[
{
	"uri": "/fmw-kubernetes/wccontent-domains/oracle-cloud/prepare-oke-environment/",
	"title": "Preparing an OKE environment",
	"tags": [],
	"description": "Running WebLogic Kubernetes Operator managed WebCenter Content domain on Oracle Kubernetes Engine (OKE).",
	"content": "Contents  Create Public SSH Key to access all the Bastion and Worker nodes Create a compartment for OKE Create Container Clusters (OKE) Create Bastion Node to access Cluster Setup OCI CLI to download kubeconfig and access OKE Cluster  Create Public SSH Key to access all the Bastion and Worker nodes Create SSH key using ssh-keygen on linux terminal to access (ssh) the Compute instances (worker/bastion) in OCI.\nssh-keygen -t rsa -N \u0026#34;\u0026#34; -b 2048 -C demokey -f id_rsa Create a compartment for OKE Within your tenancy, there must be a compartment to contain the necessary network resources (VCN, subnets, internet gateway, route table, security lists).\n Go to OCI console, and use the top-left Menu to select the Identity \u0026gt; Compartments option. Click the Create Compartment button. Enter the compartment name(For example, WCCStorage) and description(OKE compartment), the click the Create Compartment button.  Create Container Clusters (OKE)  In the Console, open the navigation menu. Go to Developer Services and click Kubernetes Clusters (OKE).  Choose a Compartment you have permission to work in. Here we will use WCCStorage compartment. On the Cluster List page, select your Compartment and click Create Cluster. In the Create Cluster dialog, select Quick Create and click Launch Workflow.  On the Create Cluster page specify the values as per your environment (like the sample values shown below)  NAME: WCCOKEPHASE1 COMPARTMENT: WCCStorage KUBERNETES VERSION: v1.18.10 CHOOSE VISIBILITY TYPE: Private SHAPE: VM.Standard.E3.Flex (Choose the available shape for worker node pool. The list shows only those shapes available in your tenancy that are supported by Container Engine for Kubernetes. See Supported Images and Shapes for Worker Nodes.) NUMBER OF NODES: 3 (The number of worker nodes to create in the node pool, placed in the regional subnet created for the \u0026lsquo;quick cluster\u0026rsquo;). Click Show Advanced Options and enter PUBLIC SSK KEY: ssh-rsa AA\u0026hellip;\u0026hellip;bmVnWgX/ demokey (The public key id_rsa.pub created at Step1)    Click Next to review the details you entered for the new cluster.\n Click Create Cluster to create the new network resources and the new cluster.  Container Engine for Kubernetes starts creating resources (as shown in the Creating cluster and associated network resources dialog). Click Close to return to the Console.  Initially, the new cluster appears in the Console with a status of Creating. When the cluster has been created, it has a status of Active.  Click on the Node Pools on Resources and then View to view the Node Pool and worker node status  You can view the status of Worker node and make sure all Node State in Active and Kubernetes Node Condition is Ready.The worker node gets listed in the kubectl command once the Kubernetes Node Condition is Ready.  To access the Cluster, Click on Access Cluster on the Cluster WCCOKEPHASE1 page.  We will be creating the bastion node and then access the Cluster.  Create Bastion Node to access Cluster Setup a bastion node for accessing internal resources. We will create the bastion node in same VCN following below steps, so that we can ssh into worker nodes. Here we will choose CIDR Block: 10.0.22.0/24 . You can choose a different block, if you want.\n  Click on the VCN Name from the Cluster Page as shown below   Next Click on Security List and then Create Security List   Create a bastion-private-sec-list security with below Ingress and Egress Rules.\nIngress Rules:\nEgress Rules:   Create a bastion-public-sec-list security with below Ingress and Egress Rules.\nIngress Rules:\nEgress Rules:   Create the bastion-route-table with Internet Gateway, so that we can add to bastion instance for internet access   Next create a Regional Public Subnet for bastion instance with name bastion-subnet with below details:\n CIDR BLOCK: 10.0.22.0/24 ROUTE TABLE: oke-bastion-routetables SUBNET ACCESS: PUBLIC SUBNET Security List: bastion-public-sec-list DHCP OPTIONS: Select the Default DHCP Options     Next Click on the Private Subnet which has Worker Nodes   And then add the bastion-private-sec-list to Worker Private Subnet, so that bastion instance can access the Worker nodes   Next Create Compute Instance oke-bastion with below details\n Name: BastionHost Image: Oracle Linux 7.X Availability Domain: Choose any AD which has limit for creating Instance VIRTUAL CLOUD NETWORK COMPARTMENT: WCCStorage( i.e., OKE Compartment) SELECT A VIRTUAL CLOUD NETWORK: Select VCN created by Quick Cluster SUBNET COMPARTMENT: WCCStorage ( i.e., OKE Compartment) SUBNET: bastion-subnet (create above) SELECT ASSIGN A PUBLIC IP ADDRESS SSH KEYS: Copy content of id_rsa.pub created in Step1     Once bastion Instance BastionHost is created, get the Public IP to ssh into the bastion instance   Login to bastion host as below\nssh -i \u0026lt;your_ssh_bastion.key\u0026gt; opc@123.456.xxx.xxx   Setup OCI CLI  Install OCI CLI bash -c \u0026#34;$(curl -L https://raw.githubusercontent.com/oracle/oci-cli/master/scripts/install/install.sh)\u0026#34;  Respond to the Installation Script Prompts. To download the kubeconfig later after setup, we need to setup the oci config file. Follow the below command and enter the details when prompted $ oci setup config    Click here to see the Sample Output   $ oci setup config This command provides a walkthrough of creating a valid CLI config file. The following links explain where to find the information required by this script: User API Signing Key, OCID and Tenancy OCID: https://docs.cloud.oracle.com/Content/API/Concepts/apisigningkey.htm#Other Region: https://docs.cloud.oracle.com/Content/General/Concepts/regions.htm General config documentation: https://docs.cloud.oracle.com/Content/API/Concepts/sdkconfig.htm Enter a location for your config [/home/opc/.oci/config]: Enter a user OCID: ocid1.user.oc1..aaaaaaaao3qji52eu4ulgqvg3k4yf7xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Enter a tenancy OCID: ocid1.tenancy.oc1..aaaaaaaaf33wodv3uhljnn5etiuafoxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Enter a region (e.g. ap-hyderabad-1, ap-melbourne-1, ap-mumbai-1, ap-osaka-1, ap-seoul-1, ap-sydney-1, ap-tokyo-1, ca-montreal-1, ca-toronto-1, eu-amsterdam-1, eu-frankfurt-1, eu-zurich-1, me-jeddah-1, sa-saopaulo-1, uk-gov-london-1, uk-london-1, us-ashburn-1, us-gov-ashburn-1, us-gov-chicago-1, us-gov-phoenix-1, us-langley-1, us-luke-1, us-phoenix-1): us-phoenix-1 Do you want to generate a new API Signing RSA key pair? (If you decline you will be asked to supply the path to an existing key.) [Y/n]: Y Enter a directory for your keys to be created [/home/opc/.oci]: Enter a name for your key [oci_api_key]: Public key written to: /home/opc/.oci/oci_api_key_public.pem Enter a passphrase for your private key (empty for no passphrase): Private key written to: /home/opc/.oci/oci_api_key.pem Fingerprint: 74:d2:f2:db:62:a9:c4:bd:9b:4f:6c:d8:31:1d:a1:d8 Config written to /home/opc/.oci/config If you haven't already uploaded your API Signing public key through the console, follow the instructions on the page linked below in the section 'How to upload the public key': https://docs.cloud.oracle.com/Content/API/Concepts/apisigningkey.htm#How2     Now you need to upload the created public key in $HOME/.oci (oci_api_key_public.pem) to OCI console Login to OCI Console and navigate to User Settings, which is in the drop down under your OCI userprofile, located at the top-right corner of the page.  On User Details page, Click Api Keys link, located near bottom-left corner of the page and then Click the Add API Key button. Copy the content of oci_api_key_public.pem and Click Add.  Now you can use the oci cli to access the OCI resources. To access the Cluster, Click on Access Cluster on the Cluster WCCOKEPHASE1 page  To access the Cluster from Bastion node perform steps as per the Local Access. $ oci -v $ mkdir -p $HOME/.kube $ oci ce cluster create-kubeconfig --cluster-id ocid1.cluster.oc1.phx.aaaaaaaaae4xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxrqgjtd --file $HOME/.kube/config --region us-phoenix-1 --token-version 2.0.0 $ export KUBECONFIG=$HOME/.kube/config  Install kubectl Client to access the Cluster $ curl -LO https://dl.k8s.io/release/v1.15.7/bin/linux/amd64/kubectl $ sudo mv kubectl /bin/ $ sudo chmod +x /bin/kubectl  Access the Cluster from bastion node $ kubectl get nodes NAME STATUS ROLES AGE VERSION 10.0.10.197 Ready node 14d v1.18.10 10.0.10.206 Ready node 14d v1.18.10 10.0.10.50 Ready node 14d v1.18.10  Install required add-ons for Oracle WebCenter Content Cluster setup  Install helm v3 $ wget https://get.helm.sh/helm-v3.1.1-linux-amd64.tar.gz $ tar -zxvf helm-v3.1.1-linux-amd64.tar.gz $ sudo mv linux-amd64/helm /bin/helm $ helm version version.BuildInfo{Version:\u0026#34;v3.1.1\u0026#34;, GitCommit:\u0026#34;afe70585407b420d0097d07b21c47dc511525ac8\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, GoVersion:\u0026#34;go1.13.8\u0026#34;}  Install git sudo yum install git -y     "
},
{
	"uri": "/fmw-kubernetes/wccontent-domains/appendix/wcc-cluster-sizing-info/",
	"title": "Domain resource sizing",
	"tags": [],
	"description": "Describes the resourse sizing information for Oracle WebCenter Content domain setup on Kubernetes cluster.",
	"content": "Oracle WebCenter Content cluster sizing recommendations    Oracle WCC Normal Usage Moderate Usage High Usage     Administration Server No of CPU core(s) : 1, Memory : 4GB No of CPU core(s) : 1, Memory : 4GB No of CPU core(s) : 1, Memory : 4GB   Number of Managed Servers 2 3 5   Configurations per Managed Server No of CPU core(s) : 2, Memory : 16GB No of CPU core(s) : 4, Memory : 16GB No of CPU core(s) : 6, Memory : 16-32GB   PV Storage Minimum 250GB Minimum 250GB Minimum 500GB    "
},
{
	"uri": "/fmw-kubernetes/wccontent-domains/adminguide/configure-load-balancer/",
	"title": "Set up a load balancer",
	"tags": [],
	"description": "Configure different load balancers for Oracle WebCenter Content domains.",
	"content": "WebLogic Kubernetes Operator supports ingress-based load balancers such as Traefik and NGINX (kubernetes/ingress-nginx). It also supports Apache webtier load balancer.\n Traefik  Configure the ingress-based Traefik load balancer for Oracle WebCenter Content domains.\n NGINX  Configure the ingress-based NGINX load balancer for Oracle WebCenter Content domain.\n Apache webtier  Configure the Apache webtier load balancer for Oracle WebCenter Content domain.\n "
},
{
	"uri": "/fmw-kubernetes/wcsites-domains/patch-and-upgrade/patch-an-image/patch-wcsites-product-image/",
	"title": "Patch a Oracle WebCenter Sites product Docker image",
	"tags": [],
	"description": "Upgrade the underlying Oracle WebCenter Sites product image in a running Oracle WebCenter Sites Kubernetes environment.",
	"content": "These instructions describe how to upgrade a new release of Oracle WebCenter Sites product Docker image in a running Oracle WebCenter Sites Kubernetes environment. A rolling upgrade approach is used to upgrade managed server pods of the domain.\nIt is expecting a Zero down time as a rolling upgrade approach is used.\n Prerequisites  Make sure Oracle WebCenter Sites domain is created and all the admin and managed pods are up and running. Make sure the database used for the Oracle WebCenter Sites domain deployment is up and running during the upgrade process.  Prepare the upgrade-domain-inputs.yaml Modify the kubernetes/create-wcsites-domain/domain-home-on-pv/upgrade/upgrade-domain-inputs.yaml. Below are given default values.\n# Name of the Admin Server adminServerName: adminserver # Unique ID identifying a domain. # This ID must not contain an underscope (\u0026#34;_\u0026#34;), and must be lowercase and unique across all domains in a Kubernetes cluster. domainUID: wcsitesinfra # Number of managed servers to generate for the domain configuredManagedServerCount: 3 #Number of managed servers running at the time of upgrade managedServerRunning: 3 # Base string used to generate managed server names managedServerNameBase: wcsites-server # Oracle WebCenter Sites Docker image. # Refer to build Oracle WebCenter Sites Docker image https://github.com/oracle/docker-images/tree/master/OracleWebCenterSites # for details on how to obtain or create the image. # tag image to a new tag for example: oracle/wcsites:12.2.1.4-21.1.1-20210122 image: oracle/wcsites:12.2.1.4-21.1.1-20210122 # Image pull policy # Legal values are \u0026#34;IfNotPresent\u0026#34;, \u0026#34;Always\u0026#34;, or \u0026#34;Never\u0026#34; imagePullPolicy: IfNotPresent # Name of the domain namespace namespace: wcsites-ns Run the upgrade script Run the upgrade script with the modified upgrade-domain-inputs.yaml file and wait for the script to be finished.\n$ sh kubernetes/create-wcsites-domain/domain-home-on-pv/upgrade/upgrade.sh -i upgrade-domain-inputs.yaml Monitor the pods rolling out incrementaly.\n$ kubectl get pods -n wcsites-ns -w Configure WebCenter Sites patch Configure WebCenter Sites patch by hitting url http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-PORT}/sites/sitespatchsetup\n"
},
{
	"uri": "/fmw-kubernetes/wccontent-domains/oracle-cloud/filesystem/",
	"title": "Preparing a file system",
	"tags": [],
	"description": "Running WebLogic Kubernetes Operator managed Oracle WebCenter Content domains on OKE",
	"content": "Create Filesystem and security list for FSS  Note: Make sure you create the filesystem and security list in the OKE created VCN\n   Login to OCI Console and go to Storage and Click File System   Click Create File System   You can create File System and Mount Targets with the default values. But in case you want to rename the file System and mount targets, follow below steps.\n Note: Make sure the Virtual Cloud Network in Mount Target refers to the one where your OKE Cluster is created and you will be accessing this file system.\n   Edit and change the File System name. You can choose any name of your choice. Following instructions will assume that the File System name chosen is WCCFS.   Edit and change the Mount Target name to WCCFS and make sure the Virtual Cloud Network selected is the one where all the instances are created. Select Public Subnet and Click Create   Once the File System is created, it lands at below page. Click on WCCFS link.   Click on Mount Commands which gives details on how to mount this file system on your instances.   Mount Command pop up gives details on what must be configured on security list to access the mount targets from instances. Note down the mount command which need to be executed on the instance   Note down the mount path and NFS server from the COMMAND TO MOUNT THE FILE SYSTEM. We will use this as NFS for Domain Home with below details. Sample from the above mount command.\n NFSServer: 10.0.20.xxx Mount Path: /WCCFS    Create the security list fss_seclist with below Ingress Rules as given in the Mount commands pop up   Create the Egress rules as below as given in the Mount commands pop up.   Make sure to add the created security list fss_security list to each subnets as shown below: Otherwise the created security list rules will not apply to the instances.   Once the security list fss_security list is added into the subnet, login to the instances and mount the file systems on to Bastion Node.\n Note: Please make sure to replace the sample NFS server address (10.0.20.235, as shown in the example below) according to your environment.\n # Run below command in same order(sequence) as a root user. # login as root sudo su # Install NFS Utils yum install nfs-utils # Create directory where you want the mount the file system sudo mkdir -p /mnt/WCCFS # Mount Command sudo mount 10.0.20.235:/WCCFS /mnt/WCCFS # Alternatively you can use: \u0026quot;mount 10.0.20.235:/WCCFS /mnt/WCCFS\u0026quot;. To persist on reboot add into /etc/fstab echo \u0026quot;10.0.20.235:/WCCFS /mnt/WCCFS nfs nfsvers=3 0 0\u0026quot; \u0026gt;\u0026gt; /etc/fstab mount -a # Change proper permissions so that all users can access the share volume sudo chown -R 1000:0 /mnt/WCCFS   Confirm that /WCCFS is now pointing to created File System\n[root@bastionhost WCCFS]# cd /mnt/WCCFS/ [root@bastionhost WCCFS]# df -h . Filesystem Size Used Avail Use% Mounted on 10.0.20.235:/WCCFS 8.0E 0 8.0E 0% /mnt/WCCFS   "
},
{
	"uri": "/fmw-kubernetes/wcsites-domains/oracle-cloud/oke/",
	"title": "Preparing an OKE environment",
	"tags": [],
	"description": "Running WebLogic Kubernetes Operator managed WebCenter Sites domains on OKE",
	"content": "Overview Create Public SSH Key to access all the Bastion/Worker nodes  Create SSH key using ssh-keygen on linux terminal.  Generating public/private rsa key pair. Your identification has been saved in \u0026lt;path\u0026gt;/id_rsa. Your public key has been saved in \u0026lt;path\u0026gt;/id_rsa.pub. The key fingerprint is: SHA256:xCi1gf1QFafbRwOM3WjUTgqEInwi6UklbuxbBeMzA6M demokey The key\u0026#39;s randomart image is: +---[RSA 2048]----+ | +++oo..++*++ | | ++==+==. oo=.+ | |Eo+o*+++o .o +o | | oo * .. o.... | | . . S . . . | | o . | | . | | | | | +----[SHA256]-----+  This will generate id_rsa and id_rsa.pub in  You will be using id_rsa.pub while creating the instance and id_rsa which is a private key, will be used during login to the instance later.  Create a compartment for OKE Within your tenancy, there must be a compartment to contain the necessary network resources (VCN, subnets, internet gateway, route table, security lists).\n Go to OCI console, and use the top-left Menu to select the Identity \u0026gt; Compartments option. Click the Create Compartment button. Enter the compartment name(WCSCDev) and description(OKE compartment), the click the Create Compartment button.  Create Virtual Cloud Network  From OCI console, use navigation menu. Under Solutions, Platform and Edge, go to Networking and click Virtual Cloud Networks (VCN) Choose a WCSCDev Compartment and then click Create Virtual Cloud Network Choose a WCSCDev Compartment and then click Networking QuickStart ? VCN with Internet Connectivity ? Start Workflow  Add Name (ex. WCNVCN)  Click Next ? Review and Create.  Create Container Clusters (OKE)  From OCI console, use navigation menu. Under Solutions, Platform and Edge, go to Developer Services and click Container Clusters (OKE) Choose a WCSCDev Compartment and then click Create Cluster *In the Cluster Creation dialog, change the placeholder value in the Name field and enter WCSOKECluster instead.  Initially, the Cluster is in \u0026ldquo;Creating\u0026rdquo; state and no buttons are accessible. Once its state is \u0026ldquo;Active\u0026rdquo;, then all the button gets are enabled.   Create Bastion Node Setup a bastion node for accessing internal resources.\nSetup OCI CLI to download kubeconfig and access OKE Cluster  Login to Bastion Node Install OCI CLI  $ bash -c \u0026#34;$(curl -L https://raw.githubusercontent.com/oracle/oci-cli/master/scripts/install/install.sh)\u0026#34;  Respond to the Installation Script Prompts. To download the kubeconfig later after setup, we need to setup the oci config file. Follow the below command and enter the details when prompted.  $ oci setup config This command provides a walkthrough of creating a valid CLI config file. The following links explain where to find the information required by this script: User OCID and Tenancy OCID: https://docs.cloud.oracle.com/Content/API/Concepts/apisigningkey.htm#Other Region: https://docs.cloud.oracle.com/Content/General/Concepts/regions.htm General config documentation: https://docs.cloud.oracle.com/Content/API/Concepts/sdkconfig.htm Enter a location for your config [/home/opc/.oci/config]: Enter a user OCID: ocid1.user.xxxxx5n3a Enter a tenancy OCID: ocid1.tenancy.xxxxxxmffq Enter a region (e.g. ap-mumbai-1, ap-seoul-1, ap-sydney-1, ap-tokyo-1, ca-toronto-1, eu-frankfurt-1, eu-zurich-1, sa-saopaulo-1, uk-london-1, us-ashburn-1, us-gov-ashburn-1, us-gov-chicago-1, us-gov-phoenix-1, us-langley-1, us-luke-1, us-phoenix-1): us-phoenix-1 Do you want to generate a new RSA key pair? (If you decline you will be asked to supply the path to an existing key.) [Y/n]: Y Enter a directory for your keys to be created [/home/opc/.oci]: Enter a name for your key [oci_api_key]: Public key written to: /home/opc/.oci/oci_api_key_public.pem Enter a passphrase for your private key (empty for no passphrase): Private key written to: /home/opc/.oci/oci_api_key.pem Fingerprint: 30:b9:a6:80:6e:b7:bb:7d:f9:79:6b:84:48:30:03:16 Config written to /home/opc/.oci/config If you haven\u0026#39;t already uploaded your public key through the console, follow the instructions on the page linked below in the section \u0026#39;How to upload the public key\u0026#39;: https://docs.cloud.oracle.com/Content/API/Concepts/apisigningkey.htm#How2  Now you need to upload the created public key in $HOME/.oci (oci_api_key_public.pem) to OCI console. Login to Console and navigate to User Settings, which is in the drop down under your OCI username in the top nav On User Details page, select \u0026ldquo;Api Keys\u0026rdquo; in the left nav and then Click the \u0026ldquo;Add Public Key\u0026rdquo; button and then copy the content of \u0026ldquo;oci_api_key_public.pem\u0026rdquo;. Click \u0026ldquo;Add\u0026rdquo;. Now you can use the oci cli to access the OCI resources.  Setup Access Kubeconfig (OKE Cluster) Install docker  Login to Bastion Node Login to instance and install the latest docker-engine and start docker service  #install docker-engine sudo yum -y install docker-engine #Enable and start docker service sudo systemctl enable docker sudo systemctl start docker #Add opc to docker group sudo /sbin/usermod -a -G docker opc  Logout and log back into the host - to ensure user is added to group correctly Check your Docker Version.  $ docker version Client: Docker Engine - Community Version: 19.03.1-ol API version: 1.40 Go version: go1.12.5 Git commit: ead9442 Built: Wed Sep 11 06:40:28 2019 OS/Arch: linux/amd64 Experimental: false Server: Docker Engine - Community Engine: Version: 19.03.1-ol API version: 1.40 (minimum version 1.12) Go version: go1.12.5 Git commit: ead9442 Built: Wed Sep 11 06:38:43 2019 OS/Arch: linux/amd64 Experimental: false Default Registry: docker.io containerd: Version: v1.2.0-rc.0-108-gc444666 GitCommit: c4446665cb9c30056f4998ed953e6d4ff22c7c39 runc: Version: spec: 1.0.1-dev GitCommit: docker-init: Version: 0.18.0 GitCommit: fec3683  If your instances are on corporate network then Configuring Proxy Requirements (run as root)  ### Create the drop-in file /etc/systemd/system/docker.service.d/http-proxy.conf that contains proxy details: cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/systemd/system/docker.service.d/http-proxy.conf [Service] Environment=\u0026#34;HTTP_PROXY=http://\u0026lt;your-company-domain\u0026gt;:80\u0026#34; Environment=\u0026#34;HTTPS_PROXY=http://\u0026lt;your-company-domain\u0026gt;:80\u0026#34; Environment=\u0026#34;NO_PROXY=localhost,127.0.0.0/8,.\u0026lt;no-proxy-domain\u0026gt;,/var/run/docker.sock\u0026#34; EOF  Restart docker daemon to load latest changes  $ sudo systemctl daemon-reload $ sudo systemctl restart docker  Verify if the proxy is configured with docker  $ docker info|grep -i proxy Install Kubernetes Packages  Login to Bastion Node Add the external kubernetes repository (run as root)  cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg exclude=kube* EOF  Set SELinux in permissive mode (effectively disabling it)  export PATH=/sbin:$PATH setenforce 0 sudo sed -i \u0026#39;s/^SELINUX=enforcing$/SELINUX=permissive/\u0026#39; /etc/selinux/config  Export proxy (if your instances are on corporate network) and install latest kubeadm, kubelet and kubectl  VERSION=1.18.4-0 sudo yum install -y kubelet-$VERSION kubeadm-$VERSION kubectl-$VERSION --disableexcludes=kubernetes ### enable kubelet service so that it auto-restart on reboot sudo systemctl enable --now kubelet  Ensure net.bridge.bridge-nf-call-iptables is set to 1 in your sysctl to avoid traffic routing issues ( run as root user)  cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sysctl --system  Update the kubelet with \u0026ndash;fail-swap-on flag to false and restart kubelet (starting in 1.8, kubelet fails to start with swap enabled)  ### run as a root user ### Update --fail-swap-on=false into /etc/sysconfig/kubelet sed -i \u0026#39;s/KUBELET_EXTRA_ARGS=/KUBELET_EXTRA_ARGS=\u0026#34;--fail-swap-on=false\u0026#34;/\u0026#39; /etc/sysconfig/kubelet cat /etc/sysconfig/kubelet ### Reload and restart kubelet systemctl daemon-reload systemctl restart kubelet Setup bastion Node to Access Kubeconfig  From OCI console, use navigation menu. Under Solutions, Platform and Edge, go to Developer Services and click Container Clusters (OKE), click on \u0026lsquo;WCSOKECluster\u0026rsquo;  Now click on \u0026ldquo;Access Cluster\u0026rdquo; then click on \u0026ldquo;Local Access\u0026rdquo;, gives details on how to download the kubeconfig to access the cluster.   $ oci -v $ mkdir -p $HOME/.kube $ oci ce cluster create-kubeconfig --cluster-id ocid1.cluster.oc1.phx.xxxxxx --file $HOME/.kube/config --region us-phoenix-1 --token-version 2.0.0 $ export KUBECONFIG=$HOME/.kube/config  Once the kubeconfig is setup, you can access the Cluster with kubectl commands from any host or laptop.  $ kubectl get nodes NAME STATUS ROLES AGE VERSION 10.0.10.2 Ready node 111m v1.18.4 10.0.10.3 Ready node 111m v1.18.4 10.0.10.4 Ready node 111m v1.18.4 Create Filesystem and security list for FSS Setup Filesystem\nCreation of OCIR Setup the OCIR for managing Docker images.\n"
},
{
	"uri": "/fmw-kubernetes/wccontent-domains/oracle-cloud/ocir/",
	"title": "Preparing OCIR",
	"tags": [],
	"description": "Running WebLogic Kubernetes Operator managed Oracle WebCenter Content domains on OKE",
	"content": "Publish images to OCIR Push all the required images to OCIR and subsequently use from there. Follow the below steps for pushing the images to OCIR\nCreate an \u0026ldquo;Auth token\u0026rdquo; Create an \u0026ldquo;Auth token\u0026rdquo; which will be used as docker password to push and pull images from OCIR. Login to OCI Console and navigate to User Settings, which is in the drop down under your OCI user-profile, located at the top-right corner of the OCI console page.  On User Details page, Click Auth Tokens link located near bottom-left corner of the page and then Click the Generate Token button: Enter a Name and Click \u0026ldquo;Generate Token\u0026rdquo;  Token will get generated  Copy the generated token.  NOTE: It will only be displayed this one time, and you will need to copy it to a secure place for further use.\n   Using the OCIR Using the Docker CLI to login to OCIR ( for phoenix : phx.ocir.io , ashburn: iad.ocir.io etc)\n docker login phx.ocir.io When promoted for username enter docker username as OCIR RepoName/oci username ( eg., axcmmdmzqtqb/oracleidentitycloudservice/myemailid@oracle.com) When prompted for your password, enter the generated Auth Token Now you can tag the WCC Docker image and push to OCIR. Sample steps as below  $ docker login phx.ocir.io $ username - axcmmdmzqtqb/oracleidentitycloudservice/myemailid@oracle.com $ password - abCXYz942,vcde (Token Generated for OCIR using user setting) $ docker tag oracle/wccontent:12.2.1.4.0-20210311104247 phx.ocir.io/axcmmdmzqtqb/oracle/wccontent:12.2.1.4.0-20210311104247 $ docker push phx.ocir.io/axcmmdmzqtqb/oracle/wccontent:12.2.1.4.0-20210311104247 This has to be done on Bastion Node for all the images.\nVerify the OCIR Images Get the OCIR repository name by logging in to Oracle Cloud Infrastructure Console. In the OCI Console, open the Navigation menu. Under Solutions and Platform, go to Developer Services and click Container Registry (OCIR) and select the your Compartment.\n"
},
{
	"uri": "/fmw-kubernetes/soa-domains/installguide/prerequisites/",
	"title": "Requirements and limitations",
	"tags": [],
	"description": "Understand the system requirements and limitations for deploying and running Oracle SOA Suite domains with the WebLogic Kubernetes Operator, including the SOA cluster sizing recommendations.",
	"content": "This section provides information about the system requirements and limitations for deploying and running Oracle SOA Suite domains with the WebLogic Kubernetes Operator.\nSystem requirements for Oracle SOA Suite domains For the current production release 22.2.2:\n Operating systems supported:  Oracle Linux 7 (UL6+) Red Hat Enterprise Linux 7 (UL3+ only with standalone Kubernetes) Oracle Linux Cloud Native Environment (OLCNE) version 1.3.   Kubernetes 1.19.15+, 1.20.11+, 1.21.5+, 1.22.5+, and 1.23.4+ (check with kubectl version). Docker 19.03.1+ (check with docker version) or CRI-O 1.20.2+ (check with crictl version | grep RuntimeVersion). Flannel networking v0.13.0-amd64 or later (check with docker images | grep flannel), Calico networking v3.16.1 or later. Helm 3.3.4+ (check with helm version --client --short). WebLogic Kubernetes Operator 3.4.0 (see the operator releases page). Oracle SOA Suite 12.2.1.4 Docker image downloaded from My Oracle Support (MOS patch 34077593). This image contains the latest bundle patch and one-off patches for Oracle SOA Suite. You must have the cluster-admin role to install the operator. The operator does not need the cluster-admin role at runtime. For more information, see the role-based access control (RBAC) documentation. We do not currently support running SOA in non-Linux containers. Additionally, see the Oracle SOA Suite documentation for other requirements such as database version.  See here for resource sizing information for Oracle SOA Suite domains set up on a Kubernetes cluster.\nLimitations Compared to running a WebLogic Server domain in Kubernetes using the operator, the following limitations currently exist for Oracle SOA Suite domains:\n In this release, Oracle SOA Suite domains are supported using the domain on a persistent volume model only, where the domain home is located in a persistent volume (PV). The \u0026ldquo;domain in image\u0026rdquo; and \u0026ldquo;model in image\u0026rdquo; models are not supported. Also, \u0026ldquo;WebLogic Deploy Tooling (WDT)\u0026rdquo; based deployments are currently not supported. Only configured clusters are supported. Dynamic clusters are not supported for Oracle SOA Suite domains. Note that you can still use all of the scaling features, but you need to define the maximum size of your cluster at domain creation time. Mixed clusters (configured servers targeted to a dynamic cluster) are not supported. The WebLogic Logging Exporter currently supports WebLogic Server logs only. Other logs will not be sent to Elasticsearch. Note, however, that you can use a sidecar with a log handling tool like Logstash or Fluentd to get logs. The WebLogic Monitoring Exporter currently supports WebLogic MBean trees only. Support for JRF and Oracle SOA Suite MBeans is not available. Also, a metrics dashboard specific to Oracle SOA Suite is not available. Instead, use the WebLogic Server dashboard to monitor the Oracle SOA Suite server metrics in Grafana. Some features such as multicast, multitenancy, production redeployment, and Node Manager (although it is used internally for the liveness probe and to start WebLogic Server instances) are not supported in this release. Features such as Java Messaging Service whole server migration, consensus leasing, and maximum availability architecture (Oracle SOA Suite EDG setup) are not supported in this release. Enabling or disabling the memory resiliency for Oracle Service Bus using the Enterprise Manager Console is not supported in this release. Zero downtime upgrade (ZDT) of the domain is not supported.  For up-to-date information about the features of WebLogic Server that are supported in Kubernetes environments, see My Oracle Support Doc ID 2349228.1.\n"
},
{
	"uri": "/fmw-kubernetes/wcsites-domains/installguide/prerequisites/",
	"title": "Requirements and limitations",
	"tags": [],
	"description": "Understand the system requirements and limitations for deploying and running Oracle WebCenter Sites domains with the WebLogic Kubernetes Operator, including the WebCenter Sites domain cluster sizing recommendations.",
	"content": "Contents  Introduction System Requirements Limitations WebCenter Sites Cluster Sizing Recommendations  Introduction This document describes the special considerations for deploying and running a WebCenter Sites domain with the WebLogic Kubernetes Operator. Other than those considerations listed here, WebCenter Sites domains work in the same way as Fusion Middleware Infrastructure domains and WebLogic Server domains.\nIn this release, WebCenter Sites domains are supported using the domain on a persistent volume model only where a WebCenter Sites domain is located in a persistent volume (PV).\nSystem Requirements  Oracle Linux 7 (UL6+) and Red Hat Enterprise Linux 7 (UL3+ only with standalone Kubernetes) are supported. Kubernetes 1.16.15+, 1.17.13+, 1.18.10+, 1.19.7+, and 1.20.6+ (check with kubectl version). Docker 18.09.1ce+, 19.03.1+ (check with docker version) or CRI-O 1.14.7 (check with crictl version | grep RuntimeVersion). Flannel networking v0.9.1-amd64 or later (check with Docker images | grep flannel). Helm 3.2.4+ (check with helm version --client --short). Oracle WebLogic Kubernetes Operator 3.3.0 (see operator releases page). Oracle WebCenterSites 12.2.1.4 Docker image (built either using imagetool or the buildDockerImage script). You must have the cluster-admin role to install the operator. The operator does not need the cluster-admin role at runtime. We do not currently support running WebCenterSites in non-Linux containers. These proxy setup are used for pulling the required binaries and source code from the respective repositories:  export NO_PROXY=\u0026quot;localhost,127.0.0.0/8,$(hostname -i),.your-company.com,/var/run/docker.sock\u0026rdquo; export no_proxy=\u0026quot;localhost,127.0.0.0/8,$(hostname -i),.your-company.com,/var/run/docker.sock\u0026rdquo; export http_proxy=http://www-proxy-your-company.com:80 export https_proxy=http://www-proxy-your-company.com:80 export HTTP_PROXY=http://www-proxy-your-company.com:80 export HTTPS_PROXY=http://www-proxy-your-company.com:80     NOTE: Add your host IP by using hostname -i and also nslookup IP addresses to the no_proxy, NO_PROXY list above.\n Limitations Compared to running a WebLogic Server domain in Kubernetes using the Operator, the following limitations currently exist for WebCenter Sites domain:\n Domain in image model is not supported in this version of the Operator. Only configured clusters are supported. Dynamic clusters are not supported for WebCenter Sites domains. Note that you can still use all of the scaling features. You just need to define the maximum size of your cluster at domain creation time. We do not currently support running WebCenter Sites in non-Linux containers. Deploying and running a WebCenter Sites domain is supported only in Operator versions 3.3.0 and later. The WebLogic Logging Exporter currently supports WebLogic Server logs only. Other logs will not be sent to Elasticsearch. Note, however, that you can use a sidecar with a log handling tool like Logstash or Fluentd to get logs. The WebLogic Monitoring Exporter currently supports the WebLogic MBean trees only. Support for JRF MBeans has not been added yet.  WebCenter Sites Cluster Sizing Recommendations    WebCenter Sites Normal Usage Moderate Usage High Usage     Admin Server No of CPU(s) : 1, Memory : 4GB No of CPU(s) : 1, Memory : 4GB No of CPU(s) : 1, Memory : 4GB   Managed Server No of Servers : 2, No of CPU(s) : 2, Memory : 16GB No of Servers : 2, No of CPU(s) : 4, Memory : 16GB No of Servers : 3, No of CPU(s) : 6, Memory : 16-32GB   PV Storage Minimum 250GB Minimum 250GB Minimum 500GB    "
},
{
	"uri": "/fmw-kubernetes/",
	"title": "Oracle Fusion Middleware on Kubernetes",
	"tags": [],
	"description": "This document lists all the Oracle Fusion Middleware products deployment supported on Kubernetes.",
	"content": "Oracle Fusion Middleware on Kubernetes Oracle supports the deployment of the following Oracle Fusion Middleware products on Kubernetes. Click on the appropriate document link below to get started on setting up the product.\n Oracle Access Management  The WebLogic Kubernetes Operator supports deployment of Oracle Access Management (OAM). Follow the instructions in this guide to set up these Oracle Access Management domains on Kubernetes.\n Oracle Identity Governance  The WebLogic Kubernetes Operator supports deployment of Oracle Identity Governance. Follow the instructions in this guide to set up Oracle Identity Governance domains on Kubernetes.\n Oracle Unified Directory  Oracle Unified Directory provides a comprehensive Directory Solution for robust Identity Management\n Oracle Unified Directory Services Manager  Oracle Unified Directory Services Manager provides an interface for managing instances of Oracle Unified Directory\n Oracle SOA Suite  The Oracle WebLogic Kubernetes Operator (the “operator”) supports deployment of Oracle SOA Suite components such as Oracle Service-Oriented Architecture (SOA), Oracle Service Bus, and Oracle Enterprise Scheduler (ESS). Follow the instructions in this guide to set up these Oracle SOA Suite domains on Kubernetes.\n Oracle WebCenter Content  WebLogic Kubernetes Operator (the “operator”) supports deployment of Oracle WebCenter Content servers such as Oracle WebCenter Content(Content Server) and Oracle WebCenter Content(Inbound Refinery Server). Follow the instructions in this guide to set up Oracle WebCenter Content domain on Kubernetes.\n Oracle WebCenter Portal  The WebLogic Kubernetes operator (the “operator”) supports deployment of Oracle WebCenter Portal. Follow the instructions in this guide to set up Oracle WebCenter Portal domain on Kubernetes.\n Oracle WebCenter Sites  The WebLogic Kubernetes Operator supports deployment of Oracle WebCenter Sites. Follow the instructions in this guide to set up Oracle WebCenter Sites domains on Kubernetes.\n Oracle Internet Directory  Oracle Internet Directory provides a comprehensive Directory Solution for robust Identity Management\n "
},
{
	"uri": "/fmw-kubernetes/soa-domains/release-notes/",
	"title": "Release Notes",
	"tags": [],
	"description": "",
	"content": "Review the latest changes and known issues for Oracle SOA Suite on Kubernetes.\nRecent changes    Date Version Change     May 31, 2022 22.2.2 Supports Oracle SOA Suite 12.2.1.4 domains deployment using April 2022 PSU and known bug fixes. Oracle SOA Suite 12.2.1.4 Docker image for this release can be downloaded from My Oracle Support (MOS patch 34077593).   February 25, 2022 22.1.2 Supports Oracle SOA Suite 12.2.1.4 domains deployment using January 2022 PSU and known bug fixes. Oracle SOA Suite 12.2.1.4 Docker image for this release can be downloaded from My Oracle Support (MOS patch 33749496).   November 30, 2021 21.4.2 Supports Oracle SOA Suite 12.2.1.4 domains deployment using October 2021 PSU and known bug fixes. Oracle SOA Suite 12.2.1.4 Docker image for this release can be downloaded from My Oracle Support (MOS patch 33467899).   August 6, 2021 21.3.2 Supports Oracle SOA Suite 12.2.1.4 domains deployment using July 2021 PSU and known bug fixes. Oracle SOA Suite 12.2.1.4 Docker image for this release can be downloaded from My Oracle Support (MOS patch 33125465).   May 31, 2021 21.2.2 Supports Oracle SOA Suite 12.2.1.4 domains deployment using April 2021 PSU and known bug fixes. Oracle SOA Suite 12.2.1.4 Docker image for this release can be downloaded from My Oracle Support (MOS patch 32794257).   February 28, 2021 21.1.2 Supports Oracle SOA Suite 12.2.1.4 domains deployment using January 2021 PSU and known bug fixes. Oracle SOA Suite 12.2.1.4 Docker image for this release can be downloaded from My Oracle Support (MOS patch 32398542).   November 30, 2020 20.4.2 Supports Oracle SOA Suite 12.2.1.4 domains deployment using October 2020 PSU and known bug fixes. Added HEALTHCHECK support for Oracle SOA Suite docker image. Oracle SOA Suite 12.2.1.4 Docker image for this release can be downloaded from My Oracle Support (MOS patch 32215749).   October 3, 2020 20.3.3 Certified Oracle WebLogic Kubernetes Operator version 3.0.1. Kubernetes 1.14.8+, 1.15.7+, 1.16.0+, 1.17.0+, and 1.18.0+ support. Flannel is the only supported CNI in this release. SSL enabling for the Administration Server and Managed Servers is supported. Only Oracle SOA Suite 12.2.1.4 is supported.    Known issues  Overriding tuning parameters is not supported using configuration overrides Deployments in WebLogic administration console display unexpected error Enterprise Manager console may display ADF_FACES-30200 error Configure the external URL access for Oracle SOA Suite composite applications Configure the external access for the Oracle Enterprise Scheduler WebServices WSDL URLs Missing gif images in Oracle Service Bus console pipeline configuration page  "
},
{
	"uri": "/fmw-kubernetes/wcsites-domains/release-notes/",
	"title": "Release Notes",
	"tags": [],
	"description": "",
	"content": "Review the latest changes and known issues for Oracle WebCenter Sites on Kubernetes.\nRecent changes    Date Version Introduces backward incompatibilities Change     May 30, 2022 22.2.2 no Supports Oracle WebCenter Sites 12.2.1.4 domains deployment using April 2022 PSU and known bug fixes - certified for Oracle WebLogic Kubernetes Operator version 3.3.0. Oracle WebCenter Sites 12.2.1.4 container image for this release can be downloaded from My Oracle Support (MOS patch 34223930).   Dec 10, 2021 21.4.3 no Certified Oracle WebLogic Kubernetes Operator version 3.3.0. Kubernetes 1.16.0+, 1.17.0+, and 1.18.0+ support. Flannel is the only supported CNI in this release. Only Webcenter Sites 12.2.1.4 is supported.   Jan 15, 2021 21.1.1 no Certified Oracle WebLogic Kubernetes Operator version 3.0.1. Kubernetes 1.14.8+, 1.15.7+, 1.1.6.0+, 1.17.0+, and 1.18.0+ support. Flannel is the only supported CNI in this release. Only Webcenter Sites 12.2.1.4 is supported.    Known issues    Issue Description     Publishing via LoadBalancer Endpoint Currenly publishing is only supported via NodePort as described in section For Publishing Setting in WebCenter Sites on page.    "
},
{
	"uri": "/fmw-kubernetes/soa-domains/adminguide/deploying-composites/supportjdev/",
	"title": "Deploy using JDeveloper",
	"tags": [],
	"description": "Deploy Oracle SOA Suite and Oracle Service Bus composite applications from Oracle JDeveloper to Oracle SOA Suite in the WebLogic Kubernetes Operator environment.",
	"content": "Learn how to deploy Oracle SOA Suite and Oracle Service Bus composite applications from Oracle JDeveloper (running outside the Kubernetes network) to an Oracle SOA Suite instance in the WebLogic Kubernetes Operator environment.\nUse JDeveloper for development and test environments only. For a production environment, you should deploy using Application Control and WLST methods.\n Deploy Oracle SOA Suite and Oracle Service Bus composite applications to Oracle SOA Suite from JDeveloper To deploy Oracle SOA Suite and Oracle Service Bus composite applications from Oracle JDeveloper, the Administration Server must be configured to expose a T3 channel. The WebLogic Kubernetes Operator provides an option to expose a T3 channel for the Administration Server using the exposeAdminT3Channel setting during domain creation, then the matching T3 service can be used to connect. By default, when exposeAdminT3Channel is set, the WebLogic Kubernetes Operator environment exposes the NodePort for the T3 channel of the NetworkAccessPoint at 30012 (use t3ChannelPort to configure the port to a different value).\nIf you miss enabling exposeAdminT3Channel during domain creation, follow Expose a T3/T3S Channel for the Administration Server to expose a T3 channel manually.\nPrerequisites   Get the Kubernetes cluster master address and verify the T3 port that will be used for creating application server connections. Use the following command to get the T3 port:\n$ kubectl get service \u0026lt;domainUID\u0026gt;-\u0026lt;AdministrationServerName\u0026gt;-external -n \u0026lt;namespace\u0026gt;-o jsonpath='{.spec.ports[0].nodePort}' For example:\n$ kubectl get service soainfra-adminserver-external -n soans -o jsonpath='{.spec.ports[0].nodePort}'   Oracle SOA Suite in the WebLogic Kubernetes Operator environment is deployed in a Reference Configuration domain. If a SOA project is developed in Classic mode JDeveloper displays a Mismatch notification in the Deploy Composite Wizard. By default, JDeveloper is in Classic mode. To develop SOA projects in Reference Configuration mode, you must manually enable this feature in JDeveloper: a. From the File menu, select Tools, then Preferences. b. Select Reference Configuration Settings. c. Select Enable Reference Configuration settings in adapters.\n  JDeveloper needs to access the Servers during deployment. In the WebLogic Kubernetes Operator environment, Administration and Managed Servers are pods and cannot be accessed directly by JDeveloper. As a workaround, you must configure the reachability of the Managed Servers:\nThe Managed Server T3 port is not exposed by default and opening this will have a security risk as the authentication method here is based on a userid/password. It is not recommended to do this on production instances.\n   Decide on an external IP address to be used to configure access to the Managed Servers. Master or worker node IP address can be used to configure Managed Server reachability. In these steps, the Kubernetes cluster master IP is used for demonstration.\n  Get the pod names of the Administration Server and Managed Servers (that is, \u0026lt;domainUID\u0026gt;-\u0026lt;server name\u0026gt;), which will be used to map in /etc/hosts.\n  Update /etc/hosts (or in Windows, C:\\Windows\\System32\\Drivers\\etc\\hosts) on the host where JDeveloper is running with the entries below, where\n\u0026lt;Master IP\u0026gt; \u0026lt;Administration Server pod name\u0026gt; \u0026lt;Master IP\u0026gt; \u0026lt;Managed Server1 pod name\u0026gt; \u0026lt;Master IP\u0026gt; \u0026lt;Managed Server2 pod name\u0026gt; Sample /etc/hosts entries looks as follows, where X.X.X.X is the master node IP address:\nX.X.X.X soainfra-adminserver X.X.X.X soainfra-soa-server1 X.X.X.X soainfra-soa-server2   Get the Kubernetes service name of the Oracle SOA Suite cluster to access externally with the master IP (or external IP):\n$ kubectl get service \u0026lt;domainUID\u0026gt;-cluster-\u0026lt;soa-cluster\u0026gt; -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl get service soainfra-cluster-soa-cluster -n soans   Create a Kubernetes service to expose the Oracle SOA Suite cluster service (\u0026lt;domainUID\u0026gt;-cluster-\u0026lt;soa-cluster\u0026gt;) externally with same port as the Managed Server:\n$ kubectl expose service \u0026lt;domainUID\u0026gt;-cluster-\u0026lt;soa-cluster\u0026gt; --name \u0026lt;domainUID\u0026gt;-\u0026lt;soa-cluster\u0026gt;-ext --external-ip=\u0026lt;Master IP\u0026gt; -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl expose service soainfra-cluster-soa-cluster --name soainfra-cluster-soa-cluster-ext --external-ip=X.X.X.X -n soans  In a production environment, exposing the SOA cluster service with an external IP address is not recommended, as it can cause message drops on the SOA Managed Servers.\n     Create an application server connection in JDeveloper   Create a new application server connection (for example wls-k8s-op-connection) in JDeveloper:   In the configuration page, provide the WebLogic Hostname as the Kubernetes Master Address.\n  Update the Port as the T3 port (default is 30012) obtained in Prerequisites.\n  Enter the WebLogic Domain value (domainUID).\n  Test the connection to verify it is successful.   Deploy SOA composite applications using JDeveloper   In JDeveloper, right-click the SOA project you want to deploy and select Deploy to display the deployment wizard.   In the Deployment Action page, select Deploy to Application Server and click Next.   In the Deployment Configuration page, select the appropriate options and click Next.   In the Select server page, select the application server connection (wls-k8s-op-connection) that was created earlier and click Next.   If the Prerequisites were configured correctly, the lookup discovers the Managed Servers for deploying the composite.   Using the application server connection, the Managed Servers (Oracle SOA Suite cluster) are listed on the SOA Servers page. Select the Oracle SOA Suite cluster and click Next.   On the Summary page, click Finish to start deploying the composites to the Oracle SOA Suite cluster.   Verify logs on JDeveloper to confirm successful deployment.   Enter the soa-infra URLs in a browser to confirm the composites are deployed on both servers of the Oracle SOA Suite cluster.   Deploy Oracle Service Bus composite applications using JDeveloper   In JDeveloper, right-click the Oracle Service Bus project you want to deploy and select Deploy to display the deployment wizard.   In the Deployment Action page, select Deploy to Application Server and click Next.   In the Select Server page, select the application server connection (wls-k8s-op-connection) that was created earlier and click Next.   On the Summary page, click Finish to start deploying the composites to the Oracle Service Bus cluster.   In JDeveloper, verify logs to confirm successful deployment.   In the Oracle Service Bus Console, click Launch Test Console to verify that the Oracle Service Bus composite application is deployed successfully.   "
},
{
	"uri": "/fmw-kubernetes/soa-domains/appendix/soa-cluster-sizing-info/",
	"title": "Domain resource sizing",
	"tags": [],
	"description": "Describes the resourse sizing information for Oracle SOA Suite domains setup on Kubernetes cluster.",
	"content": "Oracle SOA cluster sizing recommendations    Oracle SOA Normal Usage Moderate Usage High Usage     Administration Server No of CPU core(s) : 1, Memory : 4GB No of CPU core(s) : 1, Memory : 4GB No of CPU core(s) : 1, Memory : 4GB   Number of Managed Servers 2 2 4   Configurations per Managed Server No of CPU core(s) : 2, Memory : 16GB No of CPU core(s) : 4, Memory : 16GB No of CPU core(s) : 6, Memory : 16-32GB   PV Storage Minimum 250GB Minimum 250GB Minimum 500GB    "
},
{
	"uri": "/fmw-kubernetes/wcportal-domains/appendix/wcp-cluster-sizing-info/",
	"title": "Domain resource sizing",
	"tags": [],
	"description": "Describes the resourse sizing information for the Oracle WebCenter Portal domain setup on Kubernetes cluster.",
	"content": "Oracle WebCenter Portal cluster sizing recommendations    WebCenter Portal Normal Usage Moderate Usage High Usage     Admin Server No of CPU(s) : 1, Memory : 4GB No of CPU(s) : 1, Memory : 4GB No of CPU(s) : 1, Memory : 4GB   Number of Managed Server No of Servers : 2 No of Servers : 2 No of Servers : 3   Configurations per Managed Server No of CPU(s) : 2, Memory : 16GB No of CPU(s) : 4, Memory : 16GB No of CPU(s) : 6, Memory : 16-32GB   PV Storage Minimum 250GB Minimum 250GB Minimum 500GB    "
},
{
	"uri": "/fmw-kubernetes/wcsites-domains/appendix/wcsites-cluster-sizing-info/",
	"title": "Domain resource sizing",
	"tags": [],
	"description": "Describes the resourse sizing information for Oracle WebCenter Sites domains setup on Kubernetes cluster.",
	"content": "Oracle WebCenter Sites cluster sizing recommendations    Oracle WebCenter Sites Normal Usage Moderate Usage High Usage     Administration Server No of CPU core(s) : 1, Memory : 4GB No of CPU core(s) : 1, Memory : 4GB No of CPU core(s) : 1, Memory : 4GB   Managed Server No of Servers : 2, No of CPU core(s) : 2, Memory : 16GB No of Servers : 2, No of CPU core(s) : 4, Memory : 16GB No of Servers : 3, No of CPU core(s) : 6, Memory : 16-32GB   PV Storage Minimum 250GB Minimum 250GB Minimum 500GB    "
},
{
	"uri": "/fmw-kubernetes/soa-domains/patch_and_upgrade/patch-an-image/",
	"title": "Patch an image",
	"tags": [],
	"description": "Create a patched Oracle SOA Suite image using the WebLogic Image Tool.",
	"content": "Oracle releases Oracle SOA Suite images regularly with the latest bundle and recommended interim patches in My Oracle Support (MOS). However, if you need to create images with new bundle and interim patches, you can build these images using the WebLogic Image Tool.\nIf you have access to the Oracle SOA Suite patches, you can patch an existing Oracle SOA Suite image with a bundle patch and interim patches. Oracle recommends that you use the WebLogic Image Tool to patch the Oracle SOA Suite image.\n Recommendations:\n Use the WebLogic Image Tool create feature for patching the Oracle SOA Suite Docker image with a bundle patch and multiple interim patches. This is the recommended approach because it optimizes the size of the image. Use the WebLogic Image Tool update feature for patching the Oracle SOA Suite Docker image with a single interim patch. Note that the patched image size may increase considerably due to additional image layers introduced by the patch application tool.   Apply the patched Oracle SOA Suite image To update an Oracle SOA Suite domain with a patched image, first make sure the patched image is pulled or created and available on the nodes in your Kubernetes cluster. Once the patched image is available, you can follow these steps to update the Oracle SOA Suite domain with a patched image:\n Stop all servers Update user permissions of the domain PV storage Address post-installation requirements Apply the patched image  Stop all servers  Note: The following steps are applicable only for non-Zero Downtime Patching. For Zero Downtime Patching, go to Address post-installation requirements.\n Before applying the patch, stop all servers in the domain:\n  In the domain.yaml configuration file, update the spec.serverStartPolicy field value to NEVER.\n  Shut down the domain (stop all servers) by applying the updated domain.yaml file:\n$ kubectl apply -f domain.yaml   Update user permissions of the domain PV storage The Oracle SOA Suite image for release 22.2.2 has an oracle user with UID 1000, with the default group set to root. Before applying the patched image, update the user permissions of the domain persistent volume (PV) to set the group to root:\n$ sudo chown -R 1000:0 /scratch/k8s_dir/SOA Address post-installation requirements If the patches in the patched Oracle SOA Suite image have any post-installation steps, follow these steps:\n Create a Kubernetes pod with domain home access Perform post-installation steps  Create a Kubernetes pod with domain home access   Get domain home persistence volume claim details for the Oracle SOA Suite domain.\nFor example, to list the persistent volume claim details in the namespace soans:\n$ kubectl get pvc -n soans Sample output showing the persistent volume claim is soainfra-domain-pvc:\nNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE soainfra-domain-pvc Bound soainfra-domain-pv 10Gi RWX soainfra-domain-storage-class xxd   Create a YAML soapostinstall.yaml using the domain home persistence volume claim.\nFor example, using soainfra-domain-pvc per the sample output:\n Note: Replace soasuite:12.2.1.4-30761841 with the patched image in the following sample YAML:\n apiVersion: v1 kind: Pod metadata: labels: run: soapostinstall name: soapostinstall namespace: soans spec: containers: - image: soasuite:12.2.1.4-30761841 name: soapostinstall command: [\u0026quot;/bin/bash\u0026quot;, \u0026quot;-c\u0026quot;, \u0026quot;sleep infinity\u0026quot;] imagePullPolicy: IfNotPresent volumeMounts: - name: soainfra-domain-storage-volume mountPath: /u01/oracle/user_projects volumes: - name: soainfra-domain-storage-volume persistentVolumeClaim: claimName: soainfra-domain-pvc   Apply the YAML to create the Kubernetes pod:\n$ kubectl apply -f soapostinstall.yaml   Perform post-installation steps If you need to perform any post-installation steps on the domain home:\n  Start a bash shell in the soapostinstall pod:\n$ kubectl exec -it -n soans soapostinstall -- bash This opens a bash shell in the running soapostinstall pod:\n[oracle@soapostinstall oracle]$   Use the bash shell of the soapostinstall pod and perform the required steps on the domain home.\n  After successful completion of the post-installation steps, you can delete the soapostinstall pod:\n$ kubectl delete -f soapostinstall.yaml   Apply the patched image After completing the required SOA schema upgrade and post-installation steps, start up the domain:\n  In the domain.yaml configuration file, update the image field value with the patched image:\nFor example:\n image: soasuite:12.2.1.4-30761841   In case of non-Zero Downtime Patching, update the spec.serverStartPolicy field value to IF_NEEDED in domain.yaml.\n  Apply the updated domain.yaml configuration file to start up the domain.\n$ kubectl apply -f domain.yaml  Note: In case of non-Zero Downtime Patching, the complete domain startup happens, as the servers in the domain were stopped earlier. For Zero Downtime Patching, the servers in the domain are rolling restarted.\n   Verify the domain is updated with the patched image:\n$ kubectl describe domain \u0026lt;domainUID\u0026gt; -n \u0026lt;domain-namespace\u0026gt;|grep \u0026quot;Image:\u0026quot; Sample output:\n$ kubectl describe domain soainfra -n soans |grep \u0026quot;Image:\u0026quot; Image: soasuite:12.2.1.4-30761841 $   "
},
{
	"uri": "/fmw-kubernetes/soa-domains/adminguide/configure-load-balancer/",
	"title": "Set up a load balancer",
	"tags": [],
	"description": "Configure different load balancers for Oracle SOA Suite domains.",
	"content": "The WebLogic Kubernetes Operator supports ingress-based load balancers such as Traefik and NGINX (kubernetes/ingress-nginx). It also supports Apache web tier load balancer.\n Traefik  Configure the ingress-based Traefik load balancer for Oracle SOA Suite domains.\n NGINX  Configure the ingress-based NGINX load balancer for Oracle SOA Suite domains.\n Apache web tier  Configure the Apache web tier load balancer for Oracle SOA Suite domains.\n "
},
{
	"uri": "/fmw-kubernetes/wcportal-domains/manage-wcportal-domains/configure-load-balancer/",
	"title": "Set up a load balancer",
	"tags": [],
	"description": "Configure different load balancers for the Oracle WebCenter Portal domain.",
	"content": "The WebLogic Kubernetes Operator supports ingress-based load balancers such as Traefik and NGINX (kubernetes/ingress-nginx) . It also supports the Apache webtier load balancer.\n Traefik  Configure the ingress-based Traefik load balancer for an Oracle WebCenter Portal domain.\n NGINX  Configure the ingress-based NGINX load balancer for an Oracle WebCenter Portal domain.\n Apache webtier  Configure the Apache webtier load balancer for an Oracle WebCenter Portal domain.\n "
},
{
	"uri": "/fmw-kubernetes/wcsites-domains/adminguide/configure-load-balancer/",
	"title": "Set up a load balancer",
	"tags": [],
	"description": "Configure different load balancers for Oracle WebCenter Sites domains",
	"content": "The Oracle WebLogic Server Kubernetes operator supports ingress-based load balancers such as Traefik and NGINX (kubernetes/ingress-nginx).\n Traefik  Configure the ingress-based Traefik load balancer for Oracle WebCenter Sites domains.\n NGINX  Configure the ingress-based NGINX load balancer for Oracle WebCenter Sites domains.\n "
},
{
	"uri": "/fmw-kubernetes/soa-domains/adminguide/configure-load-balancer/traefik/",
	"title": "Traefik",
	"tags": [],
	"description": "Configure the ingress-based Traefik load balancer for Oracle SOA Suite domains.",
	"content": "This section provides information about how to install and configure the ingress-based Traefik load balancer (version 2.2.1 or later for production deployments) to load balance Oracle SOA Suite domain clusters. You can configure Traefik for non-SSL, SSL termination, and end-to-end SSL access of the application URL.\nFollow these steps to set up Traefik as a load balancer for an Oracle SOA Suite domain in a Kubernetes cluster:\n Install the Traefik (ingress-based) load balancer Create an Ingress for the domain Verify domain application URL access Uninstall the Traefik ingress Uninstall Traefik  Install the Traefik (ingress-based) load balancer   Use Helm to install the Traefik (ingress-based) load balancer. Use the values.yaml file in the sample but set kubernetes.namespaces specifically.\n$ cd ${WORKDIR} $ kubectl create namespace traefik $ helm repo add traefik https://helm.traefik.io/traefik --force-update Sample output:\n\u0026#34;traefik\u0026#34; has been added to your repositories   Install Traefik:\n$ helm install traefik traefik/traefik \\  --namespace traefik \\  --values charts/traefik/values.yaml \\  --set \u0026#34;kubernetes.namespaces={traefik}\u0026#34; \\  --set \u0026#34;service.type=NodePort\u0026#34; --wait    Click here to see the sample output.   LAST DEPLOYED: Sun Sep 13 21:32:00 2020 NAMESPACE: traefik STATUS: deployed REVISION: 1 TEST SUITE: None    A sample values.yaml for deployment of Traefik 2.6.x:\nimage: name: traefik tag: 2.6.0 pullPolicy: IfNotPresent ingressRoute: dashboard: enabled: true # Additional ingressRoute annotations (e.g. for kubernetes.io/ingress.class) annotations: {} # Additional ingressRoute labels (e.g. for filtering IngressRoute by custom labels) labels: {} providers: kubernetesCRD: enabled: true kubernetesIngress: enabled: true # IP used for Kubernetes Ingress endpoints ports: traefik: port: 9000 expose: true # The exposed port for this service exposedPort: 9000 # The port protocol (TCP/UDP) protocol: TCP web: port: 8000 # hostPort: 8000 expose: true exposedPort: 30305 nodePort: 30305 # The port protocol (TCP/UDP) protocol: TCP # Use nodeport if set. This is useful if you have configured Traefik in a # LoadBalancer # nodePort: 32080 # Port Redirections # Added in 2.2, you can make permanent redirects via entrypoints. # https://docs.traefik.io/routing/entrypoints/#redirection # redirectTo: websecure websecure: port: 8443 # # hostPort: 8443 expose: true exposedPort: 30443 # The port protocol (TCP/UDP) protocol: TCP nodePort: 30443   Verify the Traefik status and find the port number of the SSL and non-SSL services:\n$ kubectl get all -n traefik    Click here to see the sample output.   NAME READY STATUS RESTARTS AGE pod/traefik-5fc4947cf9-fbl9r 1/1 Running 5 7d17h NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/traefik NodePort 10.100.195.70 \u0026lt;none\u0026gt; 9000:31288/TCP,30305:30305/TCP,30443:30443/TCP 7d17h NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/traefik 1/1 1 1 7d17h NAME DESIRED CURRENT READY AGE replicaset.apps/traefik-5fc4947cf9 1 1 1 7d17h      Access the Traefik dashboard through the URL http://\u0026lt;MASTERNODE-HOSTNAME\u0026gt;:31288, with the HTTP host traefik.example.com:\n$ curl -H \u0026#34;host: \u0026lt;MASTERNODE-HOSTNAME\u0026gt;\u0026#34; http://\u0026lt;MASTERNODE-HOSTNAME\u0026gt;:31288/dashboard/  Note: Make sure that you specify a fully qualified node name for \u0026lt;MASTERNODE-HOSTNAME\u0026gt;\n   Configure Traefik to manage ingresses created in this namespace, where traefik is the Traefik namespace and soans is the namespace of the domain:\n  $ helm upgrade traefik traefik/traefik --namespace traefik --reuse-values \\  --set \u0026#34;kubernetes.namespaces={traefik,soans}\u0026#34;    Click here to see the sample output.   Release \u0026#34;traefik\u0026#34; has been upgraded. Happy Helming! NAME: traefik LAST DEPLOYED: Sun Sep 13 21:32:12 2020 NAMESPACE: traefik STATUS: deployed REVISION: 2 TEST SUITE: None    Create an ingress for the domain Create an ingress for the domain in the domain namespace by using the sample Helm chart. Here path-based routing is used for ingress. Sample values for default configuration are shown in the file ${WORKDIR}/charts/ingress-per-domain/values.yaml. By default, type is TRAEFIK, sslType is NONSSL, and domainType is soa. These values can be overridden by passing values through the command line or can be edited in the sample file values.yaml based on the type of configuration (NONSSL, SSL, and E2ESSL).\nIf needed, you can update the ingress YAML file to define more path rules (in section spec.rules.host.http.paths) based on the domain application URLs that need to be accessed. The template YAML file for the Traefik (ingress-based) load balancer is located at ${WORKDIR}/charts/ingress-per-domain/templates/traefik-ingress.yaml.\n Note: See here for all the configuration parameters.\n   Choose an appropriate LOADBALANCER_HOSTNAME for accessing the Oracle SOA Suite domain application URLs.\n$ export LOADBALANCER_HOSTNAME=\u0026lt;LOADBALANCER_HOSTNAME\u0026gt; For example, if you are executing the commands from a master node terminal, where the master hostname is LOADBALANCER_HOSTNAME:\n$ export LOADBALANCER_HOSTNAME=$(hostname -f)   Install ingress-per-domain using Helm for NONSSL configuration:\n$ cd ${WORKDIR} $ helm install soa-traefik-ingress \\  charts/ingress-per-domain \\  --namespace soans \\  --values charts/ingress-per-domain/values.yaml \\  --set \u0026#34;traefik.hostname=${LOADBALANCER_HOSTNAME}\u0026#34; Sample output:\nNAME: soa-traefik-ingress LAST DEPLOYED: Mon Jul 20 11:44:13 2020 NAMESPACE: soans STATUS: deployed REVISION: 1 TEST SUITE: None   For secured access (SSL termination and E2ESSL) to the Oracle SOA Suite application, create a certificate, and generate a Kubernetes secret:\n$ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /tmp/tls1.key -out /tmp/tls1.crt -subj \u0026#34;/CN=*\u0026#34; $ kubectl -n soans create secret tls soainfra-tls-cert --key /tmp/tls1.key --cert /tmp/tls1.crt   Create the Traefik TLSStore custom resource.\nIn case of SSL termination, Traefik should be configured to use the user-defined SSL certificate. If the user-defined SSL certificate is not configured, Traefik will create a default SSL certificate. To configure a user-defined SSL certificate for Traefik, use the TLSStore custom resource. The Kubernetes secret created with the SSL certificate should be referenced in the TLSStore object. Run the following command to create the TLSStore:\n$ cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: traefik.containo.us/v1alpha1 kind: TLSStore metadata: name: default namespace: soans spec: defaultCertificate: secretName: soainfra-tls-cert EOF   Install ingress-per-domain using Helm for SSL configuration.\nThe Kubernetes secret name should be updated in the template file.\nThe template file also contains the following annotations:\ntraefik.ingress.kubernetes.io/router.entrypoints: websecure traefik.ingress.kubernetes.io/router.tls: \u0026#34;true\u0026#34; traefik.ingress.kubernetes.io/router.middlewares: soans-wls-proxy-ssl@kubernetescrd The entry point for SSL termination access and the Middleware name should be updated in the annotation. The Middleware name should be in the form \u0026lt;namespace\u0026gt;-\u0026lt;middleware name\u0026gt;@kubernetescrd.\n$ cd ${WORKDIR} $ helm install soa-traefik-ingress \\  charts/ingress-per-domain \\  --namespace soans \\  --values charts/ingress-per-domain/values.yaml \\  --set \u0026#34;traefik.hostname=${LOADBALANCER_HOSTNAME}\u0026#34; \\  --set sslType=SSL Sample output:\nNAME: soa-traefik-ingress LAST DEPLOYED: Mon Jul 20 11:44:13 2020 NAMESPACE: soans STATUS: deployed REVISION: 1 TEST SUITE: None   Install ingress-per-domain using Helm for E2ESSL configuration.\n$ cd ${WORKDIR} $ helm install soa-traefik-ingress \\  charts/ingress-per-domain \\  --namespace soans \\  --values charts/ingress-per-domain/values.yaml \\  --set sslType=E2ESSL Sample output:\nNAME: soa-traefik-ingress LAST DEPLOYED: Fri Apr 9 09:47:27 2021 NAMESPACE: soans STATUS: deployed REVISION: 1 TEST SUITE: None   For NONSSL access to the Oracle SOA Suite application, get the details of the services by the ingress:\n$ kubectl describe ingress soainfra-traefik -n soans    Click here to see all services supported by the above deployed ingress.   Name: soainfra-traefik Namespace: soans Address: Default backend: default-http-backend:80 (\u0026lt;error: endpoints \u0026#34;default-http-backend\u0026#34; not found\u0026gt;) Rules: Host Path Backends ---- ---- -------- www.example.com /console soainfra-adminserver:7001 (10.244.0.45:7001) /em soainfra-adminserver:7001 (10.244.0.45:7001) /weblogic/ready soainfra-adminserver:7001 (10.244.0.45:7001) soainfra-cluster-soa-cluster:8001 (10.244.0.46:8001,10.244.0.47:8001) /soa-infra soainfra-cluster-soa-cluster:8001 (10.244.0.46:8001,10.244.0.47:8001) /soa/composer soainfra-cluster-soa-cluster:8001 (10.244.0.46:8001,10.244.0.47:8001) /integration/worklistapp soainfra-cluster-soa-cluster:8001 (10.244.0.46:8001,10.244.0.47:8001) Annotations: kubernetes.io/ingress.class: traefik Events: \u0026lt;none\u0026gt;      For SSL access to the Oracle SOA Suite application, get the details of the services by the above deployed ingress:\n$ kubectl describe ingress soainfra-traefik -n soans    Click here to see all services supported by the above deployed ingress.    ``` Name: soainfra-traefik Namespace: soans Address: Default backend: default-http-backend:80 (\u0026lt;error: endpoints \u0026quot;default-http-backend\u0026quot; not found\u0026gt;) TLS: soainfra-tls-cert terminates www.example.com Rules: Host Path Backends ---- ---- -------- www.example.com /console soainfra-adminserver:7001 () /em soainfra-adminserver:7001 () /weblogic/ready soainfra-adminserver:7001 () soainfra-cluster-soa-cluster:8001 () /soa-infra soainfra-cluster-soa-cluster:8001 () /soa/composer soainfra-cluster-soa-cluster:8001 () /integration/worklistapp soainfra-cluster-soa-cluster:8001 () Annotations: kubernetes.io/ingress.class: traefik meta.helm.sh/release-name: soa-traefik-ingress meta.helm.sh/release-namespace: soans traefik.ingress.kubernetes.io/router.entrypoints: websecure traefik.ingress.kubernetes.io/router.middlewares: soans-wls-proxy-ssl@kubernetescrd traefik.ingress.kubernetes.io/router.tls: true Events: \u0026lt;none\u0026gt; ```      For E2ESSL access to the Oracle SOA Suite application, get the details of the services by the above deployed ingress:\n$ kubectl describe IngressRouteTCP soainfra-traefik -n soans\t   Click here to see all services supported by the above deployed ingress.    ``` Name: soa-cluster-routetcp Namespace: soans Labels: app.kubernetes.io/managed-by=Helm Annotations: meta.helm.sh/release-name: soa-traefik-ingress meta.helm.sh/release-namespace: soans API Version: traefik.containo.us/v1alpha1 Kind: IngressRouteTCP Metadata: Creation Timestamp: 2021-04-09T09:47:27Z Generation: 1 Managed Fields: API Version: traefik.containo.us/v1alpha1 Fields Type: FieldsV1 fieldsV1: f:metadata: f:annotations: .: f:meta.helm.sh/release-name: f:meta.helm.sh/release-namespace: f:labels: .: f:app.kubernetes.io/managed-by: f:spec: .: f:entryPoints: f:routes: f:tls: .: f:passthrough: Manager: Go-http-client Operation: Update Time: 2021-04-09T09:47:27Z Resource Version: 548305 Self Link: /apis/traefik.containo.us/v1alpha1/namespaces/soans/ingressroutetcps/soa-cluster-routetcp UID: 933e524c-b773-474b-a87f-560d69f08d4b Spec: Entry Points: websecure Routes: Match: HostSNI(`HostName`) Services: Termination Delay: 400 Name: soainfra-adminserver Port: 7002 Weight: 3 Tls: Passthrough: true Events: \u0026lt;none\u0026gt; ```      To confirm that the load balancer noticed the new ingress and is successfully routing to the domain server pods, you can send a request to the URL for the \u0026ldquo;WebLogic ReadyApp framework\u0026rdquo;, which should return an HTTP 200 status code, as follows:\n$ curl -v http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER_PORT}/weblogic/ready * Trying 149.87.129.203... \u0026gt; GET http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER_PORT}/weblogic/ready HTTP/1.1 \u0026gt; User-Agent: curl/7.29.0 \u0026gt; Accept: */* \u0026gt; Proxy-Connection: Keep-Alive \u0026gt; host: ${LOADBALANCER_HOSTNAME} \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; Date: Sat, 14 Mar 2020 08:35:03 GMT \u0026lt; Vary: Accept-Encoding \u0026lt; Content-Length: 0 \u0026lt; Proxy-Connection: Keep-Alive \u0026lt; * Connection #0 to host localhost left intact   Verify domain application URL access For NONSSL configuration After setting up the Traefik (ingress-based) load balancer, verify that the domain application URLs are accessible through the non-SSL load balancer port 30305 for HTTP access. The sample URLs for Oracle SOA Suite domain of type soa are:\nhttp://${LOADBALANCER_HOSTNAME}:${LOADBALANCER_NON_SSLPORT}/weblogic/ready http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER_NON_SSLPORT}/console http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER_NON_SSLPORT}/em http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER_NON_SSLPORT}/soa-infra http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER_NON_SSLPORT}/soa/composer http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER_NON_SSLPORT}/integration/worklistapp For SSL configuration After setting up the Traefik (ingress-based) load balancer, verify that the domain applications are accessible through the SSL load balancer port 30443 for HTTPS access. The sample URLs for Oracle SOA Suite domain of type soa are:\nhttps://${LOADBALANCER_HOSTNAME}:${LOADBALANCER_SSLPORT}/weblogic/ready https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER_SSLPORT}/console https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER_SSLPORT}/em https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER_SSLPORT}/soa-infra https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER_SSLPORT}/soa/composer https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER_SSLPORT}/integration/worklistapp For E2ESSL configuration After setting up the Traefik (ingress-based) load balancer, verify that the domain applications are accessible through the SSL load balancer port 30443 for HTTPS access.\n  To access the application URLs from the browser, update /etc/hosts on the browser host (in Windows, C:\\Windows\\System32\\Drivers\\etc\\hosts) with the entries below\nX.X.X.X admin.org X.X.X.X soa.org X.X.X.X osb.org  Note: The value of X.X.X.X is the host ipaddress on which this ingress is deployed.\n  Note: If you are behind any corporate proxy, make sure to update the browser proxy settings appropriately to access the host names updated /etc/hosts file.\n   The sample URLs for Oracle SOA Suite domain of type soa are:\nhttps://admin.org:${LOADBALANCER_SSLPORT}/weblogic/ready https://admin.org:${LOADBALANCER_SSLPORT}/console https://admin.org:${LOADBALANCER_SSLPORT}/em https://soa.org:${LOADBALANCER_SSLPORT}/soa-infra https://soa.org:${LOADBALANCER_SSLPORT}/soa/composer https://soa.org:${LOADBALANCER_SSLPORT}/integration/worklistapp Uninstall the Traefik ingress Uninstall and delete the ingress deployment:\n$ helm delete soa-traefik-ingress -n soans Uninstall Traefik $ helm delete traefik -n traefik "
},
{
	"uri": "/fmw-kubernetes/wcportal-domains/manage-wcportal-domains/configure-load-balancer/traefik/",
	"title": "Traefik",
	"tags": [],
	"description": "Configure the ingress-based Traefik load balancer for an Oracle WebCenter Portal domain.",
	"content": "To load balance Oracle WebCenter Portal domain clusters, you can install the ingress-based Traefik load balancer (version 2.2.1 or later for production deployments) and configure it for non-SSL, SSL termination, and end-to-end SSL access of the application URL. Follow these steps to set up Traefik as a load balancer for an Oracle WebCenter Portal domain in a Kubernetes cluster:\n  Non-SSL and SSL termination\n Install the Traefik (ingress-based) load balancer Configure Traefik to manage ingresses Create an Ingress for the domain Verify domain application URL access Uninstall the Traefik ingress    End-to-end SSL configuration\n Install the Traefik load balancer for End-to-end SSL Configure Traefik to manage domain Create IngressRouteTCP Verify end-to-end SSL access Uninstall Traefik    Non-SSL and SSL termination Install the Traefik (ingress-based) load balancer   Use Helm to install the Traefik (ingress-based) load balancer. You can use the following values.yaml sample file and set kubernetes.namespaces as required.\n$ cd ${WORKDIR} $ kubectl create namespace traefik $ helm repo add traefik https://containous.github.io/traefik-helm-chart Sample output:\n\u0026#34;traefik\u0026#34; has been added to your repositories   Install Traefik:\n$ helm install traefik traefik/traefik \\  --namespace traefik \\  --values charts/traefik/values.yaml \\  --set \u0026#34;kubernetes.namespaces={traefik}\u0026#34; \\  --set \u0026#34;service.type=NodePort\u0026#34; --wait    Click here to see the sample output.   LAST DEPLOYED: Sun Sep 13 21:32:00 2020 NAMESPACE: traefik STATUS: deployed REVISION: 1 TEST SUITE: None    A sample values.yaml for deployment of Traefik 2.2.x looks like this:\nimage: name: traefik tag: 2.2.8 pullPolicy: IfNotPresent ingressRoute: dashboard: enabled: true annotations: {} labels: {} providers: kubernetesCRD: enabled: true kubernetesIngress: enabled: true ports: traefik: port: 9000 expose: true exposedPort: 9000 protocol: TCP web: port: 8000 expose: true exposedPort: 30305 nodePort: 30305 protocol: TCP websecure: port: 8443 expose: true exposedPort: 30443 protocol: TCP nodePort: 30443   Verify the Traefik status and find the port number of the SSL and non-SSL services:\n$ kubectl get all -n traefik    Click here to see the sample output.   NAME READY STATUS RESTARTS AGE pod/traefik-f9cf58697-29dlx 1/1 Running 0 35s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/traefik NodePort 10.100.113.37 \u0026lt;none\u0026gt; 9000:30070/TCP,30305:30305/TCP,30443:30443/TCP 35s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/traefik 1/1 1 1 36s NAME DESIRED CURRENT READY AGE replicaset.apps/traefik-f9cf58697 1 1 1 36s      Access the Traefik dashboard through the URL http://$(hostname -f):30070, with the HTTP host traefik.example.com:\n$ curl -H \u0026#34;host: $(hostname -f)\u0026#34; http://$(hostname -f):30070/dashboard/  Note: Make sure that you specify a fully qualified node name for $(hostname -f)\n   Configure Traefik to manage ingresses Configure Traefik to manage ingresses created in this namespace. In the following sample, traefik is the Traefik namespace and wcpns is the namespace of the domain:\n$ helm upgrade traefik traefik/traefik \\ --reuse-values \\ --namespace traefik \\ --set \u0026#34;kubernetes.namespaces={traefik,wcpns}\u0026#34; \\ --wait    Click here to see the sample output.   Release \u0026#34;traefik\u0026#34; has been upgraded. Happy Helming! NAME: traefik LAST DEPLOYED: Tue Jan 12 04:33:15 2021 NAMESPACE: traefik STATUS: deployed REVISION: 2 TEST SUITE: None    Create an ingress for the domain Create an ingress for the domain in the domain namespace by using the sample Helm chart. Here path-based routing is used for ingress. Sample values for default configuration are shown in the file ${WORKDIR}/charts/ingress-per-domain/values.yaml. By default, type is TRAEFIK , tls is Non-SSL. You can override these values by passing values through the command line or edit them in the sample values.yaml file based on the type of configuration (non-SSL or SSL).\n NOTE: This is not an exhaustive list of rules. You can enhance it based on the application URLs that need to be accessed externally.\n If needed, you can update the ingress YAML file to define more path rules (in section spec.rules.host.http.paths) based on the domain application URLs that need to be accessed. The template YAML file for the Traefik (ingress-based) load balancer is located at ${WORKDIR}/charts/ingress-per-domain/templates/traefik-ingress.yaml You can add new path rules like shown below .\n- path: /NewPathRule backend: serviceName: \u0026#39;Backend Service Name\u0026#39; servicePort: \u0026#39;Backend Service Port\u0026#39;   Install ingress-per-domain using Helm for non-SSL configuration:\n$ cd ${WORKDIR} $ helm install wcp-traefik-ingress \\  charts/ingress-per-domain \\  --namespace wcpns \\  --values charts/ingress-per-domain/values.yaml \\  --set \u0026#34;traefik.hostname=$(hostname -f)\u0026#34; Sample output:\nNAME: wcp-traefik-ingress LAST DEPLOYED: Mon Jul 20 11:44:13 2020 NAMESPACE: wcpns STATUS: deployed REVISION: 1 TEST SUITE: None   For secured access (SSL) to the Oracle WebCenter Portal application, create a certificate and generate a Kubernetes secret:\n$ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /tmp/tls1.key -out /tmp/tls1.crt -subj \u0026#34;/CN=*\u0026#34; $ kubectl -n wcpns create secret tls wcp-domain-tls-cert --key /tmp/tls1.key --cert /tmp/tls1.crt  Note: The value of CN is the host on which this ingress is to be deployed.\n   Create the Traefik TLSStore custom resource.\nIn case of SSL termination, Traefik should be configured to use the user-defined SSL certificate. If the user-defined SSL certificate is not configured, Traefik creates a default SSL certificate. To configure a user-defined SSL certificate for Traefik, use the TLSStore custom resource. The Kubernetes secret created with the SSL certificate should be referenced in the TLSStore object. Run the following command to create the TLSStore:\n$ cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: traefik.containo.us/v1alpha1 kind: TLSStore metadata: name: default namespace: wcpns spec: defaultCertificate: secretName: wcp-domain-tls-cert EOF   Install ingress-per-domain using Helm for SSL configuration.\nThe Kubernetes secret name should be updated in the template file.\nThe template file also contains the following annotations:\ntraefik.ingress.kubernetes.io/router.entrypoints: websecure traefik.ingress.kubernetes.io/router.tls: \u0026#34;true\u0026#34; traefik.ingress.kubernetes.io/router.middlewares: wcpns-wls-proxy-ssl@kubernetescrd The entry point for SSL access and the Middleware name should be updated in the annotation. The Middleware name should be in the form \u0026lt;namespace\u0026gt;-\u0026lt;middleware name\u0026gt;@kubernetescrd.\n$ cd ${WORKDIR} $ helm install wcp-traefik-ingress \\  charts/ingress-per-domain \\  --namespace wcpns \\  --values charts/ingress-per-domain/values.yaml \\  --set \u0026#34;traefik.hostname=$(hostname -f)\u0026#34; \\  --set sslType=SSL Sample output:\nNAME: wcp-traefik-ingress LAST DEPLOYED: Mon Jul 20 11:44:13 2020 NAMESPACE: wcpns STATUS: deployed REVISION: 1 TEST SUITE: None   For non-SSL access to the Oracle WebCenter Portal application, get the details of the services by the ingress:\n$ kubectl describe ingress wcp-domain-traefik -n wcpns    Click here to see all services supported by the above deployed ingress.   Name: wcp-domain-traefik Namespace: wcpns Address: Default backend: default-http-backend:80 (\u0026lt;error: endpoints \u0026#34;default-http-backend\u0026#34; not found\u0026gt;) Rules: Host Path Backends ---- ---- -------- www.example.com /webcenter wcp-domain-cluster-wcp-cluster:8888 (10.244.0.52:8888,10.244.0.53:8888) /console wcp-domain-adminserver:7001 (10.244.0.51:7001) /rsscrawl wcp-domain-cluster-wcp-cluster:8888 (10.244.0.52:8888,10.244.0.53:8888) /rest wcp-domain-cluster-wcp-cluster:8888 (10.244.0.52:8888,10.244.0.53:8888) /webcenterhelp wcp-domain-cluster-wcp-cluster:8888 (10.244.0.52:8888,10.244.0.53:8888) /em wcp-domain-adminserver:7001 (10.244.0.51:7001) /wsrp-tools wcp-domain-cluster-wcportlet-cluster:8889 (10.244.0.52:8889,10.244.0.53:8889) /portalTools wcp-domain-cluster-wcportlet-cluster:8889 (10.244.0.52:8889,10.244.0.53:8889) Annotations: kubernetes.io/ingress.class: traefik meta.helm.sh/release-name: wcp-traefik-ingress meta.helm.sh/release-namespace: wcpns Events: \u0026lt;none\u0026gt;      For SSL access to the Oracle WebCenter Portal application, get the details of the services by the above deployed ingress:\n$ kubectl describe ingress wcp-domain-traefik -n wcpns    Click here to see all services supported by the above deployed ingress.   Name: wcp-domain-traefik Namespace: wcpns Address: Default backend: default-http-backend:80 (\u0026lt;error: endpoints \u0026quot;default-http-backend\u0026quot; not found\u0026gt;) TLS: wcp-domain-tls-cert terminates www.example.com Rules: Host Path Backends ---- ---- -------- www.example.com /webcenter wcp-domain-cluster-wcp-cluster:8888 (10.244.0.52:8888,10.244.0.53:8888) /console wcp-domain-adminserver:7001 (10.244.0.51:7001) /rsscrawl wcp-domain-cluster-wcp-cluster:8888 (10.244.0.52:8888,10.244.0.53:8888) /rest wcp-domain-cluster-wcp-cluster:8888 (10.244.0.52:8888,10.244.0.53:8888) /webcenterhelp wcp-domain-cluster-wcp-cluster:8888 (10.244.0.52:8888,10.244.0.53:8888) /em wcp-domain-adminserver:7001 (10.244.0.51:7001) /wsrp-tools wcp-domain-cluster-wcportlet-cluster:8889 (10.244.0.52:8889,10.244.0.53:8889) /portalTools wcp-domain-cluster-wcportlet-cluster:8889 (10.244.0.52:8889,10.244.0.53:8889) Annotations: kubernetes.io/ingress.class: traefik meta.helm.sh/release-name: wcp-traefik-ingress meta.helm.sh/release-namespace: wcpns traefik.ingress.kubernetes.io/router.entrypoints: websecure traefik.ingress.kubernetes.io/router.middlewares: wcpns-wls-proxy-ssl@kubernetescrd traefik.ingress.kubernetes.io/router.tls: true Events: \u0026lt;none\u0026gt;      To confirm that the load balancer noticed the new ingress and is successfully routing to the domain server pods, you can send a request to the URL for the WebLogic ReadyApp framework, which should return an HTTP 200 status code, as follows:\n$ curl -v http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER_PORT}/weblogic/ready * Trying 149.87.129.203... \u0026gt; GET http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER_PORT}/weblogic/ready HTTP/1.1 \u0026gt; User-Agent: curl/7.29.0 \u0026gt; Accept: */* \u0026gt; Proxy-Connection: Keep-Alive \u0026gt; host: $(hostname -f) \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; Date: Sat, 14 Mar 2020 08:35:03 GMT \u0026lt; Vary: Accept-Encoding \u0026lt; Content-Length: 0 \u0026lt; Proxy-Connection: Keep-Alive \u0026lt; * Connection #0 to host localhost left intact   Verify domain application URL access For non-SSL configuration After setting up the Traefik (ingress-based) load balancer, verify that the domain application URLs are accessible through the non-SSL load balancer port 30305 for HTTP access. The sample URLs for Oracle WebCenter Portal domain are:\nhttp://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/webcenter http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/console http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/em http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/rsscrawl http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/rest http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/webcenterhelp http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/wsrp-tools http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/portalTools For SSL configuration After setting up the Traefik (ingress-based) load balancer, verify that the domain applications are accessible through the SSL load balancer port 30443 for HTTPS access. The sample URLs for Oracle WebCenter Portal domain are:\nhttps://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/webcenter https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/console https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/em https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/rsscrawl https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/rest https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/webcenterhelp https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/wsrp-tools https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/portalTools Uninstall the Traefik ingress Uninstall and delete the ingress deployment:\n$ helm delete wcp-traefik-ingress -n wcpns End-to-end SSL configuration Install the Traefik load balancer for end-to-end SSL   Use Helm to install the Traefik (ingress-based) load balancer. You can use the values.yaml sample file and set kubernetes.namespaces as required.\n$ cd ${WORKDIR} $ kubectl create namespace traefik $ helm repo add traefik https://containous.github.io/traefik-helm-chart Sample output:\n\u0026#34;traefik\u0026#34; has been added to your repositories   Install Traefik:\n$ helm install traefik traefik/traefik \\  --namespace traefik \\  --values charts/traefik/values.yaml \\  --set \u0026#34;kubernetes.namespaces={traefik}\u0026#34; \\  --set \u0026#34;service.type=NodePort\u0026#34; --wait    Click here to see the sample output.   LAST DEPLOYED: Sun Sep 13 21:32:00 2020 NAMESPACE: traefik STATUS: deployed REVISION: 1 TEST SUITE: None      Verify the Traefik operator status and find the port number of the SSL and non-SSL services:\n$ kubectl get all -n traefik    Click here to see the sample output.   NAME READY STATUS RESTARTS AGE pod/traefik-845f5d6dbb-swb96 1/1 Running 0 32s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/traefik NodePort 10.99.52.249 \u0026lt;none\u0026gt; 9000:31288/TCP,30305:30305/TCP,30443:30443/TCP 32s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/traefik 1/1 1 1 33s NAME DESIRED CURRENT READY AGE replicaset.apps/traefik-845f5d6dbb 1 1 1 33s      Access the Traefik dashboard through the URL http://$(hostname -f):31288, with the HTTP host traefik.example.com:\n$ curl -H \u0026#34;host: $(hostname -f)\u0026#34; http://$(hostname -f):31288/dashboard/  Note: Make sure that you specify a fully qualified node name for $(hostname -f).\n   Configure Traefik to manage the domain Configure Traefik to manage the domain application service created in this namespace. In the following sample, traefik is the Traefik namespace and wcpns is the namespace of the domain:\n$ helm upgrade traefik traefik/traefik --namespace traefik --reuse-values \\ --set \u0026#34;kubernetes.namespaces={traefik,wcpns}\u0026#34;    Click here to see the sample output.   Release \u0026quot;traefik\u0026quot; has been upgraded. Happy Helming! NAME: traefik LAST DEPLOYED: Sun Sep 13 21:32:12 2020 NAMESPACE: traefik STATUS: deployed REVISION: 2 TEST SUITE: None    Create IngressRouteTCP   For each backend service, create different ingresses, as Traefik does not support multiple paths or rules with annotation ssl-passthrough. For example, for wcp-domain-adminserver and wcp-domain-cluster-wcp-cluster, different ingresses must be created.\n  To enable SSL passthrough in Traefik, you can configure a TCP router. A sample YAML for IngressRouteTCP is available at ${WORKDIR}/charts/ingress-per-domain/tls/traefik-tls.yaml. The following should be updated in traefik-tls.yaml:\n The service name and the SSL port should be updated in the services. The load balancer host name should be updated in the HostSNI rule.  Sample traefik-tls.yaml:\napiVersion: traefik.containo.us/v1alpha1 kind: IngressRouteTCP metadata: name: wcp-domain-cluster-routetcp namespace: wcpns spec: entryPoints: - websecure routes: - match: HostSNI(`${LOADBALANCER_HOSTNAME}`) services: - name: wcp-domain-cluster-wcp-cluster port: 8888 weight: 3 TerminationDelay: 400 tls: passthrough: true   Create the IngressRouteTCP:\n$ kubectl apply -f traefik-tls.yaml   Verify end-to-end SSL access Verify the access to application URLs exposed through the configured service. The configured WCP cluster service enables you to access the following WCP domain URLs:\nhttps://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/webcenter https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/rsscrawl https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/rest https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/webcenterhelp Uninstall Traefik $ helm delete traefik -n traefik $ cd ${WORKDIR}/charts/ingress-per-domain/tls $ kubectl delete -f traefik-tls.yaml "
},
{
	"uri": "/fmw-kubernetes/wcsites-domains/adminguide/configure-load-balancer/traefik/",
	"title": "Traefik",
	"tags": [],
	"description": "Configure the ingress-based Traefik load balancer for Oracle WebCenter Sites domains.",
	"content": "This section provides information about how to install and configure the ingress-based Traefik load balancer (version 2.2.1 or later for production deployments) to load balance Oracle WebCenter Sites domain clusters. You can configure Traefik for access of the application URL.\nFollow these steps to set up Traefik as a load balancer for an Oracle WebCenter Sites domain in a Kubernetes cluster:\nSetting Up Loadbalancer Traefik for the WebCenter Sites Domain on K8S Follow these steps to set up Traefik as a loadbalancer for the Oracle WebCenter Sites domain:\n Install the Traefik Load Balancer Configure Traefik to Manage Ingresses Create an Ingress for the Domain Verify that You can Access the Domain URL  Install the Traefik Load Balancer   Use helm to install the Traefik load balancer. For detailed information, see this document. Use the values.yaml file in the sample but set kubernetes.namespaces specifically.\n Add the repo\n $ cd ${WORKDIR}/weblogic-kubernetes-operator $ kubectl create namespace traefik $ helm repo add traefik https://containous.github.io/traefik-helm-chart  Update the repo\n $ helm repo update  Helm Install for Traefik\n $ helm install traefik traefik/traefik \\  --namespace traefik \\  --values kubernetes/charts/traefik/values.yaml \\  --set \u0026#34;kubernetes.namespaces={traefik}\u0026#34; \\  --set \u0026#34;service.type=NodePort\u0026#34; --wait NAME:traefik-operator LAST DEPLOYED: Fri Jun 19 00:17:57 2020 NAMESPACE: traefik STATUS: deployed REVISION: 1 TEST SUITE: None   Access the Traefik dashboard through the URL http://$(hostname -f):30305, with the HTTP host traefik.example.com. NOTE: Make sure you specify full qualified node name for $(hostname -f).\n$ curl -H \u0026#39;host: $(hostname -f)\u0026#39; http://$(hostname -f):30305/ \u0026lt;a href=\u0026#34;/dashboard/\u0026#34;\u0026gt;Found\u0026lt;/a\u0026gt;. $   Configure Traefik to Manage Ingresses Configure Traefik to manage Ingresses created in this namespace: Note: Here traefik is the Traefik namespace, wcsites-ns is the namespace of the domain.\n Helm upgrade for traefik\n $ helm upgrade traefik traefik/traefik --namespace traefik --reuse-values \\  --set \u0026#34;kubernetes.namespaces={traefik,wcsites-ns}\u0026#34; NAME:traefik-operator LAST DEPLOYED: Fri Jun 19 00:18:50 2020 NAMESPACE: traefik STATUS: deployed REVISION: 2 TEST SUITE: None Create an Ingress for the Domain   Create an ingress for the domain in the domain namespace by using the sample Helm chart. Here path-based routing is used for ingress. Sample values for default configuration are shown in the file ${WORKDIR}/kubernetes/charts/ingress-per-domain/values.yaml. By default, type is TRAEFIK, sslType is NONSSL, and domainType is wcs. These values can be overridden by passing values through the command line or can be edited in the sample file values.yaml.\nIf needed, you can update the ingress YAML file to define more path rules (in section spec.rules.host.http.paths) based on the domain application URLs that need to be accessed. The template YAML file for the Traefik (ingress-based) load balancer is located at ${WORKDIR}/kubernetes/charts/ingress-per-domain/templates/traefik-ingress.yaml.\nFor detailed instructions about ingress, see this page.\nFor now, you can update the kubernetes/charts/ingress-per-domain/values.yaml with appropriate values.\n  Update the kubernetes/charts/ingress-per-domain/templates/traefik-ingress.yaml with the url routes to be load balanced.\nNOTE: This is not an exhaustive list of rules. You can enhance it based on the application urls that need to be accessed externally. These rules hold good for domain type wcs.\n  Install \u0026ldquo;ingress-per-domain\u0026rdquo; using helm.\n Helm Install ingress-per-domain\n $ helm install wcsitesinfra-ingress kubernetes/charts/ingress-per-domain \\  --namespace wcsites-ns \\  --values kubernetes/charts/ingress-per-domain/values.yaml \\  --set \u0026#34;traefik.hostname=$(hostname -f)\u0026#34; NAME: wcsitesinfra-ingress LAST DEPLOYED: Fri Jun 19 00:18:50 2020 NAMESPACE: wcsites-ns STATUS: deployed REVISION: 1 TEST SUITE: None   To confirm that the load balancer noticed the new Ingress and is successfully routing to the domain\u0026rsquo;s server pods, you can send a request to the URL for the \u0026ldquo;WebLogic ReadyApp framework\u0026rdquo; which should return a HTTP 200 status code, as shown in the example below:\n  -bash-4.2$ curl -v http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-PORT}/weblogic/ready * Trying 149.87.129.203... \u0026gt; GET http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-PORT}/weblogic/ready HTTP/1.1 \u0026gt; User-Agent: curl/7.29.0 \u0026gt; Accept: */* \u0026gt; Proxy-Connection: Keep-Alive \u0026gt; host: $(hostname -f) \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; Date: Sat, 14 Mar 2020 08:35:03 GMT \u0026lt; Vary: Accept-Encoding \u0026lt; Content-Length: 0 \u0026lt; Proxy-Connection: Keep-Alive \u0026lt; * Connection #0 to host localhost left intact Verify that You can Access the Domain URL After setting up the Traefik loadbalancer, verify that the domain applications are accessible through the loadbalancer port 30305. Through load balancer (Traefik port 30305), the following URLs are available for setting up domains of WebCenter Sites domain types:\nhttp://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-PORT}/weblogic/ready http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-PORT}/console http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-PORT}/em http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-PORT}/sites/version.jsp "
},
{
	"uri": "/fmw-kubernetes/wcportal-domains/manage-wcportal-domains/monitoring-and-publishing-logs/publishing-logs/fluentd/",
	"title": " Fluentd",
	"tags": [],
	"description": "Describes how to configure a WebCenter Portal domain to use Fluentd to send log information to Elasticsearch.",
	"content": "Overview You can configure your WebLogic domain to use Fluentd so it can send the log information to Elasticsearch.\nHere\u0026rsquo;s how this works:\n fluentd runs as a separate container in the Administration Server and Managed Server pods. The log files reside on a volume that is shared between the weblogic-server and fluentd containers. fluentd tails the domain logs files and exports them to Elasticsearch. A ConfigMap contains the filter and format rules for exporting log records.  Prerequisites It is assumed that you are editing an existing WebCenter Portal domain. However, you can make all the changes to the domain YAML before creating the domain. A complete example of a domain definition with fluentd configuration is at the end of this document.\nThese identifiers are used in the sample commands.\n wcpns: WebCenter Portal domain namespace wcp-domain: domainUID wcp-domain-domain-credentials: Kubernetes secret   The sample Elasticsearch configuration is:\nelasticsearchhost: elasticsearch.wcp-domain.sample.com elasticsearchport: 443 elasticsearchuser: username elasticsearchpassword: password Install Elasticsearch and Kibana To install Elasticsearch and Kibana, run the following command:\n$ kubectl apply -f kubernetes/samples/scripts/elasticsearch-and-kibana/elasticsearch_and_kibana.yaml Configure log files to use a volume The domain log files must be written to a volume that can be shared between the weblogic-server and fluentd containers. The following elements are required to accomplish this:\n logHome must be a path that can be shared between containers. logHomeEnabled must be set to true so that the logs are written outside the pod and persist across pod restarts. A volume must be defined on which the log files will reside. In the example, emptyDir is a volume that gets created when a Pod is created. It will persist across pod restarts but deleting the pod would delete the emptyDir content. The volumeMounts mounts the named volume created with emptyDir and establishes the base path for accessing the volume.  NOTE: For brevity, only the paths to the relevant configuration are here.\nFor Example, run : kubectl edit domain wcp-domain -n wcpns and make the following edits:\nspec: logHome: /u01/oracle/user_projects/domains/logs/wcp-domain logHomeEnabled: true serverPod: volumes: - emptyDir: {} name: weblogic-domain-storage-volume volumeMounts: - mountPath: /scratch name: weblogic-domain-storage-volume Add Elasticsearch secrets to WebLogic domain credentials Configure the fluentd container to look for Elasticsearch parameters in the domain credentials. Edit the domain credentials and add the parameters shown in the example below.\nFor example, run: kubectl edit secret wcp-domain-domain-credentials -n wcpns and add the base64 encoded values of each Elasticsearch parameter:\nelasticsearchhost: ZWxhc3RpY3NlYXJjaC5ib2JzLWJvb2tzLnNhbXBsZS5jb20= elasticsearchport: NDQz elasticsearchuser: Ym9i elasticsearchpassword: d2VsY29tZTE= Create Fluentd configuration Create a ConfigMap named fluentd-config in the namespace of the domain. The ConfigMap contains the parsing rules and Elasticsearch configuration.\nHere\u0026rsquo;s an explanation of some elements defined in the ConfigMap:\n The @type tail indicates that tail is used to obtain updates to the log file. The path of the log file obtained from the LOG_PATH environment variable that is defined in the fluentd container. The tag value of log records obtained from the DOMAIN_UID environment variable that is defined in the fluentd container. The \u0026lt;parse\u0026gt; section defines how to interpret and tag each element of a log record. The \u0026lt;match **\u0026gt; section contains the configuration information for connecting to Elasticsearch and defines the index name of each record to be the domainUID. The scheme indicates type of connection between fluentd and Elasticsearch.  The following is an example of how to create the ConfigMap:\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: v1 kind: ConfigMap metadata: labels: weblogic.domainUID: wcp-domain weblogic.resourceVersion: domain-v2 name: fluentd-config namespace: wcpns data: fluentd.conf: | \u0026lt;match fluent.**\u0026gt; @type null \u0026lt;/match\u0026gt; \u0026lt;source\u0026gt; @type tail path \u0026#34;#{ENV[\u0026#39;LOG_PATH\u0026#39;]}\u0026#34; pos_file /tmp/server.log.pos read_from_head true tag \u0026#34;#{ENV[\u0026#39;DOMAIN_UID\u0026#39;]}\u0026#34; # multiline_flush_interval 20s \u0026lt;parse\u0026gt; @type multiline format_firstline /^####/ format1 /^####\u0026lt;(?\u0026lt;timestamp\u0026gt;(.*?))\u0026gt;/ format2 / \u0026lt;(?\u0026lt;level\u0026gt;(.*?))\u0026gt;/ format3 / \u0026lt;(?\u0026lt;subSystem\u0026gt;(.*?))\u0026gt;/ format4 / \u0026lt;(?\u0026lt;serverName\u0026gt;(.*?))\u0026gt;/ format5 / \u0026lt;(?\u0026lt;serverName2\u0026gt;(.*?))\u0026gt;/ format6 / \u0026lt;(?\u0026lt;threadName\u0026gt;(.*?))\u0026gt;/ format7 / \u0026lt;(?\u0026lt;info1\u0026gt;(.*?))\u0026gt;/ format8 / \u0026lt;(?\u0026lt;info2\u0026gt;(.*?))\u0026gt;/ format9 / \u0026lt;(?\u0026lt;info3\u0026gt;(.*?))\u0026gt;/ format10 / \u0026lt;(?\u0026lt;sequenceNumber\u0026gt;(.*?))\u0026gt;/ format11 / \u0026lt;(?\u0026lt;severity\u0026gt;(.*?))\u0026gt;/ format12 / \u0026lt;(?\u0026lt;messageID\u0026gt;(.*?))\u0026gt;/ format13 / \u0026lt;(?\u0026lt;message\u0026gt;(.*?))\u0026gt;/ \u0026lt;/parse\u0026gt; \u0026lt;/source\u0026gt; \u0026lt;match **\u0026gt; @type elasticsearch host \u0026#34;#{ENV[\u0026#39;ELASTICSEARCH_HOST\u0026#39;]}\u0026#34; port \u0026#34;#{ENV[\u0026#39;ELASTICSEARCH_PORT\u0026#39;]}\u0026#34; user \u0026#34;#{ENV[\u0026#39;ELASTICSEARCH_USER\u0026#39;]}\u0026#34; password \u0026#34;#{ENV[\u0026#39;ELASTICSEARCH_PASSWORD\u0026#39;]}\u0026#34; index_name \u0026#34;#{ENV[\u0026#39;DOMAIN_UID\u0026#39;]}\u0026#34; scheme http \u0026lt;/match\u0026gt; EOF Mount the ConfigMap as a volume in the weblogic-server container Edit the domain definition and configure a volume for the ConfigMap containing the fluentd configuration.\nNOTE: For brevity, only the paths to the relevant configuration are shown.\nFor example, run: kubectl edit domain wcp-domain -n wcpns and add the following portions to the domain definition.\nspec: serverPod: volumes: - configMap: defaultMode: 420 name: fluentd-config name: fluentd-config-volume Add fluentd container Add a container to the domain to run fluentd in the Administration Server and Managed Server pods.\nThe container definition:\n Defines a LOG_PATH environment variable that points to the log location of bobbys-front-end. Defines ELASTICSEARCH_HOST, ELASTICSEARCH_PORT, ELASTICSEARCH_USER, and ELASTICSEARCH_PASSWORD environment variables that are all retrieving their values from the secret wcp-domain-domain-credentials. Includes volume mounts for the fluentd-config ConfigMap and the volume containing the domain logs.  NOTE: For brevity, only the paths to the relevant configuration are shown.\nFor example, run: kubectl edit domain wcp-domain -n wcpcns and add the following container definition.\nspec: serverPod: containers: - args: - -c - /etc/fluent.conf env: - name: DOMAIN_UID valueFrom: fieldRef: fieldPath: metadata.labels[\u0026#39;weblogic.domainUID\u0026#39;] - name: SERVER_NAME valueFrom: fieldRef: fieldPath: metadata.labels[\u0026#39;weblogic.serverName\u0026#39;] - name: LOG_PATH value: /u01/oracle/user_projects/domains/logs/wcp-domain/$(SERVER_NAME).log - name: FLUENTD_CONF value: fluentd.conf - name: FLUENT_ELASTICSEARCH_SED_DISABLE value: \u0026#34;true\u0026#34; - name: ELASTICSEARCH_HOST valueFrom: secretKeyRef: key: elasticsearchhost name: wcp-domain-domain-credentials - name: ELASTICSEARCH_PORT valueFrom: secretKeyRef: key: elasticsearchport name: wcp-domain-domain-credentials - name: ELASTICSEARCH_USER valueFrom: secretKeyRef: key: elasticsearchuser name: wcp-domain-domain-credentials optional: true - name: ELASTICSEARCH_PASSWORD valueFrom: secretKeyRef: key: elasticsearchpassword name: wcp-domain-domain-credentials optional: true image: fluent/fluentd-kubernetes-daemonset:v1.3.3-debian-elasticsearch-1.3 imagePullPolicy: IfNotPresent name: fluentd resources: {} volumeMounts: - mountPath: /fluentd/etc/fluentd.conf name: fluentd-config-volume subPath: fluentd.conf - mountPath: /scratch name: weblogic-domain-storage-volume Verify logs exported to Elasticsearch The logs are sent to Elasticsearch after you start the Administration Server and Managed Server pods after making the changes described previously.\nYou can check if the fluentd container is successfully tailing the log by executing a command like kubectl logs -f wcp-domain-adminserver -n wcpns fluentd. The log output should look similar to this:\n2019-10-01 16:23:44 +0000 [info]: #0 starting fluentd worker pid=13 ppid=9 worker=0 2019-10-01 16:23:44 +0000 [warn]: #0 /scratch/logs/bobs-bookstore/managed-server1.log not found. Continuing without tailing it. 2019-10-01 16:23:44 +0000 [info]: #0 fluentd worker is now running worker=0 2019-10-01 16:24:01 +0000 [info]: #0 following tail of /scratch/logs/bobs-bookstore/managed-server1.log When you connect to Kibana, you will see an index created for the domainUID.\nDomain example The following is a complete example of a domain custom resource with a fluentd container configured.\napiVersion: weblogic.oracle/v8 kind: Domain metadata: labels: weblogic.domainUID: wcp-domain name: wcp-domain namespace: wcpns spec: domainHome: /u01/oracle/user_projects/domains/wcp-domain domainHomeSourceType: PersistentVolume image: \u0026#34;oracle/wcportal:12.2.1.4\u0026#34; imagePullPolicy: \u0026#34;IfNotPresent\u0026#34; webLogicCredentialsSecret: name: wcp-domain-domain-credentials includeServerOutInPodLog: true logHomeEnabled: true httpAccessLogInLogHome: true logHome: /u01/oracle/user_projects/domains/logs/wcp-domain dataHome: \u0026#34;\u0026#34; serverStartPolicy: \u0026#34;IF_NEEDED\u0026#34; adminServer: serverStartState: \u0026#34;RUNNING\u0026#34; clusters: - clusterName: wcp_cluster serverStartState: \u0026#34;RUNNING\u0026#34; serverPod: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: \u0026#34;weblogic.clusterName\u0026#34; operator: In values: - $(CLUSTER_NAME) topologyKey: \u0026#34;kubernetes.io/hostname\u0026#34; replicas: 2 serverPod: containers: - args: - -c - /etc/fluent.conf env: - name: DOMAIN_UID valueFrom: fieldRef: fieldPath: metadata.labels[\u0026#39;weblogic.domainUID\u0026#39;] - name: SERVER_NAME valueFrom: fieldRef: fieldPath: metadata.labels[\u0026#39;weblogic.serverName\u0026#39;] - name: LOG_PATH value: /u01/oracle/user_projects/domains/logs/wcp-domain/$(SERVER_NAME).log - name: FLUENTD_CONF value: fluentd.conf - name: FLUENT_ELASTICSEARCH_SED_DISABLE value: \u0026#34;true\u0026#34; - name: ELASTICSEARCH_HOST valueFrom: secretKeyRef: key: elasticsearchport name: wcp-domain-domain-credentials - name: ELASTICSEARCH_PORT valueFrom: secretKeyRef: key: elasticsearchhost name: wcp-domain-domain-credentials - name: ELASTICSEARCH_USER valueFrom: secretKeyRef: key: elasticsearchuser name: wcp-domain-domain-credentials - name: ELASTICSEARCH_PASSWORD valueFrom: secretKeyRef: key: elasticsearchpassword name: wcp-domain-domain-credentials image: fluent/fluentd-kubernetes-daemonset:v1.11.5-debian-elasticsearch6-1.0 imagePullPolicy: IfNotPresent name: fluentd resources: {} volumeMounts: - mountPath: /fluentd/etc/fluentd.conf name: fluentd-config-volume subPath: fluentd.conf - mountPath: /u01/oracle/user_projects/domains name: weblogic-domain-storage-volume env: - name: JAVA_OPTIONS value: -Dweblogic.StdoutDebugEnabled=false - name: USER_MEM_ARGS value: \u0026#39;-Djava.security.egd=file:/dev/./urandom -Xms1g -Xmx2g\u0026#39; volumeMounts: - mountPath: /u01/oracle/user_projects/domains name: weblogic-domain-storage-volume volumes: - name: weblogic-domain-storage-volume persistentVolumeClaim: claimName: wcp-domain-domain-pvc - emptyDir: {} name: weblogic-domain-storage-volume - configMap: defaultMode: 420 name: fluentd-config name: fluentd-config-volume serverStartPolicy: IF_NEEDED webLogicCredentialsSecret: name: wcp-domain-domain-credentials Get the Kibana dashboard port information as shown below: -bash-4.2$ kubectl get pods -w NAME READY STATUS RESTARTS AGE elasticsearch-8bdb7cf54-mjs6s 1/1 Running 0 4m3s kibana-dbf8964b6-n8rcj 1/1 Running 0 4m3s -bash-4.2$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE elasticsearch ClusterIP 10.100.11.154 \u0026lt;none\u0026gt; 9200/TCP,9300/TCP 4m32s kibana NodePort 10.97.205.0 \u0026lt;none\u0026gt; 5601:31884/TCP 4m32s kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 71d You can access the Kibana dashboard at http://mycompany.com:kibana-nodeport/. In our example, the node port is 31884.\nCreate an Index Pattern in Kibana Create an index pattern wcp-domain* in Kibana by navigating to the dashboard through the Management option. When the servers are started, the log data is shown on the Kibana dashboard.\n"
},
{
	"uri": "/fmw-kubernetes/wcportal-domains/manage-wcportal-domains/monitoring-and-publishing-logs/monitoring-domain/",
	"title": " Monitor a WebCenter Portal domain",
	"tags": [],
	"description": "Monitor an WebCenter Portal instance using Prometheus and Grafana.",
	"content": "You can monitor a WebCenter Portal domain using Prometheus and Grafana by exporting the metrics from the domain instance using the WebLogic Monitoring Exporter. This sample shows you how to set up the WebLogic Monitoring Exporter to push the data to Prometheus.\nPrerequisites This document assumes that the Prometheus Operator is deployed on the Kubernetes cluster. If it is not already deployed, follow the steps below for deploying the Prometheus Operator.\nPrepare to use the setup monitoring script The sample scripts for setup monitoring for OracleWebCenterPortal domain are available at ${WORKDIR}/monitoring-service.\nYou must edit monitoring-inputs.yaml(or a copy of it) to provide the details of your domain. Refer to the configuration parameters below to understand the information that you must provide in this file.\nConfiguration parameters The following parameters can be provided in the inputs file.\n   Parameter Description Default     domainUID domainUID of the OracleWebCenterPortal domain. wcp-domain   domainNamespace Kubernetes namespace of the OracleWebCenterPortal domain. wcpns   setupKubePrometheusStack Boolean value indicating whether kube-prometheus-stack (Prometheus, Grafana and Alertmanager) to be installed true   additionalParamForKubePrometheusStack The script install\u0026rsquo;s kube-prometheus-stack with service.type as NodePort and values for service.nodePort as per the parameters defined in monitoring-inputs.yaml. Use additionalParamForKubePrometheusStack parameter to further configure with additional parameters as per values.yaml. Sample value to disable NodeExporter, Prometheus-Operator TLS support and Admission webhook support for PrometheusRules resources is --set nodeExporter.enabled=false --set prometheusOperator.tls.enabled=false --set prometheusOperator.admissionWebhooks.enabled=false    monitoringNamespace Kubernetes namespace for monitoring setup. monitoring   adminServerName Name of the Administration Server. AdminServer   adminServerPort Port number for the Administration Server inside the Kubernetes cluster. 7001   wcpClusterName Name of the wcpCluster. wcp_cluster   wcpManagedServerPort Port number of the managed servers in the wcpCluster. 8888   wlsMonitoringExporterTowcpCluster Boolean value indicating whether to deploy WebLogic Monitoring Exporter to wcpCluster. false   wcpPortletClusterName Name of the wcpPortletCluster. wcportlet-cluster   wcpManagedServerPort Port number of the Portlet managed servers in the wcpPortletCluster. 8889   wlsMonitoringExporterTowcpPortletCluster Boolean value indicating whether to deploy WebLogic Monitoring Exporter to wcpPortletCluster. false   exposeMonitoringNodePort Boolean value indicating if the Monitoring Services (Prometheus, Grafana and Alertmanager) is exposed outside of the Kubernetes cluster. false   prometheusNodePort Port number of the Prometheus outside the Kubernetes cluster. 32101   grafanaNodePort Port number of the Grafana outside the Kubernetes cluster. 32100   alertmanagerNodePort Port number of the Alertmanager outside the Kubernetes cluster. 32102   weblogicCredentialsSecretName Name of the Kubernetes secret which has Administration Server’s user name and password. wcp-domain-domain-credentials    Note that the values specified in the monitoring-inputs.yaml file will be used to install kube-prometheus-stack (Prometheus, Grafana and Alertmanager) and deploying WebLogic Monitoring Exporter into the OracleWebCenterPortal domain. Hence make the domain specific values to be same as that used during domain creation.\nRun the setup monitoring script Update the values in monitoring-inputs.yaml as per your requirement and run the setup-monitoring.sh script, specifying your inputs file:\n$ cd ${WORKDIR}/monitoring-service $ ./setup-monitoring.sh \\  -i monitoring-inputs.yaml The script will perform the following steps:\n Helm install prometheus-community/kube-prometheus-stack of version \u0026ldquo;16.5.0\u0026rdquo; if setupKubePrometheusStack is set to true. Deploys WebLogic Monitoring Exporter to Administration Server. Deploys WebLogic Monitoring Exporter to wcpCluster if wlsMonitoringExporterTowcpCluster is set to true. Deploys WebLogic Monitoring Exporter to wcpPortletCluster if wlsMonitoringExporterTowcpPortletCluster is set to true. Exposes the Monitoring Services (Prometheus at 32101, Grafana at 32100 and Alertmanager at 32102) outside of the Kubernetes cluster if exposeMonitoringNodePort is set to true. Imports the WebLogic Server Grafana Dashboard if setupKubePrometheusStack is set to true.  Verify the results The setup monitoring script will report failure if there was any error. However, verify that required resources were created by the script.\nVerify the kube-prometheus-stack To confirm that prometheus-community/kube-prometheus-stack was installed when setupKubePrometheusStack is set to true, run the following command:\n$ helm ls -n monitoring Sample output:\n$ helm ls -n monitoring NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION monitoring monitoring 1 2021-06-18 12:58:35.177221969 +0000 UTC deployed kube-prometheus-stack-16.5.0 0.48.0 $ Verify the Prometheus, Grafana and Alertmanager setup When exposeMonitoringNodePort was set to true, verify that monitoring services are accessible outside of the Kubernetes cluster:\n 32100 is the external port for Grafana and with credentials admin:admin 32101 is the external port for Prometheus 32102 is the external port for Alertmanager  Verify the service discovery of WebLogic Monitoring Exporter Verify whether prometheus is able to discover wls-exporter and collect the metrics:\n  Access the Prometheus dashboard at http://mycompany.com:32101/\n  Navigate to Status to see the Service Discovery details.\n  Verify that wls-exporter is listed in the discovered services.\n  Verify the WebLogic Server dashoard You can access the Grafana dashboard at http://mycompany.com:32100/.\n  Log in to Grafana dashboard with username: admin and password: admin.\n  Navigate to \u0026ldquo;WebLogic Server Dashboard\u0026rdquo; under General and verify.\nThis displays the WebLogic Server Dashboard.\n  Delete the monitoring setup To delete the monitoring setup created by Run the setup monitoring script, run the below command:\n$ cd ${WORKDIR}/monitoring-service $ ./delete-monitoring.sh \\  -i monitoring-inputs.yaml "
},
{
	"uri": "/fmw-kubernetes/oig/configure-design-console/using-the-design-console-with-nginx-non-ssl/",
	"title": "a. Using Design Console with NGINX(non-SSL)",
	"tags": [],
	"description": "Configure Design Console with NGINX(non-SSL).",
	"content": "Configure an NGINX ingress (non-SSL) to allow Design Console to connect to your Kubernetes cluster.\n  Prerequisites\n  Setup routing rules for the Design Console ingress\n  Create the ingress\n  Update the T3 channel\n  Restart the OIG Managed Server\n  Design Console client\na. Using an on-premises installed Design Console\nb. Using a container image for Design Console\n  Login to the Design Console\n  Prerequisites If you haven\u0026rsquo;t already configured an NGINX ingress controller (Non-SSL) for OIG, follow Using an Ingress with NGINX (non-SSL).\nMake sure you know the master hostname and ingress port for NGINX before proceeding e.g http://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}.\nNote: In all steps below if you are using a load balancer for your ingress instead of NodePort then replace ${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT} with `${LOADBALANCER-HOSTNAME}:${LOADBALANCER-PORT}.\nSetup routing rules for the Design Console ingress   Setup routing rules by running the following commands:\n$ cd $WORKDIR/kubernetes/design-console-ingress Edit values.yaml and ensure that tls: NONSSL and domainUID: governancedomain are set, for example:\n# Load balancer type. Supported values are: NGINX type: NGINX # Type of Configuration Supported Values are : NONSSL,SSL # tls: NONSSL tls: NONSSL # TLS secret name if the mode is SSL secretName: dc-tls-cert # WLS domain as backend to the load balancer wlsDomain: domainUID: governancedomain oimClusterName: oim_cluster oimServerT3Port: 14002   Create the ingress   Run the following command to create the ingress:\n$ cd $WORKDIR $ helm install governancedomain-nginx-designconsole kubernetes/design-console-ingress --namespace oigns --values kubernetes/design-console-ingress/values.yaml For example:\nThe output will look similar to the following:\nNAME: governancedomain-nginx-designconsole LAST DEPLOYED: Thu Mar 10 14:32:16 2022 NAMESPACE: oigns STATUS: deployed REVISION: 1 TEST SUITE: None   Run the following command to show the ingress is created successfully:\n$ kubectl describe ing governancedomain-nginx-designconsole -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl describe ing governancedomain-nginx-designconsole -n oigns The output will look similar to the following:\nName: governancedomain-nginx-designconsole Namespace: oigns Address: Default backend: default-http-backend:80 (\u0026lt;error: endpoints \u0026quot;default-http-backend\u0026quot; not found\u0026gt;) Rules: Host Path Backends ---- ---- -------- * governancedomain-cluster-oim-cluster:14002 (10.244.1.25:14002) Annotations: kubernetes.io/ingress.class: nginx meta.helm.sh/release-name: governancedomain-nginx-designconsole meta.helm.sh/release-namespace: oigns nginx.ingress.kubernetes.io/affinity: cookie nginx.ingress.kubernetes.io/enable-access-log: false Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Sync 13s nginx-ingress-controller Scheduled for sync   Update the T3 channel   Log in to the WebLogic Console using http://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/console.\n  Navigate to Environment, click Servers, and then select oim_server1.\n  Click Protocols, and then Channels.\n  Click the default T3 channel called T3Channel.\n  Click Lock and Edit.\n  Set the External Listen Address to the ingress controller hostname ${MASTERNODE-HOSTNAME}.\n  Set the External Listen Port to the ingress controller port ${MASTERNODE-PORT}.\n  Click Save.\n  Click Activate Changes.\n  Restart the OIG Managed Server Restart the OIG Managed Server for the above changes to take effect:\n$ cd $WORKDIR/kubernetes/domain-lifecycle $ ./restartServer.sh -s oim_server1 -d \u0026lt;domain_uid\u0026gt; -n \u0026lt;domain_namespace\u0026gt; For example:\n$ cd $WORKDIR/kubernetes/domain-lifecycle ./restartServer.sh -s oim_server1 -d governancedomain -n oigns Make sure the \u0026lt;domain_uid\u0026gt;-oim-server1 has a READY status of 1/1 before continuing:\n$ kubectl get pods -n oigns | grep oim-server1 The output will look similar to the following:\ngovernancedomain-oim-server1 1/1 Running 0 8m Design Console client It is possible to use Design Console from an on-premises install, or from a container image.\nUsing an on-premises installed Design Console   Install Design Console on an on-premises machine\n  Follow Login to the Design Console.\n  Using a container image for Design Console Using Docker The Design Console can be run from a container using X windows emulation.\n  On the parent machine where the Design Console is to be displayed, run xhost +.\n  Find which worker node the \u0026lt;domain\u0026gt;-oim-server1 pod is running. For example:\n$ kubectl get pods -n oigns -o wide | grep governancedomain-oim-server1 The output will look similar to the following:\ngovernancedomain-oim-server1 1/1 Running 0 31m 10.244.2.98 worker-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;   On the worker node returned above e.g worker-node2, execute the following command to find the OIG container image name:\n$ docker images Then execute the following command to start a container to run Design Console:\n$ docker run -u root --name oigdcbase -it \u0026lt;image\u0026gt; bash For example:\n$ docker run -u root -it --name oigdcbase container-registry.oracle.com/middleware/oig_cpu:12.2.1.4-jdk8-ol7-220120.1359 bash This will take you into a bash shell inside the container:\nbash-4.2#   Inside the container set the proxy, for example:\nbash-4.2# export https_proxy=http://proxy.example.com:80   Install the relevant X windows packages in the container:\nbash-4.2# yum install libXext libXrender libXtst   Execute the following outside the container to create a new Design Console image from the container:\n$ docker commit \u0026lt;container_name\u0026gt; \u0026lt;design_console_image_name\u0026gt; For example:\n$ docker commit oigdcbase oigdc   Exit the container bash session:\nbash-4.2# exit   Start a new container using the Design Console image:\n$ docker run --name oigdc -it oigdc /bin/bash This will take you into a bash shell for the container:\nbash-4.2#   In the container run the following to export the DISPLAY:\n$ export DISPLAY=\u0026lt;parent_machine_hostname:1\u0026gt;   Start the Design Console from the container:\nbash-4.2# cd idm/designconsole bash-4.2# sh xlclient.sh The Design Console login should be displayed. Now follow Login to the Design Console.\n  Using podman   On the parent machine where the Design Console is to be displayed, run xhost +.\n  Find which worker node the \u0026lt;domain\u0026gt;-oim-server1 pod is running. For example:\n$ kubectl get pods -n oigns -o wide | grep governancedomain-oim-server1 The output will look similar to the following:\ngovernancedomain-oim-server1 1/1 Running 0 31m 10.244.2.98 worker-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;   On the worker node returned above e.g worker-node2, execute the following command to find the OIG container image name:\n$ podman images Then execute the following command to start a container to run Design Console:\n$ podman run -u root --name oigdcbase -it \u0026lt;image\u0026gt; bash For example:\n$ podman run -u root -it --name oigdcbase container-registry.oracle.com/middleware/oig_cpu:12.2.1.4-jdk8-ol7-220120.1359 bash This will take you into a bash shell inside the container:\nbash-4.2#   Inside the container set the proxy, for example:\nbash-4.2# export https_proxy=http://proxy.example.com:80   Install the relevant X windows packages in the container:\nbash-4.2# yum install libXext libXrender libXtst   Execute the following outside the container to create a new Design Console image from the container:\n$ podman commit \u0026lt;container_name\u0026gt; \u0026lt;design_console_image_name\u0026gt; For example:\n$ podman commit oigdcbase oigdc   Exit the container bash session:\nbash-4.2# exit   Start a new container using the Design Console image:\n$ podman run --name oigdc -it oigdc /bin/bash This will take you into a bash shell for the container:\nbash-4.2#   In the container run the following to export the DISPLAY:\n$ export DISPLAY=\u0026lt;parent_machine_hostname:1\u0026gt;   Start the Design Console from the container:\nbash-4.2# cd idm/designconsole bash-4.2# sh xlclient.sh The Design Console login should be displayed. Now follow Login to the Design Console.\n  Login to the Design Console   Launch the Design Console and in the Oracle Identity Manager Design Console login page enter the following details:\nEnter the following details and click Login:\n Server URL: \u0026lt;url\u0026gt; User ID: xelsysadm Password: \u0026lt;password\u0026gt;.  where \u0026lt;url\u0026gt; is http://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}\n  If successful the Design Console will be displayed.\n  "
},
{
	"uri": "/fmw-kubernetes/oig/manage-oig-domains/domain-lifecycle/",
	"title": "Domain life cycle",
	"tags": [],
	"description": "Learn about the domain life cyle of an OIG domain.",
	"content": " View existing OIG servers Starting/Scaling up OIG Managed servers Stopping/Scaling down OIG Managed servers Stopping and starting the Administration Server and Managed Servers Domain lifecycle sample scripts  As OIG domains use the WebLogic Kubernetes Operator, domain lifecyle operations are managed using the WebLogic Kubernetes Operator itself.\nThis document shows the basic operations for starting, stopping and scaling servers in the OIG domain.\nFor more detailed information refer to Domain Life Cycle in the WebLogic Kubernetes Operator documentation.\nDo not use the WebLogic Server Administration Console or Oracle Enterprise Manager Console to start or stop servers.\n View existing OIG Servers The default OIG deployment starts the Administration Server (AdminServer), one OIG Managed Server (oim_server1) and one SOA Managed Server (soa_server1).\nThe deployment also creates, but doesn\u0026rsquo;t start, four extra OIG Managed Servers (oim-server2 to oim-server5) and four more SOA Managed Servers (soa_server2 to soa_server5).\nAll these servers are visible in the WebLogic Server Administration Console https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/console by navigating to Domain Structure \u0026gt; governancedomain \u0026gt; Environment \u0026gt; Servers.\nTo view the running servers using kubectl, run the following command:\n$ kubectl get pods -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get pods -n oigns The output should look similar to the following:\nNAME READY STATUS RESTARTS AGE governancedomain-adminserver 1/1 Running 0 23h governancedomain-create-fmw-infra-sample-domain-job-8cww8 0/1 Completed 0 24h governancedomain-oim-server1 1/1 Running 0 23h governancedomain-soa-server1 1/1 Running 0 23h Starting/Scaling up OIG Managed Servers The number of OIG Managed Servers running is dependent on the replicas parameter configured for the cluster. To start more OIG Managed Servers perform the following steps:\n  Run the following kubectl command to edit the domain:\n$ kubectl edit domain \u0026lt;domain_uid\u0026gt; -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl edit domain governancedomain -n oigns Note: This opens an edit session for the domain where parameters can be changed using standard vi commands.\n  In the edit session search for clusterName: oim_cluster and look for the replicas parameter. By default the replicas parameter is set to \u0026ldquo;1\u0026rdquo; hence a single OIG Managed Server is started (oim_server1):\n - clusterName: oim_cluster replicas: 1 serverPod: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - podAffinityTerm: labelSelector: matchExpressions: - key: weblogic.clusterName operator: In values: - $(CLUSTER_NAME)   To start more OIG Managed Servers, increase the replicas value as desired. In the example below, one more Managed Server will be started by setting replicas to \u0026ldquo;2\u0026rdquo;:\n - clusterName: oim_cluster replicas: 2 serverPod: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - podAffinityTerm: labelSelector: matchExpressions: - key: weblogic.clusterName operator: In values: - $(CLUSTER_NAME)   Save the file and exit (:wq)\nThe output will look similar to the following:\ndomain.weblogic.oracle/governancedomain edited   Run the following kubectl command to view the pods:\n$ kubectl get pods -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get pods -n oigns The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE governancedomain-adminserver 1/1 Running 0 23h governancedomain-create-fmw-infra-sample-domain-job-8cww8 0/1 Completed 0 24h governancedomain-oim-server1 1/1 Running 0 23h governancedomain-oim-server2 0/1 Running 0 7s governancedomain-soa-server1 1/1 Running 0 23h One new pod (governancedomain-oim-server2) is started, but currently has a READY status of 0/1. This means oim_server2 is not currently running but is in the process of starting. The server will take several minutes to start so keep executing the command until READY shows 1/1:\nNAME READY STATUS RESTARTS AGE governancedomain-adminserver 1/1 Running 0 23h governancedomain-create-fmw-infra-sample-domain-job-8cww8 0/1 Completed 0 24h governancedomain-oim-server1 1/1 Running 0 23h governancedomain-oim-server2 1/1 Running 0 5m27s governancedomain-soa-server1 1/1 Running 0 23h Note: To check what is happening during server startup when READY is 0/1, run the following command to view the log of the pod that is starting:\n$ kubectl logs \u0026lt;pod\u0026gt; -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl logs governancedomain-oim-server2 -n oigns   Stopping/Scaling down OIG Managed Servers As mentioned in the previous section, the number of OIG Managed Servers running is dependent on the replicas parameter configured for the cluster. To stop one or more OIG Managed Servers, perform the following:\n  Run the following kubectl command to edit the domain:\n$ kubectl edit domain \u0026lt;domain_uid\u0026gt; -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl edit domain governancedomain -n oigns   In the edit session search for clusterName: oim_cluster and look for the replicas parameter. In the example below replicas is set to \u0026ldquo;2\u0026rdquo; hence two OIG Managed Servers are started (oim_server1 and oim_server2):\n - clusterName: oim_cluster replicas: 2 serverPod: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - podAffinityTerm: labelSelector: matchExpressions: - key: weblogic.clusterName operator: In values: - $(CLUSTER_NAME)   To stop OIG Managed Servers, decrease the replicas value as desired. In the example below, we will stop one Managed Server by setting replicas to \u0026ldquo;1\u0026rdquo;:\n - clusterName: oim_cluster replicas: 1 serverPod: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - podAffinityTerm: labelSelector: matchExpressions: - key: weblogic.clusterName operator: In values: - $(CLUSTER_NAME)   Save the file and exit (:wq)\n  Run the following kubectl command to view the pods:\n$ kubectl get pods -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get pods -n oigns The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE governancedomain-adminserver 1/1 Running 0 23h governancedomain-create-fmw-infra-sample-domain-job-8cww8 0/1 Completed 0 24h governancedomain-oim-server1 1/1 Running 0 23h governancedomain-oim-server2 1/1 Terminating 0 7m30s governancedomain-soa-server1 1/1 Running 0 23h The exiting pod shows a STATUS of Terminating (governancedomain-oim-server2). The server may take a minute or two to stop, so keep executing the command until the pod has disappeared:\nNAME READY STATUS RESTARTS AGE governancedomain-adminserver 1/1 Running 0 23h governancedomain-create-fmw-infra-sample-domain-job-8cww8 0/1 Completed 0 24h governancedomain-oim-server1 1/1 Running 0 23h governancedomain-soa-server1 1/1 Running 0 23h   Stopping and Starting the Administration Server and Managed Servers To stop all the OIG Managed Servers and the Administration Server in one operation:\n  Run the following kubectl command to edit the domain:\n$ kubectl edit domain \u0026lt;domain_uid\u0026gt; -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl edit domain governancedomain -n oigns   In the edit session search for serverStartPolicy: IF_NEEDED:\n volumeMounts: - mountPath: /u01/oracle/user_projects/domains name: weblogic-domain-storage-volume volumes: - name: weblogic-domain-storage-volume persistentVolumeClaim: claimName: governancedomain-domain-pvc serverStartPolicy: IF_NEEDED   Change serverStartPolicy: IF_NEEDED to NEVER as follows:\n volumeMounts: - mountPath: /u01/oracle/user_projects/domains name: weblogic-domain-storage-volume volumes: - name: weblogic-domain-storage-volume persistentVolumeClaim: claimName: governancedomain-domain-pvc serverStartPolicy: NEVER   Save the file and exit (:wq).\n  Run the following kubectl command to view the pods:\n$ kubectl get pods -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get pods -n oigns The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE governancedomain-adminserver 1/1 Terminating 0 23h governancedomain-create-fmw-infra-sample-domain-job-8cww8 0/1 Completed 0 24h governancedomain-oim-server1 1/1 Terminating 0 23h governancedomain-soa-server1 1/1 Terminating 0 23h The AdminServer pod and Managed Server pods will move to a STATUS of Terminating. After a few minutes, run the command again and the pods should have disappeared:\nNAME READY STATUS RESTARTS AGE governancedomain-create-fmw-infra-sample-domain-job-8cww8 0/1 Completed 0 24h   To start the Administration Server and Managed Servers up again, repeat the previous steps but change serverStartPolicy: NEVER to IF_NEEDED as follows:\n volumeMounts: - mountPath: /u01/oracle/user_projects/domains name: weblogic-domain-storage-volume volumes: - name: weblogic-domain-storage-volume persistentVolumeClaim: claimName: governancedomain-domain-pvc serverStartPolicy: IF_NEEDED   Run the following kubectl command to view the pods:\n$ kubectl get pods -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get pods -n oigns The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE governancedomain-adminserver 0/1 Running 0 4s governancedomain-create-fmw-infra-sample-domain-job-8cww8 0/1 Completed 0 24h The Administration Server pod will start followed by the OIG Managed Servers pods. This process will take several minutes, so keep executing the command until all the pods are running with READY status 1/1 :\nNAME READY STATUS RESTARTS AGE governancedomain-adminserver 1/1 Running 0 6m57s governancedomain-create-fmw-infra-sample-domain-job-8cww8 0/1 Completed 0 24h governancedomain-oim-server1 1/1 Running 0 4m33s governancedomain-soa-server1 1/1 Running 0 4m33s   Domain lifecycle sample scripts The WebLogic Kubernetes Operator provides sample scripts to start up or shut down a specific Managed Server or cluster in a deployed domain, or the entire deployed domain.\nNote: Prior to running these scripts, you must have previously created and deployed the domain.\nThe scripts are located in the $WORKDIR/kubernetes/domain-lifecycle directory. For more information, see the README.\n"
},
{
	"uri": "/fmw-kubernetes/wccontent-domains/patch_and_upgrade/patch-an-image/",
	"title": "Patch an image",
	"tags": [],
	"description": "Create a patched Oracle WebCenter Content image using the WebLogic Image Tool.",
	"content": "Oracle aims to release Oracle WebCenter Content images regularly with latest bundle and recommended interim patches in My Oracle Support (MOS). However, if there is a need to create images with new bundle and interim patches, you can build these images using WebLogic Image Tool.\nIf you have access to the Oracle WebCenter Content patches, you can patch an existing Oracle WebCenter Content image with a bundle patch and interim patches. It is recommended to use the WebLogic Image Tool to patch the Oracle WebCenter Content image.\n Recommendations:\n Use the WebLogic Image Tool create feature for patching the Oracle WebCenter Content Docker image with a bundle patch and multiple interim patches. This is the recommended approach because it optimizes the size of the image. Use the WebLogic Image Tool update feature for patching the Oracle WebCenter Content Docker image with a single interim patch. Note that the patched image size may increase considerably due to additional image layers introduced by the patch application tool.   Apply the patched image   Update the image: field in domain.yaml configuration file with the patched image.\n  Apply the updated domain.yaml configuration file:\n$ kubectl apply -f domain.yaml    Note: The server pods will be automatically restarted (rolling restart).\n "
},
{
	"uri": "/fmw-kubernetes/oam/release-notes/",
	"title": "Release Notes",
	"tags": [],
	"description": "",
	"content": "Review the latest changes and known issues for Oracle Access Management on Kubernetes.\nRecent changes    Date Version Change     April, 2022 22.2.1 Updated for CRI-O support.   November, 2021 21.4.2 Supports Oracle Access Management domain deployment using WebLogic Kubernetes Operator 3.3.0. Voyager ingress removed as no longer supported.   October 2021 21.4.1 A) References to supported Kubernetes, Helm and Docker versions removed and replaced with Support note reference. B) Namespace and domain names changed to be consistent with Enterprise Deployment Guide for Oracle Identity and Access Management in a Kubernetes Cluster. C) Addtional post configuration tasks added. D) Upgrading a Kubernetes Cluster and Security Hardening removed as vendor specific.   November 2020 20.4.1 Initial release of Oracle Access Management on Kubernetes.    "
},
{
	"uri": "/fmw-kubernetes/oid/release-notes/",
	"title": "Release Notes",
	"tags": [],
	"description": "",
	"content": "Review the latest changes and known issues for Oracle Internet Directory on Kubernetes.\nRecent changes    Date Version Change     April, 2022 22.2.1 Updated for CRI-O support.   October, 2021 21.4.1 Initial release of Oracle Identity Directory on Kubernetes.    "
},
{
	"uri": "/fmw-kubernetes/oig/release-notes/",
	"title": "Release Notes",
	"tags": [],
	"description": "",
	"content": "Review the latest changes and known issues for Oracle Identity Governance on Kubernetes.\nRecent changes    Date Version Change     April, 2022 22.2.1 Updated for CRI-O support.   November, 2021 21.4.2 Supports Oracle Identity Governance domain deployment using WebLogic Kubernetes Operator 3.3.0. Voyager ingress removed as no longer supported.   October 2021 21.4.1 A) References to supported Kubernetes, Helm and Docker versions removed and replaced with Support note reference. B) Namespace and domain names changed to be consistent with Enterprise Deployment Guide for Oracle Identity and Access Management in a Kubernetes Cluster. C) Addtional post configuration tasks added. D) New section on how to start Design Console in a container. E) Upgrading a Kubernetes Cluster and Security Hardening removed as vendor specific.   November 2020 20.4.1 Initial release of Identity Governance on Kubernetes.    "
},
{
	"uri": "/fmw-kubernetes/oud/release-notes/",
	"title": "Release Notes",
	"tags": [],
	"description": "",
	"content": "Review the latest changes and known issues for Oracle Unified Directory on Kubernetes.\nRecent changes    Date Version Change     April, 2022 22.2.1 Updated for CRI-O support.   November 2021 21.4.2 Voyager ingress removed as no longer supported.   October 2021 21.4.1 A) References to supported Kubernetes, Helm and Docker versions removed and replaced with Support note reference. B) Namespace and domain names changed to be consistent with Enterprise Deployment Guide for Oracle Identity and Access Management in a Kubernetes Cluster. C) Upgrading a Kubernetes Cluster and Security Hardening removed as vendor specific.   November 2020 20.4.1 Initial release of Oracle Unified Directory on Kubernetes.    "
},
{
	"uri": "/fmw-kubernetes/oudsm/release-notes/",
	"title": "Release Notes",
	"tags": [],
	"description": "",
	"content": "Review the latest changes and known issues for Oracle Unified Directory Services Manager on Kubernetes.\nRecent changes    Date Version Change     April, 2022 22.2.1 Updated for CRI-O support.   November 2021 21.4.2 Voyager ingress removed as no longer supported.   October 2021 21.4.1 A) References to supported Kubernetes, Helm and Docker versions removed and replaced with Support note reference. B) Namespace and domain names changed to be consistent with Enterprise Deployment Guide for Oracle Identity and Access Management in a Kubernetes Cluster. C) Upgrading a Kubernetes Cluster and Security Hardening removed as vendor specific.   November 2020 20.4.1 Initial release of Oracle Unified Directory Services Manager on Kubernetes.    "
},
{
	"uri": "/fmw-kubernetes/wccontent-domains/release-notes/",
	"title": "Release Notes",
	"tags": [],
	"description": "",
	"content": "Review the latest changes for Oracle WebCenter Content on Kubernetes.\nRecent changes    Date Version Change     March 11, 2022 22.1.3 Supports Oracle WebCenter Content 12.2.1.4 domains deployment using January 2022 PSU and known bug fixes - certified for Oracle WebLogic Kubernetes Operator version 3.3.0. Oracle WebCenter Content 12.2.1.4 container image for this release can be downloaded from My Oracle Support (MOS patch 33771196).   December 7, 2021 21.4.3 Supports Oracle WebCenter Content 12.2.1.4 domains deployment using April 2021 PSU and known bug fixes - certified for Oracle WebLogic Kubernetes Operator version 3.2.5. Oracle WebCenter Content 12.2.1.4 container image for this release can be downloaded from My Oracle Support (MOS patch 32822360).   June 16, 2021 21.2.3 Supports Oracle WebCenter Content 12.2.1.4 domains deployment using April 2021 PSU and known bug fixes. Oracle WebCenter Content 12.2.1.4 container image for this release can be downloaded from My Oracle Support (MOS patch 32822360).   February 28, 2021 21.1.2 Certified Oracle WebLogic Kubernetes Operator version 3.1.1. Kubernetes 1.14.8+, 1.15.7+, 1.16.0+, 1.17.0+, and 1.18.0+ support. Flannel is the only supported CNI in this release. SSL enabling for the Administration Server and Managed Servers is supported. For now, only Oracle WebCenter Content 12.2.1.4 is supported.    "
},
{
	"uri": "/fmw-kubernetes/wcportal-domains/release-notes/",
	"title": "Release Notes",
	"tags": [],
	"description": "",
	"content": "Recent changes    Date Version Change     May 28, 2022 22.2.2 Only Oracle WebCenter Portal 12.2.1.4 is supported and certified with the WebLogic Kubernetes operator version 3.3.0.   June 30, 2021 21.2.3 Only Oracle WebCenter Portal 12.2.1.4 is supported and certified with the WebLogic Kubernetes operator version 3.1.1.    "
},
{
	"uri": "/fmw-kubernetes/wccontent-domains/installguide/prerequisites/",
	"title": "Requirements and limitations",
	"tags": [],
	"description": "Understand the system requirements and limitations for deploying and running Oracle WebCenter Content with the WebLogic Kubernetes Operator, including the Oracle WebCenter Content cluster sizing recommendations.",
	"content": "This section provides information about the system requirements and limitations for deploying and running Oracle WebCenter Content domains with the WebLogic Kubernetes Operator.\nSystem requirements for Oracle WebCenter Content domains For the current production release 22.1.3:\n Oracle Linux 7 (UL6+) and Red Hat Enterprise Linux 7 (UL3+ only with standalone Kubernetes) are supported. Supported Kubernetes versions are: 1.16.15+, 1.17.13+ and 1.18.10+ (check with kubectl version). Docker 18.09.1ce, 19.03.1 (check with docker version) or CRI-O 1.14.7 (check with crictl version | grep RuntimeVersion). Flannel networking v0.12.0-amd64 or later (check with docker images | grep flannel). Helm 3.4.1 (check with helm version --client --short). Oracle WebLogic Kubernetes Operator 3.3.0 (see WebLogic Kubernetes Operator releases page). Oracle WebCenter Content 12.2.1.4 Docker image downloaded from My Oracle Support (MOS patch 33771196). This image contains the latest bundle patch and one-off patches for Oracle WebCenter Content. You must have the cluster-admin role to install WebLogic Kubernetes Operator. The WebLogic Kubernetes Operator does not need the cluster-admin role at runtime. We do not currently support running Oracle WebCenter Content in non-Linux containers. Additionally, see the Oracle WebCenter Content documentation for other requirements such as database version.  See here for resourse sizing information for Oracle WebCenter Content domains setup on Kubernetes cluster.\nLimitations Compared to running a WebLogic Server domain in Kubernetes using the WebLogic Kubernetes Operator, the following limitations currently exist for Oracle WebCenter Content domains:\n In this release, Oracle WebCenter Content domains are supported using the domain on a persistent volume model only, where the domain home is located in a persistent volume (PV). The \u0026ldquo;domain in image\u0026rdquo; and \u0026ldquo;model in image\u0026rdquo; models are not supported. Also, \u0026ldquo;WebLogic Deploy Tooling (WDT)\u0026rdquo; based deployments are currently not supported. Only configured clusters are supported. Dynamic clusters are not supported for Oracle WebCenter Content domains. Note that you can still use all of the scaling features, but you need to define the maximum size of your cluster at domain creation time. Mixed clusters (configured servers targeted to a dynamic cluster) are not supported. The WebLogic Logging Exporter currently supports WebLogic Server logs only. Other logs will not be sent to Elasticsearch. Note, however, that you can use a sidecar with a log handling tool like Logstash or Fluentd to get logs. The WebLogic Monitoring Exporter currently supports WebLogic MBean trees only. Support for JRF and Oracle WebCenter Content MBeans is not available. Also, a metrics dashboard specific to Oracle WebCenter Content is not available. Instead, use the WebLogic Server dashboard to monitor the Oracle WebCenter Content server metrics in Grafana. Some features such as multicast, multitenancy, production redeployment, and Node Manager (although it is used internally for the liveness probe and to start WebLogic Server instances) are not supported in this release. Features such as Java Messaging Service whole server migration, consensus leasing, and maximum availability architecture (Oracle WebCenter Content setup) are not supported in this release. You can have multiple UCM servers on your domain but you can have only one IBR server. There is a generic limitation with all load-balancers in end-to-end SSL configuration - accessing multiple types of servers (different Managed Servers and/or Administration Server) at the same time, is currently not supported.  For up-to-date information about the features of WebLogic Server that are supported in Kubernetes environments, see My Oracle Support Doc ID 2349228.1.\n"
},
{
	"uri": "/fmw-kubernetes/wcportal-domains/installguide/prerequisites/",
	"title": "Requirements and limitations",
	"tags": [],
	"description": "Understand the system requirements and limitations for deploying and running Oracle WebCenter Portal with the WebLogic Kubernetes operator.",
	"content": "Contents  Introduction System Requirements Limitations  Introduction This document describes the special considerations for deploying and running a WebCenter Portal domain with the WebLogic Kubernetes Operator. Other than those considerations listed here, the WebCenter Portal domain works in the same way as Fusion Middleware Infrastructure and WebLogic Server domains do.\nIn this release, WebCenter Portal domain is based on the domain on a persistent volume model where a WebCenter Portal domain is located in a persistent volume (PV).\nSystem Requirements  Kubernetes 1.18.18+, 1.19.7+, and 1.20.6+ (check with kubectl version). Flannel networking v0.14.0 or later (check with docker images | grep flannel), Calico networking v3.15. Docker 19.03.11+ (check with docker version). Helm 3.4+ (check with helm version). WebLogic Kubernetes operator 3.3.0 (see the operator releases page). Oracle WebCenter Portal 12.2.1.4.0 image. These proxy setups are used for pulling the required binaries and source code from the respective repositories:  export NO_PROXY=\u0026quot;localhost,127.0.0.0/8,$(hostname -i),.your-company.com,/var/run/docker.sock\u0026rdquo; export no_proxy=\u0026quot;localhost,127.0.0.0/8,$(hostname -i),.your-company.com,/var/run/docker.sock\u0026rdquo; export http_proxy=http://www-proxy-your-company.com:80 export https_proxy=http://www-proxy-your-company.com:80 export HTTP_PROXY=http://www-proxy-your-company.com:80 export HTTPS_PROXY=http://www-proxy-your-company.com:80    NOTE: Add your host IP by using hostname -i and nslookup IP addresses to the no_proxy, NO_PROXY list above.\nLimitations Compared to running a WebLogic Server domain in Kubernetes using the operator, the following limitations currently exist for a WebCenter Portal domain:\n Domain in image model is not supported in this version of the operator. Only configured clusters are supported. Dynamic clusters are not supported on WebCenter Portal domains. Note that you can still use all of the scaling features. You just need to define the maximum size of your cluster at the time when you create a domain. At present, WebCenter Portal doesn\u0026rsquo;t run on non-Linux containers. Deploying and running a WebCenter Portal domain is supported only in the operator versions 3.3.0 and later. The WebLogic Logging Exporter currently supports WebLogic Server logs only. Other logs are not sent to Elasticsearch. Note, however, that you can use a sidecar with a log handling tool like Fluentd to get logs. The WebLogic Monitoring Exporter currently supports the WebLogic MBean trees only. Support for JRF MBeans has not been added yet.  "
},
{
	"uri": "/fmw-kubernetes/wccontent-domains/adminguide/configure-load-balancer/traefik/",
	"title": "Traefik",
	"tags": [],
	"description": "Configure the ingress-based Traefik load balancer for Oracle WebCenter Content domains.",
	"content": "This section provides information about how to install and configure the ingress-based Traefik load balancer (version 2.2.1 or later for production deployments) to load balance Oracle WebCenter Content domain clusters. You can configure Traefik for non-SSL, SSL termination and end-to-end SSL access of the application URL.\nFollow these steps to set up Traefik as a load balancer for an Oracle WebCenter Content\tdomain in a Kubernetes cluster:\n  Non-SSL and SSL termination\n Install the Traefik (ingress-based) load balancer Configure Traefik to manage ingresses Create an Ingress for the domain Verify domain application URL access Uninstall the Traefik ingress    End-to-end SSL configuration\n Install the Traefik load balancer for End-to-end SSL Configure Traefik to manage domain Create IngressRouteTCP Verify end-to-end SSL access Uninstall Traefik    Non-SSL and SSL termination Install the Traefik (ingress-based) load balancer   Use Helm to install the Traefik (ingress-based) load balancer. For detailed information, see here. Use the values.yaml file in the sample but set kubernetes.namespaces specifically.\n$ cd ${WORKDIR}/weblogic-kubernetes-operator $ kubectl create namespace traefik $ helm repo add traefik https://containous.github.io/traefik-helm-chart Sample output:\n\u0026#34;traefik\u0026#34; has been added to your repositories   Install Traefik:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator $ helm install traefik traefik/traefik \\  --namespace traefik \\  --values kubernetes/samples/scripts/charts/traefik/values.yaml \\  --set \u0026#34;kubernetes.namespaces={traefik}\u0026#34; \\  --set \u0026#34;service.type=NodePort\u0026#34; --wait    Click here to see the sample output.   NAME: traefik LAST DEPLOYED: Sun Jan 17 23:30:20 2021 NAMESPACE: traefik STATUS: deployed REVISION: 1 TEST SUITE: None    A sample values.yaml for deployment of Traefik 2.2.x:\nimage: name: traefik tag: 2.2.8 pullPolicy: IfNotPresent ingressRoute: dashboard: enabled: true # Additional ingressRoute annotations (e.g. for kubernetes.io/ingress.class) annotations: {} # Additional ingressRoute labels (e.g. for filtering IngressRoute by custom labels) labels: {} providers: kubernetesCRD: enabled: true kubernetesIngress: enabled: true # IP used for Kubernetes Ingress endpoints ports: traefik: port: 9000 expose: true # The exposed port for this service exposedPort: 9000 # The port protocol (TCP/UDP) protocol: TCP web: port: 8000 # hostPort: 8000 expose: true exposedPort: 30305 nodePort: 30305 # The port protocol (TCP/UDP) protocol: TCP # Use nodeport if set. This is useful if you have configured Traefik in a # LoadBalancer # nodePort: 32080 # Port Redirections # Added in 2.2, you can make permanent redirects via entrypoints. # https://docs.traefik.io/routing/entrypoints/#redirection # redirectTo: websecure websecure: port: 8443 # # hostPort: 8443 expose: true exposedPort: 30443 # The port protocol (TCP/UDP) protocol: TCP nodePort: 30443   Verify the Traefik status and find the port number of the SSL and non-SSL services:\n$ kubectl get all -n traefik    Click here to see the sample output.   NAME READY STATUS RESTARTS AGE pod/traefik-f9cf58697-p57nt 1/1 Running 0 22d NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/traefik NodePort 10.96.95.253 \u0026lt;none\u0026gt; 9000:32306/TCP,30305:30305/TCP,30443:30443/TCP 22d NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/traefik 1/1 1 1 22d NAME DESIRED CURRENT READY AGE replicaset.apps/traefik-f9cf58697 1 1 1 22d      Access the Traefik dashboard through the URL http://$(hostname -f):32306, with the HTTP host traefik.example.com:\n$ curl -H \u0026#34;host: $(hostname -f)\u0026#34; http://$(hostname -f):32306/dashboard/  Note: Make sure that you specify a fully qualified node name for $(hostname -f)\n   Configure Traefik to manage ingresses Configure Traefik to manage ingresses created in this namespace, where traefik is the Traefik namespace and wccns is the namespace of the domain:\n$ helm upgrade traefik traefik/traefik --namespace traefik --reuse-values \\  --set \u0026#34;kubernetes.namespaces={traefik,wccns}\u0026#34;    Click here to see the sample output.   Release \u0026#34;traefik\u0026#34; has been upgraded. Happy Helming! NAME: traefik LAST DEPLOYED: Sun Jan 17 23:43:02 2021 NAMESPACE: traefik STATUS: deployed REVISION: 2 TEST SUITE: None    Create an ingress for the domain Create an ingress for the domain in the domain namespace by using the sample Helm chart. Here path-based routing is used for ingress. Sample values for default configuration are shown in the file ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain/values.yaml. By default, type is TRAEFIK , tls is Non-SSL, and domainType is wccinfra. These values can be overridden by passing values through the command line or can be edited in the sample file values.yaml based on the type of configuration (non-SSL or SSL). If needed, you can update the ingress YAML file to define more path rules (in section spec.rules.host.http.paths) based on the domain application URLs that need to be accessed. The template YAML file for the Traefik (ingress-based) load balancer is located at ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain/templates/traefik-ingress.yaml\n  Install ingress-per-domain using Helm for non-SSL configuration:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator $ helm install wcc-traefik-ingress \\  kubernetes/samples/charts/ingress-per-domain \\  --set type=TRAEFIK \\  --namespace wccns \\  --values kubernetes/samples/charts/ingress-per-domain/values.yaml \\  --set \u0026#34;traefik.hostname=$(hostname -f)\u0026#34; --set tls=NONSSL Sample output:\nNAME: wcc-traefik-ingress LAST DEPLOYED: Sun Jan 17 23:49:09 2021 NAMESPACE: wccns STATUS: deployed REVISION: 1 TEST SUITE: None   For secured access (SSL) to the Oracle WebCenter Content application, create a certificate and generate a Kubernetes secret:\n$ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /tmp/tls1.key -out /tmp/tls1.crt -subj \u0026#34;/CN=*\u0026#34; $ kubectl -n wccns create secret tls domain1-tls-cert --key /tmp/tls1.key --cert /tmp/tls1.crt   Create Traefik Middleware custom resource\nIn case of SSL termination, Traefik must pass a custom header WL-Proxy-SSL:true to the WebLogic Server endpoints. Create the Middleware using the following command:\n$ cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: traefik.containo.us/v1alpha1 kind: Middleware metadata: name: wls-proxy-ssl namespace: wccns spec: headers: customRequestHeaders: WL-Proxy-SSL: \u0026#34;true\u0026#34; EOF   Create the Traefik TLSStore custom resource.\nIn case of SSL termination, Traefik should be configured to use the user-defined SSL certificate. If the user-defined SSL certificate is not configured, Traefik will create a default SSL certificate. To configure a user-defined SSL certificate for Traefik, use the TLSStore custom resource. The Kubernetes secret created with the SSL certificate should be referenced in the TLSStore object. Run the following command to create the TLSStore:\n$ cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: traefik.containo.us/v1alpha1 kind: TLSStore metadata: name: default namespace: wccns spec: defaultCertificate: secretName: domain1-tls-cert EOF   Install ingress-per-domain using Helm for SSL configuration.\nThe Kubernetes secret name should be updated in the template file.\nThe template file also contains the following annotations:\ntraefik.ingress.kubernetes.io/router.entrypoints: websecure traefik.ingress.kubernetes.io/router.tls: \u0026#34;true\u0026#34; traefik.ingress.kubernetes.io/router.middlewares: wccns-wls-proxy-ssl@kubernetescrd The entry point for SSL access and the Middleware name should be updated in the annotation. The Middleware name should be in the form \u0026lt;namespace\u0026gt;-\u0026lt;middleware name\u0026gt;@kubernetescrd.\n$ cd ${WORKDIR}/weblogic-kubernetes-operator $ helm install wcc-traefik-ingress \\  kubernetes/samples/charts/ingress-per-domain \\  --set type=TRAEFIK \\  --namespace wccns \\  --values kubernetes/samples/charts/ingress-per-domain/values.yaml \\  --set \u0026#34;traefik.hostname=$(hostname -f)\u0026#34; \\  --set tls=SSL Sample output:\nNAME: wcc-traefik-ingress LAST DEPLOYED: Mon Jul 20 11:44:13 2020 NAMESPACE: wccns STATUS: deployed REVISION: 1 TEST SUITE: None   For non-SSL access to the Oracle WebCenter Content application, get the details of the services by the ingress:\n$ kubectl describe ingress wccinfra-traefik -n wccns     Click here to see all services supported by the above deployed ingress.    Name: wccinfra-traefik Namespace: wccns Address: Default backend: default-http-backend:80 (\u0026lt;error: endpoints \u0026quot;default-http-backend\u0026quot; not found\u0026gt;) Rules: Host Path Backends ---- ---- -------- domain1.org /console wccinfra-adminserver:7001 (10.244.0.201:7001) /em wccinfra-adminserver:7001 (10.244.0.201:7001) /wls-exporter wccinfra-adminserver:7001 (10.244.0.201:7001) /cs wccinfra-cluster-ucm-cluster:16200 (10.244.0.202:16200,10.244.0.207:16200,10.244.0.211:16200) /adfAuthentication wccinfra-cluster-ucm-cluster:16200 (10.244.0.202:16200,10.244.0.207:16200,10.244.0.211:16200) /_ocsh wccinfra-cluster-ucm-cluster:16200 (10.244.0.202:16200,10.244.0.207:16200,10.244.0.211:16200) /_dav wccinfra-cluster-ucm-cluster:16200 (10.244.0.202:16200,10.244.0.207:16200,10.244.0.211:16200) /idcws wccinfra-cluster-ucm-cluster:16200 (10.244.0.202:16200,10.244.0.207:16200,10.244.0.211:16200) /idcnativews wccinfra-cluster-ucm-cluster:16200 (10.244.0.202:16200,10.244.0.207:16200,10.244.0.211:16200) /wsm-pm wccinfra-cluster-ucm-cluster:16200 (10.244.0.202:16200,10.244.0.207:16200,10.244.0.211:16200) /ibr wccinfra-cluster-ibr-cluster:16250 (10.244.0.203:16250) /ibr/adfAuthentication wccinfra-cluster-ibr-cluster:16250 (10.244.0.203:16250) /weblogic/ready wccinfra-cluster-ucm-cluster:16200 (10.244.0.202:16200,10.244.0.207:16200,10.244.0.211:16200) /imaging wccinfra-cluster-ipm-cluster:16000 (10.244.0.206:16000,10.244.0.209:16000,10.244.0.213:16000) /dc-console wccinfra-cluster-capture-cluster:16400 (10.244.0.204:16400,10.244.0.208:16400,10.244.0.212:16400) /dc-client wccinfra-cluster-capture-cluster:16400 (10.244.0.204:16400,10.244.0.208:16400,10.244.0.212:16400) /wcc wccinfra-cluster-wccadf-cluster:16225 (10.244.0.205:16225,10.244.0.210:16225,10.244.0.214:16225) Annotations: kubernetes.io/ingress.class: traefik meta.helm.sh/release-name: wcc-traefik-ingress meta.helm.sh/release-namespace: wccns Events: \u0026lt;none\u0026gt;      For SSL access to the Oracle WebCenter Content application, get the details of the services by the above deployed ingress:\n$ kubectl describe ingress wccinfra-traefik -n wccns     Click here to see all services supported by the above deployed ingress.    Name: wccinfra-traefik Namespace: wccns Address: Default backend: default-http-backend:80 (\u0026lt;error: endpoints \u0026quot;default-http-backend\u0026quot; not found\u0026gt;) Rules: Host Path Backends ---- ---- -------- domain1.org /console wccinfra-adminserver:7001 (10.244.0.201:7001) /em wccinfra-adminserver:7001 (10.244.0.201:7001) /wls-exporter wccinfra-adminserver:7001 (10.244.0.201:7001) /cs wccinfra-cluster-ucm-cluster:16200 (10.244.0.202:16200,10.244.0.207:16200,10.244.0.211:16200) /adfAuthentication wccinfra-cluster-ucm-cluster:16200 (10.244.0.202:16200,10.244.0.207:16200,10.244.0.211:16200) /_ocsh wccinfra-cluster-ucm-cluster:16200 (10.244.0.202:16200,10.244.0.207:16200,10.244.0.211:16200) /_dav wccinfra-cluster-ucm-cluster:16200 (10.244.0.202:16200,10.244.0.207:16200,10.244.0.211:16200) /idcws wccinfra-cluster-ucm-cluster:16200 (10.244.0.202:16200,10.244.0.207:16200,10.244.0.211:16200) /idcnativews wccinfra-cluster-ucm-cluster:16200 (10.244.0.202:16200,10.244.0.207:16200,10.244.0.211:16200) /wsm-pm wccinfra-cluster-ucm-cluster:16200 (10.244.0.202:16200,10.244.0.207:16200,10.244.0.211:16200) /ibr wccinfra-cluster-ibr-cluster:16250 (10.244.0.203:16250) /ibr/adfAuthentication wccinfra-cluster-ibr-cluster:16250 (10.244.0.203:16250) /weblogic/ready wccinfra-cluster-ucm-cluster:16200 (10.244.0.202:16200,10.244.0.207:16200,10.244.0.211:16200) /imaging wccinfra-cluster-ipm-cluster:16000 (10.244.0.206:16000,10.244.0.209:16000,10.244.0.213:16000) /dc-console wccinfra-cluster-capture-cluster:16400 (10.244.0.204:16400,10.244.0.208:16400,10.244.0.212:16400) /dc-client wccinfra-cluster-capture-cluster:16400 (10.244.0.204:16400,10.244.0.208:16400,10.244.0.212:16400) /wcc wccinfra-cluster-wccadf-cluster:16225 (10.244.0.205:16225,10.244.0.210:16225,10.244.0.214:16225) Annotations: kubernetes.io/ingress.class: traefik meta.helm.sh/release-name: wcc-traefik-ingress meta.helm.sh/release-namespace: wccns Events: \u0026lt;none\u0026gt;     To confirm that the load balancer noticed the new ingress and is successfully routing to the domain server pods, you can send a request to the URL for the \u0026ldquo;WebLogic ReadyApp framework\u0026rdquo;, which should return an HTTP 200 status code, as follows: $ curl -v http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER_PORT}/weblogic/ready * About to connect() to abc.com port 30305 (#0) * Trying 100.111.156.246... * Connected to abc.com (100.111.156.246) port 30305 (#0) \u0026gt; GET /weblogic/ready HTTP/1.1 \u0026gt; User-Agent: curl/7.29.0 \u0026gt; Host: domain1.org:30305 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; Content-Length: 0 \u0026lt; Date: Thu, 03 Dec 2020 13:16:19 GMT \u0026lt; Vary: Accept-Encoding \u0026lt; * Connection #0 to host abc.com left intact   Verify domain application URL access For non-SSL configuration After setting up the Traefik (ingress-based) load balancer, verify that the domain application URLs are accessible through the non-SSL load balancer port 30305 for HTTP access. The sample URLs for Oracle WebCenter Content domain of type wcc are:\nhttp://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/weblogic/ready http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/console http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/cs http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/ibr http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/em http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/imaging http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/dc-console http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/wcc\tFor SSL configuration After setting up the Traefik (ingress-based) load balancer, verify that the domain applications are accessible through the SSL load balancer port 30443 for HTTPS access. The sample URLs for Oracle WebCenter Content domain are:\nhttps://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/weblogic/ready https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/console https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/cs https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/ibr https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/em https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/imaging https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/dc-console https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/wcc Uninstall the Traefik ingress Uninstall and delete the ingress deployment:\n$ helm delete wcc-traefik-ingress -n wccns End-to-end SSL configuration Install the Traefik load balancer for end-to-end SSL   Use Helm to install the Traefik (ingress-based) load balancer. For detailed information, see here. Use the values.yaml file in the sample but set kubernetes.namespaces specifically.\n$ cd ${WORKDIR}/weblogic-kubernetes-operator $ kubectl create namespace traefik $ helm repo add traefik https://containous.github.io/traefik-helm-chart Sample output:\n\u0026#34;traefik\u0026#34; has been added to your repositories   Install Traefik:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator $ helm install traefik traefik/traefik \\  --namespace traefik \\  --values kubernetes/samples/scripts/charts/traefik/values.yaml \\  --set \u0026#34;kubernetes.namespaces={traefik}\u0026#34; \\  --wait    Click here to see the sample output.   NAME: traefik LAST DEPLOYED: Sun Jan 17 23:30:20 2021 NAMESPACE: traefik STATUS: deployed REVISION: 1 TEST SUITE: None      Verify the Traefik operator status and find the port number of the SSL and non-SSL services:\n$ kubectl get all -n traefik    Click here to see the sample output.   NAME READY STATUS RESTARTS AGE pod/traefik-operator-676fc64d9c-skppn 1/1 Running 0 78d NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/traefik-operator NodePort 10.109.223.59 \u0026lt;none\u0026gt; 443:30443/TCP,80:30305/TCP 78d service/traefik-operator-dashboard ClusterIP 10.110.85.194 \u0026lt;none\u0026gt; 80/TCP 78d NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/traefik-operator 1/1 1 1 78d NAME DESIRED CURRENT READY AGE replicaset.apps/traefik-operator-676fc64d9c 1 1 1 78d replicaset.apps/traefik-operator-cb78c9dc9 0 0 0 78d      Access the Traefik dashboard through the URL http://$(hostname -f):32306, with the HTTP host traefik.example.com:\n$ curl -H \u0026#34;host: $(hostname -f)\u0026#34; http://$(hostname -f):32306/dashboard/  Note: Make sure that you specify a fully qualified node name for $(hostname -f).\n   Configure Traefik to manage the domain Configure Traefik to manage the domain application service created in this namespace, where traefik is the Traefik namespace and wccns is the namespace of the domain:\n$ helm upgrade traefik traefik/traefik --namespace traefik --reuse-values \\  --set \u0026#34;kubernetes.namespaces={traefik,wccns}\u0026#34;    Click here to see the sample output.   Release \u0026#34;traefik\u0026#34; has been upgraded. Happy Helming! NAME: traefik LAST DEPLOYED: Sun Jan 17 23:43:02 2021 NAMESPACE: traefik STATUS: deployed REVISION: 2 TEST SUITE: None    Create IngressRouteTCP   To enable SSL passthrough in Traefik, you can configure a TCP router. A sample YAML for IngressRouteTCP is available at ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain/tls/traefik-tls.yaml. The following should be updated in traefik-tls.yaml:\n The service name and the SSL port should be updated in the Services. The load balancer hostname should be updated in the HostSNI rule.  Sample traefik-tls.yaml:\n  apiVersion: traefik.containo.us/v1alpha1 kind: IngressRouteTCP metadata: name: wcc-ucm-routetcp namespace: wccns spec: entryPoints: - websecure routes: - match: HostSNI(`${LOADBALANCER_HOSTNAME}`) services: - name: wccinfra-cluster-ucm-cluster port: 16201 weight: 3 TerminationDelay: 400 tls: passthrough: true  Create the IngressRouteTCP:  $ kubectl apply -f traefik-tls.yaml Verify end-to-end SSL access Verify the access to application URLs exposed through the configured service. You should be able to access the following Oracle WebCenter Content domain URLs:\nLOADBALANCER-SSLPORT is 30443\nhttps://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/console https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/cs https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/ibr https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/imaging https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/dc-console https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/wcc Uninstall Traefik $ helm delete traefik -n wccns "
},
{
	"uri": "/fmw-kubernetes/wccontent-domains/oracle-cloud/configure-load-balancer/traefik/",
	"title": "Traefik",
	"tags": [],
	"description": "Configure the ingress-based Traefik load balancer for Oracle WebCenter Content domains.",
	"content": "This section provides information about how to install and configure the ingress-based Traefik load balancer (version 2.2.8 or later for production deployments) to load balance Oracle WebCenter Content domain clusters.\nFollow these steps to set up Traefik as a load balancer for an Oracle WebCenter Content\tdomain in a Kubernetes cluster:\nContents  Install the Traefik (ingress-based) load balancer Configure Traefik to manage ingresses Create an Ingress for the domain Verify domain application URL access Uninstall the Traefik ingress  Install the Traefik (ingress-based) load balancer   Use Helm to install the Traefik (ingress-based) load balancer. For detailed information, see here. Use the values.yaml file in the sample but set kubernetes.namespaces specifically.\n$ cd ${WORKDIR}/weblogic-kubernetes-operator $ kubectl create namespace traefik $ helm repo add traefik https://containous.github.io/traefik-helm-chart Sample output:\n\u0026#34;traefik\u0026#34; has been added to your repositories   Install Traefik:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator $ helm install traefik traefik/traefik \\  --namespace traefik \\  --values kubernetes/samples/scripts/charts/traefik/values.yaml \\  --set \u0026#34;kubernetes.namespaces={traefik}\u0026#34; \\  --set \u0026#34;service.type=LoadBalancer\u0026#34; --wait    Click here to see the sample output.   NAME: traefik-operator LAST DEPLOYED: Mon Jun 1 19:31:20 2020 NAMESPACE: traefik STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: 1. Get Traefik load balancer IP or hostname: NOTE: It may take a few minutes for this to become available. You can watch the status by running: $ kubectl get svc traefik-operator --namespace traefik -w Once \u0026#39;EXTERNAL-IP\u0026#39; is no longer \u0026#39;\u0026lt;pending\u0026gt;\u0026#39;: $ kubectl describe svc traefik-operator --namespace traefik | grep Ingress | awk \u0026#39;{print $3}\u0026#39; 2. Configure DNS records corresponding to Kubernetes ingress resources to point to the load balancer IP or hostname found in step 1    A sample values.yaml for deployment of Traefik 2.2.x:   Click here to see values.yaml   image: name: traefik tag: 2.2.8 pullPolicy: IfNotPresent ingressRoute: dashboard: enabled: true # Additional ingressRoute annotations (e.g. for kubernetes.io/ingress.class) annotations: {} # Additional ingressRoute labels (e.g. for filtering IngressRoute by custom labels) labels: {} providers: kubernetesCRD: enabled: true kubernetesIngress: enabled: true # IP used for Kubernetes Ingress endpoints ports: traefik: port: 9000 expose: true # The exposed port for this service exposedPort: 9000 # The port protocol (TCP/UDP) protocol: TCP web: port: 8000 # hostPort: 8000 expose: true exposedPort: 30305 nodePort: 30305 # The port protocol (TCP/UDP) protocol: TCP # Use nodeport if set. This is useful if you have configured Traefik in a # LoadBalancer # nodePort: 32080 # Port Redirections # Added in 2.2, you can make permanent redirects via entrypoints. # https://docs.traefik.io/routing/entrypoints/#redirection # redirectTo: websecure websecure: port: 8443 # # hostPort: 8443 expose: true exposedPort: 30443 # The port protocol (TCP/UDP) protocol: TCP nodePort: 30443   \n  Verify the Traefik (load balancer) services:\n  Please note the EXTERNAL-IP of the traefik-operator service. This is the public IP address of the load balancer that you will use to access the WebLogic Server Administration Console and WebCenter Content URLs.\n$ kubectl get service -n traefik NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE traefik LoadBalancer 10.96.8.30 123.456.xx.xx 9000:30734/TCP,30305:30305/TCP,30443:30443/TCP 6d23h To print only the Traefik EXTERNAL-IP, execute this command:\n$ TRAEFIK_PUBLIC_IP=`kubectl describe svc traefik --namespace traefik | grep Ingress | awk \u0026#39;{print $3}\u0026#39;` $ echo $TRAEFIK_PUBLIC_IP 123.456.xx.xx   Verify the helm charts:\n$ helm list -n traefik NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION traefik traefik 2 2021-10-11 12:22:41.122310912 +0000 UTC deployed traefik-9.1.1 2.2.8   Verify the Traefik status and find the port number\n$ kubectl get all -n traefik    Click here to see the sample output.   NAME READY STATUS RESTARTS AGE pod/traefik-f9cf58697-xjhpl 1/1 Running 0 7d NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/traefik LoadBalancer 10.96.8.30 123.456.xx.xx 9000:30734/TCP,30305:30305/TCP,30443:30443/TCP 7d NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/traefik 1/1 1 1 7d NAME DESIRED CURRENT READY AGE replicaset.apps/traefik-f9cf58697 1 1 1 7d      Configure Traefik to manage ingresses Configure Traefik to manage ingresses created in this namespace, where traefik is the Traefik namespace and wccns is the namespace of the domain:\n$ helm upgrade traefik traefik/traefik --namespace traefik --reuse-values \\  --set \u0026#34;kubernetes.namespaces={traefik,wccns}\u0026#34;    Click here to see the sample output.   Release \u0026#34;traefik\u0026#34; has been upgraded. Happy Helming! NAME: traefik LAST DEPLOYED: Sun Jan 17 23:43:02 2021 NAMESPACE: traefik STATUS: deployed REVISION: 2 TEST SUITE: None    Create an ingress for the domain Create an ingress for the domain in the domain namespace by using the sample Helm chart. Here path-based routing is used for ingress. Sample values for default configuration are shown in the file ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain/values.yaml. By default, type is TRAEFIK , tls is Non-SSL, and domainType is wccinfra. These values can be overridden by passing values through the command line or can be edited in the sample file values.yaml based on the type of configuration (non-SSL or SSL). If needed, you can update the ingress YAML file to define more path rules (in section spec.rules.host.http.paths) based on the domain application URLs that need to be accessed. The template YAML file for the Traefik (ingress-based) load balancer is located at ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain/templates/traefik-ingress.yaml\n  Install ingress-per-domain using Helm for non-SSL configuration:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator $ helm install wcc-traefik-ingress \\  kubernetes/samples/charts/ingress-per-domain \\  --set type=TRAEFIK \\  --namespace wccns \\  --values kubernetes/samples/charts/ingress-per-domain/values.yaml \\  --set \u0026#34;traefik.hostname=\u0026#34; \\  --set tls=NONSSL Sample output:\nNAME: wcc-traefik-ingress LAST DEPLOYED: Sun Jan 17 23:49:09 2021 NAMESPACE: wccns STATUS: deployed REVISION: 1 TEST SUITE: None   Verify domain application URL access After setting up the Traefik (ingress-based) load balancer, verify that the domain application URLs are accessible through the load balancer port 30305 for HTTP access. The sample URLs for Oracle WebCenter Content domain of type wcc are:\nhttp://${TRAEFIK_PUBLIC_IP}:30305/weblogic/ready http://${TRAEFIK_PUBLIC_IP}:30305/console http://${TRAEFIK_PUBLIC_IP}:30305/cs http://${TRAEFIK_PUBLIC_IP}:30305/ibr\tUninstall the Traefik ingress Uninstall and delete the ingress deployment:\n$ helm delete wcc-traefik-ingress -n wccns "
},
{
	"uri": "/fmw-kubernetes/wccontent-domains/installguide/",
	"title": "Install Guide",
	"tags": [],
	"description": "",
	"content": "Install the WebLogic Kubernetes Operator to prepare and deploy Oracle WebCenter Content domain.\n Requirements and limitations  Understand the system requirements and limitations for deploying and running Oracle WebCenter Content with the WebLogic Kubernetes Operator, including the Oracle WebCenter Content cluster sizing recommendations.\n Prepare your environment  Prepare for creating Oracle WebCenter Content domain, including required secrets creation, persistent volume and volume claim creation, database creation, and database schema creation.\n Create Oracle WebCenter Content domain  Create Oracle WebCenter Content domain home on an existing PV or PVC and create the domain resource YAML file for deploying the generated Oracle WebCenter Content domain.\n Launch Oracle Webcenter Content Native Applications in Containers  How to launch Oracle WebCenter Content native binaries from inside containerized environment.\n "
},
{
	"uri": "/fmw-kubernetes/wccontent-domains/oracle-cloud/prepare-environment-wcc-domain/",
	"title": "Prepare environment for WCC domain",
	"tags": [],
	"description": "Prepare environment for WCC domain on Oracle Kubernetes Engine (OKE).",
	"content": "To create your Oracle WebCenter Content domain in Kubernetes OKE environment, complete the following steps:\nContents   Set up the code repository to deploy Oracle WebCenter Content domain\n  Create a namespace for the Oracle WebCenter Content domain\n  Create the imagePullSecrets\n  Install the WebLogic Kubernetes Operator\n  Prepare the environment for Oracle WebCenter Content domain\na. Upgrade the WebLogic Kubernetes Operator with the Oracle WebCenter Content domain-namespace\nb. Create a persistent storage for the Oracle WebCenter Content domain\nc. Create a Kubernetes secret with domain credentials\nd. Create a Kubernetes secret with the RCU credentials\ne. Install and start the Database\nf. Configure access to your database\ng. Run the Repository Creation Utility to set up your database schemas\n  Create Oracle WebCenter Content domain\n  Set up the code repository to deploy Oracle WebCenter Content domain Oracle WebCenter Content domain deployment on Kubernetes leverages the WebLogic Kubernetes Operator infrastructure. To deploy an Oracle WebCenter Content domain, you must set up the deployment scripts.\n  Create a working directory to set up the source code:\n$ export WORKDIR=$HOME/wcc_3.3.0 $ mkdir ${WORKDIR}   Download the supported version of the WebLogic Kubernetes Operator source code from the WebLogic Kubernetes Operator github project. Currently the supported WebLogic Kubernetes Operator version is 3.3.0:\n$ cd ${WORKDIR} $ git clone https://github.com/oracle/weblogic-kubernetes-operator.git --branch v3.3.0   Download the Oracle WebCenter Content Kubernetes deployment scripts from the WCC repository and copy them to the WebLogic Kubernetes Operator samples location:\n$ git clone https://github.com/oracle/fmw-kubernetes.git $ cp -rf ${WORKDIR}/fmw-kubernetes/OracleWebCenterContent/kubernetes/create-wcc-domain ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/\t$ cp -rf ${WORKDIR}/fmw-kubernetes/OracleWebCenterContent/kubernetes/ingress-per-domain ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/charts/ $ cp -rf ${WORKDIR}/fmw-kubernetes/OracleWebCenterContent/kubernetes/charts ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/ $ cp -rf ${WORKDIR}/fmw-kubernetes/OracleWebCenterContent/kubernetes/imagetool-scripts ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/ ``\n  Create a namespace for the Oracle WebCenter Content domain Create a Kubernetes namespace (for example, wccns) for the domain unless you intend to use the default namespace. Use the new namespace in the remaining steps in this section. For details, see Prepare to run a domain.\n $ kubectl create namespace wccns Create the imagePullSecrets Create the imagePullSecrets (in wccns namespace) so that Kubernetes Deployment can pull the image automatically from OCIR.\n Note: Create the imagePullSecret as per your environement using a sample command like this -\n $ kubectl create secret docker-registry image-secret -n wccns --docker-server=phx.ocir.io --docker-username=axxxxxxxxxxx/oracleidentitycloudservice/\u0026lt;your_user_name\u0026gt; --docker-password='vUv+xxxxxxxxxxx\u0026lt;KN7z' --docker-email=me@oracle.com The parameter values are:\nOCI Region is phoenix phx.ocir.io OCI Tenancy Name axxxxxxxxxxx ImagePullSecret Name image-secret Username and email address me@oracle.com Auth Token Password vUv+xxxxxxxxxxx\u0026lt;KN7z\nInstall the WebLogic Kubernetes Operator The WebLogic Kubernetes Operator supports the deployment of Oracle WebCenter Content domain in the Kubernetes environment.\nIn the following example commands to install the WebLogic Kubernetes Operator, opns is the namespace and op-sa is the service account created for the WebLogic Kubernetes Operator:\nCreating namespace and service account for WebLogic Kubernetes Operator $ kubectl create namespace opns $ kubectl create serviceaccount -n opns op-sa Install WebLogic Kubernetes Operator $ cd ${WORKDIR}/weblogic-kubernetes-operator $ helm install weblogic-kubernetes-operator kubernetes/charts/weblogic-operator --namespace opns --set image=phx.ocir.io/xxxxxxxxxxx/oracle/weblogic-kubernetes-operator:3.3.0 --set imagePullSecret=image-secret --set serviceAccount=op-sa --set \u0026quot;domainNamespaces={}\u0026quot; --set \u0026quot;javaLoggingLevel=FINE\u0026quot; --wait Verify the WebLogic Kubernetes Operator pod $ kubectl get pods -n opns NAME READY STATUS RESTARTS AGE weblogic-operator-779965b66c-d8265 1/1 Running 0 11d # Verify the Operator helm Charts $ helm list -n opns NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION weblogic-kubernetes-operator opns 3 2022-02-24 06:50:29.810106777 +0000 UTC deployed weblogic-operator-3.3.0 3.3.0 Prepare the environment for Oracle WebCenter Content domain Upgrade the WebLogic Kubernetes Operator with the Oracle WebCenter Content domain-namespace  $ cd ${WORKDIR}/weblogic-kubernetes-operator $ helm upgrade --reuse-values --namespace opns --set \u0026quot;domainNamespaces={wccns}\u0026quot; --wait weblogic-kubernetes-operator kubernetes/charts/weblogic-operator Create a persistent storage for the Oracle WebCenter Content domain In the Kubernetes namespace you created, create the PV and PVC for the domain by running the create-pv-pvc.sh script. Follow the instructions for using the script to create a dedicated PV and PVC for the Oracle WebCenter Content domain.\nHere we will use the NFS Server and mount path, created on this page.\n  Review the configuration parameters for PV creation here. Based on your requirements, update the values in the create-pv-pvc-inputs.yaml file located at ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-weblogic-domain-pv-pvc/. Sample configuration parameter values for the Oracle WebCenter Content domain are:\n baseName: domain domainUID: wccinfra namespace: wccns weblogicDomainStorageType:: NFS weblogicDomainStorageNFSServer:: \u0026lt;your_nfs_server_ip\u0026gt; weblogicDomainStoragePath: /\u0026lt;your_dir_name\u0026gt;   Note: Make sure to update the \u0026ldquo;weblogicDomainStorageNFSServer:\u0026rdquo; with the NFS Server IP as per your Environment\n   Ensure that the path for the weblogicDomainStoragePath property exists (if not, please refer subsection 4 of this document to create it first) and has correct access permissions, and that the folder is empty.\n  Run the create-pv-pvc.sh script:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-weblogic-domain-pv-pvc $ rm -rf output/ $ ./create-pv-pvc.sh -i create-pv-pvc-inputs.yaml -o output   The create-pv-pvc.sh script will create a subdirectory pv-pvcs under the given /path/to/output-directory directory and creates two YAML configuration files for PV and PVC. Apply these two YAML files to create the PV and PVC Kubernetes resources using the kubectl create -f command:\n$ kubectl create -f output/pv-pvcs/wccinfra-domain-pv.yaml -n wccns $ kubectl create -f output/pv-pvcs/wccinfra-domain-pvc.yaml -n wccns   Get the details of PV and PVC:\n$ kubectl describe pv wccinfra-domain-pv $ kubectl describe pvc wccinfra-domain-pvc -n wccns   Create a Kubernetes secret with domain credentials Create the Kubernetes secrets username and password of the administrative account in the same Kubernetes namespace as the domain:\n $ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-weblogic-domain-credentials $ ./create-weblogic-credentials.sh -u weblogic -p welcome1 -n wccns -d wccinfra -s wccinfra-domain-credentials For more details, see this document.\nYou can check the secret with the kubectl get secret command.\nFor example:\n  Click here to see the sample secret description.   $ kubectl get secret wccinfra-domain-credentials -o yaml -n wccns apiVersion: v1 data: password: d2VsY29tZTE= username: d2VibG9naWM= kind: Secret metadata: creationTimestamp: \u0026quot;2021-07-30T06:04:33Z\u0026quot; labels: weblogic.domainName: wccinfra weblogic.domainUID: wccinfra managedFields: - apiVersion: v1 fieldsType: FieldsV1 fieldsV1: f:data: .: {} f:password: {} f:username: {} f:metadata: f:labels: .: {} f:weblogic.domainName: {} f:weblogic.domainUID: {} f:type: {} manager: kubectl operation: Update time: \u0026quot;2021-07-30T06:04:36Z\u0026quot; name: wccinfra-domain-credentials namespace: wccns resourceVersion: \u0026quot;90770768\u0026quot; selfLink: /api/v1/namespaces/wccns/secrets/wccinfra-domain-credentials uid: 9c5dab09-15f3-4e1f-a40d-457904ddf96b type: Opaque    Create a Kubernetes secret with the RCU credentials You also need to create a Kubernetes secret containing the credentials for the database schemas. When you create your domain, it will obtain the RCU credentials from this secret.\nUse the provided sample script to create the secret:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-rcu-credentials $ ./create-rcu-credentials.sh -u weblogic -p welcome1 -a sys -q welcome1 -d wccinfra -n wccns -s wccinfra-rcu-credentials The parameter values are:\n-u username for schema owner (regular user), required.\n-p password for schema owner (regular user), required.\n-a username for SYSDBA user, required.\n-q password for SYSDBA user, required.\n-d domainUID. Example: wccinfra\n-n namespace. Example: wccns\n-s secretName. Example: wccinfra-rcu-credentials\nYou can confirm the secret was created as expected with the kubectl get secret command.\nFor example:\n  Click here to see the sample secret description.   $ kubectl get secret wccinfra-rcu-credentials -o yaml -n wccns apiVersion: v1 data: password: d2VsY29tZTE= sys_password: d2VsY29tZTE= sys_username: c3lz username: d2VibG9naWM= kind: Secret metadata: creationTimestamp: \u0026#34;2020-09-16T08:23:04Z\u0026#34; labels: weblogic.domainName: wccinfra weblogic.domainUID: wccinfra managedFields: - apiVersion: v1 fieldsType: FieldsV1 fieldsV1: f:data: .: {} f:password: {} f:sys_password: {} f:sys_username: {} f:username: {} f:metadata: f:labels: .: {} f:weblogic.domainName: {} f:weblogic.domainUID: {} f:type: {} manager: kubectl operation: Update time: \u0026#34;2020-09-16T08:23:04Z\u0026#34; name: wccinfra-rcu-credentials namespace: wccns resourceVersion: \u0026#34;3277132\u0026#34; selfLink: /api/v1/namespaces/wccns/secrets/wccinfra-rcu-credentials uid: b75f4e13-84e6-40f5-84ba-0213d85bdf30 type: Opaque    Install and start the Database This step is required only when standalone database was not already setup and the user wanted to use the database in a container. The Oracle Database Docker images are supported only for non-production use. For more details, see My Oracle Support note: Oracle Support for Database Running on Docker (Doc ID 2216342.1). For production usecase it is suggested to use a standalone db. Sample provides steps to create the database in a container.\nThe database in a container can be created with a PV attached for persisting the data or without attaching the PV. In this setup we will be creating database in a container without PV attached.\n$ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-oracle-db-service $ ./start-db-service.sh -i phx.ocir.io/xxxxxxxxxxxx/oracle/database/enterprise:x.x.x.x -s image-secret -n wccns    Click here to see the Sample Output   $ ./start-db-service.sh -i phx.ocir.io/xxxxxxxxxxxx/oracle/database/enterprise:x.x.x.x -s image-secret -n wccns Checking Status for NameSpace [wccns] Skipping the NameSpace[wccns] Creation ... NodePort[30011] ImagePullSecret[docker-store] Image[phx.ocir.io/xxxxxxxxxxxx/oracle/database/enterprise:x.x.x.x] NameSpace[wccns] service/oracle-db created deployment.apps/oracle-db created [oracle-db-8598b475c5-cx5nk] already initialized .. Checking Pod READY column for State [1/1] NAME READY STATUS RESTARTS AGE oracle-db-8598b475c5-cx5nk 1/1 Running 0 20s Service [oracle-db] found NAME READY STATUS RESTARTS AGE oracle-db-8598b475c5-cx5nk 1/1 Running 0 25s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE oracle-db LoadBalancer 10.96.74.187 \u0026lt;pending\u0026gt; 1521:30011/TCP 28s [1/30] Retrying for Oracle Database Availability... [2/30] Retrying for Oracle Database Availability... [3/30] Retrying for Oracle Database Availability... [4/30] Retrying for Oracle Database Availability... [5/30] Retrying for Oracle Database Availability... [6/30] Retrying for Oracle Database Availability... [7/30] Retrying for Oracle Database Availability... [8/30] Retrying for Oracle Database Availability... [9/30] Retrying for Oracle Database Availability... [10/30] Retrying for Oracle Database Availability... [11/30] Retrying for Oracle Database Availability... [12/30] Retrying for Oracle Database Availability... [13/30] Retrying for Oracle Database Availability... Done ! The database is ready for use . Oracle DB Service is RUNNING with NodePort [30011] Oracle DB Service URL [oracle-db.wccns.svc.cluster.local:1521/devpdb.k8s]    Once database is created successfully, you can use the database connection string, as an rcuDatabaseURL parameter in the create-domain-inputs.yaml file.\nConfigure access to your database Run a container to create rcu pod\nkubectl run rcu --generator=run-pod/v1 \\  --image phx.ocir.io/xxxxxxxxxxx/oracle/wccontent:x.x.x.x \\  --namespace wccns \\  --overrides=\u0026#39;{ \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;spec\u0026#34;: { \u0026#34;imagePullSecrets\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;image-secret\u0026#34;}] } }\u0026#39; \\  -- sleep infinity # Check the status of rcu pod kubectl get pods -n wccns Run the Repository Creation Utility to set up your database schemas Create or Drop schemas To create the database schemas for Oracle WebCenter Content, run the create-rcu-schema.sh script.\nFor example:\n# Make sure rcu pod status is running before executing this kubectl exec -n wccns -ti rcu /bin/bash # DB details export CONNECTION_STRING=your_db_host:1521/your_db_service export RCUPREFIX=your_schema_prefix echo -e welcome1\u0026#34;\\n\u0026#34;welcome1\u0026gt; /tmp/pwd.txt # Create schemas /u01/oracle/oracle_common/bin/rcu -silent -createRepository -databaseType ORACLE -connectString $CONNECTION_STRING -dbUser sys -dbRole sysdba -useSamePasswordForAllSchemaUsers true -selectDependentsForComponents true -schemaPrefix $RCUPREFIX -component CONTENT -component MDS -component STB -component OPSS -component IAU -component IAU_APPEND -component IAU_VIEWER -component WLS -tablespace USERS -tempTablespace TEMP -f \u0026lt; /tmp/pwd.txt # Drop schemas /u01/oracle/oracle_common/bin/rcu -silent -dropRepository -databaseType ORACLE -connectString $CONNECTION_STRING -dbUser sys -dbRole sysdba -selectDependentsForComponents true -schemaPrefix $RCUPREFIX -component CONTENT -component MDS -component STB -component OPSS -component IAU -component IAU_APPEND -component IAU_VIEWER -component WLS -f \u0026lt; /tmp/pwd.txt # Exit from the container exit Create Oracle WebCenter Content domain Now that you have your Docker images and you have created your RCU schemas, you are ready to create your domain. To continue, follow the Step-3 and Step-4.\n"
},
{
	"uri": "/fmw-kubernetes/wccontent-domains/installguide/prepare-your-environment/",
	"title": "Prepare your environment",
	"tags": [],
	"description": "Prepare for creating Oracle WebCenter Content domain, including required secrets creation, persistent volume and volume claim creation, database creation, and database schema creation.",
	"content": "To prepare your Oracle WebCenter Content in Kubernetes environment, complete the following steps:\n  Set up your Kubernetes cluster\n  Install Helm\n  Pull dependent images\n  Set up the code repository to deploy Oracle WebCenter Content domain\n  Obtain the Oracle WebCenter Content Docker image\n  Install the WebLogic Kubernetes Operator\n  Prepare the environment for Oracle WebCenter Content domain\na. Create a namespace for the Oracle WebCenter Content domain\nb. Create a persistent storage for the Oracle WebCenter Content domain\nc. Create a Kubernetes secret with domain credentials\nd. Create a Kubernetes secret with the RCU credentials\ne. Configure access to your database\nf. Run the Repository Creation Utility to set up your database schemas\n  Create Oracle WebCenter Content domain\n  Set up your Kubernetes cluster If you need help setting up a Kubernetes environment, check the cheat sheet.\nInstall Helm The WebLogic Kubernetes Operator uses Helm to create and deploy the necessary resources and then run it in a Kubernetes cluster. For Helm installation and usage information, see here.\nPull dependent images Obtain dependent images and add them to your local registry. Dependent images include WebLogic Kubernetes Operator, Traefik. Pull these images and add them to your local registry:\n Pull these docker images and re-tag them as shown:  To pull an image from the Oracle Container Registry, in a web browser, navigate to https://container-registry.oracle.com and log in using the Oracle Single Sign-On authentication service. If you do not already have SSO credentials, at the top of the page, click the Sign In link to create them.\nUse the web interface to accept the Oracle Standard Terms and Restrictions for the Oracle software images that you intend to deploy. Your acceptance of these terms are stored in a database that links the software images to your Oracle Single Sign-On login credentials.\nThen, pull these docker images and re-tag them:\ndocker login https://container-registry.oracle.com (enter your Oracle email Id and password) This step is required once at every node to get access to the Oracle Container Registry. WebLogic Kubernetes Operator image:\n$ docker pull container-registry.oracle.com/middleware/weblogic-kubernetes-operator:3.3.0 $ docker tag container-registry.oracle.com/middleware/weblogic-kubernetes-operator:3.3.0 oracle/weblogic-kubernetes-operator:3.3.0 Pull Traefik Image\n$ docker pull traefik:2.2.8 Set up the code repository to deploy Oracle WebCenter Content domain Oracle WebCenter Content domain deployment on Kubernetes leverages the WebLogic Kubernetes Operator infrastructure. To deploy an Oracle WebCenter Content domain, you must set up the deployment scripts.\n  Create a working directory to set up the source code:\n$ export WORKDIR=$HOME/wcc_3.3.0 $ mkdir ${WORKDIR}   Download the supported version of the WebLogic Kubernetes Operator source code from WebLogic Kubernetes Operator github project. Currently the supported WebLogic Kubernetes Operator version is 3.3.0:\n$ git clone https://github.com/oracle/weblogic-kubernetes-operator.git --branch v3.3.0   Download the Oracle WebCenter Content Kubernetes deployment scripts from the WCC repository and copy them to the WebLogic Kubernetes Operator samples location:\n$ git clone https://github.com/oracle/fmw-kubernetes.git $ cp -rf ${WORKDIR}/fmw-kubernetes/OracleWebCenterContent/kubernetes/create-wcc-domain ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/\t$ cp -rf ${WORKDIR}/fmw-kubernetes/OracleWebCenterContent/kubernetes/ingress-per-domain ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/charts/ $ cp -rf ${WORKDIR}/fmw-kubernetes/OracleWebCenterContent/kubernetes/charts ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/ $ cp -rf ${WORKDIR}/fmw-kubernetes/OracleWebCenterContent/kubernetes/imagetool-scripts ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/ ``\n  Obtain the Oracle WebCenter Content Docker image The Oracle WebCenter Content image with latest bundle patch and required interim patches can be obtained from My Oracle Support (MOS). This is the only image supported for production deployments. Follow the below steps to download the Oracle WebCenter Content image from My Oracle Support.\n  Download patch 33771196 from My Oracle Support (MOS).\n  Unzip the downloaded patch zip file.\nFor example:\n$ unzip p32822360_122140_Linux-x86-64.zip # sample output Archive: p32822360_122140_Linux-x86-64.zip inflating: wccontent-12.2.1.4.0-8-ol7-210507.0906.tar inflating: README.html   Load the image archive using the docker load command.\nFor example:\n$ docker load \u0026lt; wccontent-12.2.1.4.0-8-ol7-210507.0906.tar    Click here to see sample output   d0df970fe76a: Loading layer [==================================================\u0026gt;] 138.3MB/138.3MB 3b64a4bdc552: Loading layer [==================================================\u0026gt;] 13.45MB/13.45MB ee5141cc5c13: Loading layer [==================================================\u0026gt;] 20.99kB/20.99kB 51f637dc720f: Loading layer [==================================================\u0026gt;] 334MB/334MB ffc8b247ad07: Loading layer [==================================================\u0026gt;] 3.98GB/3.98GB cd87862f5c14: Loading layer [==================================================\u0026gt;] 4.608kB/4.608kB 12661fb5186c: Loading layer [==================================================\u0026gt;] 137.2kB/137.2kB f84db83c8dfa: Loading layer [==================================================\u0026gt;] 69.12kB/69.12kB Loaded image: oracle/wccontent:12.2.1.4.0-8-ol7-210507.0906      Run the docker inspect command to verify that the downloaded image is the latest released image. The value of label com.oracle.weblogic.imagetool.buildid must match to 29ff0886-a299-4860-9b13-fd6bb80ec354.\nFor example:\n$ docker inspect --format=\u0026#39;{{ index .Config.Labels \u0026#34;com.oracle.weblogic.imagetool.buildid\u0026#34; }}\u0026#39; oracle/wccontent:12.2.1.4.0-8-ol7-210507.0906 29ff0886-a299-4860-9b13-fd6bb80ec354   Alternatively, if you want to build and use Oracle WebCenter Content Container image, using WebLogic Image Tool, with any additional bundle patch or interim patches, then follow these steps to create the image.\n Note: The default Oracle WebCenter Content image name used for Oracle WebCenter Content domain deployment is oracle/wccontent:12.2.1.4.0. The image created must be tagged as oracle/wccontent:12.2.1.4.0 using the docker tag command. If you want to use a different name for the image, make sure to update the new image tag name in the create-domain-inputs.yaml file and also in other instances where the oracle/wccontent:12.2.1.4.0 image name is used.\n Install the WebLogic Kubernetes Operator The WebLogic Kubernetes Operator supports the deployment of Oracle WebCenter Content domain in the Kubernetes environment. Follow the steps in this document to install WebLogic Kubernetes Operator.\n Note: Optionally, you can execute these steps to send the contents of the operator’s logs to Elasticsearch.\n In the following example commands to install the WebLogic Kubernetes Operator, opns is the namespace and op-sa is the service account created for WebLogic Kubernetes Operator:\nCreating namespace and service account for WebLogic Kubernetes Operator $ kubectl create namespace opns $ kubectl create serviceaccount -n opns op-sa Install WebLogic Kubernetes Operator $ cd ${WORKDIR}/weblogic-kubernetes-operator $ helm install weblogic-kubernetes-operator kubernetes/charts/weblogic-operator --namespace opns --set image=oracle/weblogic-kubernetes-operator:3.3.0 --set serviceAccount=op-sa --set \u0026quot;domainNamespaces={}\u0026quot; --set \u0026quot;javaLoggingLevel=FINE\u0026quot; --wait Prepare the environment for Oracle WebCenter Content domain Create a namespace for the Oracle WebCenter Content domain Create a Kubernetes namespace (for example, wccns) for the domain unless you intend to use the default namespace. Use the new namespace in the remaining steps in this section. For details, see Prepare to run a domain.\n $ kubectl create namespace wccns $ cd ${WORKDIR}/weblogic-kubernetes-operator $ helm upgrade --reuse-values --namespace opns --set \u0026quot;domainNamespaces={wccns}\u0026quot; --wait weblogic-kubernetes-operator kubernetes/charts/weblogic-operator Create a persistent storage for the Oracle WebCenter Content domain In the Kubernetes namespace you created, create the PV and PVC for the domain by running the create-pv-pvc.sh script. Follow the instructions for using the script to create a dedicated PV and PVC for the Oracle WebCenter Content domain.\n  Review the configuration parameters for PV creation here. Based on your requirements, update the values in the create-pv-pvc-inputs.yaml file located at ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-weblogic-domain-pv-pvc/. Sample configuration parameter values for the Oracle WebCenter Content domain are:\n baseName: domain domainUID: wccinfra namespace: wccns weblogicDomainStorageType: HOST_PATH weblogicDomainStoragePath: /net/\u0026lt;your_host_name\u0026gt;/scratch/k8s_dir/wcc    Ensure that the path for the weblogicDomainStoragePath property exists (if not, please refer subsection 4 of this document to create it first) and has full access permissions, and that the folder is empty.\n  Run the create-pv-pvc.sh script:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-weblogic-domain-pv-pvc $ rm -rf output/ $ ./create-pv-pvc.sh -i create-pv-pvc-inputs.yaml -o output   The create-pv-pvc.sh script will create a subdirectory pv-pvcs under the given /path/to/output-directory directory and creates two YAML configuration files for PV and PVC. Apply these two YAML files to create the PV and PVC Kubernetes resources using the kubectl create -f command:\n$ kubectl create -f output/pv-pvcs/wccinfra-domain-pv.yaml -n wccns $ kubectl create -f output/pv-pvcs/wccinfra-domain-pvc.yaml -n wccns   Get the details of PV and PVC:\n$ kubectl describe pv wccinfra-domain-pv $ kubectl describe pvc wccinfra-domain-pvc -n wccns   Create a Kubernetes secret with domain credentials Create the Kubernetes secrets username and password of the administrative account in the same Kubernetes namespace as the domain:\n $ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-weblogic-domain-credentials $ ./create-weblogic-credentials.sh -u weblogic -p welcome1 -n wccns -d wccinfra -s wccinfra-domain-credentials For more details, see this document.\nYou can check the secret with the kubectl get secret command.\nFor example:\n  Click here to see the sample secret description.   $ kubectl get secret wccinfra-domain-credentials -o yaml -n wccns apiVersion: v1 data: password: d2VsY29tZTE= username: d2VibG9naWM= kind: Secret metadata: creationTimestamp: \u0026quot;2020-09-16T08:22:50Z\u0026quot; labels: weblogic.domainName: wccinfra weblogic.domainUID: wccinfra managedFields: - apiVersion: v1 fieldsType: FieldsV1 fieldsV1: f:data: .: {} f:password: {} f:username: {} f:metadata: f:labels: .: {} f:weblogic.domainName: {} f:weblogic.domainUID: {} f:type: {} manager: kubectl operation: Update time: \u0026quot;2020-09-16T08:22:50Z\u0026quot; name: wccinfra-domain-credentials namespace: wccns resourceVersion: \u0026quot;3277100\u0026quot; selfLink: /api/v1/namespaces/wccns/secrets/wccinfra-domain-credentials uid: 35a8313f-1ec2-44b0-a2bf-fee381eed57f type: Opaque    Create a Kubernetes secret with the RCU credentials You also need to create a Kubernetes secret containing the credentials for the database schemas. When you create your domain, it will obtain the RCU credentials from this secret.\nUse the provided sample script to create the secret:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-rcu-credentials $ ./create-rcu-credentials.sh -u weblogic -p welcome1 -a sys -q welcome1 -d wccinfra -n wccns -s wccinfra-rcu-credentials The parameter values are:\n-u username for schema owner (regular user), required.\n-p password for schema owner (regular user), required.\n-a username for SYSDBA user, required.\n-q password for SYSDBA user, required.\n-d domainUID. Example: wccinfra\n-n namespace. Example: wccns\n-s secretName. Example: wccinfra-rcu-credentials\nYou can confirm the secret was created as expected with the kubectl get secret command.\nFor example:\n  Click here to see the sample secret description.   $ kubectl get secret wccinfra-rcu-credentials -o yaml -n wccns apiVersion: v1 data: password: d2VsY29tZTE= sys_password: d2VsY29tZTE= sys_username: c3lz username: d2VibG9naWM= kind: Secret metadata: creationTimestamp: \u0026#34;2020-09-16T08:23:04Z\u0026#34; labels: weblogic.domainName: wccinfra weblogic.domainUID: wccinfra managedFields: - apiVersion: v1 fieldsType: FieldsV1 fieldsV1: f:data: .: {} f:password: {} f:sys_password: {} f:sys_username: {} f:username: {} f:metadata: f:labels: .: {} f:weblogic.domainName: {} f:weblogic.domainUID: {} f:type: {} manager: kubectl operation: Update time: \u0026#34;2020-09-16T08:23:04Z\u0026#34; name: wccinfra-rcu-credentials namespace: wccns resourceVersion: \u0026#34;3277132\u0026#34; selfLink: /api/v1/namespaces/wccns/secrets/wccinfra-rcu-credentials uid: b75f4e13-84e6-40f5-84ba-0213d85bdf30 type: Opaque    Configure access to your database Run a container to create rcu pod\nkubectl run rcu --generator=run-pod/v1 --image oracle/wccontent:12.2.1.4 -n wccns -- sleep infinity #check the status of rcu pod kubectl get pods -n wccns Run the Repository Creation Utility to set up your database schemas Create OR Drop schemas To create the database schemas for Oracle WebCenter Content, run the create-rcu-schema.sh script.\nFor example:\n# make sure rcu pod status is running before executing this kubectl exec -n wccns -ti rcu /bin/bash # DB details export CONNECTION_STRING=your_db_host:1521/your_db_service export RCUPREFIX=your_schema_prefix echo -e welcome1\u0026#34;\\n\u0026#34;welcome1\u0026gt; /tmp/pwd.txt # Create schemas /u01/oracle/oracle_common/bin/rcu -silent -createRepository -databaseType ORACLE -connectString $CONNECTION_STRING -dbUser sys -dbRole sysdba -useSamePasswordForAllSchemaUsers true -selectDependentsForComponents true -schemaPrefix $RCUPREFIX -component CONTENT -component MDS -component STB -component OPSS -component IAU -component IAU_APPEND -component IAU_VIEWER -component WLS -tablespace USERS -tempTablespace TEMP -f \u0026lt; /tmp/pwd.txt # Drop schemas /u01/oracle/oracle_common/bin/rcu -silent -dropRepository -databaseType ORACLE -connectString $CONNECTION_STRING -dbUser sys -dbRole sysdba -selectDependentsForComponents true -schemaPrefix $RCUPREFIX -component CONTENT -component MDS -component STB -component OPSS -component IAU -component IAU_APPEND -component IAU_VIEWER -component WLS -f \u0026lt; /tmp/pwd.txt #exit from the container exit Create Oracle WebCenter Content domain Now that you have your Docker images and you have created your RCU schemas, you are ready to create your domain. To continue, follow the instructions in Create Oracle WebCenter Content domains.\n"
},
{
	"uri": "/fmw-kubernetes/wccontent-domains/appendix/quickstart-deployment-guide/",
	"title": "Quick start deployment guide",
	"tags": [],
	"description": "Describes how to quickly get an Oracle WebCenter Content domain instance running (using the defaults, nothing special) for development and test purposes.",
	"content": "Use this Quick Start to create an Oracle WebCenter Content domain deployment in a Kubernetes cluster (on-premise environments) with WebLogic Kubernetes Operator. Note that this walkthrough is for demonstration purposes only, not for use in production. These instructions assume that you are already familiar with Kubernetes. If you need more detailed instructions, refer to the Install Guide.\nHardware requirements Supported Linux kernel for deploying and running Oracle WebCenter Content domain with the WebLogic Kubernetes Operator is Oracle Linux 7 (UL6+) and Red Hat Enterprise Linux 7 (UL3+ only with standalone Kubernetes). Refer to the prerequisites for more details.\nFor this exercise the minimum hardware requirement to create a single node Kubernetes cluster and deploy Oracle WebCenter Content domain with one UCM and IBR Cluster each.\n   Hardware Size     RAM 32GB   Disk Space 250GB+   CPU core(s) 6    See here for resourse sizing information for Oracle WebCenter Content domain setup on Kubernetes cluster.\nSet up Oracle WebCenter Content in an on-premise environment Perform the steps in this topic to create a single instance on-premise Kubernetes cluster and create an Oracle WebCenter Content domain which deploys Oracle WebCenter Content Server and Oracle WebCenter Inbound Refinery Server.\n Step 1 - Prepare a virtual machine for the Kubernetes cluster Step 2 - Set up a single instance Kubernetes cluster Step 3 - Get scripts and images Step 4 - Install the WebLogic Kubernetes Operator Step 5 - Install the Traefik (ingress-based) load balancer Step 6 - Create and configure an Oracle WebCenter Content Domain  1. Prepare a virtual machine for the Kubernetes cluster For illustration purposes, these instructions are for Oracle Linux 7u6+. If you are using a different flavor of Linux, you will need to adjust the steps accordingly.\nThese steps must be run with the root user, unless specified otherwise. Any time you see YOUR_USERID in a command, you should replace it with your actual userid.\n 1.1 Prerequisites   Choose the directories where your Docker and Kubernetes files will be stored. The Docker directory should be on a disk with a lot of free space (more than 100GB) because it will be used for the Docker file system, which contains all of your images and containers. The Kubernetes directory is used for the /var/lib/kubelet file system and persistent volume storage.\n$ export docker_dir=/u01/docker $ export kubelet_dir=/u01/kubelet $ mkdir -p $docker_dir $kubelet_dir $ ln -s $kubelet_dir /var/lib/kubelet   Verify that IPv4 forwarding is enabled on your host.\nNote: Replace eth0 with the ethernet interface name of your compute resource if it is different.\n$ /sbin/sysctl -a 2\u0026gt;\u0026amp;1|grep -s 'net.ipv4.conf.docker0.forwarding' $ /sbin/sysctl -a 2\u0026gt;\u0026amp;1|grep -s 'net.ipv4.conf.eth0.forwarding' $ /sbin/sysctl -a 2\u0026gt;\u0026amp;1|grep -s 'net.ipv4.conf.lo.forwarding' $ /sbin/sysctl -a 2\u0026gt;\u0026amp;1|grep -s 'net.ipv4.ip_nonlocal_bind' For example: Verify that all are set to 1\n$ net.ipv4.conf.docker0.forwarding = 1 $ net.ipv4.conf.eth0.forwarding = 1 $ net.ipv4.conf.lo.forwarding = 1 $ net.ipv4.ip_nonlocal_bind = 1 Solution: Set all values to 1 immediately with the following commands:\n$ /sbin/sysctl net.ipv4.conf.docker0.forwarding=1 $ /sbin/sysctl net.ipv4.conf.eth0.forwarding=1 $ /sbin/sysctl net.ipv4.conf.lo.forwarding=1 $ /sbin/sysctl net.ipv4.ip_nonlocal_bind=1 To preserve the settings post-reboot: Update the above values to 1 in files in /usr/lib/sysctl.d/, /run/sysctl.d/, and /etc/sysctl.d/\n  Verify the iptables rule for forwarding.\nKubernetes uses iptables to handle many networking and port forwarding rules. A standard Docker installation may create a firewall rule that prevents forwarding.\nVerify if the iptables rule to accept forwarding traffic is set:\n$ /sbin/iptables -L -n | awk '/Chain FORWARD / {print $4}' | tr -d \u0026quot;)\u0026quot; If the output is \u0026ldquo;DROP\u0026rdquo;, then run the following command:\n$ /sbin/iptables -P FORWARD ACCEPT Verify if the iptables rule is set properly to \u0026ldquo;ACCEPT\u0026rdquo;:\n$ /sbin/iptables -L -n | awk '/Chain FORWARD / {print $4}' | tr -d \u0026quot;)\u0026quot;   Disable and stop firewalld:\n$ systemctl disable firewalld $ systemctl stop firewalld   1.2 Install and configure Docker  Note : If you have already installed Docker with version 18.03+ and configured Docker daemon root to sufficient disk space along with proxy settings, continue to Install and configure Kubernetes\n   Make sure that you have the right operating system version:\n$ uname -a $ more /etc/oracle-release For example:\nLinux xxxxxxx 4.1.12-124.27.1.el7uek.x86_64 #2 SMP Mon May 13 08:56:17 PDT 2019 x86_64 x86_64 x86_64 GNU/Linux Oracle Linux Server release 7.6   Install the latest docker-engine and start the Docker service:\n$ yum-config-manager --enable ol7_addons $ yum install docker-engine $ systemctl enable docker $ systemctl start docker   Add your userid to the Docker group. This will allow you to run the Docker commands without root access:\n$ /sbin/usermod -a -G docker \u0026lt;YOUR_USERID\u0026gt;   Check your Docker version. It must be at least 18.03.\n$ docker version For example:\nClient: Docker Engine - Community Version: 19.03.1-ol API version: 1.40 Go version: go1.12.5 Git commit: ead9442 Built: Wed Sep 11 06:40:28 2019 OS/Arch: linux/amd64 Experimental: false Server: Docker Engine - Community Engine: Version: 19.03.1-ol API version: 1.40 (minimum version 1.12) Go version: go1.12.5 Git commit: ead9442 Built: Wed Sep 11 06:38:43 2019 OS/Arch: linux/amd64 Experimental: false Default Registry: docker.io containerd: Version: v1.2.0-rc.0-108-gc444666 GitCommit: c4446665cb9c30056f4998ed953e6d4ff22c7c39 runc: Version: 1.0.0-rc5+dev GitCommit: 4bb1fe4ace1a32d3676bb98f5d3b6a4e32bf6c58 docker-init: Version: 0.18.0 GitCommit: fec3683   Update the Docker engine configuration:\n$ mkdir -p /etc/docker $ cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/docker/daemon.json { \u0026quot;group\u0026quot;: \u0026quot;docker\u0026quot;, \u0026quot;data-root\u0026quot;: \u0026quot;/u01/docker\u0026quot; } EOF   Configure proxy settings if you are behind an HTTP proxy. On some hosts /etc/systemd/system/docker.service.d may not be available. Create this directory if it is not available.\n ### Create the drop-in file /etc/systemd/system/docker.service.d/http-proxy.conf that contains proxy details: $ cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/systemd/system/docker.service.d/http-proxy.conf [Service] Environment=\u0026quot;HTTP_PROXY=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT\u0026quot; Environment=\u0026quot;HTTPS_PROXY=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT\u0026quot; Environment=\u0026quot;NO_PROXY=localhost,127.0.0.0/8,ADD-YOUR-INTERNAL-NO-PROXY-LIST,/var/run/docker.sock\u0026quot; EOF   Restart the Docker daemon to load the latest changes:\n$ systemctl daemon-reload $ systemctl restart docker   Verify that the proxy is configured with Docker:\n$ docker info|grep -i proxy For example:\nHTTP Proxy: http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT HTTPS Proxy: http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT No Proxy: localhost,127.0.0.0/8,ADD-YOUR-INTERNAL-NO-PROXY-LIST,/var/run/docker.sock   Verify Docker installation:\n$ docker run hello-world For example:\nHello from Docker! This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the \u0026quot;hello-world\u0026quot; image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal. To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bash Share images, automate workflows, and more with a free Docker ID: https://hub.docker.com/ For more examples and ideas, visit: https://docs.docker.com/get-started/   1.3 Install and configure Kubernetes   Add the external Kubernetes repository:\n$ cat \u0026lt;\u0026lt;EOF | sudo tee /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\\$basearch enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg exclude=kubelet kubeadm kubectl EOF   Set SELinux in permissive mode (effectively disabling it):\n$ export PATH=/sbin:$PATH $ setenforce 0 $ sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config   Export proxy and install kubeadm, kubelet, and kubectl:\n### Get the nslookup IP address of the master node to use with apiserver-advertise-address during setting up Kubernetes master ### as the host may have different internal ip (hostname -i) and nslookup $HOSTNAME $ ip_addr=`nslookup $(hostname -f) | grep -m2 Address | tail -n1| awk -F: '{print $2}'| tr -d \u0026quot; \u0026quot;` $ echo $ip_addr ### Set the proxies $ export NO_PROXY=localhost,127.0.0.0/8,ADD-YOUR-INTERNAL-NO-PROXY-LIST,/var/run/docker.sock,$ip_addr $ export no_proxy=localhost,127.0.0.0/8,ADD-YOUR-INTERNAL-NO-PROXY-LIST,/var/run/docker.sock,$ip_addr $ export http_proxy=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT $ export https_proxy=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT $ export HTTPS_PROXY=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT $ export HTTP_PROXY=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT ### install kubernetes 1.18.4-1 $ VERSION=1.18.4-1 $ yum install -y kubelet-$VERSION kubeadm-$VERSION kubectl-$VERSION --disableexcludes=kubernetes ### enable kubelet service so that it auto-restart on reboot $ systemctl enable --now kubelet   Ensure net.bridge.bridge-nf-call-iptables is set to 1 in your sysctl to avoid traffic routing issues:\n$ cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF $ sysctl --system   Disable swap check:\n$ sed -i 's/KUBELET_EXTRA_ARGS=/KUBELET_EXTRA_ARGS=\u0026quot;--fail-swap-on=false\u0026quot;/' /etc/sysconfig/kubelet $ cat /etc/sysconfig/kubelet ### Reload and restart kubelet $ systemctl daemon-reload $ systemctl restart kubelet   1.4 Set up Helm   Install Helm v3.x.\na. Download Helm from https://github.com/helm/helm/releases. Example to download Helm v3.2.4:\n$ wget https://get.helm.sh/helm-v3.2.4-linux-amd64.tar.gz b. Unpack tar.gz:\n$ tar -zxvf helm-v3.2.4-linux-amd64.tar.gz c. Find the Helm binary in the unpacked directory, and move it to its desired destination:\n$ mv linux-amd64/helm /usr/bin/helm   Run helm version to verify its installation:\n$ helm version version.BuildInfo{Version:\u0026quot;v3.2.4\u0026quot;, GitCommit:\u0026quot;0ad800ef43d3b826f31a5ad8dfbb4fe05d143688\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;, GoVersion:\u0026quot;go1.13.12\u0026quot;}   2. Set up a single instance Kubernetes cluster  Notes:\n These steps must be run with the root user, unless specified otherwise! If you choose to use a different cidr block (that is, other than 10.244.0.0/16 for the --pod-network-cidr= in the kubeadm init command), then also update NO_PROXY and no_proxy with the appropriate value.  Also make sure to update kube-flannel.yaml with the new value before deploying.   Replace the following with appropriate values:  ADD-YOUR-INTERNAL-NO-PROXY-LIST REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT     2.1 Set up the master node   Create a shell script that sets up the necessary environment variables. You can append this to the user’s .bashrc so that it will run at login. You must also configure your proxy settings here if you are behind an HTTP proxy:\n## grab my IP address to pass into kubeadm init, and to add to no_proxy vars ip_addr=`nslookup $(hostname -f) | grep -m2 Address | tail -n1| awk -F: '{print $2}'| tr -d \u0026quot; \u0026quot;` export pod_network_cidr=\u0026quot;10.244.0.0/16\u0026quot; export service_cidr=\u0026quot;10.96.0.0/12\u0026quot; export PATH=$PATH:/sbin:/usr/sbin ### Set the proxies export NO_PROXY=localhost,127.0.0.0/8,ADD-YOUR-INTERNAL-NO-PROXY-LIST,/var/run/docker.sock,$ip_addr,$pod_network_cidr,$service_cidr export no_proxy=localhost,127.0.0.0/8,ADD-YOUR-INTERNAL-NO-PROXY-LIST,/var/run/docker.sock,$ip_addr,$pod_network_cidr,$service_cidr export http_proxy=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT export https_proxy=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT export HTTPS_PROXY=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT export HTTP_PROXY=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT   Source the script to set up your environment variables:\n$ . ~/.bashrc   To implement command completion, add the following to the script:\n$ [ -f /usr/share/bash-completion/bash_completion ] \u0026amp;\u0026amp; . /usr/share/bash-completion/bash_completion $ source \u0026lt;(kubectl completion bash)   Run kubeadm init to create the master node:\n$ kubeadm init \\ --pod-network-cidr=$pod_network_cidr \\ --apiserver-advertise-address=$ip_addr \\ --ignore-preflight-errors=Swap \u0026gt; /tmp/kubeadm-init.out 2\u0026gt;\u0026amp;1   Log in to the terminal with YOUR_USERID:YOUR_GROUP. Then set up the ~/.bashrc similar to steps 1 to 3 with YOUR_USERID:YOUR_GROUP.\n Note that from now on we will be using YOUR_USERID:YOUR_GROUP to execute any kubectl commands and not root.\n   Set up YOUR_USERID:YOUR_GROUP to access the Kubernetes cluster:\n$ mkdir -p $HOME/.kube $ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config $ sudo chown $(id -u):$(id -g) $HOME/.kube/config   Verify that YOUR_USERID:YOUR_GROUP is set up to access the Kubernetes cluster using the kubectl command:\n$ kubectl get nodes  Note: At this step, the node is not in ready state as we have not yet installed the pod network add-on. After the next step, the node will show status as Ready.\n   Install a pod network add-on (flannel) so that your pods can communicate with each other.\n Note: If you are using a different cidr block than 10.244.0.0/16, then download and update kube-flannel.yml with the correct cidr address before deploying into the cluster:\n $ kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.12.0/Documentation/kube-flannel.yml   Verify that the master node is in Ready status:\n$ kubectl get nodes For example:\nNAME STATUS ROLES AGE VERSION mymasternode Ready master 8m26s v1.18.4 or:\n$ kubectl get pods -n kube-system For example:\nNAME READY STATUS RESTARTS AGE pod/coredns-86c58d9df4-58p9f 1/1 Running 0 3m59s pod/coredns-86c58d9df4-mzrr5 1/1 Running 0 3m59s pod/etcd-mymasternode 1/1 Running 0 3m4s pod/kube-apiserver-node 1/1 Running 0 3m21s pod/kube-controller-manager-mymasternode 1/1 Running 0 3m25s pod/kube-flannel-ds-amd64-6npx4 1/1 Running 0 49s pod/kube-proxy-4vsgm 1/1 Running 0 3m59s pod/kube-scheduler-mymasternode 1/1 Running 0 2m58s   To schedule pods on the master node, taint the node:\n$ kubectl taint nodes --all node-role.kubernetes.io/master-   Congratulations! Your Kubernetes cluster environment is ready to deploy your Oracle WebCenter Content domain.\nFor additional references on Kubernetes cluster setup, check the cheat sheet.\n3. Get scripts and images 3.1 Set up the code repository to deploy Oracle WebCenter Content domains Follow these steps to set up the source code repository required to deploy Oracle WebCenter Content domains.\n3.2 Get required Docker images and add them to your local registry Follow these steps to set up the source code repository required to deploy Oracle WebCenter Content domains.\n3.3 Build Oracle WebCenter Content Docker image and add it to your local registry Follow these steps to set up the source code repository required to deploy Oracle WebCenter Content domains.\n Note: For test and development purposes this Oracle WebCenter Content image need not contain any product patches.\n 4. Install WebLogic Kubernetes Operator 4.1 Prepare for WebLogic Kubernetes Operator.   Create a namespace opns for the WebLogic Kubernetes Operator:\n$ kubectl create namespace opns   Create a service account op-sa for WebLogic Kubernetes Operator in the operator’s namespace:\n$ kubectl create serviceaccount -n opns op-sa   4.2 Install the WebLogic Kubernetes Operator Use Helm to install and start WebLogic Kubernetes Operator from the directory you just cloned:\n $ cd ${WORKDIR}/weblogic-kubernetes-operator $ helm install weblogic-kubernetes-operator kubernetes/charts/weblogic-operator \\ --namespace opns \\ --set image=oracle/weblogic-kubernetes-operator:3.3.0 \\ --set serviceAccount=op-sa \\ --set \u0026quot;domainNamespaces={}\u0026quot; \\ --wait 4.3 Verify the WebLogic Kubernetes Operator   Verify that the WebLogic Kubernetes Operator\u0026rsquo;s pod is running by listing the pods in the respective namespace. You should see one for the WebLogic Kubernetes Operator:\n$ kubectl get pods -n opns   Verify that the WebLogic Kubernetes Operator is up and running by viewing the operator-pod\u0026rsquo;s logs:\n$ kubectl logs -n opns -c weblogic-operator deployments/weblogic-operator   The WebLogic Kubernetes Operator v3.3.0 has been installed. Continue with the load balancer and Oracle WebCenter Content domain setup.\n5. Install the Traefik (ingress-based) load balancer WebLogic Kubernetes Operator supports these load balancers: Traefik, NGINX and Apache. Samples are provided in the documentation.\nThis Quick Start demonstrates how to install the Traefik ingress controller to provide load balancing for an Oracle WebCenter Content domain.\n  Create a namespace for Traefik:\n$ kubectl create namespace traefik   Set up Helm for 3rd party services:\n$ helm repo add traefik https://containous.github.io/traefik-helm-chart   Install the Traefik operator in the traefik namespace with the provided sample values:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator $ helm install traefik traefik/traefik \\ --namespace traefik \\ --values kubernetes/samples/scripts/charts/traefik/values.yaml \\ --set \u0026quot;kubernetes.namespaces={traefik}\u0026quot; \\ --set \u0026quot;service.type=NodePort\u0026quot; \\ --wait   6. Create and configure an Oracle WebCenter Content domain 6.1 Prepare for an Oracle WebCenter Content domain   Create a namespace that can host Oracle WebCenter Content domain:\n$ kubectl create namespace wccns   Use Helm to configure the WebLogic Kubernetes Operator to manage Oracle WebCenter Content domains in this namespace:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator $ helm upgrade weblogic-kubernetes-operator kubernetes/charts/weblogic-operator \\ --reuse-values \\ --namespace opns \\ --set \u0026quot;domainNamespaces={wccns}\u0026quot; \\ --wait   Create Kubernetes secrets.\na. Create a Kubernetes secret for the domain in the same Kubernetes namespace as the domain. In this example, the username is weblogic, the password in welcome1, and the namespace is wccns:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-weblogic-domain-credentials $ ./create-weblogic-credentials.sh \\ -u weblogic \\ -p welcome1 \\ -n wccns \\ -d wccinfra \\ -s wccinfra-domain-credentials b. Create a Kubernetes secret for the RCU in the same Kubernetes namespace as the domain:\n Schema user : WCC1 Schema password : Oradoc_db1 DB sys user password : Oradoc_db1 Domain name : wccinfra Domain Namespace : wccns Secret name : wccinfra-rcu-credentials  $ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-rcu-credentials $ ./create-rcu-credentials.sh \\ -u WCC1 \\ -p Oradoc_db1 \\ -a sys \\ -q Oradoc_db1 \\ -d wccinfra \\ -n wccns \\ -s wccinfra-rcu-credentials   Create the Kubernetes persistence volume and persistence volume claim.\na. Create the Oracle WebCenter Content domain home directory. Determine if a user already exists on your host system with uid:gid of 1000:0:\n$ sudo getent passwd 1000 If this command returns a username (which is the first field), you can skip the following useradd command. If not, create the oracle user with useradd:\n$ sudo useradd -u 1000 -g 0 oracle Create the directory that will be used for the Oracle WebCenter Content domain home:\n$ sudo mkdir /scratch/k8s_dir $ sudo chown -R 1000:0 /scratch/k8s_dir b. Update create-pv-pvc-inputs.yaml with the following values:\n baseName: domain domainUID: wccinfra namespace: wccns weblogicDomainStoragePath: /scratch/k8s_dir  $ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-weblogic-domain-pv-pvc $ cp create-pv-pvc-inputs.yaml create-pv-pvc-inputs.yaml.orig $ sed -i -e \u0026quot;s:baseName\\: weblogic-sample:baseName\\: domain:g\u0026quot; create-pv-pvc-inputs.yaml $ sed -i -e \u0026quot;s:domainUID\\::domainUID\\: wccinfra:g\u0026quot; create-pv-pvc-inputs.yaml $ sed -i -e \u0026quot;s:namespace\\: default:namespace\\: wccns:g\u0026quot; create-pv-pvc-inputs.yaml $ sed -i -e \u0026quot;s:#weblogicDomainStoragePath\\: /scratch/k8s_dir:weblogicDomainStoragePath\\: /scratch/k8s_dir:g\u0026quot; create-pv-pvc-inputs.yaml c. Run the create-pv-pvc.sh script to create the PV and PVC configuration files:\n$ ./create-pv-pvc.sh -i create-pv-pvc-inputs.yaml -o output d. Create the PV and PVC using the configuration files created in the previous step:\n$ kubectl create -f output/pv-pvcs/wccinfra-domain-pv.yaml $ kubectl create -f output/pv-pvcs/wccinfra-domain-pvc.yaml   Configure the database and create schemas for the Oracle WebCenter Content domain.\nFollow configure-database-access step and run-RCU step to set up the database connection and configure product schemas required to deploy Oracle WebCenter Content domain.\n  Now the environment is ready to start the Oracle WebCenter Content domain creation.\n6.2 Create an Oracle WebCenter Content domain   The sample scripts for Oracle WebCenter Content domain deployment are available at \u0026lt;weblogic-kubernetes-operator-project\u0026gt;/kubernetes/samples/scripts/create-wcc-domain. You must edit create-domain-inputs.yaml (or a copy of it) to provide the details for your domain.\n  Run the create-domain.sh script to create a domain:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-wcc-domain/domain-home-on-pv/ $ ./create-domain.sh -i create-domain-inputs.yaml -o output   Create a Kubernetes domain object:\nOnce the create-domain.sh is successful, it generates the output/weblogic-domains/wccinfra/domain.yaml that you can use to create the Kubernetes resource domain, which starts the domain and servers:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-wcc-domain/domain-home-on-pv $ kubectl create -f output/weblogic-domains/wccinfra/domain.yaml   Verify that the Kubernetes domain object named wccinfra is created:\n$ kubectl get domain -n wccns NAME AGE wccinfra 3m18s   Once you create the domain, introspect pod is created. This inspects the domain home and then starts the wccinfra-adminserver pod. Once the wccinfra-adminserver pod starts successfully, then the Managed Server pods are started in parallel. Watch the wccns namespace for the status of domain creation:\n$ kubectl get pods -n wccns   Verify that the Oracle WebCenter Content domain server pods and services are created and in Ready state:\n$ kubectl get all -n wccns   6.3 Configure Traefik to access in Oracle WebCenter Content domain services   Configure Traefik to manage ingresses created in the Oracle WebCenter Content domain namespace (wccns):\n$ helm upgrade traefik traefik/traefik \\ --reuse-values \\ --namespace traefik \\ --set \u0026quot;kubernetes.namespaces={traefik,wccns}\u0026quot; \\ --wait   Create an ingress for the domain in the domain namespace by using the sample Helm chart:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator $ helm install wcc-traefik-ingress kubernetes/samples/charts/ingress-per-domain \\ --namespace wccns \\ --values kubernetes/samples/charts/ingress-per-domain/values.yaml \\ --set \u0026quot;traefik.hostname=$(hostname -f)\u0026quot;   Verify the created ingress per domain details:\n$ kubectl describe ingress wccinfra-traefik -n wccns   6.4 Verify that you can access the Oracle WebCenter Content domain URL   Get the LOADBALANCER_HOSTNAME for your environment:\nexport LOADBALANCER_HOSTNAME=$(hostname -f)   The following URLs are available for Oracle WebCenter Content domain:\nCredentials: username: weblogic password: welcome1\nhttp://${LOADBALANCER_HOSTNAME}:30305/console http://${LOADBALANCER_HOSTNAME}:30305/em http://${LOADBALANCER_HOSTNAME}:30305/cs http://${LOADBALANCER_HOSTNAME}:30305/ibr http://${LOADBALANCER_HOSTNAME}:30305/imaging http://${LOADBALANCER_HOSTNAME}:30305/dc-console http://${LOADBALANCER_HOSTNAME}:30305/wcc   "
},
{
	"uri": "/fmw-kubernetes/wccontent-domains/adminguide/configure-load-balancer/nginx/",
	"title": "NGINX",
	"tags": [],
	"description": "Configure the ingress-based NGINX load balancer for Oracle WebCenter Content domain.",
	"content": "This section provides information about how to install and configure the ingress-based NGINX load balancer to load balance Oracle WebCenter Content domain clusters. You can configure NGINX for non-SSL, SSL termination, and end-to-end SSL access of the application URL.\nFollow these steps to set up NGINX as a load balancer for an Oracle WebCenter Content domain in a Kubernetes cluster:\nSee the official installation document for prerequisites.\n  Non-SSL and SSL termination\n Install the NGINX load balancer Configure NGINX to manage ingresses Verify non-SSL and SSL termination access    End-to-end SSL configuration\n Install the NGINX load balancer for End-to-end SSL Deploy tls to access individual Managed Servers Deploy tls to access Administration Server    To get repository information, enter the following Helm commands:\n$ helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx $ helm repo update Non-SSL and SSL termination Install the NGINX load balancer   Deploy the ingress-nginx controller by using Helm on the domain namespace:\nFor Kubernetes versions up to v1.18.x:\n$ helm install nginx-ingress -n wccns \\  --version=3.34.0 \\  --set controller.service.type=NodePort \\  --set controller.admissionWebhooks.enabled=false \\  ingress-nginx/ingress-nginx     Click here to see the sample output.   NAME: nginx-ingress LAST DEPLOYED: Sun Feb 7 23:19:30 2021 NAMESPACE: wccns STATUS: deployed REVISION: 2 TEST SUITE: None NOTES: The ingress-nginx controller has been installed. Get the application URL by running these commands: export HTTP_NODE_PORT=$(kubectl --namespace wccns get services -o jsonpath=\u0026#34;{.spec.ports[0].nodePort}\u0026#34; nginx-ingress-ingress-nginx-controller) export HTTPS_NODE_PORT=$(kubectl --namespace wccns get services -o jsonpath=\u0026#34;{.spec.ports[1].nodePort}\u0026#34; nginx-ingress-ingress-nginx-controller) export NODE_IP=$(kubectl --namespace wccns get nodes -o jsonpath=\u0026#34;{.items[0].status.addresses[1].address}\u0026#34;) echo \u0026#34;Visit http://$NODE_IP:$HTTP_NODE_PORTto access your application via HTTP.\u0026#34; echo \u0026#34;Visit https://$NODE_IP:$HTTPS_NODE_PORTto access your application via HTTPS.\u0026#34; An example Ingress that makes use of the controller: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx name: example namespace: foo spec: rules: - host: www.example.com http: paths: - backend: serviceName: exampleService servicePort: 80 path: / # This section is only required if TLS is to be enabled for the Ingress tls: - hosts: - www.example.com secretName: example-tls If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided: apiVersion: v1 kind: Secret metadata: name: example-tls namespace: foo data: tls.crt: \u0026lt;base64 encoded cert\u0026gt; tls.key: \u0026lt;base64 encoded key\u0026gt; type: kubernetes.io/tls      Check the status of the deployed ingress controller:\n$ kubectl --namespace wccns get services | grep ingress-nginx-controller Sample output:\nnginx-ingress-ingress-nginx-controller NodePort 10.97.189.122 \u0026lt;none\u0026gt; 80:30993/TCP,443:30232/TCP 7d2h   Configure NGINX to manage ingresses  Create an ingress for the domain in the domain namespace by using the sample Helm chart. Here path-based routing is used for ingress. Sample values for default configuration are shown in the file ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain/values.yaml. By default, type is TRAEFIK, tls is Non-SSL, and domainType is wccinfra. These values can be overridden by passing values through the command line or can be edited in the sample file values.yaml. If needed, you can update the ingress YAML file to define more path rules (in section spec.rules.host.http.paths) based on the domain application URLs that need to be accessed. Update the template YAML file for the NGINX load balancer located at ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain/templates/nginx-ingress.yaml  $ cd ${WORKDIR}/weblogic-kubernetes-operator $ helm install wccinfra-nginx-ingress kubernetes/samples/charts/ingress-per-domain \\  --namespace wccns \\  --values kubernetes/samples/charts/ingress-per-domain/values.yaml \\  --set \u0026#34;nginx.hostname=$(hostname -f)\u0026#34; \\  --set type=NGINX \\  --set tls=NONSSL Sample output:\nNAME: wccinfra-nginx-ingress LAST DEPLOYED: Sun Feb 7 23:52:38 2021 NAMESPACE: wccns STATUS: deployed REVISION: 1 TEST SUITE: None   For secured access (SSL) to the Oracle WebCenter Content application, create a certificate and generate a Kubernetes secret:\n$ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /tmp/tls1.key -out /tmp/tls1.crt -subj \u0026#34;/CN=*\u0026#34; $ kubectl -n wccns create secret tls domain1-tls-cert --key /tmp/tls1.key --cert /tmp/tls1.crt   Install ingress-per-domain using Helm for SSL configuration:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator $ helm install wccinfra-nginx-ingress kubernetes/samples/charts/ingress-per-domain \\  --namespace wccns \\  --values kubernetes/samples/charts/ingress-per-domain/values.yaml \\  --set \u0026#34;nginx.hostname=$(hostname -f)\u0026#34; \\  --set type=NGINX --set tls=SSL Sample output:\nNAME: wccinfra-nginx-ingress LAST DEPLOYED: Mon Feb 8 00:01:13 2021 NAMESPACE: wccns STATUS: deployed REVISION: 1 TEST SUITE: None   For non-SSL access or SSL to the Oracle WebCenter Content application, get the details of the services by the ingress:\n$ kubectl describe ingress wccinfra-nginx -n wccns     Click here to see the sample output of the services supported by the above deployed ingress.   Name: wccinfra-nginx Namespace: wccns Address: 10.97.189.122 Default backend: default-http-backend:80 (\u0026lt;error: endpoints \u0026#34;default-http-backend\u0026#34; not found\u0026gt;) TLS: domain1-tls-cert terminates domain1.org Rules: Host Path Backends ---- ---- -------- domain1.org /console wccinfra-adminserver:7001 (10.244.0.58:7001) /em wccinfra-adminserver:7001 (10.244.0.58:7001) /servicebus wccinfra-adminserver:7001 (10.244.0.58:7001) /cs wccinfra-cluster-ucm-cluster:16200 (10.244.0.60:16200,10.244.0.61:16200) /adfAuthentication wccinfra-cluster-ucm-cluster:16200 (10.244.0.60:16200,10.244.0.61:16200) /ibr wccinfra-cluster-ibr-cluster:16250 (10.244.0.59:16250) /ibr/adfAuthentication wccinfra-cluster-ibr-cluster:16250 (10.244.0.59:16250) /weblogic/ready wccinfra-cluster-ucm-cluster:16200 (10.244.0.60:16200,10.244.0.61:16200) /imaging wccinfra-cluster-ipm-cluster:16000 (10.244.0.206:16000,10.244.0.209:16000,10.244.0.213:16000) /dc-console wccinfra-cluster-capture-cluster:16400 (10.244.0.204:16400,10.244.0.208:16400,10.244.0.212:16400) /dc-client wccinfra-cluster-capture-cluster:16400 (10.244.0.204:16400,10.244.0.208:16400,10.244.0.212:16400) /wcc wccinfra-cluster-wccadf-cluster:16225 (10.244.0.205:16225,10.244.0.210:16225,10.244.0.214:16225) Annotations: kubernetes.io/ingress.class: nginx meta.helm.sh/release-name: wccinfra-nginx-ingress meta.helm.sh/release-namespace: wccns nginx.ingress.kubernetes.io/configuration-snippet: more_set_input_headers \u0026#34;X-Forwarded-Proto: https\u0026#34;; more_set_input_headers \u0026#34;WL-Proxy-SSL: true\u0026#34;; nginx.ingress.kubernetes.io/ingress.allow-http: false Events: \u0026lt;none\u0026gt;    Verify non-SSL and SSL termination access Non-SSL configuration Verify that the Oracle WebCenter Content domain application URLs are accessible through the LOADBALANCER-Non-SSLPORT:\nhttp://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/weblogic/ready http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/console http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/em http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/cs http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/ibr http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/imaging http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/dc-console http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/wcc SSL configuration Verify that the Oracle WebCenter Content domain application URLs are accessible through the LOADBALANCER-SSLPORT:\nhttps://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/weblogic/ready https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/console https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/em https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/cs https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/ibr https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/imaging https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/dc-console https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/wcc Uninstall the ingress Uninstall and delete the ingress-nginx deployment:\n$ helm delete wccinfra-nginx -n wccns End-to-end SSL configuration Install the NGINX load balancer for End-to-end SSL   For secured access (SSL) to the Oracle WebCenter Content application, create a certificate and generate secrets:\n$ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /tmp/tls1.key -out /tmp/tls1.crt -subj \u0026#34;/CN=*\u0026#34; $ kubectl -n wccns create secret tls domain1-tls-cert --key /tmp/tls1.key --cert /tmp/tls1.crt   Deploy the ingress-nginx controller by using Helm on the domain namespace:\nFor Kubernetes versions up to v1.18.x:\n$ helm install nginx-ingress -n wccns \\  --version=3.34.0 \\  --set controller.extraArgs.default-ssl-certificate=wccns/domain1-tls-cert \\  --set controller.service.type=NodePort \\  --set controller.admissionWebhooks.enabled=false \\  --set controller.extraArgs.enable-ssl-passthrough=true \\  ingress-nginx/ingress-nginx\t   Click here to see the sample output.   Release \u0026#34;nginx-ingress\u0026#34; has been upgraded. Happy Helming! NAME: nginx-ingress LAST DEPLOYED: Mon Feb 8 02:07:26 2021 NAMESPACE: wccns STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: The ingress-nginx controller has been installed. Get the application URL by running these commands: export HTTP_NODE_PORT=$(kubectl --namespace wccns get services -o jsonpath=\u0026#34;{.spec.ports[0].nodePort}\u0026#34; nginx-ingress-ingress-nginx-controller) export HTTPS_NODE_PORT=$(kubectl --namespace wccns get services -o jsonpath=\u0026#34;{.spec.ports[1].nodePort}\u0026#34; nginx-ingress-ingress-nginx-controller) export NODE_IP=$(kubectl --namespace wccns get nodes -o jsonpath=\u0026#34;{.items[0].status.addresses[1].address}\u0026#34;) echo \u0026#34;Visit http://$NODE_IP:$HTTP_NODE_PORTto access your application via HTTP.\u0026#34; echo \u0026#34;Visit https://$NODE_IP:$HTTPS_NODE_PORTto access your application via HTTPS.\u0026#34; An example Ingress that makes use of the controller: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx name: example namespace: foo spec: rules: - host: www.example.com http: paths: - backend: serviceName: exampleService servicePort: 80 path: / # This section is only required if TLS is to be enabled for the Ingress tls: - hosts: - www.example.com secretName: example-tls If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided: apiVersion: v1 kind: Secret metadata: name: example-tls namespace: foo data: tls.crt: \u0026lt;base64 encoded cert\u0026gt; tls.key: \u0026lt;base64 encoded key\u0026gt; type: kubernetes.io/tls      Check the status of the deployed ingress controller:\n$ kubectl --namespace wccns get services | grep ingress-nginx-controller Sample output:\nnginx-ingress-ingress-nginx-controller NodePort 10.97.189.122 \u0026lt;none\u0026gt; 80:30993/TCP,443:30232/TCP 168m   Deploy tls to access individual Managed Servers   Deploy tls to securely access the services. Only one application can be configured with ssl-passthrough. A sample tls file for NGINX is shown below for the service wccinfra-cluster-ucm-cluster and port 16201. All the applications running on port 16201 can be securely accessed through this ingress. For each backend service, create different ingresses as NGINX does not support multiple path/rules with annotation ssl-passthrough. That is, for wccinfra-cluster-ucm-cluster, wccinfra-cluster-ibr-cluster, wccinfra-cluster-ipm-cluster, wccinfra-cluster-capture-cluster, wccinfra-cluster-wccadf-cluster and wccinfra-adminserver, different ingresses must be created.\n$ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain/tls Sample nginx-ucm-tls.yaml:\n  Click here to see the content of the file nginx-ucm-tls.yaml   apiVersion: extensions/v1beta1 kind: Ingress metadata: name: wcc-ucm-ingress namespace: wccns annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/ssl-passthrough: \u0026#34;true\u0026#34; spec: tls: - hosts: - \u0026#39;domain1.org\u0026#39; secretName: domain1-tls-cert rules: - host: \u0026#39;domain1.org\u0026#39; http: paths: - path: backend: serviceName: wccinfra-cluster-ucm-cluster servicePort: 16201     Note: host is the server on which this ingress is deployed.\n   Deploy the secured ingress:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain/tls $ kubectl create -f nginx-ucm-tls.yaml   Check the services supported by the ingress:\n$ kubectl describe ingress wcc-ucm-ingress -n wccns    Click here check the services supported by the ingress.   Name: wcc-ucm-ingress Namespace: wccns Address: 10.102.97.237 Default backend: default-http-backend:80 (\u0026lt;error: endpoints \u0026#34;default-http-backend\u0026#34; not found\u0026gt;) TLS: domain1-tls-cert terminates domain1.org Rules: Host Path Backends ---- ---- -------- domain1.org wccinfra-cluster-ucm-cluster:16201 (10.244.238.136:16201,10.244.253.132:16201) Annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/ssl-passthrough: true Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Sync 62s (x2 over 106s) nginx-ingress-controller Scheduled for sync      Verify end-to-end SSL access Verify that the Oracle WebCenter Content domain application URLs are accessible through the LOADBALANCER-SSLPORT:\nhttps://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/cs Deploy tls to access Administration Server   As ssl-passthrough in NGINX works on the clusterIP of the backing service instead of individual endpoints, you must expose adminserver service created by the WebLogic Kubernetes Operator with clusterIP.\nFor example:\na. Get the name of Administration Server service:\n$ kubectl get svc -n wccns | grep wccinfra-adminserver Sample output:\nwccinfra-adminserver ClusterIP None \u0026lt;none\u0026gt; 7001/TCP,7002/TCP 7 b. Expose the Administration Server service wccinfra-adminserver and use the new service name wccinfra-adminserver-nginx-ssl:\n$ kubectl expose svc wccinfra-adminserver -n wccns --name=wccinfra-adminserver-nginx-ssl --port=7002 c. Deploy the secured ingress:\nSample nginx-admin-tls.yaml:\n  Click here to see the content of the file nginx-admin-tls.yaml   apiVersion: extensions/v1beta1 kind: Ingress metadata: name: wcc-admin-ingress namespace: wccns annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/ssl-passthrough: \u0026#34;true\u0026#34; spec: tls: - hosts: - \u0026#39;domain1.org\u0026#39; secretName: domain1-tls-cert rules: - host: \u0026#39;domain1.org\u0026#39; http: paths: - path: backend: serviceName: wccinfra-adminserver-nginx-ssl servicePort: 7002     Note: host is the server on which this ingress is deployed.\n $ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain/tls $ kubectl create -f nginx-admin-tls.yaml   Verify end-to-end SSL access Verify that the Oracle WebCenter Content Administration Server URL is accessible through the LOADBALANCER-SSLPORT:\nhttps://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/console Uninstall ingress-nginx tls $ cd weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain/tls $ kubectl delete -f nginx-ucm-tls.yaml "
},
{
	"uri": "/fmw-kubernetes/soa-domains/installguide/",
	"title": "Install Guide",
	"tags": [],
	"description": "",
	"content": "Install the WebLogic Kubernetes Operator and prepare and deploy Oracle SOA Suite domains.\n Requirements and limitations  Understand the system requirements and limitations for deploying and running Oracle SOA Suite domains with the WebLogic Kubernetes Operator, including the SOA cluster sizing recommendations.\n Prepare your environment  Prepare for creating Oracle SOA Suite domains, including required secrets creation, persistent volume and volume claim creation, database creation, and database schema creation.\n Create Oracle SOA Suite domains  Create an Oracle SOA Suite domain home on an existing PV or PVC, and create the domain resource YAML file for deploying the generated Oracle SOA Suite domain.\n "
},
{
	"uri": "/fmw-kubernetes/wcportal-domains/installguide/",
	"title": "Install Guide",
	"tags": [],
	"description": "",
	"content": "Install the WebLogic Kubernetes operator and prepare and deploy the Oracle WebCenter Portal domain.\n Requirements and limitations  Understand the system requirements and limitations for deploying and running Oracle WebCenter Portal with the WebLogic Kubernetes operator.\n Prepare your environment  Prepare for creating the Oracle WebCenter Portal domain, This preparation includes but not limited to creating required secrets, persistent volume and volume claim, and database schema.\n Create WebCenter Portal domain  Create an Oracle WebCenter Portal domain home on an existing PV or PVC, and create the domain resource YAML file for deploying the generated Oracle WebCenter Portal domain.\n Configure WebCenter Portal For Search  Set up search functionality in Oracle WebCenter Portal using Elasticsearch.\n "
},
{
	"uri": "/fmw-kubernetes/wcsites-domains/installguide/",
	"title": "Install Guide",
	"tags": [],
	"description": "",
	"content": "Install the WebLogic Kubernetes Operator and prepare and deploy Oracle Webcenter Sites domains.\n Requirements and limitations  Understand the system requirements and limitations for deploying and running Oracle WebCenter Sites domains with the WebLogic Kubernetes Operator, including the WebCenter Sites domain cluster sizing recommendations.\n Prepare your environment  Prepare for creating Oracle WebCenter Sites domains, including required secrets creation, persistent volume and volume claim creation, database creation, and database schema creation.\n Create Oracle WebCenter Sites domains  Create an Oracle WebCenter Sites domain home on an existing PV or PVC, and create the domain resource YAML file for deploying the generated Oracle WebCenter Sites domain.\n "
},
{
	"uri": "/fmw-kubernetes/soa-domains/appendix/quickstart-deployment-on-prem/",
	"title": "Quick start deployment on-premise",
	"tags": [],
	"description": "Describes how to quickly get an Oracle SOA Suite domain instance running (using the defaults, nothing special) for development and test purposes.",
	"content": "Use this Quick Start to create an Oracle SOA Suite domain deployment in a Kubernetes cluster (on-premise environments) with the WebLogic Kubernetes Operator. Note that this walkthrough is for demonstration purposes only, not for use in production. These instructions assume that you are already familiar with Kubernetes. If you need more detailed instructions, refer to the Install Guide.\nHardware requirements The Linux kernel supported for deploying and running Oracle SOA Suite domains with the operator is Oracle Linux 7 (UL6+) and Red Hat Enterprise Linux 7 (UL3+ only with standalone Kubernetes). Refer to the prerequisites for more details.\nFor this exercise, the minimum hardware requirements to create a single-node Kubernetes cluster and then deploy the soaosb (Oracle SOA Suite, Oracle Service Bus, and Enterprise Scheduler (ESS)) domain type with one Managed Server for Oracle SOA Suite and one for the Oracle Service Bus cluster, along with Oracle Database running as a container are:\n   Hardware Size     RAM 32GB   Disk Space 250GB+   CPU core(s) 6    See here for resource sizing information for Oracle SOA Suite domains set up on a Kubernetes cluster.\nSet up Oracle SOA Suite in an on-premise environment Use the steps in this topic to create a single-instance on-premise Kubernetes cluster and then create an Oracle SOA Suite soaosb domain type, which deploys a domain with Oracle SOA Suite, Oracle Service Bus, and Oracle Enterprise Scheduler (ESS).\n Step 1 - Prepare a virtual machine for the Kubernetes cluster Step 2 - Set up a single instance Kubernetes cluster Step 3 - Get scripts and images Step 4 - Install the WebLogic Kubernetes Operator Step 5 - Install the Traefik (ingress-based) load balancer Step 6 - Create and configure an Oracle SOA Suite domain  1. Prepare a virtual machine for the Kubernetes cluster For illustration purposes, these instructions are for Oracle Linux 7u6+. If you are using a different flavor of Linux, you will need to adjust the steps accordingly.\nThese steps must be run with the root user, unless specified otherwise. Any time you see YOUR_USERID in a command, you should replace it with your actual userid.\n 1.1 Prerequisites   Choose the directories where your Docker and Kubernetes files will be stored. The Docker directory should be on a disk with a lot of free space (more than 100GB) because it will be used for the Docker file system, which contains all of your images and containers. The Kubernetes directory is used for the /var/lib/kubelet file system and persistent volume storage.\n$ export docker_dir=/u01/docker $ export kubelet_dir=/u01/kubelet $ mkdir -p $docker_dir $kubelet_dir $ ln -s $kubelet_dir /var/lib/kubelet   Verify that IPv4 forwarding is enabled on your host.\nNote: Replace eth0 with the ethernet interface name of your compute resource if it is different.\n$ /sbin/sysctl -a 2\u0026gt;\u0026amp;1|grep -s \u0026#39;net.ipv4.conf.docker0.forwarding\u0026#39; $ /sbin/sysctl -a 2\u0026gt;\u0026amp;1|grep -s \u0026#39;net.ipv4.conf.eth0.forwarding\u0026#39; $ /sbin/sysctl -a 2\u0026gt;\u0026amp;1|grep -s \u0026#39;net.ipv4.conf.lo.forwarding\u0026#39; $ /sbin/sysctl -a 2\u0026gt;\u0026amp;1|grep -s \u0026#39;net.ipv4.ip_nonlocal_bind\u0026#39; For example: Verify that all are set to 1:\n$ net.ipv4.conf.docker0.forwarding = 1 $ net.ipv4.conf.eth0.forwarding = 1 $ net.ipv4.conf.lo.forwarding = 1 $ net.ipv4.ip_nonlocal_bind = 1 Solution: Set all values to 1 immediately:\n$ /sbin/sysctl net.ipv4.conf.docker0.forwarding=1 $ /sbin/sysctl net.ipv4.conf.eth0.forwarding=1 $ /sbin/sysctl net.ipv4.conf.lo.forwarding=1 $ /sbin/sysctl net.ipv4.ip_nonlocal_bind=1   To preserve the settings permanently: Update the above values to 1 in files in /usr/lib/sysctl.d/, /run/sysctl.d/, and /etc/sysctl.d/.\n  Verify the iptables rule for forwarding.\nKubernetes uses iptables to handle many networking and port forwarding rules. A standard Docker installation may create a firewall rule that prevents forwarding.\nVerify if the iptables rule to accept forwarding traffic is set:\n$ /sbin/iptables -L -n | awk \u0026#39;/Chain FORWARD / {print $4}\u0026#39; | tr -d \u0026#34;)\u0026#34; If the output is \u0026ldquo;DROP\u0026rdquo;, then run the following command:\n$ /sbin/iptables -P FORWARD ACCEPT Verify if the iptables rule is properly set to \u0026ldquo;ACCEPT\u0026rdquo;:\n$ /sbin/iptables -L -n | awk \u0026#39;/Chain FORWARD / {print $4}\u0026#39; | tr -d \u0026#34;)\u0026#34;   Disable and stop firewalld:\n$ systemctl disable firewalld $ systemctl stop firewalld   1.2 Install and configure Docker  Note: If you have already installed Docker with version 19.03.1+ and configured the Docker daemon root to sufficient disk space along with proxy settings, continue to Install and configure Kubernetes.\n   Make sure that you have the right operating system version:\n$ uname -a $ more /etc/oracle-release Example output:\nLinux xxxxxxx 4.1.12-124.27.1.el7uek.x86_64 #2 SMP Mon May 13 08:56:17 PDT 2019 x86_64 x86_64 x86_64 GNU/Linux Oracle Linux Server release 7.6   Install the latest docker-engine and start the Docker service:\n$ docker_version=\u0026#34;19.03.1.ol\u0026#34; $ yum-config-manager --enable ol7_addons $ yum install docker-engine-$docker_version $ systemctl enable docker $ systemctl start docker   Add your user ID to the Docker group to allow you to run Docker commands without root access:\n$ /sbin/usermod -a -G docker \u0026lt;YOUR_USERID\u0026gt;   Check that your Docker version is at least 18.03:\n$ docker version Example output:\nClient: Docker Engine - Community Version: 19.03.1-ol API version: 1.40 Go version: go1.12.5 Git commit: ead9442 Built: Wed Sep 11 06:40:28 2019 OS/Arch: linux/amd64 Experimental: false Server: Docker Engine - Community Engine: Version: 19.03.1-ol API version: 1.40 (minimum version 1.12) Go version: go1.12.5 Git commit: ead9442 Built: Wed Sep 11 06:38:43 2019 OS/Arch: linux/amd64 Experimental: false Default Registry: docker.io containerd: Version: v1.2.0-rc.0-108-gc444666 GitCommit: c4446665cb9c30056f4998ed953e6d4ff22c7c39 runc: Version: 1.0.0-rc5+dev GitCommit: 4bb1fe4ace1a32d3676bb98f5d3b6a4e32bf6c58 docker-init: Version: 0.18.0 GitCommit: fec3683   Update the Docker engine configuration:\n$ mkdir -p /etc/docker $ cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/docker/daemon.json { \u0026#34;group\u0026#34;: \u0026#34;docker\u0026#34;, \u0026#34;data-root\u0026#34;: \u0026#34;/u01/docker\u0026#34; } EOF   Configure proxy settings if you are behind an HTTP proxy:\n### Create the drop-in file /etc/systemd/system/docker.service.d/http-proxy.conf that contains proxy details: $ cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/systemd/system/docker.service.d/http-proxy.conf [Service] Environment=\u0026#34;HTTP_PROXY=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT\u0026#34; Environment=\u0026#34;HTTPS_PROXY=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT\u0026#34; Environment=\u0026#34;NO_PROXY=localhost,127.0.0.0/8,ADD-YOUR-INTERNAL-NO-PROXY-LIST,/var/run/docker.sock\u0026#34; EOF  Note: On some hosts /etc/systemd/system/docker.service.d may not be available. Create this directory if it is not available.\n   Restart the Docker daemon to load the latest changes:\n$ systemctl daemon-reload $ systemctl restart docker   Verify that the proxy is configured with Docker:\n$ docker info|grep -i proxy Example output:\nHTTP Proxy: http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT HTTPS Proxy: http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT No Proxy: localhost,127.0.0.0/8,ADD-YOUR-INTERNAL-NO-PROXY-LIST,/var/run/docker.sock   Verify Docker installation:\n$ docker run hello-world Example output:\nHello from Docker! This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the \u0026#34;hello-world\u0026#34; image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal. To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bash Share images, automate workflows, and more with a free Docker ID: https://hub.docker.com/ For more examples and ideas, visit: https://docs.docker.com/get-started/   1.3 Install and configure Kubernetes   Add the external Kubernetes repository:\n$ cat \u0026lt;\u0026lt;EOF | sudo tee /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\\$basearch enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg exclude=kubelet kubeadm kubectl EOF   Set SELinux in permissive mode (effectively disabling it):\n$ export PATH=/sbin:$PATH $ setenforce 0 $ sed -i \u0026#39;s/^SELINUX=enforcing$/SELINUX=permissive/\u0026#39; /etc/selinux/config   Export proxy and install kubeadm, kubelet, and kubectl:\n### Get the nslookup IP address of the master node to use with apiserver-advertise-address during setting up Kubernetes master ### as the host may have different internal ip (hostname -i) and nslookup $HOSTNAME $ ip_addr=`nslookup $(hostname -f) | grep -m2 Address | tail -n1| awk -F: \u0026#39;{print $2}\u0026#39;| tr -d \u0026#34; \u0026#34;` $ echo $ip_addr ### Set the proxies $ export NO_PROXY=localhost,127.0.0.0/8,ADD-YOUR-INTERNAL-NO-PROXY-LIST,/var/run/docker.sock,$ip_addr $ export no_proxy=localhost,127.0.0.0/8,ADD-YOUR-INTERNAL-NO-PROXY-LIST,/var/run/docker.sock,$ip_addr $ export http_proxy=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT $ export https_proxy=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT $ export HTTPS_PROXY=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT $ export HTTP_PROXY=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT ### Install Kubernetes $ VERSION=1.23.6-0 $ yum install -y kubelet-$VERSION kubeadm-$VERSION kubectl-$VERSION --disableexcludes=kubernetes ### Enable kubelet service so that it automatically restarts on reboot $ systemctl enable --now kubelet   Ensure net.bridge.bridge-nf-call-iptables is set to 1 in your sysctl to avoid traffic routing issues:\n$ cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF $ sysctl --system   Disable swap check:\n$ sed -i \u0026#39;s/KUBELET_EXTRA_ARGS=/KUBELET_EXTRA_ARGS=\u0026#34;--fail-swap-on=false\u0026#34;/\u0026#39; /etc/sysconfig/kubelet $ cat /etc/sysconfig/kubelet ### Reload and restart kubelet $ systemctl daemon-reload $ systemctl restart kubelet   From Kubernetes version v1.22 onward, kubeadm will default cgroup-driver to systemd. If your Docker is using cgroup driver as cgroupfs, set --cgroup-driver=cgroupfs for kubelet.\n$ sed -i \u0026#39;s/^KUBELET_EXTRA_ARGS=.*/KUBELET_EXTRA_ARGS=\u0026#34;--fail-swap-on=false --cgroup-driver=cgroupfs\u0026#34;/\u0026#39; /etc/sysconfig/kubelet $ cat /etc/sysconfig/kubelet ### Reload and restart kubelet $ systemctl daemon-reload $ systemctl restart kubelet   1.4 Set up Helm   Install Helm v3.3.4+.\na. Download Helm from https://github.com/helm/helm/releases.\nFor example, to download Helm v3.5.4:\n$ wget https://get.helm.sh/helm-v3.5.4-linux-amd64.tar.gz b. Unpack tar.gz:\n$ tar -zxvf helm-v3.5.4-linux-amd64.tar.gz c. Find the Helm binary in the unpacked directory, and move it to its desired destination:\n$ mv linux-amd64/helm /usr/bin/helm   Run helm version to verify its installation:\n$ helm version version.BuildInfo{Version:\u0026#34;v3.5.4\u0026#34;, GitCommit:\u0026#34;1b5edb69df3d3a08df77c9902dc17af864ff05d1\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, GoVersion:\u0026#34;go1.15.11\u0026#34;}   2. Set up a single instance Kubernetes cluster  Notes:\n These steps must be run with the root user, unless specified otherwise! If you choose to use a different CIDR block (that is, other than 10.244.0.0/16 for the --pod-network-cidr= in the kubeadm init command), then also update NO_PROXY and no_proxy with the appropriate value.  Also make sure to update kube-flannel.yaml with the new value before deploying.   Replace the following with appropriate values:  ADD-YOUR-INTERNAL-NO-PROXY-LIST REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT     2.1 Set up the master node   Create a shell script that sets up the necessary environment variables. You can append this to the user’s .bashrc so that it will run at login. You must also configure your proxy settings here if you are behind an HTTP proxy:\n## grab my IP address to pass into kubeadm init, and to add to no_proxy vars ip_addr=`nslookup $(hostname -f) | grep -m2 Address | tail -n1| awk -F: \u0026#39;{print $2}\u0026#39;| tr -d \u0026#34; \u0026#34;` export pod_network_cidr=\u0026#34;10.244.0.0/16\u0026#34; export service_cidr=\u0026#34;10.96.0.0/12\u0026#34; export PATH=$PATH:/sbin:/usr/sbin ### Set the proxies export NO_PROXY=localhost,.svc,127.0.0.0/8,ADD-YOUR-INTERNAL-NO-PROXY-LIST,/var/run/docker.sock,$ip_addr,$pod_network_cidr,$service_cidr export no_proxy=localhost,.svc,127.0.0.0/8,ADD-YOUR-INTERNAL-NO-PROXY-LIST,/var/run/docker.sock,$ip_addr,$pod_network_cidr,$service_cidr export http_proxy=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT export https_proxy=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT export HTTPS_PROXY=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT export HTTP_PROXY=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT   Source the script to set up your environment variables:\n$ . ~/.bashrc   To implement command completion, add the following to the script:\n$ [ -f /usr/share/bash-completion/bash_completion ] \u0026amp;\u0026amp; . /usr/share/bash-completion/bash_completion $ source \u0026lt;(kubectl completion bash)   Run kubeadm init to create the master node:\n$ kubeadm init \\  --pod-network-cidr=$pod_network_cidr \\  --apiserver-advertise-address=$ip_addr \\  --ignore-preflight-errors=Swap \u0026gt; /tmp/kubeadm-init.out 2\u0026gt;\u0026amp;1   Log in to the terminal with YOUR_USERID:YOUR_GROUP. Then set up the ~/.bashrc similar to steps 1 to 3 with YOUR_USERID:YOUR_GROUP.\n Note that from now on we will be using YOUR_USERID:YOUR_GROUP to execute any kubectl commands and not root.\n   Set up YOUR_USERID:YOUR_GROUP to access the Kubernetes cluster:\n$ mkdir -p $HOME/.kube $ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config $ sudo chown $(id -u):$(id -g) $HOME/.kube/config   Verify that YOUR_USERID:YOUR_GROUP is set up to access the Kubernetes cluster using the kubectl command:\n$ kubectl get nodes  Note: At this step, the node is not in ready state as we have not yet installed the pod network add-on. After the next step, the node will show status as Ready.\n   Install a pod network add-on (flannel) so that your pods can communicate with each other.\n Note: If you are using a different CIDR block than 10.244.0.0/16, then download and update kube-flannel.yml with the correct CIDR address before deploying into the cluster:\n $ wget https://raw.githubusercontent.com/flannel-io/flannel/v0.17.0/Documentation/kube-flannel.yml $ ### Update the CIDR address if you are using a CIDR block other than the default 10.244.0.0/16 $ kubectl apply -f kube-flannel.yml   Verify that the master node is in Ready status:\n$ kubectl get nodes Sample output:\nNAME STATUS ROLES AGE VERSION mymasternode Ready control-plane,master 12h v1.23.6 or:\n$ kubectl get pods -n kube-system Sample output:\nNAME READY STATUS RESTARTS AGE pod/coredns-86c58d9df4-58p9f 1/1 Running 0 3m59s pod/coredns-86c58d9df4-mzrr5 1/1 Running 0 3m59s pod/etcd-mymasternode 1/1 Running 0 3m4s pod/kube-apiserver-node 1/1 Running 0 3m21s pod/kube-controller-manager-mymasternode 1/1 Running 0 3m25s pod/kube-flannel-ds-6npx4 1/1 Running 0 49s pod/kube-proxy-4vsgm 1/1 Running 0 3m59s pod/kube-scheduler-mymasternode 1/1 Running 0 2m58s   To schedule pods on the master node, taint the node:\n$ kubectl taint nodes --all node-role.kubernetes.io/master-   Congratulations! Your Kubernetes cluster environment is ready to deploy your Oracle SOA Suite domain.\nRefer to the official documentation to set up a Kubernetes cluster.\n3. Get scripts and images 3.1 Set up the code repository to deploy Oracle SOA Suite domains Follow these steps to set up the source code repository required to deploy Oracle SOA Suite domains.\n3.2 Get required Docker images and add them to your local registry   Pull the WebLogic Kubernetes Operator image:\n$ docker pull ghcr.io/oracle/weblogic-kubernetes-operator:3.4.0 $ docker tag ghcr.io/oracle/weblogic-kubernetes-operator:3.4.0 oracle/weblogic-kubernetes-operator:3.4.0   Obtain the Oracle Database image and Oracle SOA Suite Docker image from the Oracle Container Registry:\na. For first time users, to pull an image from the Oracle Container Registry, navigate to https://container-registry.oracle.com and log in using the Oracle Single Sign-On (SSO) authentication service. If you do not already have SSO credentials, you can create an Oracle Account using: https://profile.oracle.com/myprofile/account/create-account.jspx.\nUse the web interface to accept the Oracle Standard Terms and Restrictions for the Oracle software images that you intend to deploy. Your acceptance of these terms are stored in a database that links the software images to your Oracle Single Sign-On login credentials.\nTo obtain the image, log in to the Oracle Container Registry:\n$ docker login container-registry.oracle.com b. Find and then pull the Oracle Database image for 12.2.0.1:\n$ docker pull container-registry.oracle.com/database/enterprise:12.2.0.1-slim c. Find and then pull the prebuilt Oracle SOA Suite image 12.2.1.4 install image:\n$ docker pull container-registry.oracle.com/middleware/soasuite:12.2.1.4  Note: This image does not contain any Oracle SOA Suite product patches and can only be used for test and development purposes.\n   4. Install the WebLogic Kubernetes Operator 4.1 Prepare for the WebLogic Kubernetes Operator.   Create a namespace opns for the operator:\n$ kubectl create namespace opns   Create a service account op-sa for the operator in the operator’s namespace:\n$ kubectl create serviceaccount -n opns op-sa   4.2 Install the WebLogic Kubernetes Operator Use Helm to install and start the operator from the directory you just cloned:\n$ cd ${WORKDIR} $ helm install weblogic-kubernetes-operator charts/weblogic-operator \\  --namespace opns \\  --set image=oracle/weblogic-kubernetes-operator:3.4.0 \\  --set serviceAccount=op-sa \\  --set \u0026#34;domainNamespaces={}\u0026#34; \\  --wait 4.3 Verify the WebLogic Kubernetes Operator   Verify that the operator’s pod is running by listing the pods in the operator’s namespace. You should see one for the operator:\n$ kubectl get pods -n opns   Verify that the operator is up and running by viewing the operator pod\u0026rsquo;s logs:\n$ kubectl logs -n opns -c weblogic-operator deployments/weblogic-operator   The WebLogic Kubernetes Operator v3.4.0 has been installed. Continue with the load balancer and Oracle SOA Suite domain setup.\n5. Install the Traefik (ingress-based) load balancer The WebLogic Kubernetes Operator supports these load balancers: Traefik, NGINX, and Apache. Samples are provided in the documentation.\nThis Quick Start demonstrates how to install the Traefik ingress controller to provide load balancing for an Oracle SOA Suite domain.\n  Create a namespace for Traefik:\n$ kubectl create namespace traefik   Set up Helm for 3rd party services:\n$ helm repo add traefik https://helm.traefik.io/traefik --force-update   Install the Traefik operator in the traefik namespace with the provided sample values:\n$ cd ${WORKDIR} $ helm install traefik traefik/traefik \\  --namespace traefik \\  --values charts/traefik/values.yaml \\  --set \u0026#34;kubernetes.namespaces={traefik}\u0026#34; \\  --set \u0026#34;service.type=NodePort\u0026#34; \\  --wait   6. Create and configure an Oracle SOA Suite domain 6.1 Prepare for an Oracle SOA Suite domain   Create a namespace that can host Oracle SOA Suite domains:\n$ kubectl create namespace soans   Use Helm to configure the operator to manage Oracle SOA Suite domains in this namespace:\n$ cd ${WORKDIR} $ helm upgrade weblogic-kubernetes-operator charts/weblogic-operator \\  --reuse-values \\  --namespace opns \\  --set \u0026#34;domainNamespaces={soans}\u0026#34; \\  --wait   Create Kubernetes secrets.\na. Create a Kubernetes secret for the domain in the same Kubernetes namespace as the domain. In this example, the username is weblogic, the password is Welcome1, and the namespace is soans:\n$ cd ${WORKDIR}/create-weblogic-domain-credentials $ ./create-weblogic-credentials.sh \\  -u weblogic \\  -p Welcome1 \\  -n soans \\  -d soainfra \\  -s soainfra-domain-credentials b. Create a Kubernetes secret for the RCU in the same Kubernetes namespace as the domain:\n Schema user : SOA1 Schema password : Oradoc_db1 DB sys user password : Oradoc_db1 Domain name : soainfra Domain Namespace : soans Secret name : soainfra-rcu-credentials  $ cd ${WORKDIR}/create-rcu-credentials $ ./create-rcu-credentials.sh \\  -u SOA1 \\  -p Oradoc_db1 \\  -a sys \\  -q Oradoc_db1 \\  -d soainfra \\  -n soans \\  -s soainfra-rcu-credentials   Create the Kubernetes persistence volume and persistence volume claim.\na. Create the Oracle SOA Suite domain home directory. Determine if a user already exists on your host system with uid:gid of 1000:0:\n$ sudo getent passwd 1000 If this command returns a username (which is the first field), you can skip the following useradd command. If not, create the oracle user with useradd:\n$ sudo useradd -u 1000 -g 0 oracle Create the directory that will be used for the Oracle SOA Suite domain home:\n$ sudo mkdir /scratch/k8s_dir $ sudo chown -R 1000:0 /scratch/k8s_dir b. The create-pv-pvc-inputs.yaml has the following values by default:\n baseName: domain domainUID: soainfra namespace: soans weblogicDomainStoragePath: /scratch/k8s_dir  Review and update if any changes required.\n$ cd ${WORKDIR}/create-weblogic-domain-pv-pvc $ vim create-pv-pvc-inputs.yaml c. Run the create-pv-pvc.sh script to create the PV and PVC configuration files:\n$ cd ${WORKDIR}/create-weblogic-domain-pv-pvc $ ./create-pv-pvc.sh -i create-pv-pvc-inputs.yaml -o output d. Create the PV and PVC using the configuration files created in the previous step:\n$ kubectl create -f output/pv-pvcs/soainfra-domain-pv.yaml $ kubectl create -f output/pv-pvcs/soainfra-domain-pvc.yaml   Install and configure the database for the Oracle SOA Suite domain.\nThis step is required only when a standalone database is not already set up and you want to use the database in a container.\nThe Oracle Database Docker images are supported only for non-production use. For more details, see My Oracle Support note: Oracle Support for Database Running on Docker (Doc ID 2216342.1). For production, it is suggested to use a standalone database. This example provides steps to create the database in a container.\n a. Create a database in a container:\n$ cd ${WORKDIR}/create-oracle-db-service $ ./start-db-service.sh -i container-registry.oracle.com/database/enterprise:12.2.0.1-slim -p none Once the database is successfully created, you can use the database connection string oracle-db.default.svc.cluster.local:1521/devpdb.k8s as an rcuDatabaseURL parameter in the create-domain-inputs.yaml file.\nb. Create Oracle SOA Suite schemas for the domain type (for example, soaosb).\nTo install the Oracle SOA Suite schemas, run the create-rcu-schema.sh script with the following inputs:\n -s \u0026lt;RCU PREFIX\u0026gt; -t \u0026lt;SOA domain type\u0026gt; -d \u0026lt;Oracle Database URL\u0026gt; -i \u0026lt;SOASuite image\u0026gt; -n \u0026lt;Namespace\u0026gt; -q \u0026lt;SYSDBA Database password\u0026gt; -r \u0026lt;Schema password\u0026gt; -c \u0026lt;Comma-separated variables\u0026gt; -l \u0026lt;Timeout limit in seconds. (optional). (default: 300)\u0026gt;  For example:\n$ cd ${WORKDIR}/create-rcu-schema $ ./create-rcu-schema.sh \\ -s SOA1 \\ -t soaosb \\ -d oracle-db.default.svc.cluster.local:1521/devpdb.k8s \\ -i container-registry.oracle.com/middleware/soasuite:12.2.1.4 \\ -n default \\ -q Oradoc_db1 \\ -r Oradoc_db1 \\ -c SOA_PROFILE_TYPE=SMALL,HEALTHCARE_INTEGRATION=NO   Now the environment is ready to start the Oracle SOA Suite domain creation.\n6.2 Create an Oracle SOA Suite domain   The sample scripts for Oracle SOA Suite domain deployment are available at ${WORKDIR}/create-soa-domain/domain-home-on-pv. You must edit create-domain-inputs.yaml (or a copy of it) to provide the details for your domain.\nUpdate create-domain-inputs.yaml with the following values for domain creation:\n domainType: soaosb initialManagedServerReplicas: 1  $ cd ${WORKDIR}/create-soa-domain/domain-home-on-pv/ $ cp create-domain-inputs.yaml create-domain-inputs.yaml.orig $ sed -i -e \u0026#34;s:domainType\\: soa:domainType\\: soaosb:g\u0026#34; create-domain-inputs.yaml $ sed -i -e \u0026#34;s:initialManagedServerReplicas\\: 2:initialManagedServerReplicas\\: 1:g\u0026#34; create-domain-inputs.yaml $ sed -i -e \u0026#34;s:image\\: soasuite\\:12.2.1.4:image\\: container-registry.oracle.com/middleware/soasuite\\:12.2.1.4:g\u0026#34; create-domain-inputs.yaml   Run the create-domain.sh script to create a domain:\n$ cd ${WORKDIR}/create-soa-domain/domain-home-on-pv/ $ ./create-domain.sh -i create-domain-inputs.yaml -o output   Create a Kubernetes domain object:\nOnce the create-domain.sh is successful, it generates output/weblogic-domains/soainfra/domain.yaml, which you can use to create the Kubernetes resource domain to start the domain and servers:\n$ cd ${WORKDIR}/create-soa-domain/domain-home-on-pv $ kubectl create -f output/weblogic-domains/soainfra/domain.yaml   Verify that the Kubernetes domain object named soainfra is created:\n$ kubectl get domain -n soans NAME AGE soainfra 3m18s   Once you create the domain, the introspect pod is created. This inspects the domain home and then starts the soainfra-adminserver pod. Once the soainfra-adminserver pod starts successfully, the Managed Server pods are started in parallel. Watch the soans namespace for the status of domain creation:\n$ kubectl get pods -n soans -w   Verify that the Oracle SOA Suite domain server pods and services are created and in Ready state:\n$ kubectl get all -n soans   6.3 Configure Traefik to access Oracle SOA Suite domain services   Configure Traefik to manage ingresses created in the Oracle SOA Suite domain namespace (soans):\n$ helm upgrade traefik traefik/traefik \\  --reuse-values \\  --namespace traefik \\  --set \u0026#34;kubernetes.namespaces={traefik,soans}\u0026#34; \\  --wait   Create an ingress for the domain in the domain namespace by using the sample Helm chart:\n$ cd ${WORKDIR} $ export LOADBALANCER_HOSTNAME=$(hostname -f) $ helm install soa-traefik-ingress charts/ingress-per-domain \\ --namespace soans \\ --values charts/ingress-per-domain/values.yaml \\ --set \u0026#34;traefik.hostname=${LOADBALANCER_HOSTNAME}\u0026#34; \\ --set domainType=soaosb   Verify the created ingress per domain details:\n$ kubectl describe ingress soainfra-traefik -n soans   6.4 Verify that you can access the Oracle SOA Suite domain URL   Get the LOADBALANCER_HOSTNAME for your environment:\n$ export LOADBALANCER_HOSTNAME=$(hostname -f)   Verify the following URLs are available for Oracle SOA Suite domains of domain type soaosb:\nCredentials:\nusername: weblogic password: Welcome1\nhttp://${LOADBALANCER_HOSTNAME}:30305/console http://${LOADBALANCER_HOSTNAME}:30305/em http://${LOADBALANCER_HOSTNAME}:30305/servicebus http://${LOADBALANCER_HOSTNAME}:30305/soa-infra http://${LOADBALANCER_HOSTNAME}:30305/soa/composer http://${LOADBALANCER_HOSTNAME}:30305/integration/worklistapp http://${LOADBALANCER_HOSTNAME}:30305/ess http://${LOADBALANCER_HOSTNAME}:30305/EssHealthCheck   "
},
{
	"uri": "/fmw-kubernetes/wcportal-domains/appendix/quickstart-deployment-on-prem/",
	"title": "Quick start deployment on-premise",
	"tags": [],
	"description": "Describes how to quickly get an Oracle WebCenter Portal domain instance running (using the defaults, nothing special) for development and test purposes.",
	"content": "Use this Quick Start to create an Oracle WebCenter Portal domain deployment in a Kubernetes cluster (on-premise environments) with the WebLogic Kubernetes Operator. Note that this walkthrough is for demonstration purposes only, not for use in production. These instructions assume that you are already familiar with Kubernetes. If you need more detailed instructions, refer to the Install Guide.\nHardware requirements The Linux kernel supported for deploying and running Oracle WebCenter Portal domains with the operator is Oracle Linux 7 (UL6+) and Red Hat Enterprise Linux 7 (UL3+ only with standalone Kubernetes). Refer to the prerequisites for more details.\nFor this exercise, the minimum hardware requirements to create a single-node Kubernetes cluster and then deploy the domain type with one Managed Server along with Oracle Database running as a container are:\n   Hardware Size     RAM 32GB   Disk Space 250GB+   CPU core(s) 6    See here for resource sizing information for Oracle WebCenter Portal domain set up on a Kubernetes cluster.\nSet up Oracle WebCenter Portal in an on-premise environment Use the steps in this topic to create a single-instance on-premise Kubernetes cluster and then create an Oracle WebCenter Portal domain.\n Step 1 - Prepare a virtual machine for the Kubernetes cluster Step 2 - Set up a single instance Kubernetes cluster Step 3 - Get scripts and images Step 4 - Install the WebLogic Kubernetes operator Step 5 - Install the Traefik (ingress-based) load balancer Step 6 - Create and configure an Oracle WebCenter Portal domain  1. Prepare a virtual machine for the Kubernetes cluster For illustration purposes, these instructions are for Oracle Linux 7u6+. If you are using a different flavor of Linux, you will need to adjust the steps accordingly.\nThese steps must be run with the root user, unless specified otherwise. Any time you see YOUR_USERID in a command, you should replace it with your actual userid.\n 1.1 Prerequisites   Choose the directories where your Docker and Kubernetes files will be stored. The Docker directory should be on a disk with a lot of free space (more than 100GB) because it will be used for the Docker file system, which contains all of your images and containers. The Kubernetes directory is used for the /var/lib/kubelet file system and persistent volume storage.\n$ export docker_dir=/u01/docker $ export kubelet_dir=/u01/kubelet $ mkdir -p $docker_dir $kubelet_dir $ ln -s $kubelet_dir /var/lib/kubelet   Verify that IPv4 forwarding is enabled on your host.\nNote: Replace eth0 with the ethernet interface name of your compute resource if it is different.\n$ /sbin/sysctl -a 2\u0026gt;\u0026amp;1|grep -s 'net.ipv4.conf.docker0.forwarding' $ /sbin/sysctl -a 2\u0026gt;\u0026amp;1|grep -s 'net.ipv4.conf.eth0.forwarding' $ /sbin/sysctl -a 2\u0026gt;\u0026amp;1|grep -s 'net.ipv4.conf.lo.forwarding' $ /sbin/sysctl -a 2\u0026gt;\u0026amp;1|grep -s 'net.ipv4.ip_nonlocal_bind' For example: Verify that all are set to 1:\n$ net.ipv4.conf.docker0.forwarding = 1 $ net.ipv4.conf.eth0.forwarding = 1 $ net.ipv4.conf.lo.forwarding = 1 $ net.ipv4.ip_nonlocal_bind = 1 Solution: Set all values to 1 immediately:\n$ /sbin/sysctl net.ipv4.conf.docker0.forwarding=1 $ /sbin/sysctl net.ipv4.conf.eth0.forwarding=1 $ /sbin/sysctl net.ipv4.conf.lo.forwarding=1 $ /sbin/sysctl net.ipv4.ip_nonlocal_bind=1   To preserve the settings permanently: Update the above values to 1 in files in /usr/lib/sysctl.d/, /run/sysctl.d/, and /etc/sysctl.d/.\n  Verify the iptables rule for forwarding.\nKubernetes uses iptables to handle many networking and port forwarding rules. A standard Docker installation may create a firewall rule that prevents forwarding.\nVerify if the iptables rule to accept forwarding traffic is set:\n$ /sbin/iptables -L -n | awk '/Chain FORWARD / {print $4}' | tr -d \u0026quot;)\u0026quot; If the output is \u0026ldquo;DROP\u0026rdquo;, then run the following command:\n$ /sbin/iptables -P FORWARD ACCEPT Verify if the iptables rule is properly set to \u0026ldquo;ACCEPT\u0026rdquo;:\n$ /sbin/iptables -L -n | awk '/Chain FORWARD / {print $4}' | tr -d \u0026quot;)\u0026quot;   Disable and stop firewalld:\n$ systemctl disable firewalld $ systemctl stop firewalld   1.2 Install and configure Docker  Note: If you have already installed Docker with version 18.03+ and configured the Docker daemon root to sufficient disk space along with proxy settings, continue to Install and configure Kubernetes.\n   Make sure that you have the right operating system version:\n$ uname -a $ more /etc/oracle-release Example output:\nLinux xxxxxxx 4.1.12-124.27.1.el7uek.x86_64 #2 SMP Mon May 13 08:56:17 PDT 2019 x86_64 x86_64 x86_64 GNU/Linux Oracle Linux Server release 7.6   Install the latest docker-engine and start the Docker service:\n$ yum-config-manager --enable ol7_addons $ docker_version=\u0026quot;19.03.11-ol\u0026quot; $ yum install docker-engine-$docker_version $ systemctl enable docker $ systemctl start docker   Add your user ID to the Docker group to allow you to run Docker commands without root access:\n$ /sbin/usermod -a -G docker \u0026lt;YOUR_USERID\u0026gt;   Check that your Docker version is at least 18.03:\n$ docker version Example output:\nClient: Docker Engine - Community Version: 19.03.11-ol API version: 1.40 Go version: go1.15.5 Git commit: 748876d Built: Thu Dec 3 19:36:03 2020 OS/Arch: linux/amd64 Experimental: false Server: Docker Engine - Community Engine: Version: 19.03.11-ol API version: 1.40 (minimum version 1.12) Go version: go1.15.8 Git commit: f0aae77 Built: Wed Feb 10 16:13:32 2021 OS/Arch: linux/amd64 Experimental: false Default Registry: docker.io containerd: Version: v1.3.9 GitCommit: runc: Version: 1.0.0-rc5+dev GitCommit: 4bb1fe4ace1a32d3676bb98f5d3b6a4e32bf6c58 docker-init: Version: 0.18.0 GitCommit: fec3683   Update the Docker engine configuration:\n$ mkdir -p /etc/docker $ cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/docker/daemon.json { \u0026quot;group\u0026quot;: \u0026quot;docker\u0026quot;, \u0026quot;data-root\u0026quot;: \u0026quot;/u01/docker\u0026quot; } EOF   Configure proxy settings if you are behind an HTTP proxy:\n ### Create the drop-in file /etc/systemd/system/docker.service.d/http-proxy.conf that contains proxy details: $ cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/systemd/system/docker.service.d/http-proxy.conf [Service] Environment=\u0026quot;HTTP_PROXY=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT\u0026quot; Environment=\u0026quot;HTTPS_PROXY=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT\u0026quot; Environment=\u0026quot;NO_PROXY=localhost,127.0.0.0/8,ADD-YOUR-INTERNAL-NO-PROXY-LIST,/var/run/docker.sock\u0026quot; EOF  Note: On some hosts /etc/systemd/system/docker.service.d may not be available. Create this directory if it is not available.\n   Restart the Docker daemon to load the latest changes:\n$ systemctl daemon-reload $ systemctl restart docker   Verify that the proxy is configured with Docker:\n$ docker info|grep -i proxy Example output:\nHTTP Proxy: http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT HTTPS Proxy: http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT No Proxy: localhost,127.0.0.0/8,ADD-YOUR-INTERNAL-NO-PROXY-LIST,/var/run/docker.sock   Verify Docker installation:\n$ docker run hello-world Example output:\nHello from Docker! This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the \u0026quot;hello-world\u0026quot; image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal. To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bash Share images, automate workflows, and more with a free Docker ID: https://hub.docker.com/ For more examples and ideas, visit: https://docs.docker.com/get-started/   1.3 Install and configure Kubernetes   Add the external Kubernetes repository:\n$ cat \u0026lt;\u0026lt;EOF | sudo tee /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\\$basearch enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg exclude=kubelet kubeadm kubectl EOF   Set SELinux in permissive mode (effectively disabling it):\n$ export PATH=/sbin:$PATH $ setenforce 0 $ sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config   Export proxy and install kubeadm, kubelet, and kubectl:\n### Get the nslookup IP address of the master node to use with apiserver-advertise-address during setting up Kubernetes master ### as the host may have different internal ip (hostname -i) and nslookup $HOSTNAME $ ip_addr=`nslookup $(hostname -f) | grep -m2 Address | tail -n1| awk -F: '{print $2}'| tr -d \u0026quot; \u0026quot;` $ echo $ip_addr ### Set the proxies $ export NO_PROXY=localhost,127.0.0.0/8,ADD-YOUR-INTERNAL-NO-PROXY-LIST,/var/run/docker.sock,$ip_addr $ export no_proxy=localhost,127.0.0.0/8,ADD-YOUR-INTERNAL-NO-PROXY-LIST,/var/run/docker.sock,$ip_addr $ export http_proxy=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT $ export https_proxy=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT $ export HTTPS_PROXY=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT $ export HTTP_PROXY=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT ### install kubernetes 1.20.10 $ VERSION=1.20.10 $ yum install -y kubelet-$VERSION kubeadm-$VERSION kubectl-$VERSION --disableexcludes=kubernetes ### enable kubelet service so that it auto-restart on reboot $ systemctl enable --now kubelet   Ensure net.bridge.bridge-nf-call-iptables is set to 1 in your sysctl to avoid traffic routing issues:\n$ cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF $ sysctl --system   Disable swap check:\n$ sed -i 's/KUBELET_EXTRA_ARGS=/KUBELET_EXTRA_ARGS=\u0026quot;--fail-swap-on=false\u0026quot;/' /etc/sysconfig/kubelet $ cat /etc/sysconfig/kubelet ### Reload and restart kubelet $ systemctl daemon-reload $ systemctl restart kubelet   1.4 Set up Helm   Install Helm v3.4+\na. Download Helm from https://github.com/helm/helm/releases.\nFor example, to download Helm v3.4.1:\n$ wget https://get.helm.sh/helm-v3.4.1-linux-amd64.tar.gz b. Unpack tar.gz:\n$ tar -zxvf helm-v3.4.1-linux-amd64.tar.gz c. Find the Helm binary in the unpacked directory, and move it to its desired destination:\n$ mv linux-amd64/helm /usr/bin/helm   Run helm version to verify its installation:\n$ helm version version.BuildInfo{Version:\u0026quot;v3.2.3\u0026quot;, GitCommit:\u0026quot;0ad800ef43d3b826f31a5ad8dfbb4fe05d143688\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;, GoVersion:\u0026quot;go1.13.12\u0026quot;}   2. Set up a single instance Kubernetes cluster  Notes:\n These steps must be run with the root user, unless specified otherwise! If you choose to use a different CIDR block (that is, other than 10.244.0.0/16 for the --pod-network-cidr= in the kubeadm init command), then also update NO_PROXY and no_proxy with the appropriate value.  Also make sure to update kube-flannel.yaml with the new value before deploying.   Replace the following with appropriate values:  ADD-YOUR-INTERNAL-NO-PROXY-LIST REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT     2.1 Set up the master node   Create a shell script that sets up the necessary environment variables. You can append this to the user’s .bashrc so that it will run at login. You must also configure your proxy settings here if you are behind an HTTP proxy:\n## grab my IP address to pass into kubeadm init, and to add to no_proxy vars ip_addr=`nslookup $(hostname -f) | grep -m2 Address | tail -n1| awk -F: '{print $2}'| tr -d \u0026quot; \u0026quot;` export pod_network_cidr=\u0026quot;10.244.0.0/16\u0026quot; export service_cidr=\u0026quot;10.96.0.0/12\u0026quot; export PATH=$PATH:/sbin:/usr/sbin ### Set the proxies export NO_PROXY=localhost,127.0.0.0/8,ADD-YOUR-INTERNAL-NO-PROXY-LIST,/var/run/docker.sock,$ip_addr,$pod_network_cidr,$service_cidr export no_proxy=localhost,127.0.0.0/8,ADD-YOUR-INTERNAL-NO-PROXY-LIST,/var/run/docker.sock,$ip_addr,$pod_network_cidr,$service_cidr export http_proxy=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT export https_proxy=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT export HTTPS_PROXY=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT export HTTP_PROXY=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT   Source the script to set up your environment variables:\n$ . ~/.bashrc   To implement command completion, add the following to the script:\n$ [ -f /usr/share/bash-completion/bash_completion ] \u0026amp;\u0026amp; . /usr/share/bash-completion/bash_completion $ source \u0026lt;(kubectl completion bash)   Run kubeadm init to create the master node:\n$ kubeadm init \\ --pod-network-cidr=$pod_network_cidr \\ --apiserver-advertise-address=$ip_addr \\ --ignore-preflight-errors=Swap \u0026gt; /tmp/kubeadm-init.out 2\u0026gt;\u0026amp;1   Log in to the terminal with YOUR_USERID:YOUR_GROUP. Then set up the ~/.bashrc similar to steps 1 to 3 with YOUR_USERID:YOUR_GROUP.\n Note that from now on we will be using YOUR_USERID:YOUR_GROUP to execute any kubectl commands and not root.\n   Set up YOUR_USERID:YOUR_GROUP to access the Kubernetes cluster:\n$ mkdir -p $HOME/.kube $ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config $ sudo chown $(id -u):$(id -g) $HOME/.kube/config   Verify that YOUR_USERID:YOUR_GROUP is set up to access the Kubernetes cluster using the kubectl command:\n$ kubectl get nodes  Note: At this step, the node is not in ready state as we have not yet installed the pod network add-on. After the next step, the node will show status as Ready.\n   Install a pod network add-on (flannel) so that your pods can communicate with each other.\n Note: If you are using a different CIDR block than 10.244.0.0/16, then download and update kube-flannel.yml with the correct CIDR address before deploying into the cluster:\n $ kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.12.0/Documentation/kube-flannel.yml   Verify that the master node is in Ready status:\n$ kubectl get nodes Sample output:\nNAME STATUS ROLES AGE VERSION mymasternode Ready master 8m26s v1.18.4 or:\n$ kubectl get pods -n kube-system Sample output:\nNAME READY STATUS RESTARTS AGE pod/coredns-86c58d9df4-58p9f 1/1 Running 0 3m59s pod/coredns-86c58d9df4-mzrr5 1/1 Running 0 3m59s pod/etcd-mymasternode 1/1 Running 0 3m4s pod/kube-apiserver-node 1/1 Running 0 3m21s pod/kube-controller-manager-mymasternode 1/1 Running 0 3m25s pod/kube-flannel-ds-amd64-6npx4 1/1 Running 0 49s pod/kube-proxy-4vsgm 1/1 Running 0 3m59s pod/kube-scheduler-mymasternode 1/1 Running 0 2m58s   To schedule pods on the master node, taint the node:\n$ kubectl taint nodes --all node-role.kubernetes.io/master-   Congratulations! Your Kubernetes cluster environment is ready to deploy your Oracle WebCenter Portal domain.\nFor additional references on Kubernetes cluster setup, check the cheat sheet.\n3. Get scripts and images 3.1 Set up the code repository to deploy Oracle WebCenter Portal Follow these steps to set up the source code repository required to deploy Oracle WebCenter Portal domain.\n3.2 Get required Docker images and add them to your local registry   Pull the operator image:\n$ docker pull ghcr.io/oracle/weblogic-kubernetes-operator:3.3.0   Obtain the Oracle Database image from the Oracle Container Registry:\na. For first time users, to pull an image from the Oracle Container Registry, navigate to https://container-registry.oracle.com and log in using the Oracle Single Sign-On (SSO) authentication service. If you do not already have SSO credentials, you can create an Oracle Account using:\nhttps://profile.oracle.com/myprofile/account/create-account.jspx.\nUse the web interface to accept the Oracle Standard Terms and Restrictions for the Oracle software images that you intend to deploy. Your acceptance of these terms are stored in a database that links the software images to your Oracle Single Sign-On login credentials.\nTo obtain the image, log in to the Oracle Container Registry:\n$ docker login container-registry.oracle.com b. Find and then pull the Oracle Database image for 12.2.0.1:\n$ docker pull container-registry.oracle.com/database/enterprise:12.2.0.1-slim c. Build Oracle WebCenter Portal 12.2.1.4 Image by following steps from this document.\n  4. Install the WebLogic Kubernetes operator 4.1 Prepare for the WebLogic Kubernetes operator.   Create a namespace operator-ns for the operator:\n$ kubectl create namespace operator-ns   Create a service account operator-sa for the operator in the operator’s namespace:\n$ kubectl create serviceaccount -n operator-ns operator-sa   4.2 Install the WebLogic Kubernetes operator Use Helm to install and start the operator from the directory you just cloned:\n $ cd ${WORKDIR} $ helm install weblogic-kubernetes-operator charts/weblogic-operator \\ --namespace operator-ns \\ --set image=oracle/weblogic-kubernetes-operator:3.3.0 \\ --set serviceAccount=operator-sa \\ --set \u0026quot;domainNamespaces={}\u0026quot; \\ --wait 4.3 Verify the WebLogic Kubernetes operator   Verify that the operator’s pod is running by listing the pods in the operator’s namespace. You should see one for the operator:\n$ kubectl get pods -n operator-ns   Verify that the operator is up and running by viewing the operator pod\u0026rsquo;s logs:\n$ kubectl logs -n operator-ns -c weblogic-operator deployments/weblogic-operator   The WebLogic Kubernetes operator v3.3.0 has been installed. Continue with the load balancer and Oracle WebCenter Portal domain setup.\n5. Install the Traefik (ingress-based) load balancer The WebLogic Kubernetes Operator supports three load balancers: Traefik, NGINX and Apache. Samples are provided in the documentation.\nThis Quick Start demonstrates how to install the Traefik ingress controller to provide load balancing for an Oracle WebCenter Portal domain.\n  Create a namespace for Traefik:\n$ kubectl create namespace traefik   Set up Helm for 3rd party services:\n$ helm repo add traefik https://containous.github.io/traefik-helm-chart   Install the Traefik operator in the traefik namespace with the provided sample values:\n$ cd ${WORKDIR} $ helm install traefik traefik/traefik \\ --namespace traefik \\ --values charts/traefik/values.yaml \\ --set \u0026quot;kubernetes.namespaces={traefik}\u0026quot; \\ --set \u0026quot;service.type=NodePort\u0026quot; \\ --wait   6. Create and configure an Oracle WebCenter Portal domain 6.1 Prepare for an Oracle WebCenter Portal domain   Create a namespace that can host Oracle WebCenter Portal domain:\n$ kubectl create namespace wcpns   Use Helm to configure the operator to manage Oracle WebCenter Portal domain in this namespace:\n$ cd ${WORKDIR} $ helm upgrade weblogic-kubernetes-operator charts/weblogic-operator \\ --reuse-values \\ --namespace operator-ns \\ --set \u0026quot;domainNamespaces={wcpns}\u0026quot; \\ --wait   Create Kubernetes secrets.\na. Create a Kubernetes secret for the domain in the same Kubernetes namespace as the domain. In this example, the username is weblogic, the password is welcome1, and the namespace is wcpns:\n$ cd ${WORKDIR}/create-weblogic-domain-credentials $ sh create-weblogic-credentials.sh -u weblogic -p welcome1 -n wcpns -d wcp-domain -s wcp-domain-domain-credentials b. Create a Kubernetes secret for the RCU in the same Kubernetes namespace as the domain:\n Schema user : WCP1 Schema password : Oradoc_db1 DB sys user password : Oradoc_db1 Domain name : wcp-domain Domain Namespace : wcpns Secret name : wcp-domain-rcu-credentials  $ cd ${WORKDIR}/create-rcu-credentials $ sh create-rcu-credentials.sh -u WCP1 -p Oradoc_db1 -a sys -q Oradoc_db1 -n wcpns -d wcp-domain -s wcp-domain-rcu-credentials   Create the Kubernetes persistence volume and persistence volume claim.\na. Create the Oracle WebCenter Portal domain home directory. Determine if a user already exists on your host system with uid:gid of 1000:\n$ sudo getent passwd 1000 If this command returns a username (which is the first field), you can skip the following useradd command. If not, create the oracle user with useradd:\n$ sudo useradd -u 1000 -g 1000 oracle Create the directory that will be used for the Oracle WebCenter Portal domain home:\n$ sudo mkdir /scratch/k8s_dir $ sudo chown -R 1000:1000 /scratch/k8s_dir b. Update create-pv-pvc-inputs.yaml with the following values:\n baseName: domain domainUID: wcp-domain namespace: wcpns weblogicDomainStoragePath: /scratch/k8s_dir  $ cd ${WORKDIR}/create-weblogic-domain-pv-pvc $ cp create-pv-pvc-inputs.yaml create-pv-pvc-inputs.yaml.orig $ sed -i -e \u0026quot;s:baseName\\: weblogic-sample:baseName\\: domain:g\u0026quot; create-pv-pvc-inputs.yaml $ sed -i -e \u0026quot;s:domainUID\\::domainUID\\: wcp-domain:g\u0026quot; create-pv-pvc-inputs.yaml $ sed -i -e \u0026quot;s:namespace\\: default:namespace\\: wcpns:g\u0026quot; create-pv-pvc-inputs.yaml $ sed -i -e \u0026quot;s:#weblogicDomainStoragePath\\: /scratch/k8s_dir:weblogicDomainStoragePath\\: /scratch/k8s_dir:g\u0026quot; create-pv-pvc-inputs.yaml c. Run the create-pv-pvc.sh script to create the PV and PVC configuration files:\n$ ./create-pv-pvc.sh -i create-pv-pvc-inputs.yaml -o output d. Create the PV and PVC using the configuration files created in the previous step:\n $ kubectl create -f output/pv-pvcs/wcp-domain-domain-pv.yaml $ kubectl create -f output/pv-pvcs/wcp-domain-domain-pvc.yaml   Install and configure the database for the Oracle WebCenter Portal domain.\nThis step is required only when a standalone database is not already set up and you want to use the database in a container.\nThe Oracle Database Docker images are supported only for non-production use. For more details, see My Oracle Support note: Oracle Support for Database Running on Docker (Doc ID 2216342.1). For production, it is suggested to use a standalone database. This example provides steps to create the database in a container.\n a. Create a database in a container:\n$ cd ${WORKDIR}/create-oracle-db-service $ ./start-db-service.sh -i container-registry.oracle.com/database/enterprise:12.2.0.1-slim -p none Once the database is successfully created, you can use the database connection string oracle-db.default.svc.cluster.local:1521/devpdb.k8s as an rcuDatabaseURL parameter in the create-domain-inputs.yaml file.\nb. Create Oracle WebCenter Portal schemas.\nTo create the Oracle WebCenter Portal schemas, run the following commands:\n $ ./create-rcu-schema.sh \\ -s WCP1 \\ -t wcp \\ -d oracle-db.default.svc.cluster.local:1521/devpdb.k8s \\ -i oracle/wcportal:12.2.1.4\\ -n wcpns \\ -q Oradoc_db1 \\ -r welcome1   Now the environment is ready to start the Oracle WebCenter Portal domain creation.\n6.2 Create an Oracle WebCenter Portal domain   The sample scripts for Oracle WebCenter Portal domain deployment are available at create-wcp-domain. You must edit create-domain-inputs.yaml (or a copy of it) to provide the details for your domain.\nUpdate create-domain-inputs.yaml with the following values for domain creation:\n rcuDatabaseURL: oracle-db.default.svc.cluster.local:1521/devpdb.k8s    Run the create-domain.sh script to create a domain:\n$ cd ${WORKDIR}/create-wcp-domain/domain-home-on-pv/ $ ./create-domain.sh -i create-domain-inputs.yaml -o output   Create a Kubernetes domain object:\nOnce the create-domain.sh is successful, it generates output/weblogic-domains/wcp-domain/domain.yaml, which you can use to create the Kubernetes resource domain to start the domain and servers:\n$ cd ${WORKDIR}/create-wcp-domain/domain-home-on-pv $ kubectl create -f output/weblogic-domains/wcp-domain/domain.yaml   Verify that the Kubernetes domain object named wcp-domain is created:\n$ kubectl get domain -n wcpns NAME AGE wcp-domain 3m18s   Once you create the domain, the introspect pod is created. This inspects the domain home and then starts the wcp-domain-adminserver pod. Once the wcp-domain-adminserver pod starts successfully, the Managed Server pods are started in parallel. Watch the wcpns namespace for the status of domain creation:\n$ kubectl get pods -n wcpns -w   Verify that the Oracle WebCenter Portal domain server pods and services are created and in Ready state:\n$ kubectl get all -n wcpns   6.3 Configure Traefik to access Oracle WebCenter Portal domain services   Configure Traefik to manage ingresses created in the Oracle WebCenter Portal domain namespace (wcpns):\n$ helm upgrade traefik traefik/traefik \\ --reuse-values \\ --namespace traefik \\ --set \u0026quot;kubernetes.namespaces={traefik,wcpns}\u0026quot; \\ --wait   Create an ingress for the domain in the domain namespace by using the sample Helm chart:\n$ cd ${WORKDIR} helm install wcp-traefik-ingress \\ charts/ingress-per-domain \\ --namespace wcpns \\ --values charts/ingress-per-domain/values.yaml \\ --set \u0026quot;traefik.hostname=$(hostname -f)\u0026quot;   Verify the created ingress per domain details:\n$ kubectl describe ingress wcp-domain-traefik -n wcpns   6.4 Verify that you can access the Oracle WebCenter Portal domain URL   Get the LOADBALANCER_HOSTNAME for your environment:\nexport LOADBALANCER_HOSTNAME=$(hostname -f)   Verify the following URLs are available for Oracle WebCenter Portal domain.\nCredentials:\nusername: weblogic password: welcome1\nhttp://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/webcenter http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/console http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/em http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/rsscrawl http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/rest http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/webcenterhelp   "
},
{
	"uri": "/fmw-kubernetes/wcsites-domains/appendix/quickstart-deployment-on-prem/",
	"title": "Quick start deployment on-premise",
	"tags": [],
	"description": "Describes how to quickly get an Oracle WebCenter Sites domain instance running (using the defaults, nothing special) for development and test purposes.",
	"content": "Use this Quick Start to create an Oracle WebCenter Sites domain deployment in a Kubernetes cluster (on-premise environments) with the Oracle WebLogic Server Kubernetes operator. Note that this walkthrough is for demonstration purposes only, not for use in production. These instructions assume that you are already familiar with Kubernetes. If you need more detailed instructions, refer to the Install Guide.\nHardware requirements Supported Linux kernel for deploying and running Oracle WebCenter Sites domains with the operator is Oracle Linux 7 UL6+ and Red Hat Enterprise Linux 7 (UL3+). Refer to the prerequisites for more details.\nFor this exercise the minimum hardware requirement to create a single node Kubernetes cluster and then deploy wcsitesinfra domain type with one managed server for Oracle WebCenter Sites and Oracle Database running as a container\n   Hardware Size     RAM 32GB   Disk Space 250GB+   CPU core(s) 6    See here for resourse sizing information for Oracle WebCenter Sites domains setup on Kubernetes cluster.\nSet up Oracle WebCenter Sites in an on-premise environment Perform the steps in this topic to create a single instance on-premise Kubernetes cluster and then create an Oracle WebCenter Sites wcsitesinfra domain type.\n Step 1 - Prepare a virtual machine for the Kubernetes cluster Step 2 - Set up a single instance Kubernetes cluster Step 3 - Get scripts and images Step 4 - Install the WebLogic Kubernetes Operator Step 5 - Install the Traefik (ingress-based) load balancer Step 6 - Create and configure an Oracle WebCenter Sites Domain  1. Prepare a virtual machine for the Kubernetes cluster For illustration purposes, these instructions are for Oracle Linux 7u6+. If you are using a different flavor of Linux, you will need to adjust the steps accordingly.\nThese steps must be run with the root user, unless specified otherwise. Any time you see YOUR_USERID in a command, you should replace it with your actual userid.\n 1.1 Prerequisites   Choose the directories where your Docker and Kubernetes files will be stored. The Docker directory should be on a disk with a lot of free space (more than 100GB) because it will be used for the Docker file system, which contains all of your images and containers. The Kubernetes directory is used for the /var/lib/kubelet file system and persistent volume storage.\n$ export docker_dir=/u01/docker $ export kubelet_dir=/u01/kubelet $ mkdir -p $docker_dir $kubelet_dir $ ln -s $kubelet_dir /var/lib/kubelet   Verify that IPv4 forwarding is enabled on your host.\nNote: Replace eth0 with the ethernet interface name of your compute resource if it is different.\n$ /sbin/sysctl -a 2\u0026gt;\u0026amp;1|grep -s 'net.ipv4.conf.docker0.forwarding' $ /sbin/sysctl -a 2\u0026gt;\u0026amp;1|grep -s 'net.ipv4.conf.eth0.forwarding' $ /sbin/sysctl -a 2\u0026gt;\u0026amp;1|grep -s 'net.ipv4.conf.lo.forwarding' $ /sbin/sysctl -a 2\u0026gt;\u0026amp;1|grep -s 'net.ipv4.ip_nonlocal_bind' For example: Verify that all are set to 1\n$ net.ipv4.conf.docker0.forwarding = 1 $ net.ipv4.conf.eth0.forwarding = 1 $ net.ipv4.conf.lo.forwarding = 1 $ net.ipv4.ip_nonlocal_bind = 1 Solution: Set all values to 1 immediately with the following commands:\n$ /sbin/sysctl net.ipv4.conf.docker0.forwarding=1 $ /sbin/sysctl net.ipv4.conf.eth0.forwarding=1 $ /sbin/sysctl net.ipv4.conf.lo.forwarding=1 $ /sbin/sysctl net.ipv4.ip_nonlocal_bind=1 To preserve the settings post-reboot: Update the above values to 1 in files in /usr/lib/sysctl.d/, /run/sysctl.d/, and /etc/sysctl.d/\n  Verify the iptables rule for forwarding.\nKubernetes uses iptables to handle many networking and port forwarding rules. A standard Docker installation may create a firewall rule that prevents forwarding.\nVerify if the iptables rule to accept forwarding traffic is set:\n$ /sbin/iptables -L -n | awk '/Chain FORWARD / {print $4}' | tr -d \u0026quot;)\u0026quot; If the output is \u0026ldquo;DROP\u0026rdquo;, then run the following command:\n$ /sbin/iptables -P FORWARD ACCEPT Verify if the iptables rule is set properly to \u0026ldquo;ACCEPT\u0026rdquo;:\n$ /sbin/iptables -L -n | awk '/Chain FORWARD / {print $4}' | tr -d \u0026quot;)\u0026quot;   Disable and stop firewalld:\n$ systemctl disable firewalld $ systemctl stop firewalld   1.2 Install and configure Docker  Note : If you have already installed Docker with version 18.03+ and configured Docker daemon root to sufficient disk space along with proxy settings, continue to Install and configure Kubernetes\n   Make sure that you have the right operating system version:\n$ uname -a $ more /etc/oracle-release For example:\nLinux xxxxxxx 4.1.12-124.27.1.el7uek.x86_64 #2 SMP Mon May 13 08:56:17 PDT 2019 x86_64 x86_64 x86_64 GNU/Linux Oracle Linux Server release 7.6   Install the latest docker-engine and start the Docker service:\n$ yum-config-manager --enable ol7_addons $ yum install docker-engine $ systemctl enable docker $ systemctl start docker   Add your userid to the Docker group. This will allow you to run the Docker commands without root access:\n$ /sbin/usermod -a -G docker \u0026lt;YOUR_USERID\u0026gt;   Check your Docker version. It must be at least 18.03.\n$ docker version For example:\nClient: Docker Engine - Community Version: 19.03.1-ol API version: 1.40 Go version: go1.12.5 Git commit: ead9442 Built: Wed Sep 11 06:40:28 2019 OS/Arch: linux/amd64 Experimental: false Server: Docker Engine - Community Engine: Version: 19.03.1-ol API version: 1.40 (minimum version 1.12) Go version: go1.12.5 Git commit: ead9442 Built: Wed Sep 11 06:38:43 2019 OS/Arch: linux/amd64 Experimental: false Default Registry: docker.io containerd: Version: v1.2.0-rc.0-108-gc444666 GitCommit: c4446665cb9c30056f4998ed953e6d4ff22c7c39 runc: Version: 1.0.0-rc5+dev GitCommit: 4bb1fe4ace1a32d3676bb98f5d3b6a4e32bf6c58 docker-init: Version: 0.18.0 GitCommit: fec3683   Update the Docker engine configuration:\n$ mkdir -p /etc/docker $ cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/docker/daemon.json { \u0026quot;group\u0026quot;: \u0026quot;docker\u0026quot;, \u0026quot;data-root\u0026quot;: \u0026quot;/u01/docker\u0026quot; } EOF   Configure proxy settings if you are behind an HTTP proxy. On some hosts /etc/systemd/system/docker.service.d may not be available. Create this directory if it is not available.\n ### Create the drop-in file /etc/systemd/system/docker.service.d/http-proxy.conf that contains proxy details: $ cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/systemd/system/docker.service.d/http-proxy.conf [Service] Environment=\u0026quot;HTTP_PROXY=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT\u0026quot; Environment=\u0026quot;HTTPS_PROXY=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT\u0026quot; Environment=\u0026quot;NO_PROXY=localhost,127.0.0.0/8,ADD-YOUR-INTERNAL-NO-PROXY-LIST,/var/run/docker.sock\u0026quot; EOF   Restart the Docker daemon to load the latest changes:\n$ systemctl daemon-reload $ systemctl restart docker   Verify that the proxy is configured with Docker:\n$ docker info|grep -i proxy For example:\nHTTP Proxy: http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT HTTPS Proxy: http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT No Proxy: localhost,127.0.0.0/8,ADD-YOUR-INTERNAL-NO-PROXY-LIST,/var/run/docker.sock   Verify Docker installation:\n$ docker run hello-world For example:\nHello from Docker! This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the \u0026quot;hello-world\u0026quot; image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal. To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bash Share images, automate workflows, and more with a free Docker ID: https://hub.docker.com/ For more examples and ideas, visit: https://docs.docker.com/get-started/   1.3 Install and configure Kubernetes   Add the external Kubernetes repository:\n$ cat \u0026lt;\u0026lt;EOF | sudo tee /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\\$basearch enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg exclude=kubelet kubeadm kubectl EOF   Set SELinux in permissive mode (effectively disabling it):\n$ export PATH=/sbin:$PATH $ setenforce 0 $ sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config   Export proxy and install kubeadm, kubelet, and kubectl:\n### Get the nslookup IP address of the master node to use with apiserver-advertise-address during setting up Kubernetes master ### as the host may have different internal ip (hostname -i) and nslookup $HOSTNAME $ ip_addr=`nslookup $(hostname -f) | grep -m2 Address | tail -n1| awk -F: '{print $2}'| tr -d \u0026quot; \u0026quot;` $ echo $ip_addr ### Set the proxies $ export NO_PROXY=localhost,127.0.0.0/8,ADD-YOUR-INTERNAL-NO-PROXY-LIST,/var/run/docker.sock,$ip_addr $ export no_proxy=localhost,127.0.0.0/8,ADD-YOUR-INTERNAL-NO-PROXY-LIST,/var/run/docker.sock,$ip_addr $ export http_proxy=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT $ export https_proxy=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT $ export HTTPS_PROXY=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT $ export HTTP_PROXY=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT ### install kubernetes 1.18.4-1 $ VERSION=1.18.4-1 $ yum install -y kubelet-$VERSION kubeadm-$VERSION kubectl-$VERSION --disableexcludes=kubernetes ### enable kubelet service so that it auto-restart on reboot $ systemctl enable --now kubelet   Ensure net.bridge.bridge-nf-call-iptables is set to 1 in your sysctl to avoid traffic routing issues:\n$ cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF $ sysctl --system   Disable swap check:\n$ sed -i 's/KUBELET_EXTRA_ARGS=/KUBELET_EXTRA_ARGS=\u0026quot;--fail-swap-on=false\u0026quot;/' /etc/sysconfig/kubelet $ cat /etc/sysconfig/kubelet ### Reload and restart kubelet $ systemctl daemon-reload $ systemctl restart kubelet   1.4 Set up Helm   Install Helm v3.x.\na. Download Helm from https://github.com/helm/helm/releases. Example to download Helm v3.2.4:\n$ wget https://get.helm.sh/helm-v3.2.4-linux-amd64.tar.gz b. Unpack tar.gz:\n$ tar -zxvf helm-v3.2.4-linux-amd64.tar.gz c. Find the Helm binary in the unpacked directory, and move it to its desired destination:\n$ mv linux-amd64/helm /usr/bin/helm   Run helm version to verify its installation:\n$ helm version version.BuildInfo{Version:\u0026quot;v3.2.4\u0026quot;, GitCommit:\u0026quot;0ad800ef43d3b826f31a5ad8dfbb4fe05d143688\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;, GoVersion:\u0026quot;go1.13.12\u0026quot;}   2. Set up a single instance Kubernetes cluster  Notes:\n These steps must be run with the root user, unless specified otherwise! If you choose to use a different cidr block (that is, other than 10.244.0.0/16 for the --pod-network-cidr= in the kubeadm init command), then also update NO_PROXY and no_proxy with the appropriate value.  Also make sure to update kube-flannel.yaml with the new value before deploying.   Replace the following with appropriate values:  ADD-YOUR-INTERNAL-NO-PROXY-LIST REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT     2.1 Set up the master node   Create a shell script that sets up the necessary environment variables. You can append this to the user’s .bashrc so that it will run at login. You must also configure your proxy settings here if you are behind an HTTP proxy:\n## grab my IP address to pass into kubeadm init, and to add to no_proxy vars ip_addr=`nslookup $(hostname -f) | grep -m2 Address | tail -n1| awk -F: '{print $2}'| tr -d \u0026quot; \u0026quot;` export pod_network_cidr=\u0026quot;10.244.0.0/16\u0026quot; export service_cidr=\u0026quot;10.96.0.0/12\u0026quot; export PATH=$PATH:/sbin:/usr/sbin ### Set the proxies export NO_PROXY=localhost,127.0.0.0/8,ADD-YOUR-INTERNAL-NO-PROXY-LIST,/var/run/docker.sock,$ip_addr,$pod_network_cidr,$service_cidr export no_proxy=localhost,127.0.0.0/8,ADD-YOUR-INTERNAL-NO-PROXY-LIST,/var/run/docker.sock,$ip_addr,$pod_network_cidr,$service_cidr export http_proxy=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT export https_proxy=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT export HTTPS_PROXY=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT export HTTP_PROXY=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT   Source the script to set up your environment variables:\n$ . ~/.bashrc   To implement command completion, add the following to the script:\n$ [ -f /usr/share/bash-completion/bash_completion ] \u0026amp;\u0026amp; . /usr/share/bash-completion/bash_completion $ source \u0026lt;(kubectl completion bash)   Run kubeadm init to create the master node:\n$ kubeadm init \\ --pod-network-cidr=$pod_network_cidr \\ --apiserver-advertise-address=$ip_addr \\ --ignore-preflight-errors=Swap \u0026gt; /tmp/kubeadm-init.out 2\u0026gt;\u0026amp;1   Log in to the terminal with YOUR_USERID:YOUR_GROUP. Then set up the ~/.bashrc similar to steps 1 to 3 with YOUR_USERID:YOUR_GROUP.\n Note that from now on we will be using YOUR_USERID:YOUR_GROUP to execute any kubectl commands and not root.\n   Set up YOUR_USERID:YOUR_GROUP to access the Kubernetes cluster:\n$ mkdir -p $HOME/.kube $ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config $ sudo chown $(id -u):$(id -g) $HOME/.kube/config   Verify that YOUR_USERID:YOUR_GROUP is set up to access the Kubernetes cluster using the kubectl command:\n$ kubectl get nodes  Note: At this step, the node is not in ready state as we have not yet installed the pod network add-on. After the next step, the node will show status as Ready.\n   Install a pod network add-on (flannel) so that your pods can communicate with each other.\n Note: If you are using a different cidr block than 10.244.0.0/16, then download and update kube-flannel.yml with the correct cidr address before deploying into the cluster:\n $ kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.12.0/Documentation/kube-flannel.yml   Verify that the master node is in Ready status:\n$ kubectl get nodes For example:\nNAME STATUS ROLES AGE VERSION mymasternode Ready master 8m26s v1.18.4 or:\n$ kubectl get pods -n kube-system For example:\nNAME READY STATUS RESTARTS AGE pod/coredns-86c58d9df4-58p9f 1/1 Running 0 3m59s pod/coredns-86c58d9df4-mzrr5 1/1 Running 0 3m59s pod/etcd-mymasternode 1/1 Running 0 3m4s pod/kube-apiserver-node 1/1 Running 0 3m21s pod/kube-controller-manager-mymasternode 1/1 Running 0 3m25s pod/kube-flannel-ds-amd64-6npx4 1/1 Running 0 49s pod/kube-proxy-4vsgm 1/1 Running 0 3m59s pod/kube-scheduler-mymasternode 1/1 Running 0 2m58s   To schedule pods on the master node, taint the node:\n$ kubectl taint nodes --all node-role.kubernetes.io/master-   Congratulations! Your Kubernetes cluster environment is ready to deploy your Oracle WebCenter Sites domain.\nFor additional references on Kubernetes cluster setup, check the cheat sheet.\n3. Get scripts and images 3.1 Set up the code repository to deploy Oracle WebCenter Sites domains Follow these steps to set up the source code repository required to deploy Oracle WebCenter Sites domains.\n3.2 Get required Docker images and add them to your local registry   Obtain the Oracle WebLogic Operator image from the Oracle Container Registry:\na. For first time users, to pull an image from the Oracle Container Registry, navigate to https://container-registry.oracle.com and log in using the Oracle Single Sign-On (SSO) authentication service. If you do not already have SSO credentials, click the Sign In link at the top of the page to create them.\nUse the web interface to accept the Oracle Standard Terms and Restrictions for the Oracle software images that you intend to deploy. Your acceptance of these terms are stored in a database that links the software images to your Oracle Single Sign-On login credentials.\nTo obtain the image, log in to the Oracle Container Registry:\n$ docker login container-registry.oracle.com b. Pull the operator image:\n$ docker pull container-registry.oracle.com/middleware/weblogic-kubernetes-operator:3.3.0 $ docker tag container-registry.oracle.com/middleware/weblogic-kubernetes-operator:3.3.0 oracle/weblogic-kubernetes-operator:3.3.0   Build Oracle WebCenter Sites 12.2.1.4 Image by following steps 4A, 4C, 4D and 5 from this document\n  Copy all the above built and pulled images to all the nodes in your cluster or add to a Docker registry that your cluster can access.\n  3.3 Set Up the Code Repository to Deploy Oracle WebCenter Sites Domain Oracle WebCenter Sites domain deployment on Kubernetes leverages the Oracle WebLogic Kubernetes Operator infrastructure. For deploying the Oracle WebCenter Sites domain, you need to set up the deployment scripts as below:\n  Create a working directory to set up the source code.\n$ mkdir ${WORKDIR} $ cd ${WORKDIR}   Download the WebCenter Sites kubernetes deployment scripts from this repository and copy them in to WebLogic operator samples location.\n$ git clone https://github.com/oracle/fmw-kubernetes.git   You can now use the deployment scripts from \u0026lt;work directory\u0026gt;/fmw-kubernetes/OracleWebCenterSites to set up the WebCenter Sites domain as further described in this document.\nThis will be your home directory for runnning all the required scripts.\n$ cd ${WORKDIR}/fmw-kubernetes/OracleWebCenterSites 4. Install the WebLogic Kubernetes Operator 4.1 Prepare for the WebLogic Kubernetes Operator.   Create a namespace operator-ns for the operator WORKDIR for the Oracle WebCenter Sites operator code:\n$ kubectl create namespace operator-ns   Create a service account operator-sa for the operator in the operator’s namespace:\n$ kubectl create serviceaccount -n operator-ns operator-sa   4.2 Install the WebLogic Kubernetes Operator Use Helm to install and start the operator from the directory you just cloned:\n $ cd ${WORKDIR}/fmw-kubernetes/OracleWebCenterSites $ helm install weblogic-kubernetes-operator kubernetes/charts/weblogic-operator \\ --namespace operator-ns \\ --set image=oracle/weblogic-kubernetes-operator:3.3.0 \\ --set serviceAccount=operator-sa \\ --set \u0026quot;domainNamespaces={}\u0026quot; \\ --wait 4.3 Verify the WebLogic Kubernetes Operator   Verify that the operator’s pod is running by listing the pods in the operator’s namespace. You should see one for the operator:\n$ kubectl get pods -n operator-ns   Verify that the operator is up and running by viewing the operator pod\u0026rsquo;s logs:\n$ kubectl logs -n operator-ns -c weblogic-operator deployments/weblogic-operator   The WebLogic Kubernetes Operator v3.3.0 has been installed. Continue with the load balancer and Oracle WebCenter Sites domain setup.\n5. Install the Traefik (ingress-based) load balancer The Oracle WebLogic Server Kubernetes operator supports two load balancers: Traefik, Nginx. Samples are provided in the documentation.\nThis Quick Start demonstrates how to install the Traefik ingress controller to provide load balancing for an Oracle WebCenter Sites domain.\n  Create a namespace for Traefik:\n$ kubectl create namespace traefik   Set up Helm for 3rd party services:\n$ helm repo add traefik https://containous.github.io/traefik-helm-chart   Install the Traefik operator in the traefik namespace with the provided sample values:\n$ cd ${WORKDIR}/fmw-kubernetes/OracleWebCenterSites $ helm install traefik traefik/traefik \\ --namespace traefik \\ --values kubernetes/charts/traefik/values.yaml \\ --set \u0026quot;kubernetes.namespaces={traefik}\u0026quot; \\ --set \u0026quot;service.type=NodePort\u0026quot; \\ --wait   6. Create and configure an Oracle WebCenter Sites domain 6.1 Prepare for an Oracle WebCenter Sites domain   Create a namespace that can host Oracle WebCenter Sites domains:\n$ kubectl create namespace wcsites-ns   Use Helm to configure the operator to manage Oracle WebCenter Sites domains in this namespace:\n$ cd ${WORKDIR}/fmw-kubernetes/OracleWebCenterSites $ helm upgrade weblogic-kubernetes-operator kubernetes/charts/weblogic-operator \\ --reuse-values \\ --namespace operator-ns \\ --set \u0026quot;domainNamespaces={wcsites-ns}\u0026quot; \\ --wait   Create Kubernetes secrets.\na. Create a Kubernetes secret for the domain in the same Kubernetes namespace as the domain. In this example, the username is weblogic, the password in Welcome1, and the namespace is wcsites-ns:\n$ cd ${WORKDIR}/fmw-kubernetes/OracleWebCenterSites/kubernetes/create-weblogic-domain-credentials $ ./create-weblogic-credentials.sh \\ -u weblogic \\ -p Welcome1 \\ -n wcsites-ns \\ -d wcsitesinfra \\ -s wcsitesinfra-domain-credentials b. Create a Kubernetes secret for the RCU in the same Kubernetes namespace as the domain:\n Schema user : WCS1 Schema password : Oradoc_db1 DB sys user password : Oradoc_db1 Domain name : wcsitesinfra Domain Namespace : wcsites-ns Secret name : wcsitesinfra-rcu-credentials  $ cd ${WORKDIR}/fmw-kubernetes/OracleWebCenterSites/kubernetes/create-rcu-credentials $ ./create-rcu-credentials.sh \\ -u WCS1 \\ -p Oradoc_db1 \\ -a sys \\ -q Oradoc_db1 \\ -d wcsitesinfra \\ -n wcsites-ns \\ -s wcsitesinfra-rcu-credentials   Create the Kubernetes persistence volume and persistence volume claim.\na. Create the Oracle WebCenter Sites domain home directory. Determine if a user already exists on your host system with uid:gid of 1000:\n$ sudo getent passwd 1000 If this command returns a username (which is the first field), you can skip the following useradd command. If not, create the oracle user with useradd:\n$ sudo useradd -u 1000 -g 1000 oracle Create the directory that will be used for the Oracle WebCenter Sites domain home:\n$ sudo mkdir /scratch/K8SVolume $ sudo chown -R 1000:1000 /scratch/K8SVolume b. Update create-pv-pvc-inputs.yaml with the following values:\n baseName: domain domainUID: wcsitesinfra namespace: wcsites-ns weblogicDomainStoragePath: /scratch/K8SVolume  $ cd ${WORKDIR}/fmw-kubernetes/OracleWebCenterSites/kubernetes/create-weblogic-domain-pv-pvc $ cp create-pv-pvc-inputs.yaml create-pv-pvc-inputs.yaml.orig $ sed -i -e \u0026quot;s:baseName\\: weblogic-sample:baseName\\: domain:g\u0026quot; create-pv-pvc-inputs.yaml $ sed -i -e \u0026quot;s:domainUID\\::domainUID\\: wcsitesinfra:g\u0026quot; create-pv-pvc-inputs.yaml $ sed -i -e \u0026quot;s:namespace\\: default:namespace\\: wcsites-ns:g\u0026quot; create-pv-pvc-inputs.yaml $ sed -i -e \u0026quot;s:#weblogicDomainStoragePath\\: /scratch/K8SVolume:weblogicDomainStoragePath\\: /scratch/K8SVolume:g\u0026quot; create-pv-pvc-inputs.yaml c. Run the create-pv-pvc.sh script to create the PV and PVC configuration files:\n$ ./create-pv-pvc.sh -i create-pv-pvc-inputs.yaml -o output d. Create the PV and PVC using the configuration files created in the previous step:\n$ kubectl create -f output/pv-pvcs/wcsitesinfra-domain-pv.yaml $ kubectl create -f output/pv-pvcs/wcsitesinfra-domain-pvc.yaml   Install and configure the database for the Oracle WebCenter Sites domain.\nThis step is required only when a standalone database is not already set up and you want to use the database in a container.\nThe Oracle Database Docker images are supported only for non-production use. For more details, see My Oracle Support note: Oracle Support for Database Running on Docker (Doc ID 2216342.1). For production, it is suggested to use a standalone database. This example provides steps to create the database in a container.\n   Now the environment is ready to start the Oracle WebCenter Sites domain creation.\n6.2 Create an Oracle WebCenter Sites domain   The sample scripts for Oracle WebCenter Sites domain deployment are available at \u0026lt;weblogic-kubernetes-operator-project\u0026gt;/kubernetes/create-wcsites-domain. You must edit create-domain-inputs.yaml (or a copy of it) to provide the details for your domain.\n  Run the create-domain.sh script to create a domain:\n$ cd ${WORKDIR}/fmw-kubernetes/OracleWebCenterSites/kubernetes/create-wcsites-domain/domain-home-on-pv/ $ ./create-domain.sh -i create-domain-inputs.yaml -o output   Create a Kubernetes domain object:\nOnce the create-domain.sh is successful, it generates the output/weblogic-domains/wcsitesinfra/domain.yaml that you can use to create the Kubernetes resource domain, which starts the domain and servers:\n$ cd ${WORKDIR}/fmw-kubernetes/OracleWebCenterSites/kubernetes/create-wcsites-domain/domain-home-on-pv $ kubectl create -f output/weblogic-domains/wcsitesinfra/domain.yaml   Verify that the Kubernetes domain object named wcsitesinfra is created:\n$ kubectl get domain -n wcsites-ns NAME AGE wcsitesinfra 3m18s   Once you create the domain, introspect pod is created. This inspects the domain home and then starts the wcsitesinfra-adminserver pod. Once the wcsitesinfra-adminserver pod starts successfully, then the Managed Server pods are started in parallel. Watch the wcsites-ns namespace for the status of domain creation:\n$ kubectl get pods -n wcsites-ns -w   Verify that the Oracle WebCenter Sites domain server pods and services are created and in Ready state:\n$ kubectl get all -n wcsites-ns   6.3 Configure Traefik to access in Oracle WebCenter Sites domain services   Configure Traefik to manage ingresses created in the Oracle WebCenter Sites domain namespace (wcsites-ns):\n$ helm upgrade traefik traefik/traefik \\ --reuse-values \\ --namespace traefik \\ --set \u0026quot;kubernetes.namespaces={traefik,wcsites-ns}\u0026quot; \\ --wait   Create an ingress for the domain in the domain namespace by using the sample Helm chart:\n$ cd ${WORKDIR}/fmw-kubernetes/OracleWebCenterSites $ helm install wcsitesinfra-ingress kubernetes/create-wcsites-domain/ingress-per-domain \\ --namespace wcsites-ns \\ --values kubernetes/create-wcsites-domain/ingress-per-domain/values.yaml \\ --set \u0026quot;traefik.hostname=$(hostname -f)\u0026quot; \\   Verify the created ingress per domain details:\n$ kubectl describe ingress wcsitesinfra-ingress -n wcsites-ns   6.4 Verify that you can access the Oracle WebCenter Sites domain URL   Get the LOADBALANCER_HOSTNAME for your environment:\nexport LOADBALANCER_HOSTNAME=$(hostname -f)   The following URLs are available for Oracle WebCenter Sites domains of domain type wcsitesinfra:\nCredentials: username: weblogic password: Welcome1\nhttp://${LOADBALANCER_HOSTNAME}:30305/console http://${LOADBALANCER_HOSTNAME}:30305/em http://${LOADBALANCER_HOSTNAME}:30305/sites   "
},
{
	"uri": "/fmw-kubernetes/wcsites-domains/oracle-cloud/bastion/",
	"title": "Preparing the Bastion host",
	"tags": [],
	"description": "Running WebLogic Kubernetes Operator managed Oracle WebCenter Sites domains on OKE",
	"content": "STEP 1 : Create Public Security List Create Public Security List (bastion_public_sec_list) in same VCN as that of OKE Cluster for Bastion Node\n Ingress Rules as: (where 10.0.22.0/24 is the CIDR planned to be used for bastion subnet)  Egress as:   STEP 2 : Create Private Security List Create Private Security List (bastion_private_sec_list) in same VCN as that of OKE Cluster which will be added into Worker Node subnet.\n Ingress Rules as: (where 10.0.22.0/24 is the CIDR planned to be used for bastion subnet)  Egress Rules as:   STEP 3 : Create Route Table Create Route Table (oke-bastion-routetables) with below details which will be used for bastion subnet STEP 4 : Create Bastion Subnet Create Bastion Subnet with CIDR Block : 10.0.22.0/24 , RouteTable: oke-bastion-routetables (created in step 3) , Security List: bastion_public_sec_list ( created in Step 1) and DHCP Options : Default available STEP 5 : Add Private Security to Worker Subnet for bastion access Add the private security list (bastion_private_sec_list), created at Step 2 to Worker Subnet, so that bastion node can ssh to Worker Nodes STEP 6 : Create Bastion Node Create Bastion Node with Subnet as \u0026ldquo;bastion-subnet\u0026rdquo;, created at Step 4, Add the private security list (bastion_private_sec_list), created at Step 2 to Worker Subnet, so that bastion node can ssh to Worker Nodes\n Update Name for the instance, Chose the Operating System Image, Availability Domain and Instance Type  Select the Compartment, VCN and Subnet Compartment where Cluster is created. Select the regional bastion-subnet created at Step4. Make sure to click on \u0026ldquo;Assign a public IP address\u0026rdquo;.  Once the bastion is created as shown below   STEP 7 : Access Worker Node from bastion host a. Login to bastion host\nscp -i id_rsa id_rsa opc@\u0026lt;bastion-host-address\u0026gt;:/home/opc ssh -i id_rsa opc@\u0026lt;bastion-host-address\u0026gt; b. Place a copy of id_rsa in bastion node to access worker node\nssh -i id_rsa opc@10.0.1.5 More details refer: https://docs.cloud.oracle.com/en-us/iaas/Content/Resources/Assets/whitepapers/bastion-hosts.pdf\n"
},
{
	"uri": "/fmw-kubernetes/soa-domains/installguide/prepare-your-environment/",
	"title": "Prepare your environment",
	"tags": [],
	"description": "Prepare for creating Oracle SOA Suite domains, including required secrets creation, persistent volume and volume claim creation, database creation, and database schema creation.",
	"content": "To prepare your Oracle SOA Suite in Kubernetes environment, complete the following steps.\nRefer to the troubleshooting page to troubleshoot issues during the domain deployment process.\n   Set up your Kubernetes cluster\n  Install Helm\n  Get dependent images\n  Set up the code repository to deploy Oracle SOA Suite domains\n  Obtain the Oracle SOA Suite Docker image\n  Install the WebLogic Kubernetes Operator\n  Prepare the environment for Oracle SOA Suite domains\na. Create a namespace for an Oracle SOA Suite domain\nb. Create a persistent storage for an Oracle SOA Suite domain\nc. Create a Kubernetes secret with domain credentials\nd. Create a Kubernetes secret with the RCU credentials\ne. Configure access to your database\nf. Run the Repository Creation Utility to set up your database schemas\n  Create an Oracle SOA Suite domain\n  Set up your Kubernetes cluster Refer the official Kubernetes set up documentation to set up a production grade Kubernetes cluster.\nInstall Helm The operator uses Helm to create and deploy the necessary resources and then run the operator in a Kubernetes cluster. For Helm installation and usage information, see here.\nGet dependent images Obtain dependent images and add them to your local registry.\n  For first time users, to pull an image from the Oracle Container Registry, navigate to https://container-registry.oracle.com and log in using the Oracle Single Sign-On (SSO) authentication service. If you do not already have an SSO account, you can create an Oracle Account here.\nUse the web interface to accept the Oracle Standard Terms and Restrictions for the Oracle software images that you intend to deploy. Your acceptance of these terms are stored in a database that links the software images to your Oracle Single Sign-On login credentials.\nLog in to the Oracle Container Registry (container-registry.oracle.com) from your Docker client:\n$ docker login container-registry.oracle.com   Pull the operator image:\n$ docker pull ghcr.io/oracle/weblogic-kubernetes-operator:3.4.0 $ docker tag ghcr.io/oracle/weblogic-kubernetes-operator:3.4.0 oracle/weblogic-kubernetes-operator:3.4.0   Set up the code repository to deploy Oracle SOA Suite domains Oracle SOA Suite domain deployment on Kubernetes leverages the WebLogic Kubernetes Operator infrastructure. To deploy an Oracle SOA Suite domain, you must set up the deployment scripts.\n  Create a working directory to set up the source code:\n$ mkdir $HOME/soa_22.2.2 $ cd $HOME/soa_22.2.2   Download the WebLogic Kubernetes Operator source code and Oracle SOA Suite Kubernetes deployment scripts from the SOA repository. Required artifacts are available at OracleSOASuite/kubernetes.\n$ git clone https://github.com/oracle/fmw-kubernetes.git $ export WORKDIR=$HOME/soa_22.2.2/fmw-kubernetes/OracleSOASuite/kubernetes   Obtain the Oracle SOA Suite Docker image The Oracle SOA Suite image with latest bundle patch and required interim patches can be obtained from My Oracle Support (MOS). This is the only image supported for production deployments. Follow the below steps to download the Oracle SOA Suite image from My Oracle Support.\n  Download patch 34077593 from My Oracle Support (MOS).\n  Unzip the downloaded patch zip file.\n  Load the image archive using the docker load command.\nFor example:\n$ docker load \u0026lt; soasuite-12.2.1.4-jdk8-ol7-220420.2140.tar Loaded image: oracle/soasuite:12.2.1.4-jdk8-ol7-220420.2140 $   Run the docker inspect command to verify that the downloaded image is the latest released image. The value of label com.oracle.weblogic.imagetool.buildid must match to 43e56369-4b97-4c40-93b7-60b17912f31b.\nFor example:\n$ docker inspect --format=\u0026#39;{{ index .Config.Labels \u0026#34;com.oracle.weblogic.imagetool.buildid\u0026#34; }}\u0026#39; oracle/soasuite:12.2.1.4-jdk8-ol7-220420.2140 43e56369-4b97-4c40-93b7-60b17912f31b $   If you want to build and use an Oracle SOA Suite Docker image with any additional bundle patch or interim patches that are not part of the image obtained from My Oracle Support, then follow these steps to create the image.\n Note: The default Oracle SOA Suite image name used for Oracle SOA Suite domains deployment is soasuite:12.2.1.4. The image obtained must be tagged as soasuite:12.2.1.4 using the docker tag command. If you want to use a different name for the image, make sure to update the new image tag name in the create-domain-inputs.yaml file and also in other instances where the soasuite:12.2.1.4 image name is used.\n Install the WebLogic Kubernetes Operator The WebLogic Kubernetes Operator supports the deployment of Oracle SOA Suite domains in the Kubernetes environment. Follow the steps in this document to install the operator.\n Note: Optionally, you can execute these steps to send the contents of the operator’s logs to Elasticsearch.\n In the following example commands to install the WebLogic Kubernetes Operator, opns is the namespace and op-sa is the service account created for the Operator:\n$ kubectl create namespace opns $ kubectl create serviceaccount -n opns op-sa $ cd ${WORKDIR} $ helm install weblogic-kubernetes-operator charts/weblogic-operator --namespace opns --set image=oracle/weblogic-kubernetes-operator:3.4.0 --set serviceAccount=op-sa --set \u0026quot;domainNamespaces={}\u0026quot; --set \u0026quot;javaLoggingLevel=FINE\u0026quot; --wait Prepare the environment for Oracle SOA Suite domains Create a namespace for an Oracle SOA Suite domain Create a Kubernetes namespace (for example, soans) for the domain unless you intend to use the default namespace. Use the new namespace in the remaining steps in this section. For details, see Prepare to run a domain.\n$ kubectl create namespace soans $ helm upgrade --reuse-values --namespace opns --set \u0026quot;domainNamespaces={soans}\u0026quot; --wait weblogic-kubernetes-operator charts/weblogic-operator Create a persistent storage for an Oracle SOA Suite domain In the Kubernetes namespace you created, create the PV and PVC for the domain by running the create-pv-pvc.sh script. Follow the instructions for using the script to create a dedicated PV and PVC for the Oracle SOA Suite domain.\n  Review the configuration parameters for PV creation here. Based on your requirements, update the values in the create-pv-pvc-inputs.yaml file located at ${WORKDIR}/create-weblogic-domain-pv-pvc/. Sample configuration parameter values for an Oracle SOA Suite domain are:\n baseName: domain domainUID: soainfra namespace: soans weblogicDomainStorageType: HOST_PATH weblogicDomainStoragePath: /scratch/k8s_dir/SOA    Ensure that the path for the weblogicDomainStoragePath property exists and have the ownership for 1000:0. If not, you need to create it as follows:\n$ sudo mkdir /scratch/k8s_dir/SOA $ sudo chown -R 1000:0 /scratch/k8s_dir/SOA   Run the create-pv-pvc.sh script:\n$ cd ${WORKDIR}/create-weblogic-domain-pv-pvc $ ./create-pv-pvc.sh -i create-pv-pvc-inputs.yaml -o output_soainfra   The create-pv-pvc.sh script will create a subdirectory pv-pvcs under the given /path/to/output-directory directory and creates two YAML configuration files for PV and PVC. Apply these two YAML files to create the PV and PVC Kubernetes resources using the kubectl create -f command:\n$ kubectl create -f output_soainfra/pv-pvcs/soainfra-domain-pv.yaml $ kubectl create -f output_soainfra/pv-pvcs/soainfra-domain-pvc.yaml   Create a Kubernetes secret with domain credentials Create the Kubernetes secrets username and password of the administrative account in the same Kubernetes namespace as the domain:\n $ cd ${WORKDIR}/create-weblogic-domain-credentials $ ./create-weblogic-credentials.sh -u weblogic -p Welcome1 -n soans -d soainfra -s soainfra-domain-credentials For more details, see this document.\nYou can check the secret with the kubectl get secret command.\nFor example:\n  Click here to see the sample secret description.   $ kubectl get secret soainfra-domain-credentials -o yaml -n soans apiVersion: v1 data: password: T3JhZG9jX2RiMQ== sys_password: T3JhZG9jX2RiMQ== sys_username: c3lz username: U09BMQ== kind: Secret metadata: creationTimestamp: \u0026quot;2020-06-25T14:08:16Z\u0026quot; labels: weblogic.domainName: soainfra weblogic.domainUID: soainfra managedFields: - apiVersion: v1 fieldsType: FieldsV1 fieldsV1: f:data: .: {} f:password: {} f:sys_password: {} f:sys_username: {} f:username: {} f:metadata: f:labels: .: {} f:weblogic.domainName: {} f:weblogic.domainUID: {} f:type: {} manager: kubectl operation: Update time: \u0026quot;2020-06-25T14:08:16Z\u0026quot; name: soainfra-rcu-credentials namespace: soans resourceVersion: \u0026quot;265386\u0026quot; selfLink: /api/v1/namespaces/soans/secrets/soainfra-rcu-credentials uid: 2d93941c-656b-43a4-8af2-78ca8be0f293 type: Opaque    Create a Kubernetes secret with the RCU credentials You also need to create a Kubernetes secret containing the credentials for the database schemas. When you create your domain, it will obtain the RCU credentials from this secret.\nUse the provided sample script to create the secret:\n$ cd ${WORKDIR}/create-rcu-credentials $ ./create-rcu-credentials.sh \\  -u SOA1 \\  -p Oradoc_db1 \\  -a sys \\  -q Oradoc_db1 \\  -d soainfra \\  -n soans \\  -s soainfra-rcu-credentials The parameter values are:\n -u username for schema owner (regular user), required. -p password for schema owner (regular user), required. -a username for SYSDBA user, required. -q password for SYSDBA user, required. -d domainUID. Example: soainfra -n namespace. Example: soans -s secretName. Example: soainfra-rcu-credentials  You can confirm the secret was created as expected with the kubectl get secret command.\nFor example:\n  Click here to see the sample secret description.   $ kubectl get secret soainfra-rcu-credentials -o yaml -n soans apiVersion: v1 data: password: T3JhZG9jX2RiMQ== sys_password: T3JhZG9jX2RiMQ== sys_username: c3lz username: U09BMQ== kind: Secret metadata: creationTimestamp: \u0026#34;2020-06-25T14:08:16Z\u0026#34; labels: weblogic.domainName: soainfra weblogic.domainUID: soainfra managedFields: - apiVersion: v1 fieldsType: FieldsV1 fieldsV1: f:data: .: {} f:password: {} f:sys_password: {} f:sys_username: {} f:username: {} f:metadata: f:labels: .: {} f:weblogic.domainName: {} f:weblogic.domainUID: {} f:type: {} manager: kubectl operation: Update time: \u0026#34;2020-06-25T14:08:16Z\u0026#34; name: soainfra-rcu-credentials namespace: soans resourceVersion: \u0026#34;265386\u0026#34; selfLink: /api/v1/namespaces/soans/secrets/soainfra-rcu-credentials uid: 2d93941c-656b-43a4-8af2-78ca8be0f293 type: Opaque    Configure access to your database Oracle SOA Suite domains require a database with the necessary schemas installed in them. The Repository Creation Utility (RCU) allows you to create those schemas. You must set up the database before you create your domain. There are no additional requirements added by running Oracle SOA Suite in Kubernetes; the same existing requirements apply.\nFor production deployments, you must set up and use the standalone (non-container) based database running outside of Kubernetes.\nBefore creating a domain, you will need to set up the necessary schemas in your database.\nRun the Repository Creation Utility to set up your database schemas Create schemas To create the database schemas for Oracle SOA Suite, run the create-rcu-schema.sh script.\nFor example:\n$ cd ${WORKDIR}/create-rcu-schema $ ./create-rcu-schema.sh -h usage: ./create-rcu-schema.sh -s \u0026lt;schemaPrefix\u0026gt; -t \u0026lt;schemaType\u0026gt; -d \u0026lt;dburl\u0026gt; -i \u0026lt;image\u0026gt; -u \u0026lt;imagePullPolicy\u0026gt; -p \u0026lt;docker-store\u0026gt; -n \u0026lt;namespace\u0026gt; -q \u0026lt;sysPassword\u0026gt; -r \u0026lt;schemaPassword\u0026gt; -o \u0026lt;rcuOutputDir\u0026gt; -c \u0026lt;customVariables\u0026gt; [-l] \u0026lt;timeoutLimit\u0026gt; [-h] -s RCU Schema Prefix (required) -t RCU Schema Type (optional) (supported values: osb,soa,soaosb) -d RCU Oracle Database URL (optional) (default: oracle-db.default.svc.cluster.local:1521/devpdb.k8s) -p OracleSOASuite ImagePullSecret (optional) (default: none) -i OracleSOASuite Image (optional) (default: soasuite:12.2.1.4) -u OracleSOASuite ImagePullPolicy (optional) (default: IfNotPresent) -n Namespace for RCU pod (optional) (default: default) -q password for database SYSDBA user. (optional) (default: Oradoc_db1) -r password for all schema owner (regular user). (optional) (default: Oradoc_db1) -o Output directory for the generated YAML file. (optional) (default: rcuoutput) -c Comma-separated custom variables in the format variablename=value. (optional). (default: none) -l Timeout limit in seconds. (optional). (default: 300) -h Help $ ./create-rcu-schema.sh \\  -s SOA1 \\  -t soaosb \\  -d oracle-db.default.svc.cluster.local:1521/devpdb.k8s \\  -i soasuite:12.2.1.4 \\  -n default \\  -q Oradoc_db1 \\  -r Oradoc_db1 \\  -c SOA_PROFILE_TYPE=SMALL,HEALTHCARE_INTEGRATION=NO For Oracle SOA Suite domains, the create-rcu-schema.sh script supports:\n domain types: soa, osb, and soaosb. You must specify one of these using the -t flag. For Oracle SOA Suite you must specify the Oracle SOA schema profile type using the -c flag. For example, -c SOA_PROFILE_TYPE=SMALL. Supported values for SOA_PROFILE_TYPE are SMALL, MED, and LARGE.   Note: To use the LARGE schema profile type, make sure that the partitioning feature is enabled in the Oracle Database.\n Make sure that you maintain the association between the database schemas and the matching domain just like you did in a non-Kubernetes environment. There is no specific functionality provided to help with this.\nDrop schemas If you want to drop a schema, you can use the drop-rcu-schema.sh script.\nFor example:\n$ cd ${WORKDIR}/create-rcu-schema $ ./drop-rcu-schema.sh -h usage: ./drop-rcu-schema.sh -s \u0026lt;schemaPrefix\u0026gt; -d \u0026lt;dburl\u0026gt; -n \u0026lt;namespace\u0026gt; -q \u0026lt;sysPassword\u0026gt; -r \u0026lt;schemaPassword\u0026gt; -c \u0026lt;customVariables\u0026gt; [-h] -s RCU Schema Prefix (required) -t RCU Schema Type (optional) (supported values: osb,soa,soaosb) -d Oracle Database URL (optional) (default: oracle-db.default.svc.cluster.local:1521/devpdb.k8s) -n Namespace where RCU pod is deployed (optional) (default: default) -q password for database SYSDBA user. (optional) (default: Oradoc_db1) -r password for all schema owner (regular user). (optional) (default: Oradoc_db1) -c Comma-separated custom variables in the format variablename=value. (optional). (default: none) -h Help $ ./drop-rcu-schema.sh \\  -s SOA1 \\  -t soaosb \\  -d oracle-db.default.svc.cluster.local:1521/devpdb.k8s \\  -n default \\  -q Oradoc_db1 \\  -r Oradoc_db1 \\  -c SOA_PROFILE_TYPE=SMALL,HEALTHCARE_INTEGRATION=NO For Oracle SOA Suite domains, the drop-rcu-schema.sh script supports:\n Domain types: soa, osb, and soaosb. You must specify one of these using the -t flag. For Oracle SOA Suite, you must specify the Oracle SOA schema profile type using the -c flag. For example, -c SOA_PROFILE_TYPE=SMALL. Supported values for SOA_PROFILE_TYPE are SMALL, MED, and LARGE.  Create an Oracle SOA Suite domain Now that you have your Docker images and you have created your RCU schemas, you are ready to create your domain. To continue, follow the instructions in Create Oracle SOA Suite domains.\n"
},
{
	"uri": "/fmw-kubernetes/wcsites-domains/installguide/prepare-your-environment/",
	"title": "Prepare your environment",
	"tags": [],
	"description": "Prepare for creating Oracle WebCenter Sites domains, including required secrets creation, persistent volume and volume claim creation, database creation, and database schema creation.",
	"content": "Contents This document describes the steps to set up the environment that includes setting up of a Kubernetes cluster and setting up the WebLogic Operator including the database.\n Introduction Set Up your Kubernetes Cluster Build Oracle WebCenter Sites Image Pull Other Dependent Images Set Up the Code Repository to Deploy Oracle WebCenter Sites Domain Grant Roles and Clear Stale Resources Install the WebLogic Kubernetes Operator Configure NFS Server Prepare the Environment for the WebCenter Sites Domain Configure access to your database  Introduction Set Up your Kubernetes Cluster If you need help in setting up a Kubernetes environment, check our cheat sheet.\nAfter creating Kubernetes clusters, you can optionally:\n Create load balancers to direct traffic to backend domains. Configure Kibana and Elasticsearch for your operator logs.  Build Oracle WebCenter Sites Image Build Oracle WebCenter Sites 12.2.1.4 Image by following steps 4A, 4C, 4D and 5 from this document.\nAlternatively, the Oracle WebCenter Sites Image with latest bundle patch can be obtained from My Oracle Support (MOS).\n  Download patch 34223930 from My Oracle Support (MOS).\n  Unzip the downloaded zip file.\nFor example:\n$ unzip p34223930_122140_Linux-x86-64.zip   Load the image archive using the docker load command.\nFor example:\n$ docker load \u0026lt; wcsites-12214-220419.tar.gz   View the image using the docker images command.\nFor example:\n$ docker images | grep wcsites oracle/wcsites 12.2.1.4-220419 ba99807045e0 3 weeks ago 3.88GB    Note: The default Oracle WebCenter Sites image name used for Oracle WebCenter Sites domains deployment is oracle/wcsites:12.2.1.4. The image obtained must be tagged as oracle/wcsites:12.2.1.4 using the docker tag command. If you want to use a different name for the image, make sure to update the new image tag name in the create-domain-inputs.yaml file and also in other instances where the oracle/wcsites:12.2.1.4 image name is used.\n Pull Other Dependent Images Dependent images include WebLogic Kubernetes Operator, and Traefik. Pull these images and add them to your local registry:\n Pull these Docker images and re-tag them as shown:  To pull an image from the Oracle Container Registry, in a web browser, navigate to https://container-registry.oracle.com and log in using the Oracle Single Sign-On authentication service. If you do not already have SSO credentials, at the top of the page, click the Sign In link to create them.\nUse the web interface to accept the Oracle Standard Terms and Restrictions for the Oracle software images that you intend to deploy. Your acceptance of these terms are stored in a database that links the software images to your Oracle Single Sign-On login credentials.\nThen, pull these Docker images and re-tag them:\ndocker login https://container-registry.oracle.com (enter your Oracle email Id and password) This step is required once at every node to get access to the Oracle Container Registry. WebLogic Kubernetes Operator image:\n$ docker pull container-registry.oracle.com/middleware/weblogic-kubernetes-operator:3.3.0 $ docker tag container-registry.oracle.com/middleware/weblogic-kubernetes-operator:3.3.0 oracle/weblogic-kubernetes-operator:3.3.0 Copy all the above built and pulled images to all the nodes in your cluster or add to a Docker registry that your cluster can access.  NOTE: If you\u0026rsquo;re not running Kubernetes on your development machine, you\u0026rsquo;ll need to make the Docker image available to a registry visible to your Kubernetes cluster. Upload your image to a machine running Docker and Kubernetes as follows:\n# on your build machine $ docker save Image_Name:Tag \u0026gt; Image_Name-Tag.tar $ scp Image_Name-Tag.tar YOUR_USER@YOUR_SERVER:/some/path/Image_Name-Tag.tar # on the Kubernetes server $ docker load \u0026lt; /some/path/Image_Name-Tag.tar Set Up the Code Repository to Deploy Oracle WebCenter Sites Domain Oracle WebCenter Sites domain deployment on Kubernetes leverages the Oracle WebLogic Kubernetes Operator infrastructure. For deploying the Oracle WebCenter Sites domain, you need to set up the deployment scripts as below:\n  Create a working directory to setup the source code.\n$ mkdir \u0026lt;work directory\u0026gt; $ cd \u0026lt;work directory\u0026gt;   Download the WebCenter Sites kubernetes deployment scripts from this repository .\n$ git clone https://github.com/oracle/fmw-kubernetes.git   You can now use the deployment scripts from \u0026lt;work directory\u0026gt;/fmw-kubernetes/OracleWebCenterSites to set up the WebCenter Sites domain as further described in this document.\nThis will be your home directory for runnning all the required scripts.\n$ cd \u0026lt;work directory\u0026gt;/fmw-kubernetes/OracleWebCenterSites Clear Stale Resources   To confirm if there is already a WebLogic custom resource definition, execute the following command:\n$ kubectl get crd NAME CREATED AT domains.weblogic.oracle 2020-03-14T12:10:21Z   If you find any WebLogic custom resource definition, then delete it by executing the following command:\n$ kubectl delete crd domains.weblogic.oracle customresourcedefinition.apiextensions.k8s.io \u0026#34;domains.weblogic.oracle\u0026#34; deleted   Install the WebLogic Kubernetes Operator   Create a namespace for the WebLogic Kubernetes Operator:\n$ kubectl create namespace operator-ns namespace/operator-ns created NOTE: For this exercise we are creating a namespace called \u0026ldquo;operator-ns\u0026rdquo; (can be any name).\nYou can also use:\n domainUID/domainname as wcsitesinfra Domain namespace as wcsites-ns Operator namespace as operator-ns traefik namespace as traefik    Create a service account for the WebLogic Kubernetes Operator in the Operator\u0026rsquo;s namespace:\n$ kubectl create serviceaccount -n operator-ns operator-sa serviceaccount/operator-sa created   To be able to set up the log-stash and Elasticsearch after creating the domain, set the value of the field elkIntegrationEnabled to true in the file kubernetes/charts/weblogic-operator/values.yaml.\n  Use helm to install and start the WebLogic Kubernetes Operator from the downloaded repository:\n Helm install weblogic-operator\n $ helm install weblogic-kubernetes-operator kubernetes/charts/weblogic-operator --namespace operator-ns --set image=oracle/weblogic-kubernetes-operator:3.3.0 --set serviceAccount=operator-sa --set \u0026#34;domainNamespaces={}\u0026#34; --wait NAME: weblogic-kubernetes-operator LAST DEPLOYED: Tue May 19 04:04:32 2020 NAMESPACE: opns STATUS: deployed REVISION: 1 TEST SUITE: None ``\n  To verify that the Operator\u0026rsquo;s pod is running, list the pods in the Operator\u0026rsquo;s namespace. You should see one for the WebLogic Kubernetes Operator:\n$ kubectl get pods -n operator-ns NAME READY STATUS RESTARTS AGE weblogic-operator-67df5fddc5-tlc4b 2/2 Running 0 3m15s   Then, check by viewing the Operator pod\u0026rsquo;s log as shown in the following sample log snippet:\n$ kubectl logs -n operator-ns -c weblogic-operator deployments/weblogic-operator Launching Oracle WebLogic Server Kubernetes Operator... Importing keystore /operator/internal-identity/temp/weblogic-operator.jks to /operator/internal-identity/temp/weblogic-operator.p12... Entry for alias weblogic-operator-alias successfully imported. Import command completed: 1 entries successfully imported, 0 entries failed or cancelled Warning: The -srcstorepass option is specified multiple times. All except the last one will be ignored. MAC verified OK % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 4249 0 2394 100 1855 6884 5334 --:--:-- --:--:-- --:--:-- 6899 % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 5558 0 3028 100 2530 22704 18970 --:--:-- --:--:-- --:--:-- 22766 OpenJDK 64-Bit Server VM warning: Option MaxRAMFraction was deprecated in version 10.0 and will likely be removed in a future release. VM settings: Max. Heap Size (Estimated): 14.08G Using VM: OpenJDK 64-Bit Server VM {\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:53.438+0000\u0026#34;,\u0026#34;thread\u0026#34;:1,\u0026#34;fiber\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.TuningParametersImpl\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;update\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168593438,\u0026#34;message\u0026#34;:\u0026#34;Reloading tuning parameters from Operator\u0026#39;s config map\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;} {\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:53.944+0000\u0026#34;,\u0026#34;thread\u0026#34;:1,\u0026#34;fiber\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.Main\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;main\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168593944,\u0026#34;message\u0026#34;:\u0026#34;Oracle WebLogic Server Kubernetes Operator, version: 3.3.0, implementation: master.4d4fe0a, build time: 2019-11-15T21:19:56-0500\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;} {\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:53.972+0000\u0026#34;,\u0026#34;thread\u0026#34;:11,\u0026#34;fiber\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.Main\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;begin\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168593972,\u0026#34;message\u0026#34;:\u0026#34;Operator namespace is: operator-ns\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;} {\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:54.009+0000\u0026#34;,\u0026#34;thread\u0026#34;:11,\u0026#34;fiber\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.Main\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;begin\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168594009,\u0026#34;message\u0026#34;:\u0026#34;Operator target namespaces are: operator-ns\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;} {\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:54.013+0000\u0026#34;,\u0026#34;thread\u0026#34;:11,\u0026#34;fiber\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.Main\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;begin\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168594013,\u0026#34;message\u0026#34;:\u0026#34;Operator service account is: operator-sa\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;}\t{\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:54.031+0000\u0026#34;,\u0026#34;thread\u0026#34;:11,\u0026#34;fiber\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.helpers.HealthCheckHelper\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;performK8sVersionCheck\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168594031,\u0026#34;message\u0026#34;:\u0026#34;Verifying Kubernetes minimum version\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;}\t{\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:54.286+0000\u0026#34;,\u0026#34;thread\u0026#34;:11,\u0026#34;fiber\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.helpers.ClientPool\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;getApiClient\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168594286,\u0026#34;message\u0026#34;:\u0026#34;The Kuberenetes Master URL is set to https://10.96.0.1:443\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;}\t{\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:54.673+0000\u0026#34;,\u0026#34;thread\u0026#34;:11,\u0026#34;fiber\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.helpers.HealthCheckHelper\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;createAndValidateKubernetesVersion\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168594673,\u0026#34;message\u0026#34;:\u0026#34;Kubernetes version is: v1.13.7\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;}\t{\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:55.259+0000\u0026#34;,\u0026#34;thread\u0026#34;:12,\u0026#34;fiber\u0026#34;:\u0026#34;engine-operator-thread-2-fiber-1\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.helpers.CrdHelper$CrdContext$CreateResponseStep\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;onSuccess\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168595259,\u0026#34;message\u0026#34;:\u0026#34;Create Custom Resource Definition: oracle.kubernetes.operator.calls.CallResponse@470b40c\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;}\t{\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:55.356+0000\u0026#34;,\u0026#34;thread\u0026#34;:16,\u0026#34;fiber\u0026#34;:\u0026#34;fiber-1-child-2\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.helpers.HealthCheckHelper\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;performSecurityChecks\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168595356,\u0026#34;message\u0026#34;:\u0026#34;Verifying that operator service account can access required operations on required resources in namespace operator-ns\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;}\t{\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:55.598+0000\u0026#34;,\u0026#34;thread\u0026#34;:18,\u0026#34;fiber\u0026#34;:\u0026#34;fiber-1-child-2\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.helpers.ConfigMapHelper$ScriptConfigMapContext$CreateResponseStep\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;onSuccess\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168595598,\u0026#34;message\u0026#34;:\u0026#34;Creating domain config map, operator-ns, for namespace: {1}.\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;}\t{\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:55.937+0000\u0026#34;,\u0026#34;thread\u0026#34;:21,\u0026#34;fiber\u0026#34;:\u0026#34;fiber-1\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;WARNING\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.utils.Certificates\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;getCertificate\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168595937,\u0026#34;message\u0026#34;:\u0026#34;Can\u0026#39;t read certificate at /operator/external-identity/externalOperatorCert\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\\njava.nio.file.NoSuchFileException: /operator/external-identity/externalOperatorCert\\n\\tat java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)\\n\\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)\\n\\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)\\n\\tat java.base/sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:215)\\n\\tat java.base/java.nio.file.Files.newByteChannel(Files.java:370)\\n\\tat java.base/java.nio.file.Files.newByteChannel(Files.java:421)\\n\\tat java.base/java.nio.file.Files.readAllBytes(Files.java:3205)\\n\\tat oracle.kubernetes.operator.utils.Certificates.getCertificate(Certificates.java:48)\\n\\tat oracle.kubernetes.operator.utils.Certificates.getOperatorExternalCertificateData(Certificates.java:39)\\n\\tat oracle.kubernetes.operator.rest.RestConfigImpl.getOperatorExternalCertificateData(RestConfigImpl.java:52)\\n\\tat oracle.kubernetes.operator.rest.RestServer.isExternalSslConfigured(RestServer.java:383)\\n\\tat oracle.kubernetes.operator.rest.RestServer.start(RestServer.java:199)\\n\\tat oracle.kubernetes.operator.Main.startRestServer(Main.java:353)\\n\\tat oracle.kubernetes.operator.Main.completeBegin(Main.java:198)\\n\\tat oracle.kubernetes.operator.Main$NullCompletionCallback.onCompletion(Main.java:701)\\n\\tat oracle.kubernetes.operator.work.Fiber.completionCheck(Fiber.java:475)\\n\\tat oracle.kubernetes.operator.work.Fiber.run(Fiber.java:448)\\n\\tat oracle.kubernetes.operator.work.ThreadLocalContainerResolver.lambda$wrapExecutor$0(ThreadLocalContainerResolver.java:87)\\n\\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\\n\\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\\n\\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\\n\\tat java.base/java.lang.Thread.run(Thread.java:834)\\n\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;} {\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:55.967+0000\u0026#34;,\u0026#34;thread\u0026#34;:21,\u0026#34;fiber\u0026#34;:\u0026#34;fiber-1\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.rest.RestServer\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;start\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168595967,\u0026#34;message\u0026#34;:\u0026#34;Did not start the external ssl REST server because external ssl has not been configured.\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;} {\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:57.910+0000\u0026#34;,\u0026#34;thread\u0026#34;:21,\u0026#34;fiber\u0026#34;:\u0026#34;fiber-1\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.rest.RestServer\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;start\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168597910,\u0026#34;message\u0026#34;:\u0026#34;Started the internal ssl REST server on https://0.0.0.0:8082/operator\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;}\t{\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:57.913+0000\u0026#34;,\u0026#34;thread\u0026#34;:21,\u0026#34;fiber\u0026#34;:\u0026#34;fiber-1\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.Main\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;markReadyAndStartLivenessThread\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168597913,\u0026#34;message\u0026#34;:\u0026#34;Starting Operator Liveness Thread\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;}   Configure NFS (Network File System) Server To configure NFS server, install the nfs-utils package preferably on Master node:\n$ sudo yum install nfs-utils To start the nfs-server service, and configure the service to start following a system reboot:\n$ sudo systemctl start nfs-server $ sudo systemctl enable nfs-server Create the directory you want to export as the NFS share, for example /scratch/K8SVolume:\n$ sudo mkdir -p /scratch/K8SVolume $ sudo chown -R 1000:1000 /scratch/K8SVolume host name or IP address of the NFS Server\nNote: Host name or IP address of the NFS Server and NFS Share path which is used when you create PV/PVC in further sections.\nPrepare the Environment for the WebCenter Sites Domain   Unless you would like to use the default namespace, create a Kubernetes namespace that can host one or more domains:\n$ kubectl create namespace wcsites-ns namespace/wcsites-ns created   To manage domains in this namespace, configure the Operator using helm:\n Helm upgrade weblogic-operator\n helm upgrade --reuse-values --set \u0026#34;domainNamespaces={wcsites-ns}\u0026#34; \\ --wait weblogic-kubernetes-operator kubernetes/charts/weblogic-operator --namespace operator-ns NAME: weblogic-kubernetes-operator LAST DEPLOYED: Tue May 19 04:06:23 2020 NAMESPACE: opns STATUS: deployed REVISION: 2 TEST SUITE: None ``\n  Create Kubernetes secrets:\na. Using the create-weblogic-credentials script, create a Kubernetes secret that contains the user name and password for the domain in the same Kubernetes namespace as the domain:\nOutput:\n$ sh kubernetes/create-weblogic-domain-credentials/create-weblogic-credentials.sh \\  -u weblogic -p Welcome1 -n wcsites-ns \\  -d wcsitesinfra -s wcsitesinfra-domain-credentials secret/wcsitesinfra-domain-credentials created secret/wcsitesinfra-domain-credentials labeled The secret wcsitesinfra-domain-credentials has been successfully created in the wcsites-ns namespace. Where:\n* weblogic is the weblogic username * Welcome1 is the weblogic password * wcsitesinfra is the domain name * wcsites-ns is the domain namespace * wcsitesinfra-domain-credentials is the secret name  Note: You can inspect the credentials as follows:\n$ kubectl get secret wcsitesinfra-domain-credentials -o yaml -n wcsites-ns b. Create a Kubernetes secret for the Repository Configuration Utility (user name and password) using the create-rcu-credentials.sh script in the same Kubernetes namespace as the domain:\nOutput:\n$ sh kubernetes/create-rcu-credentials/create-rcu-credentials.sh \\  -u WCS1 -p Oradoc_db1 -a sys -q Oradoc_db1 -n wcsites-ns \\  -d wcsitesinfra -s wcsitesinfra-rcu-credentials secret/wcsitesinfra-rcu-credentials created secret/wcsitesinfra-rcu-credentials labeled The secret wcsitesinfra-rcu-credentials has been successfully created in the wcsites-ns namespace. Where:\n* WCS1 is the schema user * Oradoc_db1 is the schema password * Oradoc_db1 is the database SYS users password * wcsitesinfra is the domain name * wcsites-ns is the domain namespace * wcsitesinfra-rcu-credentials is the secret name  Note: You can inspect the credentials as follows:\n$ kubectl get secret wcsitesinfra-rcu-credentials -o yaml -n wcsites-ns   Create a Kubernetes PV and PVC (Persistent Volume and Persistent Volume Claim):\na. Update the kubernetes/create-wcsites-domain/utils/create-wcsites-pv-pvc-inputs.yaml.\nReplace the token %NFS_SERVER% with the host name/IP of NFS Server created in Configure NFS Server section.\nIn the NFS Server, create a folder and grant permissions as given below:\n$ sudo rm -rf /scratch/K8SVolume/WCSites \u0026amp;\u0026amp; sudo mkdir -p /scratch/K8SVolume/WCSites \u0026amp;\u0026amp; sudo chown 1000:1000 /scratch/K8SVolume/WCSites Update the weblogicDomainStoragePath paramter with /scratch/K8SVolume/WCSites.\nb. Execute the create-pv-pvc.sh script to create the PV and PVC configuration files:\n$ sh kubernetes/create-weblogic-domain-pv-pvc/create-pv-pvc.sh \\  -i kubernetes/create-wcsites-domain/utils/create-wcsites-pv-pvc-inputs.yaml \\  -o kubernetes/create-wcsites-domain/output Input parameters being used export version=\u0026#34;create-weblogic-sample-domain-pv-pvc-inputs-v1\u0026#34; export baseName=\u0026#34;domain\u0026#34; export domainUID=\u0026#34;wcsitesinfra\u0026#34; export namespace=\u0026#34;wcsites-ns\u0026#34; export weblogicDomainStorageType=\u0026#34;HOST_PATH\u0026#34; export weblogicDomainStoragePath=\u0026#34;/scratch/K8SVolume/WCSites\u0026#34; export weblogicDomainStorageReclaimPolicy=\u0026#34;Retain\u0026#34; export weblogicDomainStorageSize=\u0026#34;10Gi\u0026#34; Generating kubernetes/create-wcsites-domain/output/pv-pvcs/wcsitesinfra-domain-pv.yaml Generating kubernetes/create-wcsites-domain/output/pv-pvcs/wcsitesinfra-domain-pvc.yaml The following files were generated: kubernetes/create-wcsites-domain/output/pv-pvcs/wcsitesinfra-domain-pv.yaml kubernetes/create-wcsites-domain/output/pv-pvcs/wcsitesinfra-domain-pvc.yaml Completed c. To create the PV and PVC, use kubectl create with output configuration files:\nOutput:\n$ kubectl create -f kubernetes/create-wcsites-domain/output/pv-pvcs/wcsitesinfra-domain-pv.yaml \\  -f kubernetes/create-wcsites-domain/output/pv-pvcs/wcsitesinfra-domain-pvc.yaml persistentvolume/wcsitesinfra-domain-pv created persistentvolumeclaim/wcsitesinfra-domain-pvc created Note: You can verify the PV and PV\u0026rsquo;s details as follows:\n$ kubectl describe pv wcsitesinfra-domain-pv -n wcsites-ns $ kubectl describe pvc wcsitesinfra-domain-pvc -n wcsites-ns   Label the nodes in the Kubernetes cluster for the targeted scheduling of the servers on particular nodes as needed:\nkubectl label node \u0026lt;node-name\u0026gt; name=abc Note: Here \u0026lt;node-name\u0026gt; is the node as displayed in the NAME field of kubectl get nodes command. abc is the label that we are defining. Label is a key, value pair and can be anything meaningful. The same should be used for nodeSelector.\nFor scheduling we can select these nodes based on the labels.\n  Configure access to your database Oracle WebCenter Sites domains require a database with the necessary schemas installed in them. The Repository Creation Utility (RCU) allows you to create those schemas. You must set up the database before you create your domain. There are no additional requirements added by running Oracle WebCenter Sites in Kubernetes; the same existing requirements apply.\nFor production deployments, you must set up and use the standalone (non-container) based database running outside of Kubernetes.\nBefore creating a domain, you will need to set up the necessary schemas in your database.\n"
},
{
	"uri": "/fmw-kubernetes/soa-domains/adminguide/deploying-composites/deploy-using-maven-ant/",
	"title": "Deploy using Maven and Ant",
	"tags": [],
	"description": "Deploy Oracle SOA Suite and Oracle Service Bus composite applications using the Maven and Ant based approach in an Oracle SOA Suite deployment.",
	"content": "Learn how to deploy Oracle SOA Suite and Oracle Service Bus composite applications using the Maven and Ant based approach in an Oracle SOA Suite in WebLogic Kubernetes Operator environment.\nBefore deploying composite applications, we need to create a Kubernetes pod in the same cluster where the Oracle SOA Suite domain is running, so that composite applications can be deployed using the internal Kubernetes Service for the Administration Server URL.\nPlace the SOA/Oracle Service Bus composite project at a share location (for example at /share/soa-deploy) mounted at /composites inside container. Make sure to provide oracle user ( uid: 1000 and gid: 0) permission to directory /share/soa-deploy, so that it is accessible and writable inside the container.\n$ sudo chown -R 1000:0 /share/soa-deploy Follow the steps in this section to create a container and then use it to deploy Oracle SOA Suite and Oracle Service Bus composite applications using Maven or Ant.\nCreate a composite deployment container Before creating a Kubernetes pod, make sure that the Oracle SOA Suite Docker image is available on a node, or you can create an image pull secret so that the pod can pull the Docker image on the host where it gets created.\n  Create an image pull secret to pull image soasuite:12.2.1.4 by the Kubernetes pod:\n$ kubectl create secret docker-registry image-secret -n soans --docker-server=your-registry.com --docker-username=xxxxxx --docker-password=xxxxxxx --docker-email=my@company.com   Create a PersistentVolume and PersistentVolumeClaim (soadeploy-pv.yaml and soadeploy-pvc.yaml) with sample composites for build and deploy placed at /share/soa-deploy.\na) Create a PersistentVolume with the sample provided (soadeploy-pv.yaml), which uses NFS (you can use hostPath or any other supported PV type):\napiVersion: v1 kind: PersistentVolume metadata: name: soadeploy-pv spec: storageClassName: soadeploy-storage-class capacity: storage: 10Gi accessModes: - ReadWriteMany # Valid values are Retain, Delete or Recycle persistentVolumeReclaimPolicy: Retain # hostPath: nfs: server: X.X.X.X path: \u0026quot;/share/soa-deploy\u0026quot; b) Apply the YAML:\n$ kubectl apply -f soadeploy-pv.yaml c) Create a PersistentVolumeClaim (soadeploy-pvc.yaml):\nkind: PersistentVolumeClaim apiVersion: v1 metadata: name: soadeploy-pvc namespace: soans spec: storageClassName: soadeploy-storage-class accessModes: - ReadWriteMany resources: requests: storage: 10Gi d) Apply the YAML:\n$ kubectl apply -f soadeploy-pvc.yaml   Create a composite deploy pod using soadeploy.yaml to mount the composites inside pod at /composites:\napiVersion: v1 kind: Pod metadata: labels: run: soadeploy name: soadeploy namespace: soans spec: imagePullSecrets: - name: image-secret containers: - image: soasuite:12.2.1.4 name: soadeploy env: - name: M2_HOME value: /u01/oracle/oracle_common/modules/org.apache.maven_3.2.5 command: [\u0026quot;/bin/bash\u0026quot;, \u0026quot;-c\u0026quot;, \u0026quot;echo 'export PATH=$PATH:$M2_HOME/bin' \u0026gt;\u0026gt; $HOME/.bashrc; sleep infinity\u0026quot;] imagePullPolicy: IfNotPresent volumeMounts: - name: mycomposite mountPath: /composites volumes: - name: mycomposite persistentVolumeClaim: claimName: soadeploy-pvc   Create the pod:\n$ kubectl apply -f soadeploy.yaml   Once the Kubernetes pod is deployed, exec into the pod to perform Maven/Ant based build and deploy:\n$ kubectl exec -it -n soans soadeploy -- bash   Maven based build and deploy  Note: Make sure to execute these commands inside the soadeploy pod.\n Set up proxy details for Maven to pull dependencies from the internet.\nIf your environment is not running behind a proxy, then skip this step. Otherwise, replace REPLACE-WITH-PROXY-HOST, REPLACE-WITH-PROXY-PORT and the value for nonProxyHosts attribute per your environment and create the settings.xml:\n$ mkdir $HOME/.m2 $ cat \u0026lt;\u0026lt;EOF \u0026gt; $HOME/.m2/settings.xml \u0026lt;settings\u0026gt; \u0026lt;proxies\u0026gt; \u0026lt;proxy\u0026gt; \u0026lt;active\u0026gt;true\u0026lt;/active\u0026gt; \u0026lt;protocol\u0026gt;http\u0026lt;/protocol\u0026gt; \u0026lt;host\u0026gt;REPLACE-WITH-PROXY-HOST\u0026lt;/host\u0026gt; \u0026lt;port\u0026gt;REPLACE-WITH-PROXY-PORT\u0026lt;/port\u0026gt; \u0026lt;nonProxyHosts\u0026gt;soainfra-cluster-soa-cluster|soainfra-adminserver\u0026lt;/nonProxyHosts\u0026gt; \u0026lt;/proxy\u0026gt; \u0026lt;/proxies\u0026gt; \u0026lt;/settings\u0026gt; EOF For Oracle SOA Suite composite applications   Set up the environment for Maven:\n#Perform Maven Sync $ cd /u01/oracle/oracle_common/plugins/maven/com/oracle/maven/oracle-maven-sync/12.2.1/ $ mvn install:install-file \\ -DpomFile=oracle-maven-sync-12.2.1.pom \\ -Dfile=oracle-maven-sync-12.2.1.jar #install Maven plugin $ mvn help:describe \\ -Dplugin=com.oracle.maven:oracle-maven-sync \\ -Ddetail #push libraries into internal repository $ mvn com.oracle.maven:oracle-maven-sync:push \\ -DoracleHome=/u01/oracle \\ -DtestingOnly=false $ mvn archetype:crawl \\ -Dcatalog=$HOME/.m2/archetype-catalog.xml \\ -DarchetypeArtifactId=oracle-soa-application \\ -DarchetypeVersion=12.2.1-4-0   Build the SOA Archive (SAR) for your sample deployment available at /composites/mavenproject/my-soa-app:\n$ cd /composites/mavenproject/my-soa-app $ mvn package The SAR will be generated at /composites/mavenproject/my-soa-app/my-project/target/sca_my-project.jar.\n  Deploy into the Oracle SOA Suite instance. For example, if the instance URL is http://soainfra-cluster-soa-cluster:8001 with credentials username: weblogic and password: Welcome1, enter the following commands:\n$ cd /composites/mavenproject/my-soa-app $ mvn pre-integration-test \\ -DoracleServerUrl=http://soainfra-cluster-soa-cluster:8001 \\ -DsarLocation=/composites/mavenproject/my-soa-app/my-project/target/sca_my-project.jar \\ -Doverwrite=true \\ -DforceDefault=true \\ -Dcomposite.partition=default \\ -Duser=weblogic -Dpassword=Welcome1   For Oracle Service Bus composite applications   Set up the environment for Maven:\n#Perform Maven Sync $ cd /u01/oracle/oracle_common/plugins/maven/com/oracle/maven/oracle-maven-sync/12.2.1/ $ mvn install:install-file \\ -DpomFile=oracle-maven-sync-12.2.1.pom \\ -Dfile=oracle-maven-sync-12.2.1.jar #push libraries into internal repository $ mvn com.oracle.maven:oracle-maven-sync:push \\ -DoracleHome=$ORACLE_HOME $ mvn archetype:crawl \\ -Dcatalog=$HOME/.m2/archetype-catalog.xml #Verify the mvn setup $ mvn help:describe \\ -DgroupId=com.oracle.servicebus.plugin \\ -DartifactId=oracle-servicebus-plugin \\ -Dversion=12.2.1-4-0   Build the Oracle Service Bus Archive (sbconfig.sbar)\nBuild sbconfig.sbar for your sample deployment, available at /composites/mavenproject/HelloWorldSB:\n$ cd /composites/mavenproject/HelloWorldSB $ mvn com.oracle.servicebus.plugin:oracle-servicebus-plugin:package The Oracle Service Bus Archive (SBAR) will be generated at /composites/mavenproject/HelloWorldSB/.data/maven/sbconfig.sbar.\n  Deploy the generated sbconfig.sbar into the Oracle Service Bus instance. For example, if the Administration URL is http://soainfra-adminserver:7001 with credentials username: weblogic and password: Welcome1, enter the following commands: :\n$ cd /composites/mavenproject/HelloWorldSB $ mvn pre-integration-test \\ -DoracleServerUrl=t3://soainfra-adminserver:7001 \\ -DoracleUsername=weblogic -DoraclePassword=Welcome1   Ant based build and deploy  Note: Make sure to execute these commands inside the soadeploy pod.\n For Oracle SOA Suite composite applications   Build an Oracle SOA Suite composite application using Ant. For example, if the composite application to be deployed is available at /composites/antproject/Project, enter the following commands:\n$ cd /u01/oracle/soa/bin $ ant -f ant-sca-package.xml \\ -DcompositeDir=/composites/antproject/Project \\ -DcompositeName=Project \\ -Drevision=0.1 The SOA Archive is generated at /composites/antproject/Project/deploy/sca_Project_rev0.1.jar, which will be used for deploying.\n  Deploy into the Oracle SOA Suite instance using Ant:\n$ cd /u01/oracle/soa/bin $ ant -f ant-sca-deploy.xml \\ -DserverURL=http://soainfra-cluster-soa-cluster:8001 \\ -DsarLocation=/composites/antproject/Project/deploy/sca_Project_rev0.1.jar \\ -Doverwrite=true \\ -Duser=weblogic -Dpassword=Welcome1   For Oracle Service Bus composite applications See Developing Services Using Oracle Service Bus to deploy Oracle Service Bus composite applications using Ant.\n"
},
{
	"uri": "/fmw-kubernetes/soa-domains/adminguide/enable-additional-url-access/",
	"title": "Enable additional URL access",
	"tags": [],
	"description": "Extend an existing ingress to enable additional application URL access for Oracle SOA Suite domains.",
	"content": "This section provides information about how to extend an existing ingress (Non-SSL and SSL termination) to enable additional application URL access for Oracle SOA Suite domains.\nThe ingress per domain created in the steps in Set up a load balancer exposes the application paths defined in template YAML files present at ${WORKDIR}/charts/ingress-per-domain/templates/.\nTo extend an existing ingress with additional application URL access:\n  Update the template YAML file at ${WORKDIR}/charts/ingress-per-domain/templates/ to define additional path rules.\nFor example, to extend an existing NGINX-based ingress with additional paths /path1 and /path2 of an Oracle SOA Suite cluster, update nginx-ingress-nonssl.yaml, nginx-ingress-ssl.yaml, or nginx-ingress-e2essl.yaml accordingly with additional paths:\n# Copyright (c) 2020, 2022, Oracle and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. {{- if eq .Values.type \u0026quot;NGINX\u0026quot; }} {{- if (eq .Values.sslType \u0026quot;NONSSL\u0026quot;) }} --- apiVersion: networking.k8s.io/v1 kind: Ingress . . spec: rules: - host: '{{ .Values.nginx.hostname }}' http: paths: # Add new paths -- start - path: /path1 backend: service: name: '{{ .Values.wlsDomain.domainUID }}-cluster-{{ .Values.wlsDomain.soaClusterName | lower | replace \u0026quot;_\u0026quot; \u0026quot;-\u0026quot; }}' port: number: {{ .Values.wlsDomain.soaManagedServerPort }} - path: /path2 backend: service: name: '{{ .Values.wlsDomain.domainUID }}-cluster-{{ .Values.wlsDomain.soaClusterName | lower | replace \u0026quot;_\u0026quot; \u0026quot;-\u0026quot; }}' port: number: {{ .Values.wlsDomain.soaManagedServerPort }} # Add new paths -- end - path: /console backend: . . {{- end }}   Get the Helm release name for the ingress installed in your domain namespace:\n$ helm ls -n \u0026lt;domain_namespace\u0026gt; For example, in the soans namespace:\n$ helm ls -n soans Sample output, showing the Helm release name for a NGINX-based ingress as soa-nginx-ingress:\nNAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION soa-nginx-ingress soans 1 2021-02-17 13:42:03.252742314 +0000 UTC deployed ingress-per-domain-0.1.0 1.0 $   To extend the existing ingress per domain with additional paths defined in the template YAML, use the helm upgrade command:\n$ cd ${WORKDIR} $ helm upgrade \u0026lt;helm_release_for_ingress\u0026gt; \\ charts/ingress-per-domain \\ --namespace \u0026lt;domain_namespace\u0026gt; \\ --reuse-values  Note: helm_release_for_ingress is the ingress name used in the corresponding helm install command for the ingress installation.\n Sample command for a NGINX-based ingress soa-nginx-ingress in the soans namespace:\n$ cd ${WORKDIR} $ helm upgrade soa-nginx-ingress \\ charts/ingress-per-domain \\ --namespace soans \\ --reuse-values This will upgrade the existing ingress to pick up the additional paths updated in the template YAML.\n  Verify that additional paths are updated into the existing ingress.\na. Get the existing ingress deployed in the domain namespace:\n$ kubectl get ingress -n \u0026lt;domain_namespace\u0026gt; For example, in the soans namespace:\n$ kubectl get ingress -n soans Sample output, showing the existing ingress as soainfra-nginx:\nNAME CLASS HOSTS ADDRESS PORTS AGE soainfra-nginx \u0026lt;none\u0026gt; domain1.org 10.109.211.160 80, 443 xxd b. Describe the ingress object and verify that new paths are available and pointing to desired backends.\nSample command and output, showing path and backend details for /path1 and /path2:\n$ kubectl describe ingress soainfra-nginx -n soans|grep path /path1 soainfra-cluster-soa-cluster:8001 (172.17.0.19:8001,172.17.0.20:8001) /path2 soainfra-cluster-soa-cluster:8001 (172.17.0.19:8001,172.17.0.20:8001)   "
},
{
	"uri": "/fmw-kubernetes/wcportal-domains/manage-wcportal-domains/monitoring-and-publishing-logs/",
	"title": "Monitor a domain and publish logs",
	"tags": [],
	"description": "Monitor Oracle WebCenter Portal and publishing logs to Elasticsearch.",
	"content": "  Monitor a WebCenter Portal domain  Monitor an WebCenter Portal instance using Prometheus and Grafana.\n Publish WebLogic Server logs into Elasticsearch  Publish WebLogic Server logs into Elasticsearch.\n "
},
{
	"uri": "/fmw-kubernetes/soa-domains/adminguide/configure-load-balancer/nginx/",
	"title": "NGINX",
	"tags": [],
	"description": "Configure the ingress-based NGINX load balancer for Oracle SOA Suite domains.",
	"content": "This section provides information about how to install and configure the ingress-based NGINX load balancer to load balance Oracle SOA Suite domain clusters. You can configure NGINX for non-SSL, SSL termination, and end-to-end SSL access of the application URL.\nFollow these steps to set up NGINX as a load balancer for an Oracle SOA Suite domain in a Kubernetes cluster:\nSee the official installation document for prerequisites.\n Install the NGINX load balancer for non-SSL and SSL termination configuration Generate secret for SSL access Install NGINX load balancer for end-to-end SSL configuration Configure NGINX to manage ingresses Verify domain application URL access Uninstall NGINX ingress Uninstall NGINX  To get repository information, enter the following Helm commands:\n$ helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx $ helm repo update Install the NGINX load balancer for non-SSL and SSL termination configuration   Deploy the ingress-nginx controller by using Helm on the domain namespace:\n$ helm install nginx-ingress -n soans \\  --set controller.service.type=NodePort \\  --set controller.admissionWebhooks.enabled=false \\  ingress-nginx/ingress-nginx    Click here to see the sample output.    NAME: nginx-ingress LAST DEPLOYED: Thu May 5 13:27:30 2022 NAMESPACE: soans STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: The ingress-nginx controller has been installed. Get the application URL by running these commands: export HTTP_NODE_PORT=$(kubectl --namespace soans get services -o jsonpath=\u0026quot;{.spec.ports[0].nodePort}\u0026quot; nginx-ingress-ingress-nginx-controller) export HTTPS_NODE_PORT=$(kubectl --namespace soans get services -o jsonpath=\u0026quot;{.spec.ports[1].nodePort}\u0026quot; nginx-ingress-ingress-nginx-controller) export NODE_IP=$(kubectl --namespace soans get nodes -o jsonpath=\u0026quot;{.items[0].status.addresses[1].address}\u0026quot;) echo \u0026quot;Visit http://$NODE_IP:$HTTP_NODE_PORT to access your application via HTTP.\u0026quot; echo \u0026quot;Visit https://$NODE_IP:$HTTPS_NODE_PORT to access your application via HTTPS.\u0026quot; An example ingress that makes use of the controller: apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: example namespace: foo spec: ingressClassName: nginx rules: - host: www.example.com http: paths: - pathType: Prefix backend: service: name: exampleService port: number: 80 path: / # This section is only required if TLS is to be enabled for the ingress tls: - hosts: - www.example.com secretName: example-tls If TLS is enabled for the ingress, a secret containing the certificate and key must also be provided: apiVersion: v1 kind: Secret metadata: name: example-tls namespace: foo data: tls.crt: \u0026lt;base64 encoded cert\u0026gt; tls.key: \u0026lt;base64 encoded key\u0026gt; type: kubernetes.io/tls      Generate secret for SSL access   For secured access (SSL and E2ESSL) to the Oracle SOA Suite application, create a certificate and generate secrets:\n$ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /tmp/tls1.key -out /tmp/tls1.crt -subj \u0026#34;/CN=domain1.org\u0026#34; $ kubectl -n soans create secret tls soainfra-tls-cert --key /tmp/tls1.key --cert /tmp/tls1.crt  Note: The value of CN is the host on which this ingress is to be deployed and secret name should be \u0026lt;domainUID\u0026gt;-tls-cert.\n   Install NGINX load balancer for end-to-end SSL configuration   Deploy the ingress-nginx controller by using Helm on the domain namespace:\n$ helm install nginx-ingress -n soans \\  --set controller.extraArgs.default-ssl-certificate=soans/soainfra-tls-cert \\  --set controller.service.type=NodePort \\  --set controller.admissionWebhooks.enabled=false \\  --set controller.extraArgs.enable-ssl-passthrough=true \\  ingress-nginx/ingress-nginx    Click here to see the sample output.   NAME: nginx-ingress LAST DEPLOYED: Thu May 5 12:21:50 2022 NAMESPACE: soans STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: The ingress-nginx controller has been installed. Get the application URL by running these commands: export HTTP_NODE_PORT=$(kubectl --namespace soans get services -o jsonpath=\u0026#34;{.spec.ports[0].nodePort}\u0026#34; nginx-ingress-ingress-nginx-controller) export HTTPS_NODE_PORT=$(kubectl --namespace soans get services -o jsonpath=\u0026#34;{.spec.ports[1].nodePort}\u0026#34; nginx-ingress-ingress-nginx-controller) export NODE_IP=$(kubectl --namespace soans get nodes -o jsonpath=\u0026#34;{.items[0].status.addresses[1].address}\u0026#34;) echo \u0026#34;Visit http://$NODE_IP:$HTTP_NODE_PORTto access your application via HTTP.\u0026#34; echo \u0026#34;Visit https://$NODE_IP:$HTTPS_NODE_PORTto access your application via HTTPS.\u0026#34; An example Ingress that makes use of the controller: apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: example namespace: foo spec: ingressClassName: nginx rules: - host: www.example.com http: paths: - pathType: Prefix backend: service: name: exampleService port: number: 80 path: / # This section is only required if TLS is to be enabled for the Ingress tls: - hosts: - www.example.com secretName: example-tls If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided: apiVersion: v1 kind: Secret metadata: name: example-tls namespace: foo data: tls.crt: \u0026lt;base64 encoded cert\u0026gt; tls.key: \u0026lt;base64 encoded key\u0026gt; type: kubernetes.io/tls      Check the status of the deployed ingress controller:\n$ kubectl --namespace soans get services | grep ingress-nginx-controller Sample output:\nnginx-ingress-ingress-nginx-controller NodePort 10.106.186.235 \u0026lt;none\u0026gt; 80:32125/TCP,443:31376/TCP 19m   Configure NGINX to manage ingresses   Choose an appropriate LOADBALANCER_HOSTNAME for accessing the Oracle SOA Suite domain application URLs.\n$ export LOADBALANCER_HOSTNAME=\u0026lt;LOADBALANCER_HOSTNAME\u0026gt; For example, if you are executing the commands from a master node terminal, where the master hostname is LOADBALANCER_HOSTNAME:\n$ export LOADBALANCER_HOSTNAME=$(hostname -f)   Create an ingress for the domain in the domain namespace by using the sample Helm chart. Here path-based routing is used for ingress. Sample values for default configuration are shown in the file ${WORKDIR}/charts/ingress-per-domain/values.yaml. By default, type is TRAEFIK , sslType is NONSSL, and domainType is soa. These values can be overridden by passing values through the command line or can be edited in the sample file values.yaml.\nIf needed, you can update the ingress YAML file to define more path rules (in section spec.rules.host.http.paths) based on the domain application URLs that need to be accessed. Update the template YAML file for the NGINX load balancer located at ${WORKDIR}/charts/ingress-per-domain/templates/nginx-ingress.yaml.\n Note: See here for all the configuration parameters.\n $ cd ${WORKDIR} $ helm install soa-nginx-ingress charts/ingress-per-domain \\  --namespace soans \\  --values charts/ingress-per-domain/values.yaml \\  --set \u0026#34;nginx.hostname=${LOADBALANCER_HOSTNAME}\u0026#34; \\  --set type=NGINX Sample output:\nNAME: soa-nginx-ingress LAST DEPLOYED: Fri Jul 24 09:34:03 2020 NAMESPACE: soans STATUS: deployed REVISION: 1 TEST SUITE: None   Install ingress-per-domain using Helm for SSL termination configuration:\n$ cd ${WORKDIR} $ helm install soa-nginx-ingress charts/ingress-per-domain \\  --namespace soans \\  --values charts/ingress-per-domain/values.yaml \\  --set \u0026#34;nginx.hostname=${LOADBALANCER_HOSTNAME}\u0026#34; \\  --set type=NGINX --set sslType=SSL Sample output:\nNAME: soa-nginx-ingress LAST DEPLOYED: Fri Jul 24 09:34:03 2020 NAMESPACE: soans STATUS: deployed REVISION: 1 TEST SUITE: None   Install ingress-per-domain using Helm for E2ESSL configuration.\n$ cd ${WORKDIR} $ helm install soa-nginx-ingress charts/ingress-per-domain \\  --namespace soans \\  --values charts/ingress-per-domain/values.yaml \\  --set type=NGINX --set sslType=E2ESSL Sample output:\nNAME: soa-nginx-ingress LAST DEPLOYED: Fri Jul 24 09:34:03 2020 NAMESPACE: soans STATUS: deployed REVISION: 1 TEST SUITE: None   For NONSSL access to the Oracle SOA Suite application, get the details of the services by the ingress:\n$ kubectl describe ingress soainfra-nginx -n soans    Click here to see the sample output of the services supported by the above deployed ingress.   Name: soainfra-nginx Namespace: soans Address: 100.111.150.225 Default backend: default-http-backend:80 (\u0026lt;error: endpoints \u0026quot;default-http-backend\u0026quot; not found\u0026gt;) Rules: Host Path Backends ---- ---- -------- domain1.org /console soainfra-adminserver:7001 (10.244.0.45:7001) /em soainfra-adminserver:7001 (10.244.0.45:7001) /weblogic/ready soainfra-adminserver:7001 (10.244.0.45:7001) / soainfra-cluster-soa-cluster:8001 (10.244.0.46:8001,10.244.0.47:8001) /soa-infra soainfra-cluster-soa-cluster:8001 (10.244.0.46:8001,10.244.0.47:8001) /soa/composer soainfra-cluster-soa-cluster:8001 (10.244.0.46:8001,10.244.0.47:8001) /integration/worklistapp soainfra-cluster-soa-cluster:8001 (10.244.0.46:8001,10.244.0.47:8001) Annotations: \u0026lt;none\u0026gt; Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal CREATE 2m32s nginx-ingress-controller Ingress soans/soainfra-nginx Normal UPDATE 94s nginx-ingress-controller Ingress soans/soainfra-nginx      For SSL access to the Oracle SOA Suite application, get the details of the services by the above deployed ingress:\n$ kubectl describe ingress soainfra-nginx -n soans    Click here to see the sample output of the services supported by the above deployed ingress.    Name: soainfra-nginx Namespace: soans Address: 100.111.150.225 Default backend: default-http-backend:80 (\u0026lt;error: endpoints \u0026quot;default-http-backend\u0026quot; not found\u0026gt;) TLS: soainfra-tls-cert terminates domain1.org Rules: Host Path Backends ---- ---- -------- domain1.org /console soainfra-adminserver:7001 (10.244.0.45:7001) /em soainfra-adminserver:7001 (10.244.0.45:7001) /weblogic/ready soainfra-adminserver:7001 (10.244.0.45:7001) / soainfra-cluster-soa-cluster:8001 (10.244.0.46:8001,10.244.0.47:8001) /soa-infra soainfra-cluster-soa-cluster:8001 (10.244.0.46:8001,10.244.0.47:8001) /soa/composer soainfra-cluster-soa-cluster:8001 (10.244.0.46:8001,10.244.0.47:8001) /integration/worklistapp soainfra-cluster-soa-cluster:8001 (10.244.0.46:8001,10.244.0.47:8001) Annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/configuration-snippet: more_set_input_headers \u0026quot;X-Forwarded-Proto: https\u0026quot;; more_set_input_headers \u0026quot;WL-Proxy-SSL: true\u0026quot;; nginx.ingress.kubernetes.io/ingress.allow-http: false Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal CREATE 3m47s nginx-ingress-controller Ingress soans/soainfra-nginx Normal UPDATE 3m25s nginx-ingress-controller Ingress soans/soainfra-nginx      For E2ESSL access to the Oracle SOA Suite application, get the details of the services by the above deployed ingress:\n$ kubectl describe ingress soainfra-nginx-e2essl -n soans    Click here to see the sample output of the services supported by the above deployed ingress.    Name: soainfra-nginx-e2essl-admin Namespace: soans Address: Default backend: default-http-backend:80 (\u0026lt;error: endpoints \u0026quot;default-http-backend\u0026quot; not found\u0026gt;) TLS: soainfra-tls-cert terminates admin.org Rules: Host Path Backends ---- ---- -------- admin.org soainfra-adminserver-nginx-ssl:7002 (10.244.0.247:7002) Annotations: kubernetes.io/ingress.class: nginx meta.helm.sh/release-name: soa-nginx-ingress meta.helm.sh/release-namespace: soans nginx.ingress.kubernetes.io/ssl-passthrough: true Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Sync 4s nginx-ingress-controller Scheduled for sync Name: soainfra-nginx-e2essl-soa Namespace: soans Address: Default backend: default-http-backend:80 (\u0026lt;error: endpoints \u0026quot;default-http-backend\u0026quot; not found\u0026gt;) TLS: soainfra-tls-cert terminates soa.org Rules: Host Path Backends ---- ---- -------- soa.org / soainfra-cluster-soa-cluster:8002 (10.244.0.249:8002) Annotations: kubernetes.io/ingress.class: nginx meta.helm.sh/release-name: soa-nginx-ingress meta.helm.sh/release-namespace: soans nginx.ingress.kubernetes.io/ssl-passthrough: true Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Sync 4s nginx-ingress-controller Scheduled for sync      Verify domain application URL access NONSSL configuration   Get the LOADBALANCER_NON_SSLPORT NodePort of NGINX using the command:\n$ LOADBALANCER_NON_SSLPORT=$(kubectl --namespace soans get services -o jsonpath=\u0026#34;{.spec.ports[0].nodePort}\u0026#34; nginx-ingress-ingress-nginx-controller) $ echo ${LOADBALANCER_NON_SSLPORT}   Verify that the Oracle SOA Suite domain application URLs are accessible through the LOADBALANCER_NON_SSLPORT:\nhttp://${LOADBALANCER_HOSTNAME}:${LOADBALANCER_NON_SSLPORT}/weblogic/ready http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER_NON_SSLPORT}/console http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER_NON_SSLPORT}/em http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER_NON_SSLPORT}/soa-infra http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER_NON_SSLPORT}/soa/composer http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER_NON_SSLPORT}/integration/worklistapp   SSL configuration   Get the LOADBALANCER_SSLPORT NodePort of NGINX using the command:\n$ LOADBALANCER_SSLPORT=$(kubectl --namespace soans get services -o jsonpath=\u0026#34;{.spec.ports[1].nodePort}\u0026#34; nginx-ingress-ingress-nginx-controller) $ echo ${LOADBALANCER_SSLPORT}   Verify that the Oracle SOA Suite domain application URLs are accessible through the LOADBALANCER_SSLPORT:\nhttps://${LOADBALANCER_HOSTNAME}:${LOADBALANCER_SSLPORT}/weblogic/ready https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER_SSLPORT}/console https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER_SSLPORT}/em https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER_SSLPORT}/soa-infra https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER_SSLPORT}/soa/composer https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER_SSLPORT}/integration/worklistapp   E2ESSL configuration   To access the SOA Suite domain application URLs from a remote browser, update the browser host config file /etc/hosts (In Windows, C:\\Windows\\System32\\Drivers\\etc\\hosts) with the IP address of the host on which the ingress is deployed with below entries:\nX.X.X.X admin.org X.X.X.X soa.org X.X.X.X osb.org  Note:\n The value of X.X.X.X is the host IP address on which this ingress is deployed. If you are behind any corporate proxy, make sure to update the browser proxy settings appropriately to access the host names updated /etc/hosts file.     Get the LOADBALANCER_SSLPORT NodePort of NGINX using the command:\n$ LOADBALANCER_SSLPORT=$(kubectl --namespace soans get services -o jsonpath=\u0026#34;{.spec.ports[1].nodePort}\u0026#34; nginx-ingress-ingress-nginx-controller) $ echo ${LOADBALANCER_SSLPORT}   Verify that the Oracle SOA Suite domain application URLs are accessible through LOADBALANCER_SSLPORT:\nhttps://admin.org:${LOADBALANCER_SSLPORT}/weblogic/ready https://admin.org:${LOADBALANCER_SSLPORT}/console https://admin.org:${LOADBALANCER_SSLPORT}/em https://soa.org:${LOADBALANCER_SSLPORT}/soa-infra https://soa.org:${LOADBALANCER_SSLPORT}/soa/composer https://soa.org:${LOADBALANCER_SSLPORT}/integration/worklistapp    Note: This is the default host name. If you have updated the host name in values.yaml, then use the updated values.\n Uninstall NGINX ingress Uninstall and delete the ingress-nginx deployment:\n$ helm delete soa-nginx-ingress -n soans Uninstall NGINX $ helm delete nginx-ingress -n soans "
},
{
	"uri": "/fmw-kubernetes/wcportal-domains/manage-wcportal-domains/configure-load-balancer/nginx/",
	"title": "NGINX",
	"tags": [],
	"description": "Configure the ingress-based NGINX load balancer for an Oracle WebCenter Portal domain.",
	"content": "To load balance Oracle WebCenter Portal domain clusters, you can install the ingress-based NGINX load balancer and configure NGINX for non-SSL, SSL termination, and end-to-end SSL access of the application URL. Follow these steps to set up NGINX as a load balancer for an Oracle WebCenter Portal domain in a Kubernetes cluster:\nSee the official installation document for prerequisites.\n  Non-SSL and SSL termination\n Install the NGINX load balancer Configure NGINX to manage ingresses Verify non-SSL and SSL termination access    End-to-end SSL configuration\n Install the NGINX load balancer for End-to-end SSL Deploy tls to access the services Verify end-to-end SSL access    Non-SSL and SSL termination To get repository information, enter the following Helm commands:\n$ helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx $ helm repo update Install the NGINX load balancer   Deploy the ingress-nginx controller by using Helm on the domain namespace:\n$ helm install nginx-ingress ingress-nginx/ingress-nginx -n wcpns \\ --set controller.service.type=NodePort \\ --set controller.admissionWebhooks.enabled=false    Click here to see the sample output.    NAME: nginx-ingress LAST DEPLOYED: Tue Jan 12 21:13:54 2021 NAMESPACE: wcpns STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: The ingress-nginx controller has been installed. Get the application URL by running these commands: export HTTP_NODE_PORT=30305 export HTTPS_NODE_PORT=$(kubectl --namespace wcpns get services -o jsonpath=\u0026quot;{.spec.ports[1].nodePort}\u0026quot; nginx-ingress-ingress-nginx-controller) export NODE_IP=$(kubectl --namespace wcpns get nodes -o jsonpath=\u0026quot;{.items[0].status.addresses[1].address}\u0026quot;) echo \u0026quot;Visit http://$NODE_IP:$HTTP_NODE_PORT to access your application via HTTP.\u0026quot; echo \u0026quot;Visit https://$NODE_IP:$HTTPS_NODE_PORT to access your application via HTTPS.\u0026quot; An example Ingress that makes use of the controller: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx name: example namespace: foo spec: rules: - host: www.example.com http: paths: - backend: serviceName: exampleService servicePort: 80 path: / # This section is only required if TLS is to be enabled for the Ingress tls: - hosts: - www.example.com secretName: example-tls If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided: apiVersion: v1 kind: Secret metadata: name: example-tls namespace: foo data: tls.crt: \u0026lt;base64 encoded cert\u0026gt; tls.key: \u0026lt;base64 encoded key\u0026gt; type: kubernetes.io/tls      Check the status of the deployed ingress controller:\n$ kubectl --namespace wcpns get services | grep ingress-nginx-controller Sample output:\nnginx-ingress-ingress-nginx-controller NodePort 10.101.123.106 \u0026lt;none\u0026gt; 80:30305/TCP,443:31856/TCP 2m12s   Configure NGINX to manage ingresses  Create an ingress for the domain in the domain namespace by using the sample Helm chart. Here path-based routing is used for ingress. Sample values for default configuration are shown in the file ${WORKDIR}/charts/ingress-per-domain/values.yaml. By default, type is TRAEFIK, tls is Non-SSL. You can override these values by passing values through the command line or edit them in the sample values.yaml file.   NOTE: This is not an exhaustive list of rules. You can enhance it based on the application URLs that need to be accessed externally.\n If needed, you can update the ingress YAML file to define more path rules (in section spec.rules.host.http.paths) based on the domain application URLs that need to be accessed. Update the template YAML file for the NGINX load balancer located at ${WORKDIR}/charts/ingress-per-domain/templates/nginx-ingress.yaml You can add new path rules like shown below .\n- path: /NewPathRule backend: serviceName: \u0026#39;Backend Service Name\u0026#39; servicePort: \u0026#39;Backend Service Port\u0026#39; $ cd ${WORKDIR} $ helm install wcp-domain-nginx charts/ingress-per-domain \\  --namespace wcpns \\  --values charts/ingress-per-domain/values.yaml \\  --set \u0026#34;nginx.hostname=$(hostname -f)\u0026#34; \\  --set type=NGINX ``` Sample output: ```bash NAME: wcp-domain-nginx LAST DEPLOYED: Fri Jul 24 09:34:03 2020 NAMESPACE: wcpns STATUS: deployed REVISION: 1 TEST SUITE: None ``` 1. For secured access (SSL) to the Oracle WebCenter Portal application, create a certificate and generate a Kubernetes secret: ```bash $ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /tmp/tls1.key -out /tmp/tls1.crt -subj \u0026#34;/CN=*\u0026#34; $ kubectl -n wcpns create secret tls wcp-domain-tls-cert --key /tmp/tls1.key --cert /tmp/tls1.crt   Install ingress-per-domain using Helm for SSL configuration:\n$ cd ${WORKDIR} $ helm install wcp-domain-nginx charts/ingress-per-domain \\  --namespace wcpns \\  --values charts/ingress-per-domain/values.yaml \\  --set \u0026#34;nginx.hostname=$(hostname -f)\u0026#34; \\  --set type=NGINX --set sslType=SSL   For non-SSL access to the Oracle WebCenter Portal application, get the details of the services by the ingress:\n$ kubectl describe ingress wcp-domain-nginx -n wcpns    Click here to see the sample output of the services supported by the above deployed ingress.    Name: wcp-domain-nginx Namespace: wcpns Address: 10.101.123.106 Default backend: default-http-backend:80 (\u0026lt;error: endpoints \u0026quot;default-http-backend\u0026quot; not found\u0026gt;) Rules: Host Path Backends ---- ---- -------- * /webcenter wcp-domain-cluster-wcp-cluster:8888 (10.244.0.52:8888,10.244.0.53:8888) /console wcp-domain-adminserver:7001 (10.244.0.51:7001) /rsscrawl wcp-domain-cluster-wcp-cluster:8888 (10.244.0.53:8888) /rest wcp-domain-cluster-wcp-cluster:8888 (10.244.0.53:8888) /webcenterhelp wcp-domain-cluster-wcp-cluster:8888 (10.244.0.53:8888) /wsrp-tools wcp-domain-cluster-wcportlet-cluster:8889 (10.244.0.53:8889) /portalTools wcp-domain-cluster-wcportlet-cluster:8889 (10.244.0.53:8889) /em wcp-domain-adminserver:7001 (10.244.0.51:7001) Annotations: meta.helm.sh/release-name: wcp-domain-nginx meta.helm.sh/release-namespace: wcpns nginx.com/sticky-cookie-services: serviceName=wcp-domain-cluster-wcp-cluster srv_id expires=1h path=/; nginx.ingress.kubernetes.io/proxy-connect-timeout: 1800 nginx.ingress.kubernetes.io/proxy-read-timeout: 1800 nginx.ingress.kubernetes.io/proxy-send-timeout: 1800 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Sync 48m (x2 over 48m) nginx-ingress-controller Scheduled for sync      For SSL access to the Oracle WebCenter Portal application, get the details of the services by the above deployed ingress:\n$ kubectl describe ingress wcp-domain-nginx -n wcpns    Click here to see the sample output of the services supported by the above deployed ingress.   Name: wcp-domain-nginx Namespace: wcpns Address: 10.106.220.140 Default backend: default-http-backend:80 (\u0026lt;error: endpoints \u0026quot;default-http-backend\u0026quot; not found\u0026gt;) TLS: wcp-domain-tls-cert terminates mydomain.com Rules: Host Path Backends ---- ---- -------- * /webcenter wcp-domain-cluster-wcp-cluster:8888 (10.244.0.43:8888,10.244.0.44:8888) /console wcp-domain-adminserver:7001 (10.244.0.42:7001) /rsscrawl wcp-domain-cluster-wcp-cluster:8888 (10.244.0.43:8888,10.244.0.44:8888) /webcenterhelp wcp-domain-cluster-wcp-cluster:8888 (10.244.0.43:8888,10.244.0.44:8888) /rest wcp-domain-cluster-wcp-cluster:8888 (10.244.0.43:8888,10.244.0.44:8888) /em wcp-domain-adminserver:7001 (10.244.0.42:7001) /wsrp-tools wcp-domain-cluster-wcportlet-cluster:8889 (10.244.0.43:8889,10.244.0.44:8889) /portalTools wcp-domain-cluster-wcportlet-cluster:8889 (10.244.0.43:8889,10.244.0.44:8889) Annotations: kubernetes.io/ingress.class: nginx meta.helm.sh/release-name: wcp-domain-nginx meta.helm.sh/release-namespace: wcpns nginx.ingress.kubernetes.io/affinity: cookie nginx.ingress.kubernetes.io/affinity-mode: persistent nginx.ingress.kubernetes.io/configuration-snippet: more_set_input_headers \u0026quot;X-Forwarded-Proto: https\u0026quot;; more_set_input_headers \u0026quot;WL-Proxy-SSL: true\u0026quot;; nginx.ingress.kubernetes.io/ingress.allow-http: false nginx.ingress.kubernetes.io/proxy-connect-timeout: 1800 nginx.ingress.kubernetes.io/proxy-read-timeout: 1800 nginx.ingress.kubernetes.io/proxy-send-timeout: 1800 nginx.ingress.kubernetes.io/session-cookie-expires: 172800 nginx.ingress.kubernetes.io/session-cookie-max-age: 172800 nginx.ingress.kubernetes.io/session-cookie-name: stickyid nginx.ingress.kubernetes.io/ssl-redirect: false Events: \u0026lt;none\u0026gt;      Verify non-SSL and SSL termination access Verify that the Oracle WebCenter Portal domain application URLs are accessible through the nginx NodePort LOADBALANCER-NODEPORT 30305:\nhttp://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-NODEPORT}/console http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-NODEPORT}/em http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-NODEPORT}/webcenter http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-NODEPORT}/rsscrawl http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-NODEPORT}/rest http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-NODEPORT}/webcenterhelp http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-NODEPORT}/wsrp-tools http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-NODEPORT}/portalTools Uninstall the ingress Uninstall and delete the ingress-nginx deployment:\n$ helm delete wcp-domain-nginx -n wcpns $ helm delete nginx-ingress -n wcpns End-to-end SSL configuration Install the NGINX load balancer for End-to-end SSL   For secured access (SSL) to the Oracle WebCenter Portal application, create a certificate and generate secrets:\n$ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /tmp/tls1.key -out /tmp/tls1.crt -subj \u0026#34;/CN=domain1.org\u0026#34; $ kubectl -n wcpns create secret tls wcp-domain-tls-cert --key /tmp/tls1.key --cert /tmp/tls1.crt  Note: The value of CN is the host on which this ingress is to be deployed.\n   Deploy the ingress-nginx controller by using Helm on the domain namespace:\n$ helm install nginx-ingress -n wcpns \\  --set controller.extraArgs.default-ssl-certificate=wcpns/wcp-domain-tls-cert \\  --set controller.service.type=NodePort \\  --set controller.admissionWebhooks.enabled=false \\  --set controller.extraArgs.enable-ssl-passthrough=true \\  ingress-nginx/ingress-nginx    Click here to see the sample output.   NAME: nginx-ingress LAST DEPLOYED: Tue Sep 15 08:40:47 2020 NAMESPACE: wcpns STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: The ingress-nginx controller has been installed. Get the application URL by running these commands: export HTTP_NODE_PORT=$(kubectl --namespace wcpns get services -o jsonpath=\u0026#34;{.spec.ports[0].nodePort}\u0026#34; nginx-ingress-ingress-nginx-controller) export HTTPS_NODE_PORT=$(kubectl --namespace wcpns get services -o jsonpath=\u0026#34;{.spec.ports[1].nodePort}\u0026#34; nginx-ingress-ingress-nginx-controller) export NODE_IP=$(kubectl --namespace wcpns get nodes -o jsonpath=\u0026#34;{.items[0].status.addresses[1].address}\u0026#34;) echo \u0026#34;Visit http://$NODE_IP:$HTTP_NODE_PORTto access your application via HTTP.\u0026#34; echo \u0026#34;Visit https://$NODE_IP:$HTTPS_NODE_PORTto access your application via HTTPS.\u0026#34; An example Ingress that makes use of the controller: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx name: example namespace: foo spec: rules: - host: www.example.com http: paths: - backend: serviceName: exampleService servicePort: 80 path: / # This section is only required if TLS is to be enabled for the Ingress tls: - hosts: - www.example.com secretName: example-tls If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided: apiVersion: v1 kind: Secret metadata: name: example-tls namespace: foo data: tls.crt: \u0026lt;base64 encoded cert\u0026gt; tls.key: \u0026lt;base64 encoded key\u0026gt; type: kubernetes.io/tls      Check the status of the deployed ingress controller:\n$ kubectl --namespace wcpns get services | grep ingress-nginx-controller Sample output:\nnginx-ingress-ingress-nginx-controller NodePort 10.96.177.215 \u0026lt;none\u0026gt; 80:32748/TCP,443:31940/TCP 23s   Deploy tls to access services   Deploy tls to securely access the services. Only one application can be configured with ssl-passthrough. A sample tls file for NGINX is shown below for the service wcp-domain-cluster-wcp-cluster and port 8889. All the applications running on port 8889 can be securely accessed through this ingress.\n  For each backend service, create different ingresses, as NGINX does not support multiple paths or rules with annotation ssl-passthrough. For example, for wcp-domain-adminserver and wcp-domain-cluster-wcp-cluster, different ingresses must be created.\n  As ssl-passthrough in NGINX works on the clusterIP of the backing service instead of individual endpoints, you must expose wcp-domain-cluster-wcp-cluster created by the operator with clusterIP.\nFor example:\na. Get the name of wcp-domain cluster service:\n$ kubectl get svc -n wcpns | grep wcp-domain-cluster-wcp-cluster Sample output:\nwcp-domain-cluster-wcp-cluster ClusterIP 10.102.128.124 \u0026lt;none\u0026gt; 8888/TCP,8889/TCP 62m   Deploy the secured ingress:\n$ cd ${WORKDIR}/charts/ingress-per-domain/tls $ kubectl create -f nginx-tls.yaml  Note: The default nginx-tls.yaml contains the backend for WebCenter Portal service with domainUID wcp-domain. You need to create similar tls configuration YAML files separately for each backend service.\n   Click here to check the content of the file nginx-tls.yaml    apiVersion: extensions/v1beta1 kind: Ingress metadata: name: wcpns-ingress namespace: wcpns annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/ssl-passthrough: \u0026quot;true\u0026quot; spec: tls: - hosts: - domain1.org secretName: wcp-domain-tls-cert rules: - host: domain1.org http: paths: - path: backend: serviceName: wcp-domain-cluster-wcp-cluster servicePort: 8889     Note: Host is the server on which this ingress is deployed.\n   Check the services supported by the ingress:\n$ kubectl describe ingress wcpns-ingress -n wcpns   Verify end-to-end SSL access Verify that the Oracle WebCenter Portal domain application URLs are accessible through the LOADBALANCER-SSLPORT 30233:\nhttps://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/webcenter https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/rsscrawl https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/webcenterhelp https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/rest https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/wsrp-tools https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/portalTools Uninstall ingress-nginx tls $ cd ${WORKDIR}/charts/ingress-per-domain/tls $ kubectl delete -f nginx-tls.yaml $ helm delete nginx-ingress -n wcpns "
},
{
	"uri": "/fmw-kubernetes/wcsites-domains/adminguide/configure-load-balancer/nginx/",
	"title": "NGINX",
	"tags": [],
	"description": "Configure the ingress-based NGINX load balancer for Oracle WebCenter Sites domains.",
	"content": "This section provides information about how to install and configure the ingress-based NGINX load balancer to load balance Oracle WebCenter Sites domain clusters. You can configure NGINX for access of the application URL.\nFollow these steps to set up NGINX as a load balancer for an Oracle WebCenter Sites domain in a Kubernetes cluster:\nSee the official installation document for prerequisites.\nAdd the Helm repos (if not added) Add following Helm repos\n$ helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx $ helm repo update Install the NGINX load balancer Here is the helm install command with the default value for http port.\n```bash $ helm install nginx-ingress -n wcsites-ns \\ --set controller.service.type=NodePort \\ --set controller.admissionWebhooks.enabled=false \\ --set controller.service.nodePorts.http=30305 \\ ingress-nginx/ingress-nginx ```  Create an Ingress for the Domain   Create an ingress for the domain in the domain namespace by using the sample Helm chart. Here path-based routing is used for ingress. Sample values for default configuration are shown in the file ${WORKDIR}/kubernetes/charts/ingress-per-domain/values.yaml. By default, type is TRAEFIK , sslType is NONSSL, and domainType is wcs. These values can be overridden by passing values through the command line or can be edited in the sample file values.yaml.\nIf needed, you can update the ingress YAML file to define more path rules (in section spec.rules.host.http.paths) based on the domain application URLs that need to be accessed. Update the template YAML file for the NGINX load balancer located at ${WORKDIR}/kubernetes/charts/ingress-per-domain/templates/nginx-ingress.yaml.\nFor detailed instructions about ingress, see this page.\nFor now, you can update the kubernetes/charts/ingress-per-domain/values.yaml with appropriate values.\n  Update the kubernetes/charts/ingress-per-domain/templates/nginx-ingress.yaml with the url routes to be load balanced.\nNOTE: This is not an exhaustive list of rules. You can enhance it based on the application urls that need to be accessed externally. These rules hold good for domain type wcs.\n  Install \u0026ldquo;ingress-per-domain\u0026rdquo; using helm.\n Helm Install ingress-per-domain\n $ helm install wcsitesinfra-ingress kubernetes/charts/ingress-per-domain \\  --namespace wcsites-ns \\  --values kubernetes/charts/ingress-per-domain/values.yaml \\  --set \u0026#34;nginx.hostname=$(hostname -f)\u0026#34; \\  --set type=NGINX NAME: wcsitesinfra-ingress LAST DEPLOYED: Fri July 9 00:18:50 2020 NAMESPACE: wcsites-ns STATUS: deployed REVISION: 1 TEST SUITE: None   To confirm that the load balancer noticed the new Ingress and is successfully routing to the domain\u0026rsquo;s server pods, you can send a request to the URL for the \u0026ldquo;WebLogic ReadyApp framework\u0026rdquo; which should return a HTTP 200 status code, as shown in the example below:\n  -bash-4.2$ curl -v http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-PORT}/weblogic/ready * Trying 149.87.129.203... \u0026gt; GET http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-PORT}/weblogic/ready HTTP/1.1 \u0026gt; User-Agent: curl/7.29.0 \u0026gt; Accept: */* \u0026gt; Proxy-Connection: Keep-Alive \u0026gt; host: $(hostname -f) \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; Date: Sat, 14 Mar 2020 08:35:03 GMT \u0026lt; Vary: Accept-Encoding \u0026lt; Content-Length: 0 \u0026lt; Proxy-Connection: Keep-Alive \u0026lt; * Connection #0 to host localhost left intact Verify that You can Access the Domain URL After setting up the nginx loadbalancer, verify that the domain applications are accessible through the loadbalancer port 30305. Through load balancer (nginx port 30305), the following URLs are available for setting up domains of WebCenter Sites domain types:\nhttp://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-PORT}/weblogic/ready http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-PORT}/console http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-PORT}/em http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-PORT}/sites/version.jsp "
},
{
	"uri": "/fmw-kubernetes/wcportal-domains/manage-wcportal-domains/monitoring-and-publishing-logs/publishing-logs/",
	"title": "Publish WebLogic Server logs into Elasticsearch",
	"tags": [],
	"description": "Publish WebLogic Server logs into Elasticsearch.",
	"content": "To publish WebLogic Server logs into Elasticsearch, you can configure your WebCenter Portal domain to use Fluentd, WebLogic Logging Exporter or Logstash.  Fluentd  Describes how to configure a WebCenter Portal domain to use Fluentd to send log information to Elasticsearch.\n WebLogic logging exporter  Use the WebLogic Logging Exporter to publish the WebLogic Server logs to Elasticsearch.\n Logstash  Describes how to configure a WebCenter Portal domain to use logstash and publish the WebLogic Server logs to Elasticsearch.\n "
},
{
	"uri": "/fmw-kubernetes/soa-domains/patch_and_upgrade/upgrade-operator-release/",
	"title": "Upgrade an operator release",
	"tags": [],
	"description": "Upgrade the WebLogic Kubernetes Operator release to a newer version.",
	"content": "To upgrade the WebLogic Kubernetes operator, use the helm upgrade command with new Helm chart and operator image. See the steps here to pull the operator image and set up the Oracle SOA Suite repository that contains the operator chart. To upgrade the operator run the following command:\n$ cd ${WORKDIR} $ helm upgrade \\ --reuse-values \\ --set image=oracle/weblogic-kubernetes-operator:3.4.0 \\ --namespace weblogic-operator-namespace \\ --wait \\ weblogic-kubernetes-operator \\ charts/weblogic-operator  Note: When the WebLogic Kubernetes Operator is upgraded from release version 3.2.1 to 3.3.0 or later, it may be expected that the Administration Server pod in the domain gets restarted.\n Post upgrade steps From operator 3.1.1, the T3 channel Kubernetes service name extension is changed from -external to -ext. If the Administration Server was configured to expose a T3 channel in your domain, then follow these steps to recreate the Kubernetes service (for T3 channel) with the new name -ext.\n Note: If these steps are not performed, then the domain restart using spec.serverStartPolicy, would fail to bring up the servers.\n   Get the existing Kubernetes service name for T3 channel from the domain namespace. For example, if the domainUID is soainfra, and the Administration Server name is adminserver, then the service would be:\nsoainfra-adminserver-external   Delete the existing Kubernetes service for T3 channel, so that operator 3.1.1 creates a new one:\n$ kubectl delete service \u0026lt;T3 channel service\u0026gt; --namespace \u0026lt;domain-namespace\u0026gt; For example, if the domainUID is soainfra, the Administration Server name is adminserver and domain namespace is soans, then the command would be:\n$ kubectl delete service soainfra-adminserver-external --namespace soans   Then the operator automatically creates a new Kubernetes service with -ext instead of -external:\nsoainfra-adminserver-ext   "
},
{
	"uri": "/fmw-kubernetes/wcsites-domains/patch-and-upgrade/upgrade-operator-release/",
	"title": "Upgrade an operator release",
	"tags": [],
	"description": "Upgrade the WebLogic Kubernetes Operator release to a newer version.",
	"content": "These instructions apply to upgrading operators within the 3.x release family as additional versions are released.\nTo upgrade the Kubernetes operator, use the helm upgrade command. When upgrading the operator, the helm upgrade command requires that you supply a new Helm chart and image. For example:\n$ helm upgrade \\ --reuse-values \\ --set image=oracle/weblogic-kubernetes-operator:3.0.2 \\ --namespace weblogic-operator-namespace \\ --wait \\ weblogic-operator \\ kubernetes/charts/weblogic-operator "
},
{
	"uri": "/fmw-kubernetes/oig/configure-design-console/using-the-design-console-with-nginx-ssl/",
	"title": "b. Using Design Console with NGINX(SSL)",
	"tags": [],
	"description": "Configure Design Console with NGINX(SSL).",
	"content": "Configure an NGINX ingress (SSL) to allow Design Console to connect to your Kubernetes cluster.\n  Prerequisites\n  Setup routing rules for the Design Console ingress\n  Create the ingress\n  Update the T3 channel\n  Restart the OIG Managed Server\n  Design Console client\na. Using an on-premises installed Design Console\nb. Using a container image for Design Console\n  Login to the Design Console\n  Prerequisites If you haven\u0026rsquo;t already configured an NGINX ingress controller (SSL) for OIG, follow Using an Ingress with NGINX (SSL).\nMake sure you know the master hostname and ingress port for NGINX before proceeding e.g https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}. Also make sure you know the Kubernetes secret for SSL that was generated e.g governancedomain-tls-cert.\nSetup routing rules for the Design Console ingress   Setup routing rules by running the following commands:\n$ cd $WORKDIR/kubernetes/design-console-ingress Edit values.yaml and ensure that tls: SSL is set. Change domainUID: and secretName: to match the values for your \u0026lt;domain_uid\u0026gt; and your SSL Kubernetes secret, for example:\n# Load balancer type. Supported values are: NGINX type: NGINX # Type of Configuration Supported Values are : NONSSL,SSL # tls: NONSSL tls: SSL # TLS secret name if the mode is SSL secretName: governancedomain-tls-cert # WLS domain as backend to the load balancer wlsDomain: domainUID: governancedomain oimClusterName: oim_cluster oimServerT3Port: 14002   Create the ingress   Run the following command to create the ingress:\n$ cd $WORKDIR $ helm install governancedomain-nginx-designconsole kubernetes/design-console-ingress --namespace oigns --values kubernetes/design-console-ingress/values.yaml The output will look similar to the following:\nNAME: governancedomain-nginx-designconsole Mon Thu Mar 10 14:42:16 2022 NAMESPACE: oigns STATUS: deployed REVISION: 1 TEST SUITE: None   Run the following command to show the ingress is created successfully:\n$ kubectl describe ing governancedomain-nginx-designconsole -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl describe ing governancedomain-nginx-designconsole -n oigns The output will look similar to the following:\nName: governancedomain-nginx-designconsole Namespace: oigns Address: Default backend: default-http-backend:80 (\u0026lt;error: endpoints \u0026quot;default-http-backend\u0026quot; not found\u0026gt;) Rules: Host Path Backends ---- ---- -------- * governancedomain-cluster-oim-cluster:14002 (10.244.2.103:14002) Annotations: kubernetes.io/ingress.class: nginx meta.helm.sh/release-name: governancedomain-nginx-designconsole meta.helm.sh/release-namespace: oigns nginx.ingress.kubernetes.io/affinity: cookie nginx.ingress.kubernetes.io/configuration-snippet: more_set_input_headers \u0026quot;X-Forwarded-Proto: https\u0026quot;; more_set_input_headers \u0026quot;WL-Proxy-SSL: true\u0026quot;; nginx.ingress.kubernetes.io/enable-access-log: false nginx.ingress.kubernetes.io/ingress.allow-http: false nginx.ingress.kubernetes.io/proxy-buffer-size: 2000k Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Sync 6s nginx-ingress-controller Scheduled for sync   Update the T3 channel   Log in to the WebLogic Console using https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/console.\n  Navigate to Environment, click Servers, and then select oim_server1.\n  Click Protocols, and then Channels.\n  Click the default T3 channel called T3Channel.\n  Click Lock and Edit.\n  Set the External Listen Address to the ingress controller hostname ${MASTERNODE-HOSTNAME}.\n  Set the External Listen Port to the ingress controller port ${MASTERNODE-PORT}.\n  Click Save.\n  Click Activate Changes.\n  Restart the OIG Managed Server Restart the OIG Managed Server for the above changes to take effect:\n$ cd $WORKDIR/kubernetes/domain-lifecycle $ ./restartServer.sh -s oim_server1 -d \u0026lt;domain_uid\u0026gt; -n \u0026lt;domain_namespace\u0026gt; For example:\n$ cd $WORKDIR/kubernetes/domain-lifecycle ./restartServer.sh -s oim_server1 -d governancedomain -n oigns Make sure the \u0026lt;domain_uid\u0026gt;-oim-server1 has a READY status of 1/1 before continuing:\n$ kubectl get pods -n oigns | grep oim-server1 The output will look similar to the following:\ngovernancedomain-oim-server1 1/1 Running 0 8m Design Console Client It is possible to use Design Console from an on-premises install, or from a container image.\nUsing an on-premises installed Design Console The instructions below should be performed on the client where Design Console is installed.\n  Import the CA certificate into the java keystore\nIf in Generate SSL Certificate you requested a certificate from a Certificate Authority (CA), then you must import the CA certificate (e.g cacert.crt) that signed your certificate, into the java truststore used by Design Console.\nIf in Generate SSL Certificate you generated a self-signed certicate (e.g tls.crt), you must import the self-signed certificate into the java truststore used by Design Console.\nImport the certificate using the following command:\n$ keytool -import -trustcacerts -alias dc -file \u0026lt;certificate\u0026gt; -keystore $JAVA_HOME/jre/lib/security/cacerts where \u0026lt;certificate\u0026gt; is the CA certificate, or self-signed certicate.\n  Once complete follow Login to the Design Console.\n  Using a container image for Design Console Using Docker The Design Console can be run from a container using X windows emulation.\n  On the parent machine where the Design Console is to be displayed, run xhost +.\n  Find which worker node the \u0026lt;domain\u0026gt;-oim-server1 pod is running. For example:\n$ kubectl get pods -n oigns -o wide | grep governancedomain-oim-server1 The output will look similar to the following:\ngovernancedomain-oim-server1 1/1 Running 0 31m 10.244.2.98 worker-node2   On the worker node returned above e.g worker-node2, execute the following command to find the OIG container image name:\n$ docker images Then execute the following command to start a container to run Design Console:\n$ docker run -u root --name oigdcbase -it \u0026lt;image\u0026gt; bash For example:\n$ docker run -u root -it --name oigdcbase container-registry.oracle.com/middleware/oig_cpu:12.2.1.4-jdk8-ol7-220120.1359 bash This will take you into a bash shell inside the container:\nbash-4.2#   Inside the container set the proxy, for example:\nbash-4.2# export https_proxy=http://proxy.example.com:80   Install the relevant X windows packages in the container:\nbash-4.2# yum install libXext libXrender libXtst   Execute the following outside the container to create a new Design Console image from the container:\n$ docker commit \u0026lt;container_name\u0026gt; \u0026lt;design_console_image_name\u0026gt; For example:\n$ docker commit oigdcbase oigdc   Exit the container bash session:\nbash-4.2# exit   Start a new container using the Design Console image:\n$ docker run --name oigdc -it oigdc /bin/bash This will take you into a bash shell for the container:\nbash-4.2#   Copy the Ingress CA certificate into the container\nIf in Generate SSL Certificate you requested a certificate from a Certificate Authority (CA), then you must copy the CA certificate (e.g cacert.crt) that signed your certificate, into the container\nIf in Generate SSL Certificate you generated a self-signed certicate (e.g tls.crt), you must copy the self-signed certificate into the container\nNote: You will have to copy the certificate over to the worker node where the oigdc image is created before running the following.\nRun the following command outside the container:\n$ cd \u0026lt;workdir\u0026gt;/ssl $ docker cp \u0026lt;certificate\u0026gt; \u0026lt;container_name\u0026gt;:/u01/jdk/jre/lib/security/\u0026lt;certificate\u0026gt; For example:\n$ cd /scratch/OIGK8S/ssl $ docker cp tls.crt oigdc:/u01/jdk/jre/lib/security/tls.crt   Import the certificate using the following command:\nbash-4.2# /u01/jdk/bin/keytool -import -trustcacerts -alias dc -file /u01/jdk/jre/lib/security/\u0026lt;certificate\u0026gt; -keystore /u01/jdk/jre/lib/security/cacerts For example:\nbash-4.2# /u01/jdk/bin/keytool -import -trustcacerts -alias dc -file /u01/jdk/jre/lib/security/tls.crt -keystore /u01/jdk/jre/lib/security/cacerts   In the container run the following to export the DISPLAY:\n$ export DISPLAY=\u0026lt;parent_machine_hostname:1\u0026gt;   Start the Design Console from the container:\nbash-4.2# cd idm/designconsole bash-4.2# sh xlclient.sh The Design Console login should be displayed. Now follow Login to the Design Console.\n  Using podman   On the parent machine where the Design Console is to be displayed, run xhost +.\n  Find which worker node the \u0026lt;domain\u0026gt;-oim-server1 pod is running. For example:\n$ kubectl get pods -n oigns -o wide | grep governancedomain-oim-server1 The output will look similar to the following:\ngovernancedomain-oim-server1 1/1 Running 0 19h 10.244.2.55 worker-node2 \u0026lt;none\u0026gt;   On the worker node returned above e.g worker-node2, execute the following command to find the OIG container image name:\n$ podman images Then execute the following command to start a container to run Design Console:\n$ podman run -u root --name oigdcbase -it \u0026lt;image\u0026gt; bash For example:\n$ podman run -u root -it --name oigdcbase container-registry.oracle.com/middleware/oig_cpu:12.2.1.4-jdk8-ol7-220120.1359 bash This will take you into a bash shell inside the container:\nbash-4.2#   Inside the container set the proxy, for example:\nbash-4.2# export https_proxy=http://proxy.example.com:80   Install the relevant X windows packages in the container:\nbash-4.2# yum install libXext libXrender libXtst   Execute the following outside the container to create a new Design Console image from the container:\n$ podman commit \u0026lt;container_name\u0026gt; \u0026lt;design_console_image_name\u0026gt; For example:\n$ podman commit oigdcbase oigdc   Exit the container bash session:\nbash-4.2# exit   Start a new container using the Design Console image:\n$ podman run --name oigdc -it oigdc /bin/bash This will take you into a bash shell for the container:\nbash-4.2#   Copy the Ingress CA certificate into the container\nIf in Generate SSL Certificate you requested a certificate from a Certificate Authority (CA), then you must copy the CA certificate (e.g cacert.crt) that signed your certificate, into the container\nIf in Generate SSL Certificate you generated a self-signed certicate (e.g tls.crt), you must copy the self-signed certificate into the container\nNote: You will have to copy the certificate over to the worker node where the oigdc image is created before running the following.\nRun the following command outside the container:\n$ cd \u0026lt;workdir\u0026gt;/ssl $ podman cp \u0026lt;certificate\u0026gt; \u0026lt;container_name\u0026gt;:/u01/jdk/jre/lib/security/\u0026lt;certificate\u0026gt; For example:\n$ cd /scratch/OIGK8S/ssl $ podman cp tls.crt oigdc:/u01/jdk/jre/lib/security/tls.crt   Inside the container, import the certificate using the following command:\nbash-4.2# /u01/jdk/bin/keytool -import -trustcacerts -alias dc -file /u01/jdk/jre/lib/security/\u0026lt;certificate\u0026gt; -keystore /u01/jdk/jre/lib/security/cacerts For example:\nbash-4.2# /u01/jdk/bin/keytool -import -trustcacerts -alias dc -file /u01/jdk/jre/lib/security/tls.crt -keystore /u01/jdk/jre/lib/security/cacerts   In the container run the following to export the DISPLAY:\n$ export DISPLAY=\u0026lt;parent_machine_hostname:1\u0026gt;   Start the Design Console from the container:\nbash-4.2# cd idm/designconsole bash-4.2# sh xlclient.sh The Design Console login should be displayed. Now follow Login to the Design Console.\n  Login to the Design Console   Launch the Design Console and in the Oracle Identity Manager Design Console login page enter the following details:\nEnter the following details and click Login:\n Server URL: \u0026lt;url\u0026gt; User ID: xelsysadm Password: \u0026lt;password\u0026gt;.  where \u0026lt;url\u0026gt; is where \u0026lt;url\u0026gt; is https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}.\n  If successful the Design Console will be displayed.\n  "
},
{
	"uri": "/fmw-kubernetes/wccontent-domains/adminguide/weblogic-monitoring-exporter-setup/",
	"title": "Monitor an Oracle WebCenter Content domain",
	"tags": [],
	"description": "Use the WebLogic Monitoring Exporter to monitor an Oracle WebCenter Content instance using Prometheus and Grafana.",
	"content": "You can monitor a WebCenter Content domain using Prometheus and Grafana by exporting the metrics from the domain instance using the WebLogic Monitoring Exporter. This sample shows you how to set up the WebLogic Monitoring Exporter to push the data to Prometheus.\nPrerequisites This document assumes that the Prometheus Operator is deployed on the Kubernetes cluster. If it is not already deployed, follow the steps below for deploying the Prometheus Operator.\nDeploy Prometheus and Grafana Refer to the compatibility matrix of Kube Prometheus and clone the release version of the kube-prometheus repository according to the Kubernetes version of your cluster.\nClone the kube-prometheus project $ git clone https://github.com/coreos/kube-prometheus.git Label the nodes Kube-Prometheus requires all the exporter nodes to be labelled with kubernetes.io/os=linux. If a node is not labelled, then you must label it using the following command:\n$ kubectl label nodes --all kubernetes.io/os=linux Create Prometheus and Grafana resources Change to the kube-prometheus directory and execute the following commands to create the namespace and CRDs:\nNOTE: Wait for a minute for each command to process.\n$ cd kube-prometheus $ kubectl create -f manifests/setup $ until kubectl get servicemonitors --all-namespaces ; do date; sleep 1; echo \u0026#34;\u0026#34;; done $ kubectl create -f manifests/ Provide external access To provide external access for Grafana, Prometheus, and Alertmanager, execute the commands below:\n$ kubectl patch svc grafana -n monitoring --type=json -p \u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/type\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;NodePort\u0026#34; },{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/ports/0/nodePort\u0026#34;, \u0026#34;value\u0026#34;: 32100 }]\u0026#39; $ kubectl patch svc prometheus-k8s -n monitoring --type=json -p \u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/type\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;NodePort\u0026#34; },{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/ports/0/nodePort\u0026#34;, \u0026#34;value\u0026#34;: 32101 }]\u0026#39; $ kubectl patch svc alertmanager-main -n monitoring --type=json -p \u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/type\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;NodePort\u0026#34; },{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/ports/0/nodePort\u0026#34;, \u0026#34;value\u0026#34;: 32102 }]\u0026#39; NOTE:\n 32100 is the external port for Grafana 32101 is the external port for Prometheus 32102 is the external port for Alertmanager   Set Up the WebLogic Monitoring Exporter Set up the WebLogic Monitoring Exporter that will collect WebLogic Server metrics and monitor your Oracle WebCenter Content domain.\nGenerate the WebLogic Monitoring Exporter Deployment Package Two packages are required as the listening ports are different for the Administration Server and Managed Servers. One binary required for the Admin Server (wls-exporter-as.war) and one for Managed Cluster (wls-exporter-ms.war). Set the required proxies and then run the script getX.X.X.sh to generate two binaries:\nDownload WebLogic Monitoring Exporter Download WebLogic Monitoring Exporter package from https://github.com/oracle/weblogic-monitoring-exporter/releases Download wls-exporter.war and getX.X.X.sh\nCreate configuration file for WebLogic Monitoring Exporter In this step we will create the configuration file for WebLogic Monitoring Exporter.The configuration will have the server port for serving the webapp, metrics to be scraped from the WebLogic server etc.\n  Click here to see sample content for config-admin `config-admin.yaml`.   metricsNameSnakeCase: true restPort: 7001 queries: - key: name keyName: location prefix: wls_server_ applicationRuntimes: key: name keyName: app componentRuntimes: prefix: wls_webapp_config_ type: WebAppComponentRuntime key: name values: [deploymentState, contextRoot, sourceInfo, openSessionsHighCount, openSessionsCurrentCount, sessionsOpenedTotalCount, sessionCookieMaxAgeSecs, sessionInvalidationIntervalSecs, sessionTimeoutSecs, singleThreadedServletPoolSize, sessionIDLength, servletReloadCheckSecs, jSPPageCheckSecs] servlets: prefix: wls_servlet_ key: servletName - JVMRuntime: prefix: wls_jvm_ key: name - executeQueueRuntimes: prefix: wls_socketmuxer_ key: name values: [pendingRequestCurrentCount] - workManagerRuntimes: prefix: wls_workmanager_ key: name values: [stuckThreadCount, pendingRequests, completedRequests] - threadPoolRuntime: prefix: wls_threadpool_ key: name values: [executeThreadTotalCount, queueLength, stuckThreadCount, hoggingThreadCount] - JMSRuntime: key: name keyName: jmsruntime prefix: wls_jmsruntime_ JMSServers: prefix: wls_jms_ key: name keyName: jmsserver destinations: prefix: wls_jms_dest_ key: name keyName: destination - persistentStoreRuntimes: prefix: wls_persistentstore_ key: name - JDBCServiceRuntime: JDBCDataSourceRuntimeMBeans: prefix: wls_datasource_ key: name - JTARuntime: prefix: wls_jta_ key: name    In this step we will generate the deployemt package. We have to generate three separate packages with restPort as 7001 16200 and 16250 in config.yaml. The three packages are required as the listening ports for AdminServer, Oracle WebCenter Content server \u0026amp; Oracle WebCenter Inbound Refinery server.\nUse getX.X.X.sh script to update the configuration file into wls-exporter package. Below a sample usage\nSet the required proxies and then run the script getX.X.X.sh\n$ cd kubernetes/samples/scripts/create-wcc-domain/utils/weblogic-monitoring-exporter $ sh get1.2.0.sh config-admin.yaml Output:\n./get1.2.0.sh config-admin.yaml % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 642 100 642 0 0 1186 0 --:--:-- --:--:-- --:--:-- 1184 100 2033k 100 2033k 0 0 846k 0 0:00:02 0:00:02 --:--:-- 1527k created /tmp/ci-6gGPjopn3l /tmp/ci-6gGPjopn3l /\u0026lt;your_path\u0026gt;/prometheus/weblogic_monitor_exporter in temp dir adding: config.yml (deflated 63%) Generate the packages for Managed Servers/clusters with the different configuration file.\nDeploy the WebLogic Monitoring Exporter Follow these steps to deploy the package in the WebLogic Server instances:\n  In the Administration Server and Managed Servers, deploy the WebLogic Monitoring Exporter (wls-exporter.war) separately using the Oracle Enterprise Manager.\n  Select the servers to which the Exporter WAR should be deployed:\n  Set the application name. The application name must be different if it is deployed separately in the Administration Server and Managed Servers. Make sure the context-root for both the deployments is wls-exporter:\n  Click Install and start application.\n  Then deploy the WebLogic Monitoring Exporter application.\n  Activate the changes to start the application. If the application is started and the port is exposed, then you can access the WebLogic Monitoring Exporter console using this URL: http://\u0026lt;server:port\u0026gt;/wls-exporter.\n  Repeat same steps for ucm, ibr, ipm, capture and wccadf servers.\n  Configure Prometheus Operator Prometheus enables you to collect metrics from the WebLogic Monitoring Exporter. The Prometheus Operator identifies the targets using service discovery. To get the WebLogic Monitoring Exporter end point discovered as a target, you must create a service monitor pointing to the service.\nSee the following sample service monitor deployment YAML configuration file located at\nkubernetes/samples/scripts/create-wccontent-domains/utils/weblogic-monitoring-exporter/wls-exporter.yaml.\nServiceMonitor for wls-exporter:\n  Click here to see sample content for wls-exporter.yaml   apiVersion: v1 kind: Secret metadata: name: basic-auth namespace: monitoring data: password: d2VsY29tZTE= # welcome1 i.e.'WebLogic password' user: d2VibG9naWM= # weblogic i.e. 'WebLogic username' type: Opaque --- apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: wls-exporter-wccinfra namespace: monitoring labels: k8s-app: wls-exporter spec: namespaceSelector: matchNames: - wccns selector: matchLabels: weblogic.domainName: wccinfra endpoints: - basicAuth: password: name: basic-auth key: password username: name: basic-auth key: user port: default relabelings: - action: labelmap regex: __meta_kubernetes_service_label_(.+) interval: 10s honorLabels: true path: /wls-exporter/metrics    The exporting of metrics from wls-exporter requires basicAuth so a Kubernetes Secret is created with the user name and password that are base64 encoded. This Secret will be used in the ServiceMonitor deployment.\nWhen generating the base64 encoded strings for the user name and password, observe if a new line character is appended in the encoded string. This line character causes an authentication failure. To avoid a new line string, use the following example:\n$ echo -n \u0026quot;welcome1\u0026quot; | base64 d2VsY29tZTE= In the deployment YAML configuration for wls-exporter shown above, weblogic.domainName: wccinfra is used as a label under spec.selector.matchLabels, so all the services will be selected for the service monitor. If you don\u0026rsquo;t use this label, you should create separate service monitors for each server - if the server name is used as matching labels in spec.selector.matchLabels. Doing so will require you to relabel the configuration because Prometheus, by default, ignores the labels provided in the wls-exporter.\nBy default, Prometheus does not store all the labels provided by the target. In the service monitor deployment YAML configuration, you must mention the relabeling configuration (spec.endpoints.relabelings) so that certain labels provided by weblogic-monitoring-exporter (required for the Grafana dashboard) are stored in Prometheus. Do not delete the following section from the configuration YAML file:\nrelabelings: - action: labelmap regex: __meta_kubernetes_service_label_(.+) Add RoleBinding and Role for the WebLogic Domain Namespace The RoleBinding is required for Prometheus to access the endpoints provided by the WebLogic Monitoring Exporter. You need to add RoleBinding for the namespace under which the WebLogic Servers pods are running in the Kubernetes cluster. Edit the kube-prometheus/manifests/prometheus-roleBindingSpecificNamespaces.yaml file in the Prometheus Operator deployment manifests and add the RoleBinding for the namespace (wccns) similar to the following example:\n- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: prometheus-k8s namespace: wccns roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: prometheus-k8s subjects: - kind: ServiceAccount name: prometheus-k8s namespace: monitoring Similarly, add the Role for the namespace under which the WebLogic Servers pods are running in the Kubernetes cluster. Edit kube-prometheus/manifests/prometheus-roleSpecificNamespaces.yaml in the Prometheus Operator deployment manifests and add the Role for the namespace (wccns) similar to the following example:\n- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: prometheus-k8s namespace: wccns rules: - apiGroups: - \u0026quot;\u0026quot; resources: - services - endpoints - pods verbs: - get - list - watch Then apply prometheus-roleBindingSpecificNamespaces.yaml and prometheus-roleSpecificNamespaces.yaml for the RoleBinding and Role to take effect in the cluster.\n$ kubectl apply -f kube-prometheus/manifests/prometheus-roleBindingSpecificNamespaces.yaml $ kubectl apply -f kube-prometheus/manifests/prometheus-roleSpecificNamespaces.yaml Deploy the Service Monitor To deploy the service monitor, use the above wls-exporter.yaml deployment YAML and run the following command:\n$ cd kubernetes/samples/scripts/create-wccontent-domains/utils/weblogic-monitoring-exporter/ $ kubectl create -f wls-exporter.yaml Enable Prometheus to Discover the Service After the deployment of the service monitor, Prometheus should be able to discover wls-exporter and export metrics.\nYou can access the Prometheus dashboard at http://mycompany.com:32101/.\nDeploy Grafana Dashboard To view the domain metrics, deploy the Grafana dashboard provided in the WebLogic Monitoring Exporter.\nYou can access the Grafana dashboard at http://mycompany.com:32100/.\n  Log in to Grafana dashboard with admin/admin.\n  Go to Settings, then select DataSources, and then Add Data Source.\nHTTP URL: Prometheus URL http://mycompany.com:32101/\nAuth: Enable Basic Auth\nBasic Auth Details: WebLogic credentials provided in step Configure Prometheus Operator\n  Download the weblogic_dashboard.json file from here.\n  Click Add and then Import. Paste the modified JSON in the Paste JSON block, and then load it.\nThis displays the WebLogic Server Dashboard.\n  "
},
{
	"uri": "/fmw-kubernetes/oam/",
	"title": "Oracle Access Management",
	"tags": [],
	"description": "The WebLogic Kubernetes Operator supports deployment of Oracle Access Management (OAM). Follow the instructions in this guide to set up these Oracle Access Management domains on Kubernetes.",
	"content": "The WebLogic Kubernetes Operator supports deployment of Oracle Access Management (OAM).\nIn this release, OAM domains are supported using the “domain on a persistent volume” model only, where the domain home is located in a persistent volume (PV).\nThe WebLogic Kubernetes Operator has several key features to assist you with deploying and managing Oracle Access Management domains in a Kubernetes environment. You can:\n Create OAM instances in a Kubernetes persistent volume. This persistent volume can reside in an NFS file system or other Kubernetes volume types. Start servers based on declarative startup parameters and desired states. Expose the OAM Services through external access. Scale OAM domains by starting and stopping Managed Servers on demand. Publish operator and WebLogic Server logs into Elasticsearch and interact with them in Kibana. Monitor the OAM instance using Prometheus and Grafana.  Current production release The current production release for the Oracle Access Management domain deployment on Kubernetes is 22.2.1. This release uses the WebLogic Kubernetes Operator version 3.3.0.\nThis release of the documentation can also be used for 3.1.X and 3.2.0 WebLogic Kubernetes Operator. For 3.0.X WebLogic Kubernetes Operator refer to Version 21.4.1\nRecent changes and known issues See the Release Notes for recent changes and known issues for Oracle Access Management domain deployment on Kubernetes.\nLimitations See here for limitations in this release.\nGetting started This documentation explains how to configure OAM on a Kubernetes cluster where no other Oracle Identity Management products will be deployed. For detailed information about this type of deployment , start at Prerequisites and follow this documentation sequentially.\nIf performing an Enterprise Deployment where multiple Oracle Identity Management products are deployed, refer to the Enterprise Deployment Guide for Oracle Identity and Access Management in a Kubernetes Cluster instead.\nDocumentation for earlier releases To view documentation for an earlier release, see:\n Version 21.4.2 Version 21.4.1  "
},
{
	"uri": "/fmw-kubernetes/oig/",
	"title": "Oracle Identity Governance",
	"tags": [],
	"description": "The WebLogic Kubernetes Operator supports deployment of Oracle Identity Governance. Follow the instructions in this guide to set up Oracle Identity Governance domains on Kubernetes.",
	"content": "The WebLogic Kubernetes Operator supports deployment of Oracle Identity Governance (OIG).\nIn this release, OIG domains are supported using the “domain on a persistent volume” model only, where the domain home is located in a persistent volume (PV).\nThe operator has several key features to assist you with deploying and managing OIG domains in a Kubernetes environment. You can:\n Create OIG instances in a Kubernetes persistent volume. This persistent volume can reside in an NFS file system or other Kubernetes volume types. Start servers based on declarative startup parameters and desired states. Expose the OIG Services for external access. Scale OIG domains by starting and stopping Managed Servers on demand. Publish operator and WebLogic Server logs into Elasticsearch and interact with them in Kibana. Monitor the OIG instance using Prometheus and Grafana.  Current production release The current production release for the Oracle Identity Governance domain deployment on Kubernetes is 22.2.1. This release uses the WebLogic Kubernetes Operator version 3.3.0.\nThis release of the documentation can also be used for 3.1.X and 3.2.0 WebLogic Kubernetes Operator. For 3.0.X WebLogic Kubernetes Operator refer to Version 21.4.1\nRecent changes and known issues See the Release Notes for recent changes and known issues for Oracle Identity Governance domain deployment on Kubernetes.\nLimitations See here for limitations in this release.\nGetting started This documentation explains how to configure OIG on a Kubernetes cluster where no other Oracle Identity Management products will be deployed. For detailed information about this type of deployment, start at Prerequisites and follow this documentation sequentially.\nIf performing an Enterprise Deployment, refer to the Enterprise Deployment Guide for Oracle Identity and Access Management in a Kubernetes Cluster instead.\nDocumentation for earlier releases To view documentation for an earlier release, see:\n Version 21.4.2 Version 21.4.1  "
},
{
	"uri": "/fmw-kubernetes/oud/",
	"title": "Oracle Unified Directory",
	"tags": [],
	"description": "Oracle Unified Directory provides a comprehensive Directory Solution for robust Identity Management",
	"content": "Oracle Unified Directory provides a comprehensive Directory Solution for robust Identity Management. Oracle Unified Directory is an all-in-one directory solution with storage, proxy, synchronization and virtualization capabilities. While unifying the approach, it provides all the services required for high-performance Enterprise and carrier-grade environments. Oracle Unified Directory ensures scalability to billions of entries, ease of installation, elastic deployments, enterprise manageability and effective monitoring.\nThis project supports deployment of Oracle Unified Directory (OUD) container images based on the 12cPS4 (12.2.1.4.0) release within a Kubernetes environment. The OUD container image refers to binaries for OUD Release 12.2.1.4.0 and it has the capability to create different types of OUD Instances (Directory Service, Proxy, Replication) in containers.\nThis project has several key features to assist you with deploying and managing Oracle Unified Directory in a Kubernetes environment. You can:\n Create Oracle Unified Directory instances in a Kubernetes persistent volume (PV). This PV can reside in an NFS file system or other Kubernetes volume types. Start servers based on declarative startup parameters and desired states. Expose the Oracle Unified Directory services for external access. Scale Oracle Unified Directory by starting and stopping servers on demand. Monitor the Oracle Unified Directory instance using Prometheus and Grafana.  Current production release The current production release for the Oracle Unified Directory 12c PS4 (12.2.1.4.0) deployment on Kubernetes is 22.2.1.\nRecent changes and known issues See the Release Notes for recent changes and known issues for Oracle Unified Directory deployment on Kubernetes.\nGetting started This documentation explains how to configure OUD on a Kubernetes cluster where no other Oracle Identity Management products will be deployed. For detailed information about this type of deployment, start at Prerequisites and follow this documentation sequentially.\nIf performing an Enterprise Deployment, refer to the Enterprise Deployment Guide for Oracle Identity and Access Management in a Kubernetes Cluster instead.\nDocumentation for earlier releases To view documentation for an earlier release, see:\n Version 21.4.2 Version 21.4.1  "
},
{
	"uri": "/fmw-kubernetes/oudsm/",
	"title": "Oracle Unified Directory Services Manager",
	"tags": [],
	"description": "Oracle Unified Directory Services Manager provides an interface for managing instances of Oracle Unified Directory",
	"content": "Oracle Unified Directory Services Manager (OUDSM) is an interface for managing instances of Oracle Unified Directory. Oracle Unified Directory Services Manager enables you to configure the structure of the directory, define objects in the directory, add and configure users, groups, and other entries. Oracle Unified Directory Services Manager is also the interface you use to manage entries, schema, security, and other directory features.\nThis project supports deployment of Oracle Unified Directory Services Manager images based on the 12cPS4 (12.2.1.4.0) release within a Kubernetes environment. The Oracle Unified Directory Services Manager Image refers to binaries for Oracle Unified Directory Services Manager Release 12.2.1.4.0.\nFollow the instructions in this guide to set up Oracle Unified Directory Services Manager on Kubernetes.\nCurrent production release The current production release for the Oracle Unified Directory 12c PS4 (12.2.1.4.0) deployment on Kubernetes is 22.2.1.\nRecent changes and known issues See the Release Notes for recent changes and known issues for Oracle Unified Directory deployment on Kubernetes.\nGetting started This documentation explains how to configure OUDSM on a Kubernetes cluster where no other Oracle Identity Management products will be deployed. For detailed information about this type of deployment, start at Prerequisites and follow this documentation sequentially.\nIf performing an Enterprise Deployment, refer to the Enterprise Deployment Guide for Oracle Identity and Access Management in a Kubernetes Cluster instead.\nDocumentation for earlier releases To view documentation for an earlier release, see:\n Version 21.4.2 Version 21.4.1  "
},
{
	"uri": "/fmw-kubernetes/wcportal-domains/installguide/prepare-your-environment/",
	"title": "Prepare your environment",
	"tags": [],
	"description": "Prepare for creating the Oracle WebCenter Portal domain, This preparation includes but not limited to creating required secrets, persistent volume and volume claim, and database schema.",
	"content": "Set up the environment, including setting up a Kubernetes cluster and the Weblogic Kubernetes Operator.\n  Install Helm\n  Set Up your Kubernetes Cluster\n  Obtain the Oracle WebCenter Portal Docker Image\n  Pull Other Dependent Images\n  Set Up the Code Repository to Deploy Oracle WebCenter Portal Domain\n  Grant Roles and Clear Stale Resources\n  Install the WebLogic Kubernetes Operator\n  Prepare the Environment for the WebCenter Portal Domain\na. Create a namespace for an Oracle WebCenter Portal domain\nb. Create a Kubernetes secret with domain credentials\nc. Create a Kubernetes secret with the RCU credentials\nd. Create a persistent storage for an Oracle WebCenter Portal domain\ne. Configure access to your database\nf. Run the Repository Creation Utility to set up your database schemas\n  Install Helm The operator uses Helm to create and deploy the necessary resources and then run the operator in a Kubernetes cluster. For Helm installation and usage information, see here.\nSet Up your Kubernetes Cluster If you need help in setting up a Kubernetes environment, check our cheat sheet.\nAfter creating Kubernetes clusters, you can optionally:\n Create load balancers to direct traffic to backend domain Configure Kibana and Elasticsearch for your operator logs  Obtain the Oracle WebCenter Portal Docker Image The Oracle WebCenter Portal image with latest bundle patch and required interim patches can be obtained from My Oracle Support (MOS). This is the only image supported for production deployments. Follow the below steps to download the Oracle WebCenter Portal image from My Oracle Support.\n  Download patch 33807917 from My Oracle Support (MOS).\n  Unzip the downloaded patch zip file.\n  Load the image archive using the docker load command.\nFor example:\n$ docker load \u0026lt; wcportal-12.2.1.4-jdk8-ol7-220203.0823.tar Loaded image: oracle/wcportal:12.2.1.4-jdk8-ol7-220203.0823   If you want to build and use an Oracle WebCenter Portal Docker image with any additional bundle patch or interim patches that are not part of the image obtained from My Oracle Support, then follow these steps to create the image.\n Note: The default Oracle WebCenter Portal image name used for Oracle WebCenter Portal domain deployment is oracle/wcportal:12.2.1.4. The image obtained must be tagged as oracle/wcportal:12.2.1.4 using the docker tag command. If you want to use a different name for the image, make sure to update the new image tag name in the create-domain-inputs.yaml file and also in other instances where the oracle/wcportal:12.2.1.4 image name is used.\n Pull Other Dependent Images Dependent images include WebLogic Kubernetes Operator, database, and Traefik. Pull these images and add them to your local registry:\n Pull these docker images and re-tag them as shown:  To pull an image from the Oracle Container Registry, in a web browser, navigate to https://container-registry.oracle.com and log in using the Oracle Single Sign-On authentication service. If you do not already have SSO credentials, at the top of the page, click the Sign In link to create them.\nUse the web interface to accept the Oracle Standard Terms and Restrictions for the Oracle software images that you intend to deploy. Your acceptance of these terms are stored in a database that links the software images to your Oracle Single Sign-On login credentials.\nThen, pull these docker images:\n$ docker login https://container-registry.oracle.com (enter your Oracle email Id and password) #This step is required once at every node to get access to the Oracle Container Registry. WebLogic Kubernetes Operator image:\n$ docker pull ghcr.io/oracle/weblogic-kubernetes-operator:3.3.0 Copy all the built and pulled images to all the nodes in your cluster or add to a Docker registry that your cluster can access.  NOTE: If you\u0026rsquo;re not running Kubernetes on your development machine, you\u0026rsquo;ll need to make the Docker image available to a registry visible to your Kubernetes cluster. Upload your image to a machine running Docker and Kubernetes as follows:\n# on your build machine $ docker save Image_Name:Tag \u0026gt; Image_Name-Tag.tar $ scp Image_Name-Tag.tar YOUR_USER@YOUR_SERVER:/some/path/Image_Name-Tag.tar # on the Kubernetes server $ docker load \u0026lt; /some/path/Image_Name-Tag.tar Set Up the Code Repository to Deploy Oracle WebCenter Portal Domain Oracle WebCenter Portal domain deployment on Kubernetes leverages the WebLogic Kubernetes Operator infrastructure. For deploying the Oracle WebCenter Portal domain, you need to set up the deployment scripts as below:\n  Create a working directory to set up the source code.\n$ mkdir $HOME/wcp_22.2.2 $ cd $HOME/wcp_22.2.2   Download the Oracle WebCenter Portal Kubernetes deployment scripts from the Github repository. Required artifacts are available at OracleWeCenterPortal/kubernetes.\n$ git clone https://github.com/oracle/fmw-kubernetes.git $ export WORKDIR=$HOME/wcp_22.2.2/fmw-kubernetes/OracleWebCenterPortal/kubernetes/   You can now use the deployment scripts from \u0026lt;$WORKDIR\u0026gt; to set up the WebCenter Portal domain as described later in this document.\nGrant Roles and Clear Stale Resources   To confirm if there is already a WebLogic custom resource definition, execute the following command:\n$ kubectl get crd NAME CREATED AT domains.weblogic.oracle 2020-03-14T12:10:21Z   Delete the WebLogic custom resource definition, if you find any, by executing the following command:\n$ kubectl delete crd domains.weblogic.oracle customresourcedefinition.apiextensions.k8s.io \u0026#34;domains.weblogic.oracle\u0026#34; deleted   Install the WebLogic Kubernetes Operator   Create a namespace for the WebLogic Kubernetes Operator:\n$ kubectl create namespace operator-ns namespace/operator-ns created NOTE: In this procedure, the namespace is called “operator-ns”. You can use any name.\nYou can use:\n domainUID/domainname as wcp-domain Domain namespace as wcpns Operator namespace as operator-ns traefik namespace as traefik    Create a service account for the WebLogic Kubernetes Operator in the operator\u0026rsquo;s namespace:\n$ kubectl create serviceaccount -n operator-ns operator-sa serviceaccount/operator-sa created   To be able to set up the log-stash and Elasticsearch after creating the domain, set the value of the field elkIntegrationEnabled to true in the file kubernetes/charts/weblogic-operator/values.yaml.\n  Use helm to install and start the WebLogic Kubernetes Operator from the downloaded repository:\n Helm install weblogic-operator\n $ cd ${WORKDIR} $ helm install weblogic-kubernetes-operator charts/weblogic-operator --namespace operator-ns --set serviceAccount=operator-sa --set \u0026#34;domainNamespaces={}\u0026#34; --wait NAME: weblogic-kubernetes-operator LAST DEPLOYED: Wed Jan 6 01:47:33 2021 NAMESPACE: operator-ns STATUS: deployed REVISION: 1 TEST SUITE: None ``\n  To verify that the operator\u0026rsquo;s pod is running, list the pods in the operator\u0026rsquo;s namespace. You should see one for the WebLogic Kubernetes Operator:\n$ kubectl get pods -n operator-ns NAME READY STATUS RESTARTS AGE weblogic-operator-67df5fddc5-tlc4b 2/2 Running 0 3m15s   Then, check by viewing the Operator pod\u0026rsquo;s log as shown in the following sample log snippet:\n$ kubectl logs -n operator-ns -c weblogic-operator deployments/weblogic-operator Launching Oracle WebLogic Server Kubernetes Operator... Importing keystore /operator/internal-identity/temp/weblogic-operator.jks to /operator/internal-identity/temp/weblogic-operator.p12... Entry for alias weblogic-operator-alias successfully imported. Import command completed: 1 entries successfully imported, 0 entries failed or cancelled Warning: The -srcstorepass option is specified multiple times. All except the last one will be ignored. MAC verified OK % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 4249 0 2394 100 1855 6884 5334 --:--:-- --:--:-- --:--:-- 6899 % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 5558 0 3028 100 2530 22704 18970 --:--:-- --:--:-- --:--:-- 22766 OpenJDK 64-Bit Server VM warning: Option MaxRAMFraction was deprecated in version 10.0 and will likely be removed in a future release. VM settings: Max. Heap Size (Estimated): 14.08G Using VM: OpenJDK 64-Bit Server VM {\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:53.438+0000\u0026#34;,\u0026#34;thread\u0026#34;:1,\u0026#34;fiber\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.TuningParametersImpl\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;update\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168593438,\u0026#34;message\u0026#34;:\u0026#34;Reloading tuning parameters from Operator\u0026#39;s config map\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;} {\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:53.944+0000\u0026#34;,\u0026#34;thread\u0026#34;:1,\u0026#34;fiber\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.Main\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;main\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168593944,\u0026#34;message\u0026#34;:\u0026#34;Oracle WebLogic Server Kubernetes Operator, version: 3.0.4, implementation: master.4d4fe0a, build time: 2019-11-15T21:19:56-0500\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;} {\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:53.972+0000\u0026#34;,\u0026#34;thread\u0026#34;:11,\u0026#34;fiber\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.Main\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;begin\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168593972,\u0026#34;message\u0026#34;:\u0026#34;Operator namespace is: operator-ns\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;} {\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:54.009+0000\u0026#34;,\u0026#34;thread\u0026#34;:11,\u0026#34;fiber\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.Main\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;begin\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168594009,\u0026#34;message\u0026#34;:\u0026#34;Operator target namespaces are: operator-ns\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;} {\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:54.013+0000\u0026#34;,\u0026#34;thread\u0026#34;:11,\u0026#34;fiber\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.Main\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;begin\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168594013,\u0026#34;message\u0026#34;:\u0026#34;Operator service account is: operator-sa\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;}\t{\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:54.031+0000\u0026#34;,\u0026#34;thread\u0026#34;:11,\u0026#34;fiber\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.helpers.HealthCheckHelper\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;performK8sVersionCheck\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168594031,\u0026#34;message\u0026#34;:\u0026#34;Verifying Kubernetes minimum version\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;}\t{\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:54.286+0000\u0026#34;,\u0026#34;thread\u0026#34;:11,\u0026#34;fiber\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.helpers.ClientPool\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;getApiClient\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168594286,\u0026#34;message\u0026#34;:\u0026#34;The Kuberenetes Master URL is set to https://10.96.0.1:443\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;}\t{\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:54.673+0000\u0026#34;,\u0026#34;thread\u0026#34;:11,\u0026#34;fiber\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.helpers.HealthCheckHelper\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;createAndValidateKubernetesVersion\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168594673,\u0026#34;message\u0026#34;:\u0026#34;Kubernetes version is: v1.13.7\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;}\t{\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:55.259+0000\u0026#34;,\u0026#34;thread\u0026#34;:12,\u0026#34;fiber\u0026#34;:\u0026#34;engine-operator-thread-2-fiber-1\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.helpers.CrdHelper$CrdContext$CreateResponseStep\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;onSuccess\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168595259,\u0026#34;message\u0026#34;:\u0026#34;Create Custom Resource Definition: oracle.kubernetes.operator.calls.CallResponse@470b40c\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;}\t{\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:55.356+0000\u0026#34;,\u0026#34;thread\u0026#34;:16,\u0026#34;fiber\u0026#34;:\u0026#34;fiber-1-child-2\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.helpers.HealthCheckHelper\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;performSecurityChecks\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168595356,\u0026#34;message\u0026#34;:\u0026#34;Verifying that operator service account can access required operations on required resources in namespace operator-ns\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;}\t{\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:55.598+0000\u0026#34;,\u0026#34;thread\u0026#34;:18,\u0026#34;fiber\u0026#34;:\u0026#34;fiber-1-child-2\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.helpers.ConfigMapHelper$ScriptConfigMapContext$CreateResponseStep\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;onSuccess\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168595598,\u0026#34;message\u0026#34;:\u0026#34;Creating domain config map, operator-ns, for namespace: {1}.\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;}\t{\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:55.937+0000\u0026#34;,\u0026#34;thread\u0026#34;:21,\u0026#34;fiber\u0026#34;:\u0026#34;fiber-1\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;WARNING\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.utils.Certificates\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;getCertificate\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168595937,\u0026#34;message\u0026#34;:\u0026#34;Can\u0026#39;t read certificate at /operator/external-identity/externalOperatorCert\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\\njava.nio.file.NoSuchFileException: /operator/external-identity/externalOperatorCert\\n\\tat java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)\\n\\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)\\n\\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)\\n\\tat java.base/sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:215)\\n\\tat java.base/java.nio.file.Files.newByteChannel(Files.java:370)\\n\\tat java.base/java.nio.file.Files.newByteChannel(Files.java:421)\\n\\tat java.base/java.nio.file.Files.readAllBytes(Files.java:3205)\\n\\tat oracle.kubernetes.operator.utils.Certificates.getCertificate(Certificates.java:48)\\n\\tat oracle.kubernetes.operator.utils.Certificates.getOperatorExternalCertificateData(Certificates.java:39)\\n\\tat oracle.kubernetes.operator.rest.RestConfigImpl.getOperatorExternalCertificateData(RestConfigImpl.java:52)\\n\\tat oracle.kubernetes.operator.rest.RestServer.isExternalSslConfigured(RestServer.java:383)\\n\\tat oracle.kubernetes.operator.rest.RestServer.start(RestServer.java:199)\\n\\tat oracle.kubernetes.operator.Main.startRestServer(Main.java:353)\\n\\tat oracle.kubernetes.operator.Main.completeBegin(Main.java:198)\\n\\tat oracle.kubernetes.operator.Main$NullCompletionCallback.onCompletion(Main.java:701)\\n\\tat oracle.kubernetes.operator.work.Fiber.completionCheck(Fiber.java:475)\\n\\tat oracle.kubernetes.operator.work.Fiber.run(Fiber.java:448)\\n\\tat oracle.kubernetes.operator.work.ThreadLocalContainerResolver.lambda$wrapExecutor$0(ThreadLocalContainerResolver.java:87)\\n\\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\\n\\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\\n\\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\\n\\tat java.base/java.lang.Thread.run(Thread.java:834)\\n\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;} {\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:55.967+0000\u0026#34;,\u0026#34;thread\u0026#34;:21,\u0026#34;fiber\u0026#34;:\u0026#34;fiber-1\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.rest.RestServer\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;start\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168595967,\u0026#34;message\u0026#34;:\u0026#34;Did not start the external ssl REST server because external ssl has not been configured.\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;} {\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:57.910+0000\u0026#34;,\u0026#34;thread\u0026#34;:21,\u0026#34;fiber\u0026#34;:\u0026#34;fiber-1\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.rest.RestServer\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;start\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168597910,\u0026#34;message\u0026#34;:\u0026#34;Started the internal ssl REST server on https://0.0.0.0:8082/operator\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;}\t{\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:57.913+0000\u0026#34;,\u0026#34;thread\u0026#34;:21,\u0026#34;fiber\u0026#34;:\u0026#34;fiber-1\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.Main\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;markReadyAndStartLivenessThread\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168597913,\u0026#34;message\u0026#34;:\u0026#34;Starting Operator Liveness Thread\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;}   Prepare the Environment for the WebCenter Portal Domain Create a namespace for an Oracle WebCenter Portal domain Unless you want to use the default namespace, create a Kubernetes namespace that can host one or more domains:\n$ kubectl create namespace wcpns namespace/wcpns created To manage domain in this namespace, configure the operator using helm:\n Helm upgrade weblogic-operator\n $ helm upgrade --reuse-values --set \u0026#34;domainNamespaces={wcpns}\u0026#34; \\  --wait weblogic-kubernetes-operator charts/weblogic-operator --namespace operator-ns NAME: weblogic-kubernetes-operator LAST DEPLOYED: Wed Jan 6 01:52:58 2021 NAMESPACE: operator-ns STATUS: deployed REVISION: 2 Create a Kubernetes secret with domain credentials Using the create-weblogic-credentials script, create a Kubernetes secret that contains the user name and password for the domain in the same Kubernetes namespace as the domain:\n$ cd ${WORKDIR}/create-weblogic-domain-credentials $ ./create-weblogic-credentials.sh -u weblogic -p welcome1 -n wcpns -d wcp-domain -s wcp-domain-domain-credentials secret/wcp-domain-domain-credentials created secret/wcp-domain-domain-credentials labeled The secret wcp-domain-domain-credentials has been successfully created in the wcpns namespace. Where: * weblogic is the weblogic username * welcome1 is the weblogic password * wcp-domain is the domain name * wcpns is the domain namespace * wcp-domain-domain-credentials is the secret name Note: You can inspect the credentials as follows:  $ kubectl get secret wcp-domain-domain-credentials -o yaml -n wcpns Create a Kubernetes secret with the RCU credentials Create a Kubernetes secret for the Repository Configuration Utility (user name and password) using the create-rcu-credentials.sh script in the same Kubernetes namespace as the domain:\n$ cd ${WORKDIR}/create-rcu-credentials $ sh create-rcu-credentials.sh \\  -u WCP1 -p welcome1 -a sys -q Oradoc_db1 -n wcpns \\  -d wcp-domain -s wcp-domain-rcu-credentials secret/wcp-domain-rcu-credentials created secret/wcp-domain-rcu-credentials labeled The secret wcp-domain-rcu-credentials has been successfully created in the wcpns namespace. Where: * WCP1 is the schema user * welcome1 is the schema password * Oradoc_db1 is the database SYS users password * wcp-domain is the domain name * wcpns is the domain namespace * wcp-domain-rcu-credentials is the secret name Note: You can inspect the credentials as follows:  $ kubectl get secret wcp-domain-rcu-credentials -o yaml -n wcpns Create a persistent storage for an Oracle WebCenter Portal domain Create a Kubernetes PV and PVC (Persistent Volume and Persistent Volume Claim):\nIn the Kubernetes namespace you created, create the PV and PVC for the domain by running the create-pv-pvc.sh script. Follow the instructions for using the script to create a dedicated PV and PVC for the Oracle WebCenter Portal domain.\n  Review the configuration parameters for PV creation here. Based on your requirements, update the values in the create-pv-pvc-inputs.yaml file located at ${WORKDIR}/create-weblogic-domain-pv-pvc/. Sample configuration parameter values for an Oracle WebCenter Portal domain are:\n baseName: domain domainUID: wcp-domain namespace: wcpns weblogicDomainStorageType: HOST_PATH weblogicDomainStoragePath: /scratch/kubevolume    Ensure that the path for the weblogicDomainStoragePath property exists (create one if it doesn\u0026rsquo;t exist), that it has full access permissions, and that the folder is empty.\n  Run the create-pv-pvc.sh script:\n$ cd ${WORKDIR}/create-weblogic-domain-pv-pvc $ ./create-pv-pvc.sh -i create-pv-pvc-inputs.yaml -o output Input parameters being used export version=\u0026#34;create-weblogic-sample-domain-pv-pvc-inputs-v1\u0026#34; export baseName=\u0026#34;domain\u0026#34; export domainUID=\u0026#34;wcp-domain\u0026#34; export namespace=\u0026#34;wcpns\u0026#34; export weblogicDomainStorageType=\u0026#34;HOST_PATH\u0026#34; export weblogicDomainStoragePath=\u0026#34;/scratch/kubevolume\u0026#34; export weblogicDomainStorageReclaimPolicy=\u0026#34;Retain\u0026#34; export weblogicDomainStorageSize=\u0026#34;10Gi\u0026#34; Generating output/pv-pvcs/wcp-domain-domain-pv.yaml Generating output/pv-pvcs/wcp-domain-domain-pvc.yaml The following files were generated: output/pv-pvcs/wcp-domain-domain-pv.yaml output/pv-pvcs/wcp-domain-domain-pvc.yaml   The create-pv-pvc.sh script creates a subdirectory pv-pvcs under the given /path/to/output-directory directory and creates two YAML configuration files for PV and PVC. Apply these two YAML files to create the PV and PVC Kubernetes resources using the kubectl create -f command:\n$ kubectl create -f output/pv-pvcs/wcp-domain-domain-pv.yaml $ kubectl create -f output/pv-pvcs/wcp-domain-domain-pvc.yaml   Configure access to your database Oracle WebCenter Portal domain requires a database which is configured with the necessary schemas. The Repository Creation Utility (RCU) allows you to create those schemas. You must set up the database before you create your domain.\nFor production deployments, you must set up and use a standalone (non-container) database running outside of Kubernetes.\nBefore creating a domain, you need to set up the necessary schemas in your database.\nRun the Repository Creation Utility to set up your database schemas To create the database schemas for Oracle WebCenter Portal domain, run the create-rcu-schema.sh script.\n$ cd ${WORKDIR}/create-rcu-schema $ sh create-rcu-schema.sh -h usage: create-rcu-schema.sh -s \u0026lt;schemaPrefix\u0026gt; -t \u0026lt;schemaType\u0026gt; -d \u0026lt;dburl\u0026gt; -i \u0026lt;image\u0026gt; -u \u0026lt;imagePullPolicy\u0026gt; -p \u0026lt;docker-store\u0026gt; -n \u0026lt;namespace\u0026gt; -q \u0026lt;sysPassword\u0026gt; -r \u0026lt;schemaPassword\u0026gt; -o \u0026lt;rcuOutputDir\u0026gt; [-h] -s RCU Schema Prefix (required) -t RCU Schema Type (optional) (supported values: wcp(default), wcpp) -d RCU Oracle Database URL (optional) (default: oracle-db.default.svc.cluster.local:1521/devpdb.k8s) -p FMW Infrastructure ImagePullSecret (optional) (default: none) -i Oracle WebCenter Portal Image (optional) (default: oracle/wcportal:12.2.1.4) -u FMW Infrastructure ImagePullPolicy (optional) (default: IfNotPresent) -n Namespace for RCU pod (optional) (default: default) -q password for database SYSDBA user. (optional) (default: Oradoc_db1) -r password for all schema owner (regular user). (optional) (default: Oradoc_db1) -o Output directory for the generated YAML file. (optional) (default: rcuoutput) -c Comma-separated variables in the format variablename=value. (optional). (default: none) -h Help $ ./create-rcu-schema.sh \\  -s WCP1 \\  -t wcp \\  -d oracle-db.default.svc.cluster.local:1521/devpdb.k8s \\  -i oracle/wcportal:12.2.1.4\\  -n wcpns \\  -q Oradoc_db1 \\  -r welcome1  Where RCU Schema type wcp generates webcenter portal related schema and wcpp generates webcenter portal plus portlet schemas.\n "
},
{
	"uri": "/fmw-kubernetes/oam/prerequisites/",
	"title": "Prerequisites",
	"tags": [],
	"description": "System requirements and limitations for deploying and running an OAM domain home",
	"content": "Introduction This document provides information about the system requirements and limitations for deploying and running OAM domains with the WebLogic Kubernetes Operator 3.3.0.\nSystem requirements for oam domains   A running Kubernetes cluster that meets the following requirements:\n The Kubernetes cluster must have sufficient nodes and resources. An installation of Helm is required on the Kubernetes cluster. Helm is used to create and deploy the necessary resources and run the WebLogic Kubernetes Operator in a Kubernetes cluster A supported container engine must be installed and running on the Kubernetes cluster. The Kubernetes cluster and container engine must meet the minimum version requirements outlined in document ID 2723908.1 on My Oracle Support. You must have the cluster-admin role to install the WebLogic Kubernetes Operator. The nodes in the Kubernetes cluster must have access to a persistent volume such as a Network File System (NFS) mount or a shared file system. The system clocks on node of the Kubernetes cluster must be synchronized. Run the date command simultaneously on all the nodes in each cluster and then syncrhonize accordingly.    A running Oracle Database 12.2.0.1 or later. The database must be a supported version for OAM as outlined in Oracle Fusion Middleware 12c certifications. It must meet the requirements as outlined in About Database Requirements for an Oracle Fusion Middleware Installation and in RCU Requirements for Oracle Databases.\n  Note: This documentation does not tell you how to install a Kubernetes cluster, Helm, the container engine, or how to push container images to a container registry. Please refer to your vendor specific documentation for this information.\nLimitations Compared to running a WebLogic Server domain in Kubernetes using the operator, the following limitations currently exist for OAM domains:\n In this release, OAM domains are supported using the “domain on a persistent volume” model only, where the domain home is located in a persistent volume (PV).The \u0026ldquo;domain in image\u0026rdquo; model is not supported. Only configured clusters are supported. Dynamic clusters are not supported for OAM domains. Note that you can still use all of the scaling features, you just need to define the maximum size of your cluster at domain creation time. The WebLogic Monitoring Exporter currently supports the WebLogic MBean trees only. Support for JRF MBeans has not been added yet. We do not currently support running OAM in non-Linux containers.  "
},
{
	"uri": "/fmw-kubernetes/oid/prerequisites/",
	"title": "Prerequisites",
	"tags": [],
	"description": "Prerequisites for deploying and running Oracle Internet Directory in a Kubernetes environment.",
	"content": "Introduction This document provides information about the system requirements for deploying and running Oracle Internet Directory 12c PS4 (12.2.1.4.0) in a Kubernetes environment.\nSystem Requirements for Oracle Internet Directory on Kubernetes   A running Kubernetes cluster that meets the following requirements:\n The Kubernetes cluster must have sufficient nodes and resources. An installation of Helm is required on the Kubernetes cluster. Helm is used to create and deploy the necessary resources on the Kubernetes cluster. A supported container engine must be installed and running on the Kubernetes cluster. The Kubernetes cluster and container engine must meet the minimum version requirements outlined in document ID 2723908.1 on My Oracle Support. The nodes in the Kubernetes cluster must have access to a persistent volume such as a Network File System (NFS) mount or a shared file system.    A running Oracle Database 12.2.0.1 or later. The database must be a supported version for OID as outlined in Oracle Fusion Middleware 12c certifications. It must meet the requirements as outlined in About Database Requirements for an Oracle Fusion Middleware Installation and in RCU Requirements for Oracle Databases.\n  Note: This documentation does not tell you how to install a Kubernetes cluster, Helm, the container engine, or how to push container images to a container registry. Please refer to your vendor specific documentation for this information.\n"
},
{
	"uri": "/fmw-kubernetes/oig/prerequisites/",
	"title": "Prerequisites",
	"tags": [],
	"description": "System requirements and limitations for deploying and running an OIG domain",
	"content": "Introduction This document provides information about the system requirements and limitations for deploying and running OIG domains with the WebLogic Kubernetes Operator 3.3.0.\nSystem requirements for OIG domains   A running Kubernetes cluster that meets the following requirements:\n The Kubernetes cluster must have sufficient nodes and resources. An installation of Helm is required on the Kubernetes cluster. Helm is used to create and deploy the necessary resources and run the WebLogic Kubernetes Operator in a Kubernetes cluster A supported container engine must be installed and running on the Kubernetes cluster. The Kubernetes cluster and container engine must meet the minimum version requirements outlined in document ID 2723908.1 on My Oracle Support. You must have the cluster-admin role to install the WebLogic Kubernetes Operator. The nodes in the Kubernetes cluster must have access to a persistent volume such as a Network File System (NFS) mount or a shared file system. The system clocks on node of the Kubernetes cluster must be synchronized. Run the date command simultaneously on all the nodes in each cluster and then syncrhonize accordingly.    A running Oracle Database 12.2.0.1 or later. The database must be a supported version for OIG as outlined in Oracle Fusion Middleware 12c certifications. It must meet the requirements as outlined in About Database Requirements for an Oracle Fusion Middleware Installation and in RCU Requirements for Oracle Databases.\n  Note: This documentation does not tell you how to install a Kubernetes cluster, Helm, the container engine, or how to push container images to a container registry. Please refer to your vendor specific documentation for this information.\nLimitations Compared to running a WebLogic Server domain in Kubernetes using the operator, the following limitations currently exist for OIG domains:\n In this release, OIG domains are supported using the “domain on a persistent volume” model only, where the domain home is located in a persistent volume (PV). The \u0026ldquo;domain in image\u0026rdquo; model is not supported. Only configured clusters are supported. Dynamic clusters are not supported for OIG domains. Note that you can still use all of the scaling features, you just need to define the maximum size of your cluster at domain creation time. The WebLogic Monitoring Exporter currently supports the WebLogic MBean trees only. Support for JRF MBeans has not been added yet. We do not currently support running OIG in non-Linux containers.  "
},
{
	"uri": "/fmw-kubernetes/oud/prerequisites/",
	"title": "Prerequisites",
	"tags": [],
	"description": "Oracle Unified Directory Prerequisites.",
	"content": "Introduction This document provides information about the system requirements for deploying and running Oracle Unified Directory 12c PS4 (12.2.1.4.0) in a Kubernetes environment.\nSystem Requirements for Oracle Unified Directory on Kubernetes  A running Kubernetes cluster that meets the following requirements:  The Kubernetes cluster must have sufficient nodes and resources. An installation of Helm is required on the Kubernetes cluster. Helm is used to create and deploy the necessary resources on the Kubernetes cluster. A supported container engine must be installed and running on the Kubernetes cluster. The Kubernetes cluster and container engine must meet the minimum version requirements outlined in document ID 2723908.1 on My Oracle Support. The nodes in the Kubernetes cluster must have access to a persistent volume such as a Network File System (NFS) mount or a shared file system.    Note: This documentation does not tell you how to install a Kubernetes cluster, Helm, the container engine, or how to push container images to a container registry. Please refer to your vendor specific documentation for this information.\n"
},
{
	"uri": "/fmw-kubernetes/oudsm/prerequisites/",
	"title": "Prerequisites",
	"tags": [],
	"description": "Oracle Unified Directory Services Manager Prerequisites.",
	"content": "Introduction This document provides information about the system requirements for deploying and running Oracle Unified Directory Services Manager 12c PS4 (12.2.1.4.0) in a Kubernetes environment.\nSystem Requirements for Oracle Unified Directory Services Manager on Kubernetes  A running Kubernetes cluster that meets the following requirements:  The Kubernetes cluster must have sufficient nodes and resources. An installation of Helm is required on the Kubernetes cluster. Helm is used to create and deploy the necessary resources on the Kubernetes cluster. A supported container engine must be installed and running on the Kubernetes cluster. The Kubernetes cluster and container engine must meet the minimum version requirements outlined in document ID 2723908.1 on My Oracle Support. The nodes in the Kubernetes cluster must have access to a persistent volume such as a Network File System (NFS) mount or a shared file system.    Note: This documentation does not tell you how to install a Kubernetes cluster, Helm, the container engine, or how to push container images to a container registry. Please refer to your vendor specific documentation for this information.\n"
},
{
	"uri": "/fmw-kubernetes/wccontent-domains/patch_and_upgrade/upgrade-operator-release/",
	"title": "Upgrade an WebLogic Kubernetes Operator release",
	"tags": [],
	"description": "Upgrade the WebLogic Kubernetes Operator release to a newer version.",
	"content": "These instructions apply to upgrading WebLogic Kubernetes Operators within the 3.x release family as additional versions are released.\nTo upgrade WebLogic Kubernetes Operator, use the helm upgrade command. Make sure that the weblogic-kubernetes-operator repository on your local machine is at the WebLogic Kubernetes Operator release to which you are upgrading. When upgrading the WebLogic Kubernetes Operator, the helm upgrade command requires that you supply a new Helm chart and image. For example:\n$ helm upgrade \\ --reuse-values \\ --set image=oracle/weblogic-kubernetes-operator:3.3.0 \\ --namespace weblogic-operator-namespace \\ --wait \\ weblogic-kubernetes-operator \\ kubernetes/charts/weblogic-operator "
},
{
	"uri": "/fmw-kubernetes/wcportal-domains/manage-wcportal-domains/monitoring-and-publishing-logs/publishing-logs/weblogiclogging/",
	"title": "WebLogic logging exporter",
	"tags": [],
	"description": "Use the WebLogic Logging Exporter to publish the WebLogic Server logs to Elasticsearch.",
	"content": "The WebLogic Logging Exporter adds a log event handler to WebLogic Server which enables WebLogic Server to push the logs to Elasticsearch in Kubernetes by using the Elasticsearch REST API. For more details, see to the WebLogic Logging Exporter project.\nThis sample shows you how to publish WebLogic Server logs to Elasticsearch and view them in Kibana. For publishing operator logs, see this sample.\nPrerequisites This document assumes that you have already set up Elasticsearch and Kibana for logs collection. If you have not, please see this document.\n Download the WebLogic Logging Exporter binaries The pre-built binaries are available on the WebLogic Logging Exporter Releases page.\nDownload:\n weblogic-logging-exporter-1.0.0.jar from the release page snakeyaml-1.25.jar from Maven Central  These identifiers are used in the sample commands.\n wcpns: WebCenter Portal domain namespace wcp-domain: domainUID wcp-domain-adminserver: Administration Server pod name   Copy the JAR Files to the WebLogic Domain Home Copy the weblogic-logging-exporter-1.0.0.jar and snakeyaml-1.25.jar files to the domain home directory in the Administration Server pod.\n$ kubectl cp \u0026lt;file-to-copy\u0026gt; \u0026lt;namespace\u0026gt;/\u0026lt;Administration-Server-pod\u0026gt;:\u0026lt;domainhome\u0026gt; $ kubectl cp snakeyaml-1.25.jar wcpns/wcp-domain-adminserver:/u01/oracle/user_projects/domains/wcp-domain/ $ kubectl cp weblogic-logging-exporter-1.0.0.jar wcpns/wcp-domain-adminserver:/u01/oracle/user_projects/domains/wcp-domain/ Add a Startup Class to the Domain Configuration   In the WebLogic Server Administration Console, in the left navigation pane, expand Environment, and then select Startup and Shutdown Classes.\n  Add a new startup class. You may choose any descriptive name, however, the class name must be weblogic.logging.exporter.Startup.\n  Target the startup class to each server from which you want to export logs.\n  In your config.xml file located at, /u01/oracle/user_projects/domains/wcp-domain/config/config.xml the newly added startup-class must exist as shown below:\n$ kubectl exec -it wcp-domain-adminserver -n wcpns cat /u01/oracle/user_projects/domains/wcp-domain/config/config.xml \u0026lt;startup-class\u0026gt; \u0026lt;name\u0026gt;weblogic-logging-exporter\u0026lt;/name\u0026gt; \u0026lt;target\u0026gt;AdminServer,wcp_cluster\u0026lt;/target\u0026gt; \u0026lt;class-name\u0026gt;weblogic.logging.exporter.Startup\u0026lt;/class-name\u0026gt; \u0026lt;/startup-class\u0026gt;   Update the WebLogic Server CLASSPATH  Copy the setDomainEnv.sh file from the pod to a local folder:  $ kubectl cp wcpns/wcp-domain-adminserver:/u01/oracle/user_projects/domains/wcp-domain/bin/setDomainEnv.sh $PWD/setDomainEnv.sh tar: Removing leading `/\u0026#39; from member names Ignore exception: tar: Removing leading '/' from member names\n Update the server class path in setDomainEnv.sh:  CLASSPATH=/u01/oracle/user_projects/domains/wcp-domain/weblogic-logging-exporter-1.0.0.jar:/u01/oracle/user_projects/domains/wcp-domain/snakeyaml-1.25.jar:${CLASSPATH} export CLASSPATH  Copy back the modified setDomainEnv.sh file to the pod:  $ kubectl cp setDomainEnv.sh wcpns/wcp-domain-adminserver:/u01/oracle/user_projects/domains/wcp-domain/bin/setDomainEnv.sh Create a Configuration File for the WebLogic Logging Exporter   Specify the Elasticsearch server host and port number in the file: \u0026lt;$WORKDIR\u0026gt;/logging-services/weblogic-logging-exporter/WebLogicLoggingExporter.yaml\nExample:\nweblogicLoggingIndexName: wls publishHost: elasticsearch.default.svc.cluster.local publishPort: 9300 domainUID: wcp-domain weblogicLoggingExporterEnabled: true weblogicLoggingExporterSeverity: TRACE weblogicLoggingExporterBulkSize: 1   Copy the WebLogicLoggingExporter.yaml file to the domain home directory in the WebLogic Administration Server pod:\n  $ kubectl cp \u0026lt;$WORKDIR\u0026gt;/logging-services/weblogic-logging-exporter/WebLogicLoggingExporter.yaml wcpns/wcp-domain-adminserver:/u01/oracle/user_projects/domains/wcp-domain/config/ Restart the Servers in the Domain To restart the servers, stop and then start them using the following commands:\nTo stop the servers:\n$ kubectl patch domain wcp-domain -n wcpns --type=\u0026#39;json\u0026#39; -p=\u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/serverStartPolicy\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;NEVER\u0026#34; }]\u0026#39; To start the servers:\n$ kubectl patch domain wcp-domain -n wcpns --type=\u0026#39;json\u0026#39; -p=\u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/serverStartPolicy\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;IF_NEEDED\u0026#34; }]\u0026#39; After all the servers are restarted, see their server logs to check that the weblogic-logging-exporter class is called, as shown below:\n======================= WebLogic Logging Exporter Startup class called Reading configuration from file name: /u01/oracle/user_projects/domains/wcp-domain/config/WebLogicLoggingExporter.yaml Config{weblogicLoggingIndexName='wls', publishHost='domain.host.com', publishPort=9200, weblogicLoggingExporterSeverity='Notice', weblogicLoggingExporterBulkSize='2', enabled=true, weblogicLoggingExporterFilters=FilterConfig{expression='NOT(MSGID = 'BEA-000449')', servers=[]}], domainUID='wcp-domain'} Create an Index Pattern in Kibana Create an index pattern wls* in Kibana by navigating to the dashboard through the Management option. After the servers are started, the log data is displayed on the Kibana dashboard:\n"
},
{
	"uri": "/fmw-kubernetes/oig/manage-oig-domains/wlst-admin-operations/",
	"title": "WLST administration operations",
	"tags": [],
	"description": "Describes the steps for WLST administration using helper pod running in the same Kubernetes Cluster as OIG Domain.",
	"content": "Invoke WLST and access Administration Server To use WLST to administer the OIG domain, use a helper pod in the same Kubernetes cluster as the OIG Domain.\n  Check to see if the helper pod exists by running:\n$ kubectl get pods -n \u0026lt;domain_namespace\u0026gt; | grep helper For example:\n$ kubectl get pods -n oigns | grep helper The output should look similar to the following:\nhelper 1/1 Running 0 26h If the helper pod doesn\u0026rsquo;t exist then see Step 1 in Prepare your environment  to create it.\n  Run the following command to start a bash shell in the helper pod:\n$ kubectl exec -it helper -n \u0026lt;domain_namespace\u0026gt; -- /bin/bash For example:\n$ kubectl exec -it helper -n oigns -- /bin/bash This will take you into a bash shell in the running helper pod:\n[oracle@helper ~]$   Connect to WLST using the following commands:\n[oracle@helper ~]$ cd $ORACLE_HOME/oracle_common/common/bin [oracle@helper ~]$ ./wlst.sh The output will look similar to the following:\nInitializing WebLogic Scripting Tool (WLST) ... Jython scans all the jar files it can find at first startup. Depending on the system, this process may take a few minutes to complete, and WLST may not return a prompt right away. Welcome to WebLogic Server Administration Scripting Shell Type help() for help on available commands wls:/offline\u0026gt;   To access t3 for the Administration Server connect as follows:\nwls:/offline\u0026gt; connect(\u0026#39;weblogic\u0026#39;,\u0026#39;\u0026lt;password\u0026gt;\u0026#39;,\u0026#39;t3://governancedomain-adminserver:7001\u0026#39;) The output will look similar to the following:\nConnecting to t3://governancedomain-adminserver:7001 with userid weblogic ... Successfully connected to Admin Server \u0026quot;AdminServer\u0026quot; that belongs to domain \u0026quot;governancedomain\u0026quot;. Warning: An insecure protocol was used to connect to the server. To ensure on-the-wire security, the SSL port or Admin port should be used instead. wls:/governancedomain/serverConfig/\u0026gt; Or to access t3 for the OIG Cluster service, connect as follows:\nwls:/offline\u0026gt; connect(\u0026#39;weblogic\u0026#39;,\u0026#39;\u0026lt;password\u0026gt;\u0026#39;,\u0026#39;t3://governancedomain-cluster-oim-cluster:14000\u0026#39;) The output will look similar to the following:\nConnecting to t3://governancedomain-cluster-oim-cluster:14000 with userid weblogic ... Successfully connected to managed Server \u0026quot;oim_server1\u0026quot; that belongs to domain \u0026quot;governancedomain\u0026quot;. Warning: An insecure protocol was used to connect to the server. To ensure on-the-wire security, the SSL port or Admin port should be used instead. wls:/governancedomain/serverConfig/\u0026gt;   Sample operations For a full list of WLST operations refer to WebLogic Server WLST Online and Offline Command Reference.\nDisplay servers wls:/governancedomain/serverConfig/\u0026gt; cd('/Servers') wls:/governancedomain/serverConfig/Servers\u0026gt; ls () dr-- AdminServer dr-- oim_server1 dr-- oim_server2 dr-- oim_server3 dr-- oim_server4 dr-- oim_server5 dr-- soa_server1 dr-- soa_server2 dr-- soa_server3 dr-- soa_server4 dr-- soa_server5 wls:/governancedomain/serverConfig/Servers\u0026gt; Performing WLST administration via SSL   By default the SSL port is not enabled for the Administration Server or OIG Managed Servers. To configure the SSL port for the Administration Server and Managed Servers login to WebLogic Administration console https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/console and navigate to Lock \u0026amp; Edit -\u0026gt; Environment -\u0026gt;Servers -\u0026gt; server_name -\u0026gt;Configuration -\u0026gt; General -\u0026gt; SSL Listen Port Enabled -\u0026gt; Provide SSL Port ( For Administration Server: 7002 and for OIG Managed Server (oim_server1): 14101) - \u0026gt; Save -\u0026gt; Activate Changes.\nNote: If configuring the OIG Managed Servers for SSL you must enable SSL on the same port for all servers (oim_server1 through oim_server4)\n  Create a myscripts directory as follows:\n$ cd $WORKDIR/kubernetes $ mkdir myscripts $ cd myscripts   Create a sample yaml template file in the myscripts directory called \u0026lt;domain_uid\u0026gt;-adminserver-ssl.yaml to create a Kubernetes service for the Administration Server:\nNote: Update the domainName, domainUID and namespace based on your environment.\napiVersion: v1 kind: Service metadata: labels: serviceType: SERVER weblogic.domainName: governancedomain weblogic.domainUID: governancedomain weblogic.resourceVersion: domain-v2 weblogic.serverName: AdminServer name: governancedomain-adminserver-ssl namespace: oigns spec: clusterIP: None ports: - name: default port: 7002 protocol: TCP targetPort: 7002 selector: weblogic.createdByOperator: \u0026quot;true\u0026quot; weblogic.domainUID: governancedomain weblogic.serverName: AdminServer type: ClusterIP and create the following sample yaml template file \u0026lt;domain_uid\u0026gt;-oim-cluster-ssl.yaml for the OIG Managed Server:\napiVersion: v1 kind: Service metadata: labels: serviceType: SERVER weblogic.domainName: governancedomain weblogic.domainUID: governancedomain weblogic.resourceVersion: domain-v2 name: governancedomain-cluster-oim-cluster-ssl namespace: oigns spec: clusterIP: None ports: - name: default port: 14101 protocol: TCP targetPort: 14101 selector: weblogic.clusterName: oim_cluster weblogic.createdByOperator: \u0026quot;true\u0026quot; weblogic.domainUID: governancedomain type: ClusterIP   Apply the template using the following command for the Administration Server:\n$ kubectl apply -f governancedomain-adminserver-ssl.yaml service/governancedomain-adminserver-ssl created or using the following command for the OIG Managed Server:\n$ kubectl apply -f governancedomain-oim-cluster-ssl.yaml service/governancedomain-cluster-oim-cluster-ssl created   Validate that the Kubernetes Services to access SSL ports are created successfully:\n$ kubectl get svc -n \u0026lt;domain_namespace\u0026gt; |grep ssl For example:\n$ kubectl get svc -n oigns |grep ssl The output will look similar to the following:\ngovernancedomain-adminserver-ssl ClusterIP None \u0026lt;none\u0026gt; 7002/TCP 74s governancedomain-cluster-oim-cluster-ssl ClusterIP None \u0026lt;none\u0026gt; 14101/TCP 21s   Connect to a bash shell of the helper pod:\n$ kubectl exec -it helper -n oigns -- /bin/bash   In the bash shell run the following:\n[oracle@governancedomain-adminserver oracle]$ export WLST_PROPERTIES=\u0026#34;-Dweblogic.security.SSL.ignoreHostnameVerification=true -Dweblogic.security.TrustKeyStore=DemoTrust\u0026#34; [oracle@governancedomain-adminserver oracle]$ cd /u01/oracle/oracle_common/common/bin [oracle@governancedomain-adminserver oracle]$ ./wlst.sh Initializing WebLogic Scripting Tool (WLST) ... Welcome to WebLogic Server Administration Scripting Shell Type help() for help on available commands wls:/offline\u0026gt; Connect to the Administration Server t3s service:\nwls:/offline\u0026gt; connect(\u0026#39;weblogic\u0026#39;,\u0026#39;\u0026lt;password\u0026gt;\u0026#39;,\u0026#39;t3s://governancedomain-adminserver-ssl:7002\u0026#39;) Connecting to t3s://governancedomain-adminserver-ssl:7002 with userid weblogic ... \u0026lt;Mar 10, 2022 4:51:43 PM GMT\u0026gt; \u0026lt;Info\u0026gt; \u0026lt;Security\u0026gt; \u0026lt;BEA-090905\u0026gt; \u0026lt;Disabling the CryptoJ JCE Provider self-integrity check for better startup performance. To enable this check, specify -Dweblogic.security.allowCryptoJDefaultJCEVerification=true.\u0026gt; \u0026lt;Mar 10, 2022 4:51:43 PM GMT\u0026gt; \u0026lt;Info\u0026gt; \u0026lt;Security\u0026gt; \u0026lt;BEA-090906\u0026gt; \u0026lt;Changing the default Random Number Generator in RSA CryptoJ from ECDRBG128 to HMACDRBG. To disable this change, specify -Dweblogic.security.allowCryptoJDefaultPRNG=true.\u0026gt; \u0026lt;Mar 10, 2022 4:51:43 PM GMT\u0026gt; \u0026lt;Info\u0026gt; \u0026lt;Security\u0026gt; \u0026lt;BEA-090909\u0026gt; \u0026lt;Using the configured custom SSL Hostname Verifier implementation: weblogic.security.utils.SSLWLSHostnameVerifier$NullHostnameVerifier.\u0026gt; Successfully connected to Admin Server \u0026#34;AdminServer\u0026#34; that belongs to domain \u0026#34;governancedomain\u0026#34;. wls:/governancedomain/serverConfig/\u0026gt; To connect to the OIG Managed Server t3s service:\nwls:/offline\u0026gt; connect(\u0026#39;weblogic\u0026#39;,\u0026#39;\u0026lt;password\u0026gt;\u0026#39;,\u0026#39;t3s://governancedomain-cluster-oim-cluster-ssl:14101\u0026#39;) Connecting to t3s://governancedomain-cluster-oim-cluster-ssl:14101 with userid weblogic ... \u0026lt;Mar 10, 2022 4:53:06 PM GMT\u0026gt; \u0026lt;Info\u0026gt; \u0026lt;Security\u0026gt; \u0026lt;BEA-090905\u0026gt; \u0026lt;Disabling the CryptoJ JCE Provider self-integrity check for better startup performance. To enable this check, specify -Dweblogic.security.allowCryptoJDefaultJCEVerification=true.\u0026gt; \u0026lt;Mar 10, 2022 4:53:06 PM GMT\u0026gt; \u0026lt;Info\u0026gt; \u0026lt;Security\u0026gt; \u0026lt;BEA-090906\u0026gt; \u0026lt;Changing the default Random Number Generator in RSA CryptoJ from ECDRBG128 to HMACDRBG. To disable this change, specify -Dweblogic.security.allowCryptoJDefaultPRNG=true.\u0026gt; \u0026lt;Mar 10, 2022 4:53:06 PM GMT\u0026gt; \u0026lt;Info\u0026gt; \u0026lt;Security\u0026gt; \u0026lt;BEA-090909\u0026gt; \u0026lt;Using the configured custom SSL Hostname Verifier implementation: weblogic.security.utils.SSLWLSHostnameVerifier$NullHostnameVerifier.\u0026gt; Successfully connected to managed Server \u0026#34;oim_server1\u0026#34; that belongs to domain \u0026#34;governancedomain\u0026#34;. wls:/governancedomain/serverConfig/\u0026gt;   "
},
{
	"uri": "/fmw-kubernetes/soa-domains/adminguide/deploying-composites/deploy-artifacts/",
	"title": "Deploy using composites in a persistent volume or image",
	"tags": [],
	"description": "Deploy Oracle SOA Suite and Oracle Service Bus composite applications artifacts in a persistent volume or in an image.",
	"content": "Learn how to deploy Oracle SOA Suite and Oracle Service Bus composite applications artifacts in a Kubernetes persistent volume or in an image to an Oracle SOA Suite environment deployed using a WebLogic Kubernetes Operator.\nThe deployment methods described in Deploy using JDeveloper and Deploy using Maven and Ant are manual processes. If you have the deployment artifacts (archives) already built, then you can package them either into a Kubernetes persistent volume or in an image and use this automated process to deploy the artifacts to an Oracle SOA Suite domain.\nPrepare to use the deploy artifacts script The sample scripts for deploying artifacts are available at ${WORKDIR}/create-soa-domain/domain-home-on-pv/\nYou must edit deploy-artifacts-inputs.yaml (or a copy of it) to provide the details of your domain and artifacts. Refer to the configuration parameters below to understand the information that you must provide in this file.\nConfiguration parameters The following parameters can be provided in the inputs file.\n   Parameter Definition Default     adminPort Port number of the Administration Server inside the Kubernetes cluster. 7001   adminServerName Name of the Administration Server. AdminServer   domainUID Unique ID that is used to identify the domain. This ID cannot contain any characters that are not valid in a Kubernetes service name. soainfra   domainType Type of the domain. Mandatory input for Oracle SOA Suite domains. You must provide one of the supported domain type values: soa (deploys artifacts into an Oracle SOA Suite domain), osb (deploys artifacts into an Oracle Service Bus domain), or soaosb (deploys artifacts into both Oracle SOA Suite and Oracle Service Bus domains). soa   soaClusterName Name of the SOA WebLogic Server cluster instance in the domain. By default, the cluster name is soa_cluster. This configuration parameter is applicable only for soa and soaosb domain types. soa_cluster   image SOA Suite Docker image. The artifacts deployment process requires Oracle SOA Suite 12.2.1.4. Refer to Obtain the Oracle SOA Suite Docker image for details on how to obtain or create the image. soasuite:12.2.1.4   imagePullPolicy Oracle SOA Suite Docker image pull policy. Valid values are IfNotPresent, Always, Never. IfNotPresent   imagePullSecretName Name of the Kubernetes secret to access the Docker Store to pull the Oracle SOA Suite Docker image. The presence of the secret will be validated when this parameter is specified.    weblogicCredentialsSecretName Name of the Kubernetes secret for the Administration Server\u0026rsquo;s user name and password. If not specified, then the value is derived from the domainUID as \u0026lt;domainUID\u0026gt;-weblogic-credentials. soainfra-domain-credentials   namespace Kubernetes namespace in which the domain was created. soans   artifactsSourceType The deploy artifacts source type. Set to PersistentVolume for deploy artifacts available in a persistent volume and Image for deploy artifacts available as an image. Image   persistentVolumeClaimName Name of the persistent volume claim created that hosts the deployment artifacts. If not specified, the value is derived from the domainUID as \u0026lt;domainUID\u0026gt;-deploy-artifacts-pvc. soainfra-deploy-artifacts-pvc   artifactsImage Deploy artifacts image. Required if artifactsSourceType is Image. artifacts:12.2.1.4   artifactsImagePullPolicy Deploy artifacts image pull policy. Valid values are IfNotPresent, Always, Never. IfNotPresent   artifactsImagePullSecretName Name of the Kubernetes secret to access the deploy artifacts image. The presence of the secret will be validated when this parameter is specified.    deployScriptFilesDir Directory on the host machine to locate the required files to deploy artifacts to the Oracle SOA Suite domain, including the script that is specified in the deployScriptName parameter. By default, this directory is set to the relative path deploy. deploy   deployScriptsMountPath Mount path where the deploy artifacts scripts are located inside a pod. The deploy-artifacts.sh script creates a Kubernetes job to run the script (specified by the deployScriptName parameter) in a Kubernetes pod to deploy the artifacts. Files in the deployScriptFilesDir directory are mounted to this location in the pod, so that the Kubernetes pod can use the scripts and supporting files to deploy artifacts. /u01/weblogic   deployScriptName Script that the deploy artifacts script uses to deploy artifacts to the Oracle SOA Suite domain. For Oracle SOA Suite, the script placed in the soa directory is used. For Oracle Service Bus, the script placed in the osb directory is used. The deploy-artifacts.sh script creates a Kubernetes job to run this script to deploy artifacts. The script is located in the in-pod directory that is specified by the deployScriptsMountPath parameter. deploy.sh   soaArtifactsArchivePath Directory inside container where Oracle SOA Suite archives are placed. /u01/sarchives   osbArtifactsArchivePath Directory inside container where Oracle Service Bus archives are placed. /u01/sbarchives    The sample demonstrates how to deploy Oracle SOA Suite composites or Oracle Service Bus applications to an Oracle SOA Suite domain home.\nRun the deploy artifacts script Run the deploy artifacts script, specifying your inputs file and an output directory to store the generated artifacts:\n$ ./deploy-artifacts.sh \\ -i deploy-artifacts-inputs.yaml \\ -o \u0026lt;path to output-directory\u0026gt; The script performs the following steps:\n Creates a directory for the generated Kubernetes YAML files for the artifacts deployment process if it does not already exist. The path name is \u0026lt;path to output-directory\u0026gt;/weblogic-domains/\u0026lt;domainUID\u0026gt;/\u0026lt;YYYYMMDD-hhmmss\u0026gt;. If the directory already exists, its contents must be removed before running this script. Creates a Kubernetes job that starts a utility Oracle SOA Suite container and run scripts to deploy artifacts provided either in an image or in a persistent volume.  Deploy artifacts from an image   Create an image with artifacts\na. A sample Dockerfile to create the artifacts in an image is available at $WORKDIR/create-soa-domain/domain-home-on-pv/deploy-docker-file. This expects the Oracle SOA Suite related archives to be available in the soa directory and Oracle Service Bus archives to be available in the osb directory.\nb. Create the soa directory and copy the Oracle SOA Suite archives to be deployed to the directory:\n$ cd $WORKDIR/create-soa-domain/domain-home-on-pv/deploy-docker-file $ mkdir soa $ cp /path/sca_sampleBPEL.jar soa c. Create the osb directory and copy the Oracle Service Bus archives to be deployed to the directory:\n$ cd $WORKDIR/create-soa-domain/domain-home-on-pv/deploy-docker-file $ mkdir osb $ cp /path/simple_sbconfig.jar osb d. Create the image using build.sh. This script creates the image with default tag 12.2.1.4 (artifacts:12.2.1.4):\n$ cd $WORKDIR/create-soa-domain/domain-home-on-pv/deploy-docker-file $ ./build.sh -h Usage: build.sh -t [tag] Builds a Docker Image with Oracle SOA/OSB artifacts Parameters: -h: view usage -t: tag for image, default is 12.2.1.4    Click here to see sample output of script with tag 12.2.1.4-v1    ``` $ ./build.sh -t 12.2.1.4-v1 Sending build context to Docker daemon 36.35kB Step 1/13 : FROM busybox ---\u0026gt; 16ea53ea7c65 Step 2/13 : ARG SOA_ARTIFACTS_ARCHIVE_PATH=/u01/sarchives ---\u0026gt; Using cache ---\u0026gt; 411edf07f267 Step 3/13 : ARG OSB_ARTIFACTS_ARCHIVE_PATH=/u01/sbarchives ---\u0026gt; Using cache ---\u0026gt; c4214b9cf0ae Step 4/13 : ARG USER=oracle ---\u0026gt; Using cache ---\u0026gt; c8ebcd5ee546 Step 5/13 : ARG USERID=1000 ---\u0026gt; Using cache ---\u0026gt; 5780beb0c3cf Step 6/13 : ARG GROUP=root ---\u0026gt; Using cache ---\u0026gt; 048e67c71f92 Step 7/13 : ENV SOA_ARTIFACTS_ARCHIVE_PATH=${SOA_ARTIFACTS_ARCHIVE_PATH} ---\u0026gt; Using cache ---\u0026gt; 31ae33cfd9bb Step 8/13 : ENV OSB_ARTIFACTS_ARCHIVE_PATH=${OSB_ARTIFACTS_ARCHIVE_PATH} ---\u0026gt; Using cache ---\u0026gt; 79602bf64dc0 Step 9/13 : RUN adduser -D -u ${USERID} -G $GROUP $USER ---\u0026gt; Using cache ---\u0026gt; 07c12cea52f9 Step 10/13 : COPY soa/ ${SOA_ARTIFACTS_ARCHIVE_PATH}/ ---\u0026gt; bfeb138516d8 Step 11/13 : COPY osb/ ${OSB_ARTIFACTS_ARCHIVE_PATH}/ ---\u0026gt; 0359a11f8f76 Step 12/13 : RUN chown -R $USER:$GROUP ${SOA_ARTIFACTS_ARCHIVE_PATH}/ ${OSB_ARTIFACTS_ARCHIVE_PATH}/ ---\u0026gt; Running in 285fb2bd8434 Removing intermediate container 285fb2bd8434 ---\u0026gt; 2e8d8c337de0 Step 13/13 : USER $USER ---\u0026gt; Running in c9db494e46ab Removing intermediate container c9db494e46ab ---\u0026gt; 40295aa15317 Successfully built 40295aa15317 Successfully tagged artifacts:12.2.1.4-v1 INFO: Artifacts image for Oracle SOA suite is ready to be extended. --\u0026gt; artifacts:12.2.1.4-v1 INFO: Build completed in 4 seconds. ```      Update the image details in deploy-artifacts-inputs.yaml for parameter artifactsImage and invoke deploy-artifacts.sh to perform deployment of artifacts.\n  Click here to see sample output of deployment for domainType of soaosb   $ ./deploy-artifacts.sh -i deploy-artifacts-inputs.yaml -o out-deploy Input parameters being used export version=\u0026quot;deploy-artifacts-inputs-v1\u0026quot; export adminPort=\u0026quot;7001\u0026quot; export adminServerName=\u0026quot;AdminServer\u0026quot; export domainUID=\u0026quot;soainfra\u0026quot; export domainType=\u0026quot;soaosb\u0026quot; export soaClusterName=\u0026quot;soa_cluster\u0026quot; export soaManagedServerPort=\u0026quot;8001\u0026quot; export image=\u0026quot;soasuite:12.2.1.4\u0026quot; export imagePullPolicy=\u0026quot;IfNotPresent\u0026quot; export weblogicCredentialsSecretName=\u0026quot;soainfra-domain-credentials\u0026quot; export namespace=\u0026quot;soans\u0026quot; export artifactsSourceType=\u0026quot;Image\u0026quot; export artifactsImage=\u0026quot;artifacts:12.2.1.4-v1\u0026quot; export artifactsImagePullPolicy=\u0026quot;IfNotPresent\u0026quot; export deployScriptsMountPath=\u0026quot;/u01/weblogic\u0026quot; export deployScriptName=\u0026quot;deploy.sh\u0026quot; export deployScriptFilesDir=\u0026quot;deploy\u0026quot; export soaArtifactsArchivePath=\u0026quot;/u01/sarchives\u0026quot; export osbArtifactsArchivePath=\u0026quot;/u01/sbarchives\u0026quot; Generating out-deploy/deploy-artifacts/soainfra/20211022-152335/deploy-artifacts-job.yaml Checking to see if the secret soainfra-domain-credentials exists in namespace soans configmap/soainfra-deploy-scripts-soa-job-cm created Checking the configmap soainfra-deploy-scripts-soa-job-cm was created configmap/soainfra-deploy-scripts-soa-job-cm labeled configmap/soainfra-deploy-scripts-osb-job-cm created Checking the configmap soainfra-deploy-scripts-osb-job-cm was created configmap/soainfra-deploy-scripts-osb-job-cm labeled Checking if object type job with name soainfra-deploy-artifacts-job-20211022-152335 exists Deploying artifacts by creating the job out-deploy/deploy-artifacts/soainfra/20211022-152335/deploy-artifacts-job.yaml job.batch/soainfra-deploy-artifacts-job-20211022-152335 created Waiting for the job to complete... status on iteration 1 of 20 for soainfra pod soainfra-deploy-artifacts-job-20211022-152335-r7ffj status is NotReady status on iteration 2 of 20 for soainfra pod soainfra-deploy-artifacts-job-20211022-152335-r7ffj status is Completed configmap \u0026quot;soainfra-deploy-scripts-soa-job-cm\u0026quot; deleted configmap \u0026quot;soainfra-deploy-scripts-osb-job-cm\u0026quot; deleted The following files were generated: out-deploy/deploy-artifacts/soainfra/20211022-152335/deploy-artifacts-inputs.yaml out-deploy/deploy-artifacts/soainfra/20211022-152335/deploy-artifacts-job.yaml Completed $ kubectl get all -n soans|grep deploy pod/soainfra-deploy-artifacts-job-20211022-152335-r7ffj 0/2 Completed 0 15m job.batch/soainfra-deploy-artifacts-job-20211022-152335 1/1 43s 15m $     Note: When you are running the script for domainType soaosb, a deployment pod is created with two containers, one for Oracle SOA Suite artifacts deployments and another for Oracle Service Bus artifacts deployments. When the deployment completes for one container while other container is still running, the pod status will move from Ready to NotReady. Once both the deployments complete successfully, the status of the pod moves to Completed.\n   Deploy artifacts from a persistent volume   Copy the artifacts for Oracle SOA Suite to the soa directory and Oracle Service Bus to the osb directory at the share location. For example, with location /share, artifacts for Oracle SOA Suite are in /share/soa and Oracle Service Bus are in /share/osb.\n$ ls /share/soa sca_sampleBPEL.jar $ $ ls /share/osb/ simple_sbconfig.jar $   Create a PersistentVolume with the sample provided (artifacts-pv.yaml):\napiVersion: v1 kind: PersistentVolume metadata: name: soainfra-deploy-artifacts-pv spec: storageClassName: deploy-storage-class capacity: storage: 10Gi accessModes: - ReadOnlyMany persistentVolumeReclaimPolicy: Retain hostPath: path: \u0026quot;/share\u0026quot; $ kubectl apply -f artifacts-pv.yaml   Create a PersistentVolumeClaim with the sample provided (artifacts-pvc.yaml):\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: soainfra-deploy-artifacts-pvc namespace: soans spec: storageClassName: deploy-storage-class accessModes: - ReadOnlyMany resources: requests: storage: 10Gi $ kubectl apply -f artifacts-pvc.yaml   Update the artifactsSourceType to PersistentVolume and provide the name for persistentVolumeClaimName in deploy-artifacts-inputs.yaml.\n  Invoke deploy-artifacts.sh to deploy artifacts for artifacts present in persistentVolumeClaimName.\n  Click here to see sample output of deployment for domainType of soaosb   $ ./deploy-artifacts.sh -i deploy-artifacts-inputs.yaml -o out-deploy Input parameters being used export version=\u0026quot;deploy-artifacts-inputs-v1\u0026quot; export adminPort=\u0026quot;7001\u0026quot; export adminServerName=\u0026quot;AdminServer\u0026quot; export domainUID=\u0026quot;soainfra\u0026quot; export domainType=\u0026quot;soaosb\u0026quot; export soaClusterName=\u0026quot;soa_cluster\u0026quot; export soaManagedServerPort=\u0026quot;8001\u0026quot; export image=\u0026quot;soasuite:12.2.1.4\u0026quot; export imagePullPolicy=\u0026quot;IfNotPresent\u0026quot; export weblogicCredentialsSecretName=\u0026quot;soainfra-domain-credentials\u0026quot; export namespace=\u0026quot;soans\u0026quot; export artifactsSourceType=\u0026quot;PersistentVolume\u0026quot; export persistentVolumeClaimName=\u0026quot;soainfra-deploy-artifacts-pvc\u0026quot; export deployScriptsMountPath=\u0026quot;/u01/weblogic\u0026quot; export deployScriptName=\u0026quot;deploy.sh\u0026quot; export deployScriptFilesDir=\u0026quot;deploy\u0026quot; export soaArtifactsArchivePath=\u0026quot;/u01/sarchives\u0026quot; export osbArtifactsArchivePath=\u0026quot;/u01/sbarchives\u0026quot; Generating out-deploy/deploy-artifacts/soainfra/20211022-164735/deploy-artifacts-job.yaml Checking to see if the secret soainfra-domain-credentials exists in namespace soans configmap/soainfra-deploy-scripts-soa-job-cm created Checking the configmap soainfra-deploy-scripts-soa-job-cm was created configmap/soainfra-deploy-scripts-soa-job-cm labeled configmap/soainfra-deploy-scripts-osb-job-cm created Checking the configmap soainfra-deploy-scripts-osb-job-cm was created configmap/soainfra-deploy-scripts-osb-job-cm labeled Checking if object type job with name soainfra-deploy-artifacts-job-20211022-164735 exists Deploying artifacts by creating the job out-deploy/deploy-artifacts/soainfra/20211022-164735/deploy-artifacts-job.yaml job.batch/soainfra-deploy-artifacts-job-20211022-164735 created Waiting for the job to complete... status on iteration 1 of 20 for soainfra pod soainfra-deploy-artifacts-job-20211022-164735-66fvn status is NotReady status on iteration 2 of 20 for soainfra pod soainfra-deploy-artifacts-job-20211022-164735-66fvn status is Completed configmap \u0026quot;soainfra-deploy-scripts-soa-job-cm\u0026quot; deleted configmap \u0026quot;soainfra-deploy-scripts-osb-job-cm\u0026quot; deleted The following files were generated: out-deploy/deploy-artifacts/soainfra/20211022-164735/deploy-artifacts-inputs.yaml out-deploy/deploy-artifacts/soainfra/20211022-164735/deploy-artifacts-job.yaml Completed $ kubectl get all -n soans |grep deploy pod/soainfra-deploy-artifacts-job-20211022-164735-66fvn 0/2 Completed 0 3m1s job.batch/soainfra-deploy-artifacts-job-20211022-164735 1/1 37s 3m1s $     Note: When you are running the script for domainType of soaosb, a deployment pod is created with two containers, one for Oracle SOA Suite artifacts deployments and one for Oracle Service Bus artifacts deployments. When the deployment completes for one container while other container is still running, the pod status moves from Ready to NotReady. Once both the deployments successfully complete, the status of the pod moves to Completed.\n   Verify the deployment logs To confirm the deployment of artifacts was successful, verify the output using the kubectl logs command:\n Note: Replace \u0026lt;YYYYMMDD-hhmmss\u0026gt;, \u0026lt;domainUID\u0026gt; and \u0026lt;namespace\u0026gt; with values for your environment.\n For Oracle SOA Suite artifacts:\n$ kubectl logs job.batch/\u0026lt;domainUID\u0026gt;-deploy-artifacts-job-\u0026lt;YYYYMMDD-hhmmss\u0026gt; -n \u0026lt;namespace\u0026gt; soa-deploy-artifacts-job For Oracle Service Bus artifacts:\n$ kubectl logs job.batch/\u0026lt;domainUID\u0026gt;-deploy-artifacts-job-\u0026lt;YYYYMMDD-hhmmss\u0026gt; -n \u0026lt;namespace\u0026gt; osb-deploy-artifacts-job "
},
{
	"uri": "/fmw-kubernetes/wccontent-domains/adminguide/",
	"title": "Administration Guide",
	"tags": [],
	"description": "Describes how to use some of the common utility tools and configurations to administer Oracle WebCenter Content domains.",
	"content": "Administer Oracle WebCenter Content domains in Kubernetes.\n Set up a load balancer  Configure different load balancers for Oracle WebCenter Content domains.\n Monitor an Oracle WebCenter Content domain  Use the WebLogic Monitoring Exporter to monitor an Oracle WebCenter Content instance using Prometheus and Grafana.\n Elasticsearch integration for logs  Monitor an Oracle WebCenter Sites domain and publish the WebLogic Server logs to Elasticsearch.\n Publish logs to Elasticsearch  Use the WebLogic Logging Exporter to publish the WebLogic Server logs to Elasticsearch.\n Publish logs to Elasticsearch Using Fluentd  Configure a WebLogic domain to use Fluentd to send log information to Elasticsearch.\n Configure an additional mount or shared space to a domain for Imaging and Capture  Configure an additional mount or shared space to a domain, for WebCenter Imaging and WebCenter Capture\n "
},
{
	"uri": "/fmw-kubernetes/wccontent-domains/installguide/create-wccontent-domains/",
	"title": "Create Oracle WebCenter Content domain",
	"tags": [],
	"description": "Create Oracle WebCenter Content domain home on an existing PV or PVC and create the domain resource YAML file for deploying the generated Oracle WebCenter Content domain.",
	"content": "The WebCenter Content deployment scripts demonstrate the creation of Oracle WebCenter Content domain home on an existing Kubernetes persistent volume (PV) and persistent volume claim (PVC). The scripts also generate the domain YAML file, which can then be used to start the Kubernetes artifacts of the corresponding domain.\nPrerequisites Before you begin, complete the following steps:\n Review the Domain resource documentation. Review the requirements and limitations. Ensure that you have executed all the preliminary steps in Prepare your environment. Ensure that the database schemas were created and the WebLogic Kubernetes Operator are running.  Prepare to use the create domain script The sample scripts for Oracle WebCenter Content domain deployment are available at ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-wcc-domain.\nYou must edit create-domain-inputs.yaml (or a copy of it) located under ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-wcc-domain/domian-home-on-pv to provide the details for your domain. Refer to the configuration parameters below to understand the information that you must provide in this file.\nConfiguration parameters The following parameters can be provided in the inputs file.\n   Parameter Definition Default     sslEnabled Boolean indicating whether to enable SSL for each WebLogic Server instance. false   adminPort Port number for the Administration Server inside the Kubernetes cluster. 7001   adminServerSSLPort SSL port number of the Administration Server inside the Kubernetes cluster. 7002   adminNodePort Port number of the Administration Server outside the Kubernetes cluster. 30701   adminServerName Name of the Administration Server. AdminServer   clusterName Name of the WebLogic cluster instance to generate for the domain. By default the cluster name is ucm_cluster \u0026amp; ibr_cluster for the WebCenter Content domain. ucm_cluster   configuredManagedServerCount Number of Managed Server instances to generate for the domain. 5   createDomainFilesDir Directory on the host machine to locate all the files to create a WebLogic domain, including the script that is specified in the createDomainScriptName property. By default, this directory is set to the relative path wlst, and the create script will use the built-in WLST offline scripts in the wlst directory to create the WebLogic domain. An absolute path is also supported to point to an arbitrary directory in the file system. The built-in scripts can be replaced by the user-provided scripts as long as those files are in the specified directory. Files in this directory are put into a Kubernetes config map, which in turn is mounted to the createDomainScriptsMountPath, so that the Kubernetes pod can use the scripts and supporting files to create a domain home. wlst   createDomainScriptsMountPath Mount path where the create domain scripts are located inside a pod. The create-domain.sh script creates a Kubernetes job to run the script (specified in the createDomainScriptName property) in a Kubernetes pod to create a domain home. Files in the createDomainFilesDir directory are mounted to this location in the pod, so that the Kubernetes pod can use the scripts and supporting files to create a domain home. /u01/weblogic   createDomainScriptName Script that the create domain script uses to create a WebLogic domain. The create-domain.sh script creates a Kubernetes job to run this script to create a domain home. The script is located in the in-pod directory that is specified in the createDomainScriptsMountPath property. If you need to provide your own scripts to create the domain home, instead of using the built-it scripts, you must use this property to set the name of the script that you want the create domain job to run. create-domain-job.sh   domainHome Home directory of the WebCenter Content domain. If not specified, the value is derived from the domainUID as /shared/domains/\u0026lt;domainUID\u0026gt;. /u01/oracle/user_projects/domains/wccinfra   domainPVMountPath Mount path of the domain persistent volume. /u01/oracle/user_projects   domainUID Unique ID that will be used to identify this particular domain. Used as the name of the generated WebLogic domain as well as the name of the Kubernetes domain resource. This ID must be unique across all domains in a Kubernetes cluster. This ID cannot contain any character that is not valid in a Kubernetes service name. wccinfra   exposeAdminNodePort Boolean indicating if the Administration Server is exposed outside of the Kubernetes cluster. false   exposeAdminT3Channel Boolean indicating if the T3 administrative channel is exposed outside the Kubernetes cluster. false   image WebCenter Content Docker image. WebLogic Kubernetes Operator requires Oracle WebCenter Content 12.2.1.4.0 Refer to Obtain the Oracle WebCenter Content Docker image for details on how to obtain or create the image. oracle/wccontent:12.2.1.4.0   imagePullPolicy WebLogic Docker image pull policy. Legal values are IfNotPresent, Always, or Never. IfNotPresent   imagePullSecretName Name of the Kubernetes secret to access the Docker Store to pull the WebLogic Server Docker image. The presence of the secret will be validated when this parameter is specified.    includeServerOutInPodLog Boolean indicating whether to include the server .out to the pod\u0026rsquo;s stdout. true   initialManagedServerReplicas Number of Managed Servers to initially start for the domain. 3   javaOptions Java options for starting the Administration Server and Managed Servers. A Java option can have references to one or more of the following pre-defined variables to obtain WebLogic domain information: $(DOMAIN_NAME), $(DOMAIN_HOME), $(ADMIN_NAME), $(ADMIN_PORT), and $(SERVER_NAME). If sslEnabled is set to true and the WebLogic demo certificate is used, add -Dweblogic.security.SSL.ignoreHostnameVerification=true to allow the Managed Servers to connect to the Administration Server while booting up. The WebLogic generated demo certificate in this environment typically contains a host name that is different from the runtime container\u0026rsquo;s host name. -Dweblogic.StdoutDebugEnabled=false   logHome The in-pod location for the domain log, server logs, server out, and Node Manager log files. If not specified, the value is derived from the domainUID as /shared/logs/\u0026lt;domainUID\u0026gt;. /u01/oracle/user_projects/domains/logs/wccinfra   managedServerNameBase Base string used to generate Managed Server names. ucm_server   managedServerPort Port number for each Managed Server. By default the managedServerPort is 16200 for the ucm_server \u0026amp; managedServerPort is 16250 for the ibr_server. 16200   managedServerSSLPort SSL port number for each Managed Server. By default the managedServerSSLPort is 16201 for the ucm_server \u0026amp; managedServerSSLPort is 16251 for the ibr_server. 16201   namespace Kubernetes namespace in which to create the domain. wccns   persistentVolumeClaimName Name of the persistent volume claim created to host the domain home. If not specified, the value is derived from the domainUID as \u0026lt;domainUID\u0026gt;-weblogic-sample-pvc. wccinfra-domain-pvc   productionModeEnabled Boolean indicating if production mode is enabled for the domain. true   serverStartPolicy Determines which WebLogic Server instances will be started. Legal values are NEVER, IF_NEEDED, ADMIN_ONLY. IF_NEEDED   t3ChannelPort Port for the t3 channel of the NetworkAccessPoint. 30012   t3PublicAddress Public address for the T3 channel. This should be set to the public address of the Kubernetes cluster. This would typically be a load balancer address. For development environments only: In a single server (all-in-one) Kubernetes deployment, this may be set to the address of the master, or at the very least, it must be set to the address of one of the worker nodes. If not provided, the script will attempt to set it to the IP address of the Kubernetes cluster   weblogicCredentialsSecretName Name of the Kubernetes secret for the Administration Server\u0026rsquo;s user name and password. If not specified, then the value is derived from the domainUID as \u0026lt;domainUID\u0026gt;-weblogic-credentials. wccinfra-domain-credentials   weblogicImagePullSecretName Name of the Kubernetes secret for the Docker Store, used to pull the WebLogic Server image.    serverPodCpuRequest, serverPodMemoryRequest, serverPodCpuCLimit, serverPodMemoryLimit The maximum amount of compute resources allowed, and minimum amount of compute resources required, for each server pod. Please refer to the Kubernetes documentation on Managing Compute Resources for Containers for details. Resource requests and resource limits are not specified.   rcuSchemaPrefix The schema prefix to use in the database, for example WCC1. You may wish to make this the same as the domainUID in order to simplify matching domains to their RCU schemas. WCC1   rcuDatabaseURL The database URL. \u0026lt;YOUR DATABASE CONNECTION DETAILS\u0026gt;   rcuCredentialsSecret The Kubernetes secret containing the database credentials. wccinfra-rcu-credentials   ipmEnabled Boolean indicating whether to enable WebCenter Imaging application false   captureEnabled Boolean indicating whether to enable WebCenter Capture application false   adfuiEnabled Boolean indicating whether to enable WebCenter ADF UI application false    Note that the names of the Kubernetes resources in the generated YAML files may be formed with the value of some of the properties specified in the create-inputs.yaml file. Those properties include the adminServerName, clusterName and managedServerNameBase. If those values contain any characters that are invalid in a Kubernetes service name, those characters are converted to valid values in the generated YAML files. For example, an uppercase letter is converted to a lowercase letter and an underscore (\u0026quot;_\u0026quot;) is converted to a hyphen (\u0026quot;-\u0026quot;).\n Note: The properties ipmEnabled, captureEnabled, adfuiEnabled are set to false by default and should be updated to true if you need to enable the respective applications.\n The sample demonstrates how to create the Oracle WebCenter Content domain home and associated Kubernetes resources for that domain. In addition, the sample provides the capability for users to supply their own scripts to create the domain home for other use cases. The generated domain YAML file could also be modified to cover more use cases.\nRun the create domain script Run the create domain script, specifying your inputs file and an output directory to store the generated artifacts:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-wcc-domain/domain-home-on-pv/ $ ./create-domain.sh \\ -i create-domain-inputs.yaml \\ -o \u0026lt;path to output-directory\u0026gt; The script will perform the following steps:\n  Create a directory for the generated Kubernetes YAML files for this domain if it does not already exist. The path name is \u0026lt;path to output-directory\u0026gt;/weblogic-domains/\u0026lt;domainUID\u0026gt;. If the directory already exists, its contents must be removed before using this script.\n  Create a Kubernetes job that will start up a utility Oracle WebCenter Content container and run offline WLST scripts to create the domain on the shared storage.\n  Run and wait for the job to finish.\n  Create a Kubernetes domain YAML file, domain.yaml, in the \u0026ldquo;output\u0026rdquo; directory that was created above. This YAML file can be used to create the Kubernetes resource using the kubectl create -f or kubectl apply -f command.\n  Run managed-server-wrapper script, which intrenally applies the domain YAML. This script also applies initial configurations for Managed Server containers and readies Managed Servers for future inter-container communications.\n$ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-wcc-domain/domain-home-on-pv/ $ ./start-managed-servers-wrapper.sh -o \u0026lt;path_to_output_directory\u0026gt; -p \u0026lt;load_balancer_port\u0026gt;   Create a convenient utility script, delete-domain-job.yaml, to clean up the domain home created by the create script.\n  The default domain created by the script has the following characteristics:\n An Administration Server named AdminServer listening on port 7001. A configured cluster named ucm_cluster of size 3. A configured cluster named ibr_cluster of size 1. A configured cluster named ipm_cluster of size 3. A configured cluster named capture_cluster of size 3. A configured cluster named wccadf_cluster of size 3. Managed Servers, named ucm_cluster listening on port 16200. Managed Servers, named ibr_cluster listening on port 16250. Managed Servers, named ipm_cluster listening on port 16000. Managed Servers, named capture_cluster listening on port 16400. Managed Servers, named wccadf_cluster listening on port 16225. Log files that are located in /shared/logs/\u0026lt;domainUID\u0026gt;.  Verify the results The create domain script will verify that the domain was created, and will report failure if there was any error. However, it may be desirable to manually verify the domain, even if just to gain familiarity with the various Kubernetes objects that were created by the script.\nGenerated YAML files with the default inputs   Click here to see sample content of the generated `domain.yaml`.   $ cat output/weblogic-domains/wccinfra/domain.yaml # Copyright (c) 2021, Oracle and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # # This is an example of how to define a Domain resource. # apiVersion: \u0026quot;weblogic.oracle/v8\u0026quot; kind: Domain metadata: name: wccinfra namespace: wccns labels: weblogic.domainUID: wccinfra spec: # The WebLogic Domain Home domainHome: /u01/oracle/user_projects/domains/wccinfra maxClusterConcurrentStartup: 1 # The domain home source type # Set to PersistentVolume for domain-in-pv, Image for domain-in-image, or FromModel for model-in-image domainHomeSourceType: PersistentVolume # The WebLogic Server Docker image that WebLogic Kubernetes Operator uses to start the domain image: \u0026quot;oracle/wccontent:12.2.1.4.0\u0026quot; # imagePullPolicy defaults to \u0026quot;Always\u0026quot; if image version is :latest imagePullPolicy: \u0026quot;IfNotPresent\u0026quot; # Identify which Secret contains the credentials for pulling an image #imagePullSecrets: #- name: # Identify which Secret contains the WebLogic Admin credentials (note that there is an example of # how to create that Secret at the end of this file) webLogicCredentialsSecret: name: wccinfra-domain-credentials # Whether to include the server out file into the pod's stdout, default is true includeServerOutInPodLog: true # Whether to enable log home logHomeEnabled: true # Whether to write HTTP access log file to log home httpAccessLogInLogHome: true # The in-pod location for domain log, server logs, server out, and Node Manager log files logHome: /u01/oracle/user_projects/domains/logs/wccinfra # An (optional) in-pod location for data storage of default and custom file stores. # If not specified or the value is either not set or empty (e.g. dataHome: \u0026quot;\u0026quot;) then the # data storage directories are determined from the WebLogic domain home configuration. dataHome: \u0026quot;\u0026quot; # serverStartPolicy legal values are \u0026quot;NEVER\u0026quot;, \u0026quot;IF_NEEDED\u0026quot;, or \u0026quot;ADMIN_ONLY\u0026quot; # This determines which WebLogic Servers the WebLogic Kubernetes Operator will start up when it discovers this Domain # - \u0026quot;NEVER\u0026quot; will not start any server in the domain # - \u0026quot;ADMIN_ONLY\u0026quot; will start up only the administration server (no managed servers will be started) # - \u0026quot;IF_NEEDED\u0026quot; will start all non-clustered servers, including the administration server and clustered servers up to the replica count serverStartPolicy: \u0026quot;IF_NEEDED\u0026quot; serverPod: # an (optional) list of environment variable to be set on the servers env: - name: JAVA_OPTIONS value: \u0026quot;-Dweblogic.StdoutDebugEnabled=false\u0026quot; - name: USER_MEM_ARGS value: \u0026quot;-Djava.security.egd=file:/dev/./urandom -Xms256m -Xmx512m \u0026quot; volumes: - name: weblogic-domain-storage-volume persistentVolumeClaim: claimName: wccinfra-domain-pvc volumeMounts: - mountPath: /u01/oracle/user_projects/domains name: weblogic-domain-storage-volume # adminServer is used to configure the desired behavior for starting the administration server. adminServer: # serverStartState legal values are \u0026quot;RUNNING\u0026quot; or \u0026quot;ADMIN\u0026quot; # \u0026quot;RUNNING\u0026quot; means the listed server will be started up to \u0026quot;RUNNING\u0026quot; mode # \u0026quot;ADMIN\u0026quot; means the listed server will be start up to \u0026quot;ADMIN\u0026quot; mode serverStartState: \u0026quot;RUNNING\u0026quot; adminService: channels: # The Admin Server's NodePort - channelName: default nodePort: 30701 # Uncomment to export the T3Channel as a service # - channelName: T3Channel # clusters is used to configure the desired behavior for starting member servers of a cluster. # If you use this entry, then the rules will be applied to ALL servers that are members of the named clusters. clusters: - clusterName: ibr_cluster serverService: precreateService: true serverStartState: \u0026quot;RUNNING\u0026quot; serverPod: # Instructs Kubernetes scheduler to prefer nodes for new cluster members where there are not # already members of the same cluster. affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: \u0026quot;weblogic.clusterName\u0026quot; operator: In values: - $(CLUSTER_NAME) topologyKey: \u0026quot;kubernetes.io/hostname\u0026quot; replicas: 1 serverStartPolicy: \u0026quot;IF_NEEDED\u0026quot; # The number of managed servers to start for unlisted clusters # replicas: 1 # Istio # configuration: # istio: # enabled: # readinessPort: - clusterName: ucm_cluster clusterService: annotations: traefik.ingress.kubernetes.io/affinity: \u0026quot;true\u0026quot; traefik.ingress.kubernetes.io/service.sticky.cookie: \u0026quot;true\u0026quot; traefik.ingress.kubernetes.io/session-cookie-name: JSESSIONID serverService: precreateService: true serverStartState: \u0026quot;RUNNING\u0026quot; serverPod: # Instructs Kubernetes scheduler to prefer nodes for new cluster members where there are not # already members of the same cluster. affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: \u0026quot;weblogic.clusterName\u0026quot; operator: In values: - $(CLUSTER_NAME) topologyKey: \u0026quot;kubernetes.io/hostname\u0026quot; replicas: 3 serverStartPolicy: \u0026quot;IF_NEEDED\u0026quot; # The number of managed servers to start for unlisted clusters # replicas: 1 - clusterName: ipm_cluster clusterService: annotations: traefik.ingress.kubernetes.io/affinity: \u0026quot;true\u0026quot; traefik.ingress.kubernetes.io/service.sticky.cookie: \u0026quot;true\u0026quot; traefik.ingress.kubernetes.io/session-cookie-name: JSESSIONID serverService: precreateService: true serverStartState: \u0026quot;RUNNING\u0026quot; serverPod: # Instructs Kubernetes scheduler to prefer nodes for new cluster members where there are not # already members of the same cluster. affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: \u0026quot;weblogic.clusterName\u0026quot; operator: In values: - $(CLUSTER_NAME) topologyKey: \u0026quot;kubernetes.io/hostname\u0026quot; replicas: 3 # The number of managed servers to start for unlisted clusters # replicas: 1 - clusterName: capture_cluster clusterService: annotations: traefik.ingress.kubernetes.io/affinity: \u0026quot;true\u0026quot; traefik.ingress.kubernetes.io/service.sticky.cookie: \u0026quot;true\u0026quot; traefik.ingress.kubernetes.io/session-cookie-name: JSESSIONID serverService: precreateService: true serverStartState: \u0026quot;RUNNING\u0026quot; serverPod: # Instructs Kubernetes scheduler to prefer nodes for new cluster members where there are not # already members of the same cluster. affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: \u0026quot;weblogic.clusterName\u0026quot; operator: In values: - $(CLUSTER_NAME) topologyKey: \u0026quot;kubernetes.io/hostname\u0026quot; replicas: 3 # The number of managed servers to start for unlisted clusters # replicas: 1 - clusterName: wccadf_cluster clusterService: annotations: traefik.ingress.kubernetes.io/affinity: \u0026quot;true\u0026quot; traefik.ingress.kubernetes.io/service.sticky.cookie: \u0026quot;true\u0026quot; traefik.ingress.kubernetes.io/session-cookie-name: WCCSID serverService: precreateService: true serverStartState: \u0026quot;RUNNING\u0026quot; serverPod: # Instructs Kubernetes scheduler to prefer nodes for new cluster members where there are not # already members of the same cluster. affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: \u0026quot;weblogic.clusterName\u0026quot; operator: In values: - $(CLUSTER_NAME) topologyKey: \u0026quot;kubernetes.io/hostname\u0026quot; replicas: 3 # The number of managed servers to start for unlisted clusters # replicas: 1    Verify the domain To confirm that the domain was created, enter the following command:\n$ kubectl describe domain DOMAINUID -n NAMESPACE Replace DOMAINUID with the domainUID and NAMESPACE with the actual namespace.\n  Click here to see a sample domain description.   $ kubectl describe domain wccinfra -n wccns Name: wccinfra Namespace: wccns Labels: weblogic.domainUID=wccinfra Annotations: API Version: weblogic.oracle/v8 Kind: Domain Metadata: Creation Timestamp: 2020-11-23T12:48:13Z Generation: 7 Managed Fields: API Version: weblogic.oracle/v8 Fields Type: FieldsV1 fieldsV1: f:metadata: f:annotations: .: f:kubectl.kubernetes.io/last-applied-configuration: f:labels: .: f:weblogic.domainUID: Manager: kubectl Operation: Update Time: 2020-11-23T13:50:28Z API Version: weblogic.oracle/v8 Fields Type: FieldsV1 fieldsV1: f:status: .: f:clusters: f:conditions: f:servers: f:startTime: Manager: OpenAPI-Generator Operation: Update Time: 2020-12-03T10:20:52Z Resource Version: 18267402 Self Link: /apis/weblogic.oracle/v8/namespaces/wccns/domains/wccinfra UID: 1a866c30-9b29-4281-bd2b-df80914efdff Spec: Admin Server: Admin Service: Channels: Channel Name: default Node Port: 30701 Server Start State: RUNNING Clusters: Cluster Name: ibr_cluster Replicas: 1 Server Pod: Affinity: Pod Anti Affinity: Preferred During Scheduling Ignored During Execution: Pod Affinity Term: Label Selector: Match Expressions: Key: weblogic.clusterName Operator: In Values: $(CLUSTER_NAME) Topology Key: kubernetes.io/hostname Weight: 100 Server Service: Precreate Service: true Server Start Policy: IF_NEEDED Server Start State: RUNNING Cluster Name: ucm_cluster Cluster Service: Annotations: traefik.ingress.kubernetes.io/affinity: true traefik.ingress.kubernetes.io/service.sticky.cookie: true traefik.ingress.kubernetes.io/session-cookie-name: JSESSIONID Replicas: 3 Server Pod: Affinity: Pod Anti Affinity: Preferred During Scheduling Ignored During Execution: Pod Affinity Term: Label Selector: Match Expressions: Key: weblogic.clusterName Operator: In Values: $(CLUSTER_NAME) Topology Key: kubernetes.io/hostname Weight: 100 Server Service: Precreate Service: true Server Start Policy: IF_NEEDED Server Start State: RUNNING Cluster Name: ipm_cluster Cluster Service: Annotations: traefik.ingress.kubernetes.io/affinity: true traefik.ingress.kubernetes.io/service.sticky.cookie: true traefik.ingress.kubernetes.io/session-cookie-name: JSESSIONID Replicas: 3 Server Pod: Affinity: Pod Anti Affinity: Preferred During Scheduling Ignored During Execution: Pod Affinity Term: Label Selector: Match Expressions: Key: weblogic.clusterName Operator: In Values: $(CLUSTER_NAME) Topology Key: kubernetes.io/hostname Weight: 100 Server Service: Precreate Service: true Server Start State: RUNNING Cluster Name: capture_cluster Cluster Service: Annotations: traefik.ingress.kubernetes.io/affinity: true traefik.ingress.kubernetes.io/service.sticky.cookie: true traefik.ingress.kubernetes.io/session-cookie-name: JSESSIONID Replicas: 3 Server Pod: Affinity: Pod Anti Affinity: Preferred During Scheduling Ignored During Execution: Pod Affinity Term: Label Selector: Match Expressions: Key: weblogic.clusterName Operator: In Values: $(CLUSTER_NAME) Topology Key: kubernetes.io/hostname Weight: 100 Server Service: Precreate Service: true Server Start State: RUNNING Cluster Name: wccadf_cluster Cluster Service: Annotations: traefik.ingress.kubernetes.io/affinity: true traefik.ingress.kubernetes.io/service.sticky.cookie: true traefik.ingress.kubernetes.io/session-cookie-name: WCCSID Replicas: 3 Server Pod: Affinity: Pod Anti Affinity: Preferred During Scheduling Ignored During Execution: Pod Affinity Term: Label Selector: Match Expressions: Key: weblogic.clusterName Operator: In Values: $(CLUSTER_NAME) Topology Key: kubernetes.io/hostname Weight: 100 Server Service: Precreate Service: true Server Start State: RUNNING Data Home: Domain Home: /u01/oracle/user_projects/domains/wccinfra Domain Home Source Type: PersistentVolume Http Access Log In Log Home: true Image: oracle/wccontent_ora_final_it:12.2.1.4.0 Image Pull Policy: IfNotPresent Include Server Out In Pod Log: true Log Home: /u01/oracle/user_projects/domains/logs/wccinfra Log Home Enabled: true Max Cluster Concurrent Startup: 1 Server Pod: Env: Name: JAVA_OPTIONS Value: -Dweblogic.StdoutDebugEnabled=false Name: USER_MEM_ARGS Value: -Djava.security.egd=file:/dev/./urandom -Xms256m -Xmx512m Volume Mounts: Mount Path: /u01/oracle/user_projects/domains Name: weblogic-domain-storage-volume Volumes: Name: weblogic-domain-storage-volume Persistent Volume Claim: Claim Name: wccinfra-domain-pvc Server Start Policy: IF_NEEDED Web Logic Credentials Secret: Name: wccinfra-domain-credentials Status: Clusters: Cluster Name: ibr_cluster Maximum Replicas: 5 Minimum Replicas: 0 Ready Replicas: 1 Replicas: 1 Replicas Goal: 1 Cluster Name: ucm_cluster Maximum Replicas: 5 Minimum Replicas: 0 Ready Replicas: 3 Replicas: 3 Replicas Goal: 3 Cluster Name: ipm_cluster Maximum Replicas: 5 Minimum Replicas: 0 Ready Replicas: 3 Replicas: 3 Replicas Goal: 3 Cluster Name: capture_cluster Maximum Replicas: 5 Minimum Replicas: 0 Ready Replicas: 3 Replicas: 3 Replicas Goal: 3 Cluster Name: wccadf_cluster Maximum Replicas: 5 Minimum Replicas: 0 Ready Replicas: 3 Replicas: 3 Replicas Goal: 3 Conditions: Last Transition Time: 2020-11-23T13:58:41.070Z Reason: ServersReady Status: True Type: Available Servers: Desired State: RUNNING Health: Activation Time: 2020-11-25T16:55:24.930Z Overall Health: ok Subsystems: Subsystem Name: ServerRuntime Symptoms: Node Name: MyNodeName Server Name: AdminServer State: RUNNING Cluster Name: ibr_cluster Desired State: RUNNING Health: Activation Time: 2020-11-30T12:23:27.603Z Overall Health: ok Subsystems: Subsystem Name: ServerRuntime Symptoms: Node Name: MyNodeName Server Name: ibr_server1 State: RUNNING Cluster Name: ibr_cluster Desired State: SHUTDOWN Server Name: ibr_server2 Cluster Name: ibr_cluster Desired State: SHUTDOWN Server Name: ibr_server3 Cluster Name: ibr_cluster Desired State: SHUTDOWN Server Name: ibr_server4 Cluster Name: ibr_cluster Desired State: SHUTDOWN Server Name: ibr_server5 Cluster Name: ucm_cluster Desired State: RUNNING Health: Activation Time: 2020-12-02T14:10:37.992Z Overall Health: ok Subsystems: Subsystem Name: ServerRuntime Symptoms: Node Name: MyNodeName Server Name: ucm_server1 State: RUNNING Cluster Name: ucm_cluster Desired State: RUNNING Health: Activation Time: 2020-12-01T04:51:19.886Z Overall Health: ok Subsystems: Subsystem Name: ServerRuntime Symptoms: Node Name: MyNodeName Server Name: ucm_server2 State: RUNNING Cluster Name: ucm_cluster Desired State: SHUTDOWN Server Name: ucm_server3 Cluster Name: ucm_cluster Desired State: SHUTDOWN Server Name: ucm_server4 Cluster Name: ucm_cluster Desired State: SHUTDOWN Server Name: ucm_server5 Cluster Name: ipm_cluster Desired State: RUNNING Health: Activation Time: 2020-12-01T04:51:19.886Z Overall Health: ok Subsystems: Subsystem Name: ServerRuntime Symptoms: Node Name: MyNodeName Server Name: ipm_server1 State: RUNNING Cluster Name: ipm_cluster Desired State: SHUTDOWN Server Name: ipm_server2 Cluster Name: ipm_cluster Desired State: SHUTDOWN Server Name: ipm_server3 Cluster Name: ipm_cluster Desired State: SHUTDOWN Server Name: ipm_server4 Cluster Name: ipm_cluster Desired State: SHUTDOWN Server Name: ipm_server5 Cluster Name: capture_cluster Desired State: RUNNING Health: Activation Time: 2020-12-01T04:51:19.886Z Overall Health: ok Subsystems: Subsystem Name: ServerRuntime Symptoms: Node Name: MyNodeName Server Name: capture_server1 State: RUNNING Cluster Name: capture_cluster Desired State: SHUTDOWN Server Name: capture_server2 Cluster Name: capture_cluster Desired State: SHUTDOWN Server Name: capture_server3 Cluster Name: capture_cluster Desired State: SHUTDOWN Server Name: capture_server4 Cluster Name: capture_cluster Desired State: SHUTDOWN Server Name: capture_server5 Cluster Name: wccadf_cluster Desired State: RUNNING Health: Activation Time: 2020-12-01T04:51:19.886Z Overall Health: ok Subsystems: Subsystem Name: ServerRuntime Symptoms: Node Name: MyNodeName Server Name: wccadf_server1 State: RUNNING Cluster Name: wccadf_cluster Desired State: SHUTDOWN Server Name: wccadf_server2 Cluster Name: wccadf_cluster Desired State: SHUTDOWN Server Name: wccadf_server3 Cluster Name: wccadf_cluster Desired State: SHUTDOWN Server Name: wccadf_server4 Cluster Name: wccadf_cluster Desired State: SHUTDOWN Server Name: wccadf_server5 Start Time: 2020-11-23T12:48:13.756Z Events: \u0026lt;none\u0026gt;    In the Status section of the output, the available servers and clusters are listed. Note that if this command is issued soon after the script finishes, there may be no servers available yet, or perhaps only the Administration Server but no Managed Servers. WebLogic Kubernetes Operator will start up the Administration Server first and wait for it to become ready before starting the Managed Servers.\nVerify the pods Enter the following command to see the pods running the servers:\n$ kubectl get pods -n NAMESPACE Here is an example of the output of this command. You can verify that an Administration Server and Managed Servers for ucm, ibr, ipm, capture and wccadf cluster are running.\n$ kubectl get pod -n wccns NAME READY STATUS RESTARTS AGE rcu 1/1 Running 0 78d wccinfra-adminserver 1/1 Running 0 9d wccinfra-create-fmw-infra-sample-domain-job-l8r9d 0/1 Completed 0 9d wccinfra-ibr-server1 1/1 Running 0 9d wccinfra-ucm-server1 1/1 Running 0 9d wccinfra-ucm-server2 1/1 Running 0 9d wccinfra-ucm-server3 1/1 Running 0 9d wccinfra-ipm-server1 1/1 Running 0 9d wccinfra-ipm-server2 1/1 Running 0 9d wccinfra-ipm-server3 1/1 Running 0 9d wccinfra-capture-server1 1/1 Running 0 9d wccinfra-capture-server2 1/1 Running 0 9d wccinfra-capture-server3 1/1 Running 0 9d wccinfra-wccadf-server1 1/1 Running 0 9d wccinfra-wccadf-server2 1/1 Running 0 9d wccinfra-wccadf-server3 1/1 Running 0 9d Verify the services Enter the following command to see the services for the domain:\n$ kubectl get services -n NAMESPACE Here is an example of the output of this command.\n  Click here to see a sample list of services.   $ kubectl get services -n wccns NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE wccinfra-adminserver ClusterIP None \u0026lt;none\u0026gt; 7001/TCP 9d wccinfra-adminserver-external NodePort 10.104.100.193 \u0026lt;none\u0026gt; 7001:30701/TCP 9d wccinfra-cluster-ibr-cluster ClusterIP 10.98.100.212 \u0026lt;none\u0026gt; 16250/TCP 114s wccinfra-cluster-ucm-cluster ClusterIP 10.108.47.178 \u0026lt;none\u0026gt; 16200/TCP 9d wccinfra-cluster-ipm-cluster ClusterIP 10.108.217.111 \u0026lt;none\u0026gt; 16000/TCP 9d wccinfra-cluster-capture-cluster ClusterIP 10.110.193.252 \u0026lt;none\u0026gt; 16400/TCP 9d wccinfra-cluster-wccadf-cluster ClusterIP 10.109.191.247 \u0026lt;none\u0026gt; 16225/TCP 9d wccinfra-ibr-server1 ClusterIP None \u0026lt;none\u0026gt; 16250/TCP 9d wccinfra-ibr-server2 ClusterIP 10.97.253.44 \u0026lt;none\u0026gt; 16250/TCP 9d wccinfra-ibr-server3 ClusterIP 10.110.183.48 \u0026lt;none\u0026gt; 16250/TCP 9d wccinfra-ibr-server4 ClusterIP 10.108.228.158 \u0026lt;none\u0026gt; 16250/TCP 9d wccinfra-ibr-server5 ClusterIP 10.101.29.140 \u0026lt;none\u0026gt; 16250/TCP 9d wccinfra-ucm-server1 ClusterIP None \u0026lt;none\u0026gt; 16200/TCP 9d wccinfra-ucm-server2 ClusterIP None \u0026lt;none\u0026gt; 16200/TCP 9d wccinfra-ucm-server3 ClusterIP None \u0026lt;none\u0026gt; 16200/TCP 9d wccinfra-ucm-server4 ClusterIP 10.109.25.242 \u0026lt;none\u0026gt; 16200/TCP 9d wccinfra-ucm-server5 ClusterIP 10.109.193.26 \u0026lt;none\u0026gt; 16200/TCP 9d wccinfra-ipm-server1 ClusterIP None \u0026lt;none\u0026gt; 16000/TCP 9d wccinfra-ipm-server2 ClusterIP None \u0026lt;none\u0026gt; 16000/TCP 9d wccinfra-ipm-server3 ClusterIP None \u0026lt;none\u0026gt; 16000/TCP 9d wccinfra-ipm-server4 ClusterIP 10.111.215.108 \u0026lt;none\u0026gt; 16000/TCP 9d wccinfra-ipm-server5 ClusterIP 10.109.220.10 \u0026lt;none\u0026gt; 16000/TCP 9d wccinfra-capture-server1 ClusterIP None \u0026lt;none\u0026gt; 16400/TCP 9d wccinfra-capture-server2 ClusterIP None \u0026lt;none\u0026gt; 16400/TCP 9d wccinfra-capture-server3 ClusterIP None \u0026lt;none\u0026gt; 16400/TCP 9d wccinfra-capture-server4 ClusterIP 10.109.72.216 \u0026lt;none\u0026gt; 16400/TCP 9d wccinfra-capture-server5 ClusterIP 10.102.90.234 \u0026lt;none\u0026gt; 16400/TCP 9d wccinfra-wccadf-server1 ClusterIP None \u0026lt;none\u0026gt; 16225/TCP 9d wccinfra-wccadf-server2 ClusterIP None \u0026lt;none\u0026gt; 16225/TCP 9d wccinfra-wccadf-server3 ClusterIP None \u0026lt;none\u0026gt; 16225/TCP 9d wccinfra-wccadf-server4 ClusterIP 10.99.91.229 \u0026lt;none\u0026gt; 16225/TCP 9d wccinfra-wccadf-server5 ClusterIP 10.105.114.38 \u0026lt;none\u0026gt; 16225/TCP 9d    Configure an additional mount or shared space to a domain for Imaging and Capture Optionally, if you want to configure an additional mount or shared space to a domain, for WebCenter Imaging and WebCenter Capture applications for file imports, refer to the Configure an Additional Mount or Shared-Space to a Domain for Imaging and Capture.\n"
},
{
	"uri": "/fmw-kubernetes/wccontent-domains/appendix/docker-k8s-hardening/",
	"title": "Security hardening",
	"tags": [],
	"description": "Review resources for the Docker and Kubernetes cluster hardening.",
	"content": "Securing a Kubernetes cluster involves hardening on multiple fronts - securing the API servers, etcd, nodes, container images, container run-time, and the cluster network. Apply principles of defense in depth, principle of least privilege, and minimize the attack surface. Use security tools such as Kube-Bench to verify the cluster\u0026rsquo;s security posture. Since Kubernetes is evolving rapidly refer to Kubernetes Security Overview for the latest information on securing a Kubernetes cluster. Also ensure the deployed Docker containers follow the Docker Security guidance.\nThis section provides references on how to securely configure Docker and Kubernetes.\nReferences   Docker hardening\n https://docs.docker.com/engine/security/security/ https://blog.aquasec.com/docker-security-best-practices    Kubernetes hardening\n https://kubernetes.io/docs/concepts/security/overview/ https://kubernetes.io/docs/concepts/security/pod-security-standards/ https://blogs.oracle.com/developers/5-best-practices-for-kubernetes-security    Security best practices for Oracle WebLogic Server Running in Docker and Kubernetes\n https://blogs.oracle.com/weblogicserver/security-best-practices-for-weblogic-server-running-in-docker-and-kubernetes    "
},
{
	"uri": "/fmw-kubernetes/wccontent-domains/adminguide/elasticsearch-integration/",
	"title": "Elasticsearch integration for logs",
	"tags": [],
	"description": "Monitor an Oracle WebCenter Sites domain and publish the WebLogic Server logs to Elasticsearch.",
	"content": "1. Integrate Elasticsearch to WebLogic Kubernetes Operator For reference information, see Elasticsearch integration for the WebLogic Kubernetes Operator.\nTo enable elasticsearch integration, you must edit file kubernetes/charts/weblogic-operator/values.yaml before deploying the WebLogic Kubernetes Operator.\n# elkIntegrationEnabled specifies whether or not ELK integration is enabled. elkIntegrationEnabled: true # logStashImage specifies the docker image containing logstash. # This parameter is ignored if 'elkIntegrationEnabled' is false. logStashImage: \u0026quot;logstash:6.6.0\u0026quot; # elasticSearchHost specifies the hostname of where Elasticsearch is running. # This parameter is ignored if 'elkIntegrationEnabled' is false. elasticSearchHost: \u0026quot;elasticsearch.default.svc.cluster.local\u0026quot; # elasticSearchPort specifies the port number of where Elasticsearch is running. # This parameter is ignored if 'elkIntegrationEnabled' is false. elasticSearchPort: 9200 After you\u0026rsquo;ve deployed WebLogic Kubernetes Operator and made the above changes, the weblogic-operator pod will have additional Logstash container. The Logstash container will push the weblogic-operator logs to the configured Elasticsearch server.\n2. Publish WebLogic Server and WebCenter Content Logs using Logstash Pod You can publish the WebLogic Server logs to Elasticsearch Server using Logstash pod. This Logstash pod must have access to the shared domain home. For the WebCenter Content wccinfra, you can use the persistent volume of the domain home in the Logstash pod. The steps to create the Logstash pod are as follows:\nGet the persistent volume details of the domain home of the WebLogic Server(s). The following command will list the persistent volume details in the namespace - \u0026ldquo;wccns\u0026rdquo;:\n$ kubectl get pv -n wccns NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE wccinfra-domain-pv 10Gi RWX Retain Bound wccns/wccinfra-domain-pvc wccinfra-domain-storage-class 33d Create the deployment yaml for Logstash pod. The mounted persistent volume of the domain home will provide access to the WebLogic server logs to Logstash pod. Given below is a sample Logstash deployment yaml.\napiVersion: apps/v1 kind: Deployment metadata: name: logstash-wls namespace: wccns spec: selector: matchLabels: app: \u0026quot;logstash-wls\u0026quot; template: # create pods using pod definition in this template metadata: labels: app: logstash-wls spec: volumes: - name: weblogic-domain-storage-volume persistentVolumeClaim: claimName: wccinfra-domain-pvc - name: shared-logs emptyDir: {} containers: - name: logstash image: logstash:6.6.0 command: [\u0026quot;/bin/sh\u0026quot;] args: [\u0026quot;/usr/share/logstash/bin/logstash\u0026quot;, \u0026quot;-f\u0026quot;, \u0026quot;/u01/oracle/user_projects/domains/logstash.conf\u0026quot;] imagePullPolicy: IfNotPresent volumeMounts: - mountPath: /u01/oracle/user_projects/domains name: weblogic-domain-storage-volume - name: shared-logs mountPath: /shared-logs ports: - containerPort: 5044 name: logstash Sample Logstash configuration file is located at kubernetes/samples/scripts/create-wcc-domain/logstash/logstash.conf\n$ vi kubernetes/samples/scripts/create-wcc-domain/logstash/logstash.conf input { file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/logs/wccinfra/AdminServer.log\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/logs/wccinfra/ucm_server*.log\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/logs/wccinfra/ibr_server*.log\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/logs/wccinfra/ipm_server*.log\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/logs/wccinfra/capture_server*.log\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/logs/wccinfra/wccadf_server*.log\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/logs/wccinfra/AdminServer.out\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/logs/wccinfra/ucm_server*.out\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/logs/wccinfra/ibr_server*.out\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/logs/wccinfra/ipm_server*.out\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/logs/wccinfra/capture_server*.out\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/logs/wccinfra/wccadf_server*.out\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/wccinfra/servers/AdminServer/logs/AdminServer-diagnostic.log\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/wccinfra/servers/**/logs/ucm_server*.log\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/wccinfra/servers/**/logs/ibr_server*.log\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/wccinfra/servers/**/logs/ipm_server*.log\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/wccinfra/servers/**/logs/capture_server*.log\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/wccinfra/servers/**/logs/wccadf_server*.log\u0026quot; start_position =\u0026gt; beginning } } filter { grok { match =\u0026gt; [ \u0026quot;message\u0026quot;, \u0026quot;\u0026lt;%{DATA:log_timestamp}\u0026gt; \u0026lt;%{WORD:log_level}\u0026gt; \u0026lt;%{WORD:thread}\u0026gt; \u0026lt;%{HOSTNAME:hostname}\u0026gt; \u0026lt;%{HOSTNAME:servername}\u0026gt; \u0026lt;%{DATA:timer}\u0026gt; \u0026lt;\u0026lt;%{DATA:kernel}\u0026gt;\u0026gt; \u0026lt;\u0026gt; \u0026lt;%{DATA:uuid}\u0026gt; \u0026lt;%{NUMBER:timestamp}\u0026gt; \u0026lt;%{DATA:misc}\u0026gt; \u0026lt;%{DATA:log_number}\u0026gt; \u0026lt;%{DATA:log_message}\u0026gt;\u0026quot; ] } } output { elasticsearch { hosts =\u0026gt; [\u0026quot;elasticsearch.default.svc.cluster.local:9200\u0026quot;] } } Here ** means that all ucm_server.log and ibr_server.log from any servers under wccinfra will be pushed to Logstash.\n$ kubectl cp kubernetes/samples/scripts/create-wcc-domain/logstash/logstash.conf wccns/wccinfra-adminserver:/u01/oracle/user_projects/domains/logstash.conf Deploy Logstash pod After you have created the Logstash deployment yaml and Logstash configuration file, deploy Logstash using following command:\n$ kubectl create -f kubernetes/samples/scripts/create-wcc-domain/logstash/logstash.yaml 3. Test the deployment of Elasticsearch and Kibana The WebLogic Kubernetes Operator also provides a sample deployment of Elasticsearch and Kibana for testing purpose. You can deploy Elasticsearch and Kibana on the Kubernetes cluster as shown below:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator/ $ kubectl create -f kubernetes/samples/scripts/elasticsearch-and-kibana/elasticsearch_and_kibana.yaml Get the Kibana dashboard port information as shown below: Wait for pods to start:\n-bash-4.2$ kubectl get pods -w NAME READY STATUS RESTARTS AGE elasticsearch-8bdb7cf54-mjs6s 1/1 Running 0 4m3s kibana-dbf8964b6-n8rcj 1/1 Running 0 4m3s -bash-4.2$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE elasticsearch ClusterIP 10.105.205.157 \u0026lt;none\u0026gt; 9200/TCP,9300/TCP 10d kibana NodePort 10.98.104.41 \u0026lt;none\u0026gt; 5601:30412/TCP 10d kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 42d You can access the Kibana dashboard at http://\u0026lt;your_hostname\u0026gt;:30412/. In our example, the node port would be 30412.\nCreate an Index Pattern in Kibana Create an index pattern logstash-* in Kibana \u0026gt; Management. After the servers are started, you will see the log data in the Kibana dashboard.\n"
},
{
	"uri": "/fmw-kubernetes/wcsites-domains/oracle-cloud/filesystem/",
	"title": "Preparing a file system",
	"tags": [],
	"description": "Running WebLogic Kubernetes Operator managed Oracle WebCenter Sites domains on OKE",
	"content": "Create Filesystem and security list for FSS Note: Make sure you create the filesystem and security list in the OKE created VCN\n Login to OCI Console and go to File Storage and Click \u0026ldquo;File System\u0026rdquo;  Click \u0026ldquo;Create File System\u0026rdquo;  You can create File System and Mount Targets with the default values. But in case you want to rename the file System and mount targets, follow below steps. Note: Make Sure the Virtual Cloud Network in Mount Target refers to the one where your instances are created and you will be accessing this file system. Edit and change the File System name to say \u0026ldquo;WCSFileSystem\u0026rdquo;  Edit and change the Mount Target name to WCSMountTarget and make sure the Virtual Cloud Network selected is \u0026ldquo;WCSVCN\u0026rdquo; the one where all the instances are created. Select Public Subnet. Click \u0026ldquo;Create\u0026rdquo;  Once the File System is created, it lands at below page. Click on \u0026ldquo;WCSFileSystem\u0026rdquo; link.  Click on Mount Commands which gives details on how to mount this file system on your instances.  Mount Command pop up gives details on what must be configured on security list to access the mount targets from instances. Note down the mount command which need to be executed on the instance  Create the security list \u0026ldquo;fss_security list \u0026quot; with below Ingress Rules as given in the Mount commands pop up.  Create the Egress rules as below as given in the Mount commands pop up.  Make sure to add the created security list \u0026ldquo;fss_security list \u0026quot; to each subnets as shown below: Otherwise the created security list rules will not apply to the instances.  Once the created security list \u0026ldquo;fss_security list \u0026quot; is added into the subnet, login to the instances and mount the file systems on to Bastion Node  #login as root sudo su #Install NFS Utils yum install nfs-utils #Create directory where you want the mount the file system mkdir -p /mnt/WCSFileSystem #Give proper permissions so that all users can access the share volume chmod 777 /mnt/WCSFileSystem # Alternatively you can use: \u0026quot;mount 10.0.0.7:/WCSFileSystem /mnt/WCSFileSystem\u0026quot;. To persist on reboot add into /etc/fstab echo \u0026quot;10.0.0.7:/WCSFileSystem /mnt/WCSFileSystem nfs nfsvers=3 0 0\u0026quot; \u0026gt;\u0026gt; /etc/fstab mount -a cd /mnt/WCSFileSystem  Confirm that /WCSFileSystem is now pointing to created File System  [root@wcsbastioninstance WCSFileSystem]# df -h . Filesystem Size Used Avail Use% Mounted on 10.0.0.7:/WCSFileSystem 8.0E 0 8.0E 0% /mnt/WCSFileSystem [root@wcsbastioninstance WCSFileSystem]# "
},
{
	"uri": "/fmw-kubernetes/soa-domains/installguide/create-soa-domains/",
	"title": "Create Oracle SOA Suite domains",
	"tags": [],
	"description": "Create an Oracle SOA Suite domain home on an existing PV or PVC, and create the domain resource YAML file for deploying the generated Oracle SOA Suite domain.",
	"content": "The SOA deployment scripts demonstrate the creation of an Oracle SOA Suite domain home on an existing Kubernetes persistent volume (PV) and persistent volume claim (PVC). The scripts also generate the domain YAML file, which can then be used to start the Kubernetes artifacts of the corresponding domain.\nPrerequisites Before you begin, complete the following steps:\n Review the Domain resource documentation. Review the requirements and limitations. Ensure that you have executed all the preliminary steps in Prepare your environment. Ensure that the database and the WebLogic Kubernetes Operator are running.  Prepare to use the create domain script The sample scripts for Oracle SOA Suite domain deployment are available at ${WORKDIR}/create-soa-domain.\nYou must edit create-domain-inputs.yaml (or a copy of it) to provide the details for your domain. Refer to the configuration parameters below to understand the information that you must provide in this file.\nConfiguration parameters The following parameters can be provided in the inputs file.\n   Parameter Definition Default     sslEnabled Boolean value indicating whether to enable SSL for each WebLogic Server instance. false   adminPort Port number for the Administration Server inside the Kubernetes cluster. 7001   adminServerSSLPort SSL port number of the Administration Server inside the Kubernetes cluster. 7002   adminNodePort Port number of the Administration Server outside the Kubernetes cluster. 30701   adminServerName Name of the Administration Server. AdminServer   configuredManagedServerCount Number of Managed Server instances to generate for the domain. 5   soaClusterName Name of the SOA WebLogic Server cluster instance to generate for the domain. By default, the cluster name is soa_cluster. This configuration parameter is applicable only for soa and soaosb domain types. soa_cluster   osbClusterName Name of the Oracle Service Bus WebLogic Server cluster instance to generate for the domain. By default, the cluster name is osb_cluster. This configuration parameter is applicable only for osb and soaosb domain types. osb_cluster   createDomainFilesDir Directory on the host machine to locate all the files to create a WebLogic Server domain, including the script that is specified in the createDomainScriptName parameter. By default, this directory is set to the relative path wlst, and the create script will use the built-in WLST offline scripts in the wlst directory to create the WebLogic Server domain. An absolute path is also supported to point to an arbitrary directory in the file system. The built-in scripts can be replaced by the user-provided scripts as long as those files are in the specified directory. Files in this directory are put into a Kubernetes config map, which in turn is mounted to the createDomainScriptsMountPath, so that the Kubernetes pod can use the scripts and supporting files to create a domain home. wlst   createDomainScriptsMountPath Mount path where the create domain scripts are located inside a pod. The create-domain.sh script creates a Kubernetes job to run the script (specified by the createDomainScriptName parameter) in a Kubernetes pod to create a domain home. Files in the createDomainFilesDir directory are mounted to this location in the pod, so that the Kubernetes pod can use the scripts and supporting files to create a domain home. /u01/weblogic   createDomainScriptName Script that the create domain script uses to create a WebLogic Server domain. The create-domain.sh script creates a Kubernetes job to run this script to create a domain home. The script is located in the in-pod directory that is specified by the createDomainScriptsMountPath parameter. If you need to provide your own scripts to create the domain home, instead of using the built-in scripts, you must use this property to set the name of the script that you want the create domain job to run. create-domain-job.sh   domainHome Home directory of the SOA domain. If not specified, the value is derived from the domainUID as /shared/domains/\u0026lt;domainUID\u0026gt;. /u01/oracle/user_projects/domains/soainfra   domainPVMountPath Mount path of the domain persistent volume. /u01/oracle/user_projects   domainUID Unique ID that will be used to identify this particular domain. Used as the name of the generated WebLogic Server domain as well as the name of the Kubernetes domain resource. This ID must be unique across all domains in a Kubernetes cluster. This ID cannot contain any character that is not valid in a Kubernetes service name. soainfra   domainType Type of the domain. Mandatory input for Oracle SOA Suite domains. You must provide one of the supported domain type values: soa (deploys a SOA domain with Enterprise Scheduler (ESS)), osb (deploys an Oracle Service Bus domain), and soaosb (deploys a domain with SOA, Oracle Service Bus, and Enterprise Scheduler (ESS)). soa   exposeAdminNodePort Boolean value indicating if the Administration Server is exposed outside of the Kubernetes cluster. false   exposeAdminT3Channel Boolean value indicating if the T3 administrative channel is exposed outside the Kubernetes cluster. false   httpAccessLogInLogHome Boolean value indicating if server HTTP access log files should be written to the same directory as logHome. If false, server HTTP access log files will be written to the directory specified in the WebLogic Server domain home configuration. true   image SOA Suite Docker image. The operator requires Oracle SOA Suite 12.2.1.4. Refer to Obtain the Oracle SOA Suite Docker image for details on how to obtain or create the image. soasuite:12.2.1.4   imagePullPolicy Oracle SOA Suite Docker image pull policy. Valid values are IfNotPresent, Always, Never. IfNotPresent   imagePullSecretName Name of the Kubernetes secret to access the Docker Store to pull the WebLogic Server Docker image. The presence of the secret will be validated when this parameter is specified.    includeServerOutInPodLog Boolean value indicating whether to include the server .out to the pod\u0026rsquo;s stdout. true   initialManagedServerReplicas Number of Managed Servers to initially start for the domain. 2   javaOptions Java options for starting the Administration Server and Managed Servers. A Java option can have references to one or more of the following predefined variables to obtain WebLogic Server domain information: $(DOMAIN_NAME), $(DOMAIN_HOME), $(ADMIN_NAME), $(ADMIN_PORT), and $(SERVER_NAME). If sslEnabled is set to true and the WebLogic Server demo certificate is used, add -Dweblogic.security.SSL.ignoreHostnameVerification=true to allow the Managed Servers to connect to the Administration Server while booting up. The WebLogic Server generated demo certificate in this environment typically contains a host name that is different from the runtime container\u0026rsquo;s host name. -Dweblogic.StdoutDebugEnabled=false   logHome The in-pod location for the domain log, server logs, server out, and Node Manager log files. If not specified, the value is derived from the domainUID as /shared/logs/\u0026lt;domainUID\u0026gt;. /u01/oracle/user_projects/domains/logs/soainfra   soaManagedServerNameBase Base string used to generate Managed Server names in the SOA cluster. The default value is soa_server. This configuration parameter is applicable only for soa and soaosb domain types. soa_server   osbManagedServerNameBase Base string used to generate Managed Server names in the Oracle Service Bus cluster. The default value is osb_server. This configuration parameter is applicable only for osb and soaosb domain types. osb_server   soaManagedServerPort Port number for each Managed Server in the SOA cluster. This configuration parameter is applicable only for soa and soaosb domain types. 8001   osbManagedServerPort Port number for each Managed Server in the Oracle Service Bus cluster. This configuration parameter is applicable only for osb and soaosb domain types. 9001   soaManagedServerSSLPort SSL port number for each Managed Server in the SOA cluster. This configuration parameter is applicable only for soa and soaosb domain types. 8002   osbManagedServerSSLPort SSL port number for each Managed Server in the Oracle Service Bus cluster. This configuration parameter is applicable only for osb and soaosb domain types. 9002   namespace Kubernetes namespace in which to create the domain. soans   persistentVolumeClaimName Name of the persistent volume claim created to host the domain home. If not specified, the value is derived from the domainUID as \u0026lt;domainUID\u0026gt;-weblogic-sample-pvc. soainfra-domain-pvc   productionModeEnabled Boolean value indicating if production mode is enabled for the domain. true   serverStartPolicy Determines which WebLogic Server instances will be started. Valid values are NEVER, IF_NEEDED, ADMIN_ONLY. IF_NEEDED   t3ChannelPort Port for the T3 channel of the NetworkAccessPoint. 30012   t3PublicAddress Public address for the T3 channel. This should be set to the public address of the Kubernetes cluster. This would typically be a load balancer address. For development environments only: In a single server (all-in-one) Kubernetes deployment, this may be set to the address of the master, or at the very least, it must be set to the address of one of the worker nodes. If not provided, the script will attempt to set it to the IP address of the Kubernetes cluster.   weblogicCredentialsSecretName Name of the Kubernetes secret for the Administration Server\u0026rsquo;s user name and password. If not specified, then the value is derived from the domainUID as \u0026lt;domainUID\u0026gt;-weblogic-credentials. soainfra-domain-credentials   weblogicImagePullSecretName Name of the Kubernetes secret for the Docker Store, used to pull the WebLogic Server image.    serverPodCpuRequest, serverPodMemoryRequest, serverPodCpuCLimit, serverPodMemoryLimit The maximum amount of compute resources allowed, and minimum amount of compute resources required, for each server pod. Refer to the Kubernetes documentation on Managing Compute Resources for Containers for details. Resource requests and resource limits are not specified.   rcuSchemaPrefix The schema prefix to use in the database. For example SOA1. You may wish to make this the same as the domainUID in order to simplify matching domains to their RCU schemas. SOA1   rcuDatabaseURL The database URL. oracle-db.default.svc.cluster.local:1521/devpdb.k8s   rcuCredentialsSecret The Kubernetes secret containing the database credentials. soainfra-rcu-credentials   persistentStore The persistent store for \u0026lsquo;JMS servers\u0026rsquo; and \u0026lsquo;Transaction log store\u0026rsquo; in the domain. Valid values are jdbc, file. jdbc    Note that the names of the Kubernetes resources in the generated YAML files may be formed with the value of some of the properties specified in the create-domain-inputs.yaml file. Those properties include the adminServerName, soaClusterName, and soaManagedServerNameBase etc. If those values contain any characters that are invalid in a Kubernetes service name, those characters are converted to valid values in the generated YAML files. For example, an uppercase letter is converted to a lowercase letter and an underscore (\u0026quot;_\u0026quot;) is converted to a hyphen (\u0026quot;-\u0026quot;).\nThe sample demonstrates how to create an Oracle SOA Suite domain home and associated Kubernetes resources for the domain. In addition, the sample provides the capability for users to supply their own scripts to create the domain home for other use cases. The generated domain YAML file could also be modified to cover more use cases.\nRun the create domain script Run the create domain script, specifying your inputs file and an output directory to store the generated artifacts:\n$ ./create-domain.sh \\ -i create-domain-inputs.yaml \\ -o \u0026lt;path to output-directory\u0026gt; The script will perform the following steps:\n  Create a directory for the generated Kubernetes YAML files for this domain if it does not already exist. The path name is \u0026lt;path to output-directory\u0026gt;/weblogic-domains/\u0026lt;domainUID\u0026gt;. If the directory already exists, its contents must be removed before using this script.\n  Create a Kubernetes job that will start up a utility Oracle SOA Suite container and run offline WLST scripts to create the domain on the shared storage.\n  Run and wait for the job to finish.\n  Create a Kubernetes domain YAML file, domain.yaml, in the \u0026ldquo;output\u0026rdquo; directory that was created above. This YAML file can be used to create the Kubernetes resource using the kubectl create -f or kubectl apply -f command:\n$ kubectl apply -f \u0026lt;path to output-directory\u0026gt;/weblogic-domains/\u0026lt;domainUID\u0026gt;/domain.yaml   Create a convenient utility script, delete-domain-job.yaml, to clean up the domain home created by the create script.\n  The default domain created by the script has the following characteristics:\n An Administration Server named AdminServer listening on port 7001. A configured cluster named soa_cluster of size 5. Two Managed Servers, named soa_server1 and soa_server2, listening on port 8001. Log files that are located in /shared/logs/\u0026lt;domainUID\u0026gt;. SOA Infra, SOA Composer, and WorklistApp applications deployed.  Refer to the troubleshooting page to troubleshoot issues during the domain creation.\n Verify the results The create domain script verifies that the domain was created, and reports failure if there is an error. However, it may be desirable to manually verify the domain, even if just to gain familiarity with the various Kubernetes objects that were created by the script.\nGenerated YAML files with the default inputs   Click here to see sample content of the generated `domain.yaml` for `soaosb` domainType that creates SOA and Oracle Service Bus clusters.   $ cat output/weblogic-domains/soainfra/domain.yaml # Copyright (c) 2020, 2021, Oracle and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # # This is an example of how to define a Domain resource. # apiVersion: \u0026quot;weblogic.oracle/v8\u0026quot; kind: Domain metadata: name: soainfra namespace: soans labels: weblogic.domainUID: soainfra spec: # The WebLogic Domain Home domainHome: /u01/oracle/user_projects/domains/soainfra # The domain home source type # Set to PersistentVolume for domain-in-pv, Image for domain-in-image, or FromModel for model-in-image domainHomeSourceType: PersistentVolume # The WebLogic Server image that the Operator uses to start the domain image: \u0026quot;soasuite:12.2.1.4\u0026quot; # imagePullPolicy defaults to \u0026quot;Always\u0026quot; if image version is :latest imagePullPolicy: \u0026quot;IfNotPresent\u0026quot; # Identify which Secret contains the credentials for pulling an image #imagePullSecrets: #- name: # Identify which Secret contains the WebLogic Admin credentials (note that there is an example of # how to create that Secret at the end of this file) webLogicCredentialsSecret: name: soainfra-domain-credentials # Whether to include the server out file into the pod's stdout, default is true includeServerOutInPodLog: true # Whether to enable log home logHomeEnabled: true # Whether to write HTTP access log file to log home httpAccessLogInLogHome: true # The in-pod location for domain log, server logs, server out, introspector out, and Node Manager log files logHome: /u01/oracle/user_projects/domains/logs/soainfra # An (optional) in-pod location for data storage of default and custom file stores. # If not specified or the value is either not set or empty (e.g. dataHome: \u0026quot;\u0026quot;) then the # data storage directories are determined from the WebLogic domain home configuration. dataHome: \u0026quot;\u0026quot; # serverStartPolicy legal values are \u0026quot;NEVER\u0026quot;, \u0026quot;IF_NEEDED\u0026quot;, or \u0026quot;ADMIN_ONLY\u0026quot; # This determines which WebLogic Servers the Operator will start up when it discovers this Domain # - \u0026quot;NEVER\u0026quot; will not start any server in the domain # - \u0026quot;ADMIN_ONLY\u0026quot; will start up only the administration server (no managed servers will be started) # - \u0026quot;IF_NEEDED\u0026quot; will start all non-clustered servers, including the administration server and clustered servers up to the replica count serverStartPolicy: \u0026quot;IF_NEEDED\u0026quot; serverPod: # an (optional) list of environment variable to be set on the servers env: - name: JAVA_OPTIONS value: \u0026quot;-Dweblogic.StdoutDebugEnabled=false -Dweblogic.ssl.Enabled=true -Dweblogic.security.SSL.ignoreHostnameVerification=true\u0026quot; - name: USER_MEM_ARGS value: \u0026quot;-Djava.security.egd=file:/dev/./urandom -Xms256m -Xmx1024m \u0026quot; volumes: - name: weblogic-domain-storage-volume persistentVolumeClaim: claimName: soainfra-domain-pvc volumeMounts: - mountPath: /u01/oracle/user_projects name: weblogic-domain-storage-volume # adminServer is used to configure the desired behavior for starting the administration server. adminServer: # serverStartState legal values are \u0026quot;RUNNING\u0026quot; or \u0026quot;ADMIN\u0026quot; # \u0026quot;RUNNING\u0026quot; means the listed server will be started up to \u0026quot;RUNNING\u0026quot; mode # \u0026quot;ADMIN\u0026quot; means the listed server will be start up to \u0026quot;ADMIN\u0026quot; mode serverStartState: \u0026quot;RUNNING\u0026quot; adminService: channels: # The Admin Server's NodePort # - channelName: default # nodePort: 30701 # Uncomment to export the T3Channel as a service - channelName: T3Channel serverPod: # an (optional) list of environment variable to be set on the admin servers env: - name: USER_MEM_ARGS value: \u0026quot;-Djava.security.egd=file:/dev/./urandom -Xms512m -Xmx1024m \u0026quot; # clusters is used to configure the desired behavior for starting member servers of a cluster. # If you use this entry, then the rules will be applied to ALL servers that are members of the named clusters. clusters: - clusterName: osb_cluster serverService: precreateService: true serverStartState: \u0026quot;RUNNING\u0026quot; serverPod: env: # This parameter can be used to pass in new system properties, use the space delimiter to append multiple values. # Do not change the below value, only append new values to it. - name: K8S_REFCONF_OVERRIDES value: \u0026quot;-Doracle.sb.tracking.resiliency.MemoryMetricEnabled=false \u0026quot; # Instructs Kubernetes scheduler to prefer nodes for new cluster members where there are not # already members of the same cluster. affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: \u0026quot;weblogic.clusterName\u0026quot; operator: In values: - $(CLUSTER_NAME) topologyKey: \u0026quot;kubernetes.io/hostname\u0026quot; replicas: 2 # The number of managed servers to start for unlisted clusters # replicas: 1 # Istio # configuration: # istio: # enabled: # readinessPort: - clusterName: soa_cluster serverService: precreateService: true serverStartState: \u0026quot;RUNNING\u0026quot; serverPod: env: # This parameter can be used to pass in new system properties, use the space delimiter to append multiple values. # Do not change the below value, only append new values to it. - name: K8S_REFCONF_OVERRIDES value: \u0026quot;-Doracle.soa.tracking.resiliency.MemoryMetricEnabled=false \u0026quot; # Instructs Kubernetes scheduler to prefer nodes for new cluster members where there are not # already members of the same cluster. affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: \u0026quot;weblogic.clusterName\u0026quot; operator: In values: - $(CLUSTER_NAME) topologyKey: \u0026quot;kubernetes.io/hostname\u0026quot; replicas: 2 # The number of managed servers to start for unlisted clusters # replicas: 1    Verify the domain To confirm that the domain was created, enter the following command:\n$ kubectl describe domain DOMAINUID -n NAMESPACE Replace DOMAINUID with the domainUID and NAMESPACE with the actual namespace.\n  Click here to see a sample domain description.   $ kubectl describe domain soainfra -n soans Name: soainfra Namespace: soans Labels: weblogic.domainUID=soainfra Annotations: \u0026lt;none\u0026gt; API Version: weblogic.oracle/v8 Kind: Domain Metadata: Creation Timestamp: 2021-03-01T05:27:38Z Generation: 1 Managed Fields: API Version: weblogic.oracle/v8 Fields Type: FieldsV1 fieldsV1: f:metadata: f:labels: .: f:weblogic.domainUID: Manager: kubectl Operation: Update Time: 2021-03-01T05:27:38Z API Version: weblogic.oracle/v8 Fields Type: FieldsV1 fieldsV1: f:status: .: f:clusters: f:conditions: f:introspectJobFailureCount: f:servers: f:startTime: Manager: Kubernetes Java Client Operation: Update Time: 2021-03-02T10:26:59Z Resource Version: 13351862 Self Link: /apis/weblogic.oracle/v8/namespaces/soans/domains/soainfra UID: 295dfc48-999e-45e3-b275-9d752587b8d9 Spec: Admin Server: Admin Service: Channels: Channel Name: T3Channel Server Pod: Env: Name: USER_MEM_ARGS Value: -Djava.security.egd=file:/dev/./urandom -Xms512m -Xmx1024m Server Start State: RUNNING Clusters: Cluster Name: osb_cluster Replicas: 2 Server Pod: Affinity: Pod Anti Affinity: Preferred During Scheduling Ignored During Execution: Pod Affinity Term: Label Selector: Match Expressions: Key: weblogic.clusterName Operator: In Values: $(CLUSTER_NAME) Topology Key: kubernetes.io/hostname Weight: 100 Env: Name: K8S_REFCONF_OVERRIDES Value: -Doracle.sb.tracking.resiliency.MemoryMetricEnabled=false Server Service: Precreate Service: true Server Start State: RUNNING Cluster Name: soa_cluster Replicas: 2 Server Pod: Affinity: Pod Anti Affinity: Preferred During Scheduling Ignored During Execution: Pod Affinity Term: Label Selector: Match Expressions: Key: weblogic.clusterName Operator: In Values: $(CLUSTER_NAME) Topology Key: kubernetes.io/hostname Weight: 100 Env: Name: K8S_REFCONF_OVERRIDES Value: -Doracle.soa.tracking.resiliency.MemoryMetricEnabled=false Server Service: Precreate Service: true Server Start State: RUNNING Data Home: Domain Home: /u01/oracle/user_projects/domains/soainfra Domain Home Source Type: PersistentVolume Http Access Log In Log Home: true Image: soasuite:12.2.1.4 Image Pull Policy: IfNotPresent Include Server Out In Pod Log: true Log Home: /u01/oracle/user_projects/domains/logs/soainfra Log Home Enabled: true Server Pod: Env: Name: JAVA_OPTIONS Value: -Dweblogic.StdoutDebugEnabled=false -Dweblogic.ssl.Enabled=true -Dweblogic.security.SSL.ignoreHostnameVerification=true Name: USER_MEM_ARGS Value: -Djava.security.egd=file:/dev/./urandom -Xms256m -Xmx1024m Volume Mounts: Mount Path: /u01/oracle/user_projects Name: weblogic-domain-storage-volume Volumes: Name: weblogic-domain-storage-volume Persistent Volume Claim: Claim Name: soainfra-domain-pvc Server Start Policy: IF_NEEDED Web Logic Credentials Secret: Name: soainfra-domain-credentials Status: Clusters: Cluster Name: osb_cluster Maximum Replicas: 5 Minimum Replicas: 0 Ready Replicas: 2 Replicas: 2 Replicas Goal: 2 Cluster Name: soa_cluster Maximum Replicas: 5 Minimum Replicas: 0 Ready Replicas: 2 Replicas: 2 Replicas Goal: 2 Conditions: Last Transition Time: 2021-03-02T10:26:59.683Z Reason: ManagedServersStarting Status: True Type: Progressing Introspect Job Failure Count: 0 Servers: Desired State: RUNNING Node Name: k8sdev Server Name: AdminServer State: UNKNOWN Cluster Name: osb_cluster Desired State: RUNNING Node Name: k8sdev Server Name: osb_server1 State: UNKNOWN Cluster Name: osb_cluster Desired State: RUNNING Node Name: k8sdev Server Name: osb_server2 State: UNKNOWN Cluster Name: osb_cluster Desired State: SHUTDOWN Server Name: osb_server3 Cluster Name: osb_cluster Desired State: SHUTDOWN Server Name: osb_server4 Cluster Name: osb_cluster Desired State: SHUTDOWN Server Name: osb_server5 Cluster Name: soa_cluster Desired State: RUNNING Node Name: k8sdev Server Name: soa_server1 State: UNKNOWN Cluster Name: soa_cluster Desired State: RUNNING Node Name: k8sdev Server Name: soa_server2 State: UNKNOWN Cluster Name: soa_cluster Desired State: SHUTDOWN Server Name: soa_server3 Cluster Name: soa_cluster Desired State: SHUTDOWN Server Name: soa_server4 Cluster Name: soa_cluster Desired State: SHUTDOWN Server Name: soa_server5 Start Time: 2021-03-01T05:27:38.844Z Events: \u0026lt;none\u0026gt;    In the Status section of the output, the available servers and clusters are listed. Note that if this command is issued very soon after the script finishes, there may be no servers available yet, or perhaps only the Administration Server but no Managed Servers. The operator will start up the Administration Server first and wait for it to become ready before starting the Managed Servers.\nVerify the pods Enter the following command to see the pods running the servers:\n$ kubectl get pods -n NAMESPACE Here is an example of the output of this command. You can verify that an Administration Server and two Managed Servers for each cluster (SOA and Oracle Service Bus) are running for soaosb domain type.\n$ kubectl get pods -n soans NAME READY STATUS RESTARTS AGE soainfra-adminserver 1/1 Running 0 53m soainfra-osb-server1 1/1 Running 0 50m soainfra-osb-server2 1/1 Running 0 50m soainfra-soa-server1 1/1 Running 0 50m soainfra-soa-server2 1/1 Running 0 50m Verify the services Enter the following command to see the services for the domain:\n$ kubectl get services -n NAMESPACE Here is an example of the output of this command. You can verify that services for Administration Server and Managed Servers (for SOA and Oracle Service Bus clusters) are created for soaosb domain type.\n  Click here to see a sample list of services.   $ kubectl get services -n soans NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE soainfra-adminserver ClusterIP None \u0026lt;none\u0026gt; 30012/TCP,7001/TCP,7002/TCP 54m soainfra-cluster-osb-cluster ClusterIP 10.100.138.57 \u0026lt;none\u0026gt; 9001/TCP,9002/TCP 51m soainfra-cluster-soa-cluster ClusterIP 10.99.117.240 \u0026lt;none\u0026gt; 8001/TCP,8002/TCP 51m soainfra-osb-server1 ClusterIP None \u0026lt;none\u0026gt; 9001/TCP,9002/TCP 51m soainfra-osb-server2 ClusterIP None \u0026lt;none\u0026gt; 9001/TCP,9002/TCP 51m soainfra-osb-server3 ClusterIP 10.108.71.8 \u0026lt;none\u0026gt; 9001/TCP,9002/TCP 51m soainfra-osb-server4 ClusterIP 10.100.1.144 \u0026lt;none\u0026gt; 9001/TCP,9002/TCP 51m soainfra-osb-server5 ClusterIP 10.108.57.147 \u0026lt;none\u0026gt; 9001/TCP,9002/TCP 51m soainfra-soa-server1 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP,8002/TCP 51m soainfra-soa-server2 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP,8002/TCP 51m soainfra-soa-server3 ClusterIP 10.98.160.126 \u0026lt;none\u0026gt; 8001/TCP,8002/TCP 51m soainfra-soa-server4 ClusterIP 10.105.164.133 \u0026lt;none\u0026gt; 8001/TCP,8002/TCP 51m soainfra-soa-server5 ClusterIP 10.109.168.179 \u0026lt;none\u0026gt; 8001/TCP,8002/TCP 51m    "
},
{
	"uri": "/fmw-kubernetes/wcsites-domains/installguide/create-wcsites-domains/",
	"title": "Create Oracle WebCenter Sites domains",
	"tags": [],
	"description": "Create an Oracle WebCenter Sites domain home on an existing PV or PVC, and create the domain resource YAML file for deploying the generated Oracle WebCenter Sites domain.",
	"content": "Contents  Introduction Prerequisites Prepare the WebCenter Sites Domain Creation Input File Create the WebCenter Sites Domain Initialize the WebCenter Sites Domain Verify the WebCenter Sites Domain Expose WebCenter Sites Services Load Balance With an Ingress Controller or A Web Server Configure WebCenter Sites Settings in WebCenter Sites Property Management For Publishing Setting in WebCenter Sites Customization  Introduction This document details on how to use sample scripts to demonstrate the creation of a WebCenter Sites domain home on an existing Kubernetes persistent volume (PV) and persistent volume claim (PVC). The scripts also generate the domain YAML file, which can then be used to start the Kubernetes artifacts of the corresponding domain.\nPrerequisites  Ensure that you have completed all of the steps under prepare-your-environment. Ensure that the database and the WebLogic Kubernetes Operator is up.  Prepare the WebCenter Sites Domain Creation Input File If required, domain creation inputs can be customized by editing create-domain-inputs.yaml as described below:\nPlease note that the sample scripts for the WebCenter Sites domain deployment are available from the previously downloaded repository at kubernetes/create-wcsites-domain/domain-home-on-pv/.\nMake a copy of the create-domain-inputs.yaml file before updating the default values.\nThe default domain created by the script has the following characteristics:\n An Administration Server named AdminServer listening on port 7001. A configured cluster named wcsites_cluster of size 5. Managed Server, named wcsites_server1, listening on port 8001. Log files that are located in /shared/logs/\u0026lt;domainUID\u0026gt;.  Configuration parameters The following parameters can be provided in the inputs file:\n   Parameter Definition Default     adminPort Port number for the Administration Server inside the Kubernetes cluster. 7001   adminServerName Name of the Administration Server. AdminServer   clusterName Name of the WebLogic cluster instance to generate for the domain. By default the cluster name is wcsites_cluster for the WebCenter Sites domain. wcsites_cluster   configuredManagedServerCount Number of Managed Server instances for the domain. 3   createDomainFilesDir Directory on the host machine to locate all the files that you need to create a WebLogic domain, including the script that is specified in the createDomainScriptName property. By default, this directory is set to the relative path wlst, and the create script will use the built-in WLST offline scripts in the wlst directory to create the WebLogic domain. An absolute path is also supported to point to an arbitrary directory in the file system. The built-in scripts can be replaced by the user-provided scripts or model files as long as those files are in the specified directory. Files in this directory are put into a Kubernetes config map, which in turn is mounted to the createDomainScriptsMountPath, so that the Kubernetes pod can use the scripts and supporting files to create a domain home. wlst   createDomainScriptsMountPath Mount path where the create domain scripts are located inside a pod. The create-domain.sh script creates a Kubernetes job to run the script (specified in the createDomainScriptName property) in a Kubernetes pod to create a domain home. Files in the createDomainFilesDir directory are mounted to this location in the pod, so that the Kubernetes pod can use the scripts and supporting files to create a domain home. /u01/weblogic   createDomainScriptName Script that the create domain script uses to create a WebLogic domain. The create-domain.sh script creates a Kubernetes job to run this script to create a domain home. The script is located in the in-pod directory that is specified in the createDomainScriptsMountPath property. If you need to provide your own scripts to create the domain home, instead of using the built-it scripts, you must use this property to set the name of the script that you want the create domain job to run. create-domain-job.sh   domainHome Home directory of the WebCenter Sites domain. This field cannot be modified. /u01/oracle/user_projects/domains/wcsitesinfra   domainPVMountPath Mount path of the domain persistent volume. This field cannot be modified. /u01/oracle/user_projects/domains   domainUID Unique ID that will be used to identify this particular domain. Used as the name of the generated WebLogic domain as well as the name of the Kubernetes domain resource. This ID must be unique across all domains in a Kubernetes cluster. This ID cannot contain any character that is not valid in a Kubernetes service name. wcsitesinfra   exposeAdminNodePort Boolean indicating if the Administration Server is exposed outside of the Kubernetes cluster. false   exposeAdminT3Channel Boolean indicating if the T3 administrative channel is exposed outside the Kubernetes cluster. false   image WebCenter Sites Docker image. The Operator requires WebCenter Sites release 12.2.1.4.0. Refer to WebCenter Sites Docker image for details on how to obtain or create the image. oracle/wcsites:12.2.1.4   imagePullPolicy WebLogic Docker image pull policy. Legal values are IfNotPresent, Always, or Never IfNotPresent   imagePullSecretName Name of the Kubernetes secret to access the Docker Store to pull the WebLogic Server Docker image. The presence of the secret will be validated when this parameter is specified.    includeServerOutInPodLog Boolean indicating whether to include the server.out to the pod\u0026rsquo;s stdout. true   initialManagedServerReplicas Number of Managed Server to initially start for the domain. 1   javaOptions Java options for starting the Administration Server and Managed Servers. A Java option can include references to one or more of the following pre-defined variables to obtain WebLogic domain information: $(DOMAIN_NAME), $(DOMAIN_HOME), $(ADMIN_NAME), $(ADMIN_PORT), and $(SERVER_NAME). -Dweblogic.StdoutDebugEnabled=false   logHome The in-pod location for the domain log, server logs, server out, and Node Manager log files. This field cannot be modified. /u01/oracle/user_projects/logs/wcsitesinfra   managedServerNameBase Base string used to generate Managed Server names. wcsites_server   managedServerPort Port number for each Managed Server. 8001   namespace Kubernetes namespace in which to create the domain. wcsites-ns   persistentVolumeClaimName Name of the persistent volume claim created to host the domain home. If not specified, the value is derived from the domainUID as \u0026lt;domainUID\u0026gt;-weblogic-sample-pvc. wcsitesinfra-domain-pvc   productionModeEnabled Boolean indicating if production mode is enabled for the domain. true   serverStartPolicy Determines which WebLogic Server instances will be started. Legal values are NEVER, IF_NEEDED, ADMIN_ONLY. IF_NEEDED   t3ChannelPort Port for the T3 channel of the NetworkAccessPoint. 30012   t3PublicAddress Public address for the T3 channel. This should be set to the public address of the Kubernetes cluster. This would typically be a load balancer address. For development environments only: In a single server (all-in-one) Kubernetes deployment, this may be set to the address of the master, or at the very least, it must be set to the address of one of the worker nodes. If not provided, the script will attempt to set it to the IP address of the Kubernetes cluster.   weblogicCredentialsSecretName Name of the Kubernetes secret for the Administration Server\u0026rsquo;s user name and password. If not specified, then the value is derived from the domainUID as \u0026lt;domainUID\u0026gt;-weblogic-credentials. wcsites-domain-credentials   weblogicImagePullSecretName Name of the Kubernetes secret for the Docker Store, used to pull the WebLogic Server image.    serverPodCpuRequest, serverPodMemoryRequest, serverPodCpuCLimit, serverPodMemoryLimit The maximum amount of compute resources allowed and minimum amount of compute resources required for each server pod. Please refer to the Kubernetes documentation on Managing Compute Resources for Containers for details. Resource requests and resource limits are not specified. Refer to WebCenter Sites Cluster Sizing Recommendations for more details.   rcuSchemaPrefix The schema prefix to use in the database, for example WCS1. You may wish to make this the same as the domainUID in order to simplify matching domains to their RCU schemas. WCS1   rcuDatabaseURL The database URL. oracle-db.wcsitesdb-ns.svc.cluster.local:1521/devpdb.k8s   rcuCredentialsSecret The loadbalancer hostname to be provided. wcsites-rcu-credentials   loadBalancerHostName Hostname for the final url accessible outside K8S environment. abc.def.com   loadBalancerPortNumber Port for the final url accessible outside K8S environment. 30305   loadBalancerProtocol Protocol for the final url accessible outside K8S environment. http   loadBalancerType Loadbalancer name that will be used. Example: Traefik or \u0026quot;\u0026rdquo; traefik   unicastPort Starting range of uniciast port that application will use. 50000   sitesSamples Sites to be installed without samples sites by default, else true. false    You can form the names of the Kubernetes resources in the generated YAML files with the value of these properties specified in the create-domain-inputs.yaml file: adminServerName , clusterName and managedServerNameBase . Characters that are invalid in a Kubernetes service name are converted to valid values in the generated YAML files. For example, an uppercase letter is converted to a lowercase letter and an underscore (\u0026quot;_\u0026quot;) is converted to a hyphen (\u0026quot;-\u0026quot;) .\nThe sample demonstrates how to create a WebCenter Sites domain home and associated Kubernetes resources for a domain that has one cluster only. In addition, the sample provides the capability for users to supply their own scripts to create the domain home for other use cases. You can modify the generated domain YAML file to include more use cases.\nCreate the WebCenter Sites Domain   Understanding the syntax of the create-domain.sh script:\n$ ./create-domain.sh \\ -i create-domain-inputs.yaml \\ -o /\u0026lt;path to output-directory\u0026gt; The script performs the following functions:\n  Creates a directory for the generated Kubernetes YAML files for this domain if it does not already exist. The path name is /\u0026lt;path to output-directory\u0026gt;/weblogic-domains/\u0026lt;domainUID\u0026gt;. If the directory already exists, remove its content before using this script.\n  Creates a Kubernetes job that will start up a utility WebCenter Sites container and run offline WLST scripts to create the domain on the shared storage.\n  Runs and waits for the job to finish.\n  Creates a Kubernetes domain YAML file, domain.yaml, in the directory that is created above. This YAML file can be used to create the Kubernetes resource using the kubectl create -f or kubectl apply -f command:\n$ kubectl apply -f ../\u0026lt;path to output-directory\u0026gt;/weblogic-domains/\u0026lt;domainUID\u0026gt;/domain.yaml   Creates a convenient utility script, delete-domain-job.yaml, to clean up the domain home created by the create script.\n    Now, run the create-domain.sh sample script below, pointing it at the create-domain-inputs inputs file and an output directory like below:\nbash-4.2$ rm -rf kubernetes/create-wcsites-domain/output/weblogic-domains bash-4.2$ sh kubernetes/create-wcsites-domain/domain-home-on-pv/create-domain.sh \\  -i kubernetes/create-wcsites-domain/domain-home-on-pv/create-domain-inputs.yaml \\  -o kubernetes/create-wcsites-domain/output Input parameters being used export version=\u0026#34;create-weblogic-sample-domain-inputs-v1\u0026#34; export adminPort=\u0026#34;7001\u0026#34; export adminServerName=\u0026#34;adminserver\u0026#34; export domainUID=\u0026#34;wcsitesinfra\u0026#34; export domainHome=\u0026#34;/u01/oracle/user_projects/domains/$domainUID\u0026#34; export serverStartPolicy=\u0026#34;IF_NEEDED\u0026#34; export clusterName=\u0026#34;wcsites_cluster\u0026#34; export configuredManagedServerCount=\u0026#34;3\u0026#34; export initialManagedServerReplicas=\u0026#34;1\u0026#34; export managedServerNameBase=\u0026#34;wcsites_server\u0026#34; export managedServerPort=\u0026#34;8001\u0026#34; export image=\u0026#34;oracle/wcsites:12.2.1.4\u0026#34; export imagePullPolicy=\u0026#34;IfNotPresent\u0026#34; export productionModeEnabled=\u0026#34;true\u0026#34; export weblogicCredentialsSecretName=\u0026#34;wcsitesinfra-domain-credentials\u0026#34; export includeServerOutInPodLog=\u0026#34;true\u0026#34; export logHome=\u0026#34;/u01/oracle/user_projects/domains/logs/$domainUID\u0026#34; export t3ChannelPort=\u0026#34;30012\u0026#34; export exposeAdminT3Channel=\u0026#34;false\u0026#34; export adminNodePort=\u0026#34;30701\u0026#34; export exposeAdminNodePort=\u0026#34;false\u0026#34; export namespace=\u0026#34;wcsites-ns\u0026#34; javaOptions=-Dweblogic.StdoutDebugEnabled=false -Xms2g export persistentVolumeClaimName=\u0026#34;wcsitesinfra-domain-pvc\u0026#34; export domainPVMountPath=\u0026#34;/u01/oracle/user_projects/domains\u0026#34; export createDomainScriptsMountPath=\u0026#34;/u01/weblogic\u0026#34; export createDomainScriptName=\u0026#34;create-domain-job.sh\u0026#34; export createDomainFilesDir=\u0026#34;wlst\u0026#34; export rcuSchemaPrefix=\u0026#34;WCS1\u0026#34; export rcuDatabaseURL=\u0026#34;oracle-db.wcsitesdb-ns.svc.cluster.local:1521/devpdb.k8s\u0026#34; export rcuCredentialsSecret=\u0026#34;wcsitesinfra-rcu-credentials\u0026#34; export loadBalancerHostName=\u0026#34;abc.def.com\u0026#34; export loadBalancerPortNumber=\u0026#34;30305\u0026#34; export loadBalancerProtocol=\u0026#34;http\u0026#34; export loadBalancerType=\u0026#34;traefik\u0026#34; export unicastPort=\u0026#34;50000\u0026#34; export sitesSamples=\u0026#34;true\u0026#34; Generating kubernetes/create-wcsites-domain/output/weblogic-domains/wcsitesinfra/create-domain-job.yaml Generating kubernetes/create-wcsites-domain/output/weblogic-domains/wcsitesinfra/delete-domain-job.yaml Generating kubernetes/create-wcsites-domain/output/weblogic-domains/wcsitesinfra/domain.yaml Checking to see if the secret wcsitesinfra-domain-credentials exists in namespace wcsites-ns configmap/wcsitesinfra-create-fmw-infra-sample-domain-job-cm created Checking the configmap wcsitesinfra-create-fmw-infra-sample-domain-job-cm was created configmap/wcsitesinfra-create-fmw-infra-sample-domain-job-cm labeled Checking if object type job with name wcsitesinfra-create-fmw-infra-sample-domain-job exists No resources found. $loadBalancerType is NOT empty Creating the domain by creating the job kubernetes/create-wcsites-domain/output/weblogic-domains/wcsitesinfra/create-domain-job.yaml job.batch/wcsitesinfra-create-fmw-infra-sample-domain-job created Waiting for the job to complete... status on iteration 1 of 20 pod wcsitesinfra-create-fmw-infra-sample-domain-job-6l7zh status is Running status on iteration 2 of 20 pod wcsitesinfra-create-fmw-infra-sample-domain-job-6l7zh status is Running status on iteration 3 of 20 pod wcsitesinfra-create-fmw-infra-sample-domain-job-6l7zh status is Running status on iteration 4 of 20 pod wcsitesinfra-create-fmw-infra-sample-domain-job-6l7zh status is Running status on iteration 5 of 20 pod wcsitesinfra-create-fmw-infra-sample-domain-job-6l7zh status is Running status on iteration 6 of 20 pod wcsitesinfra-create-fmw-infra-sample-domain-job-6l7zh status is Running status on iteration 7 of 20 pod wcsitesinfra-create-fmw-infra-sample-domain-job-6l7zh status is Running status on iteration 8 of 20 pod wcsitesinfra-create-fmw-infra-sample-domain-job-6l7zh status is Running status on iteration 9 of 20 pod wcsitesinfra-create-fmw-infra-sample-domain-job-6l7zh status is Running status on iteration 10 of 20 pod wcsitesinfra-create-fmw-infra-sample-domain-job-6l7zh status is Running status on iteration 11 of 20 pod wcsitesinfra-create-fmw-infra-sample-domain-job-6l7zh status is Running status on iteration 12 of 20 pod wcsitesinfra-create-fmw-infra-sample-domain-job-6l7zh status is Running status on iteration 13 of 20 pod wcsitesinfra-create-fmw-infra-sample-domain-job-6l7zh status is Running status on iteration 14 of 20 pod wcsitesinfra-create-fmw-infra-sample-domain-job-6l7zh status is Running status on iteration 15 of 20 pod wcsitesinfra-create-fmw-infra-sample-domain-job-6l7zh status is Running status on iteration 16 of 20 pod wcsitesinfra-create-fmw-infra-sample-domain-job-6l7zh status is Completed Domain wcsitesinfra was created and will be started by the WebLogic Kubernetes Operator The following files were generated: kubernetes/create-wcsites-domain/output/weblogic-domains/wcsitesinfra/create-domain-inputs.yaml kubernetes/create-wcsites-domain/output/weblogic-domains/wcsitesinfra/create-domain-job.yaml kubernetes/create-wcsites-domain/output/weblogic-domains/wcsitesinfra/domain.yaml Completed\t  To monitor the above domain creation logs:\n$ kubectl get pods -n wcsites-ns |grep wcsitesinfra-create wcsitesinfra-create-fmw-infra-sample-domain-job-6l7zh 1/1 Running 0 6s $ kubectl get pods -n wcsites-ns | grep wcsitesinfra-create | awk \u0026#39;{print $1}\u0026#39; | xargs kubectl -n wcsites-ns logs -f SAMPLE OUTPUT:\nThe domain will be created using the script /u01/weblogic/createSitesDomain.sh Install Automation -\u0026gt; Starting automation script [mkdir] Created dir: /u01/wcs-wls-docker-install/work [echo] [3/14/20 7:54 AM] Work Directory=/u01/wcs-wls-docker-install/work [echo] [3/14/20 7:54 AM] DB URL: jdbc:oracle:thin:@ [echo] [3/14/20 7:54 AM] Info -\u0026gt; The script.db.connectstring has been set. [echo] [3/14/20 7:54 AM] Info.setDBConnectStringPropertey -\u0026gt; setting oracle-db.wcsitesdb-ns.svc.cluster.local:1521/devpdb.k8s [echo] [3/14/20 7:54 AM] Validation -\u0026gt; Checking if full path to JAVA executable is correctly specified [exec] java version \u0026quot;1.8.0_241\u0026quot; [exec] Java(TM) SE Runtime Environment (build 1.8.0_241-b07) [exec] Java HotSpot(TM) 64-Bit Server VM (build 25.241-b07, mixed mode) [echo] [3/14/20 7:54 AM] Validation -\u0026gt; Checking database connection [echo] [3/14/20 7:54 AM] dbUrl-----------------: jdbc:oracle:thin:@oracle-db.wcsitesdb-ns.svc.cluster.local:1521/devpdb.k8s [echo] [3/14/20 7:54 AM] Database Connection --\u0026gt; Success! [echo] [3/14/20 7:54 AM] 1st phase: WebCenter Sites installation started... [copy] Copying 1 file to /u01/wcs-wls-docker-install/work [copy] Copying /u01/wcs-wls-docker-install/rcu.rsp to /u01/wcs-wls-docker-install/work/rcu.rsp [echo] [3/14/20 7:54 AM] 1st phase: WebCenter Sites installation completed [echo] [3/14/20 7:54 AM] 2nd phase: WebCenter Sites RCU configuration started... [echo] [3/14/20 7:54 AM] Installation -\u0026gt; Repository Creation Utility - creates schema [echo] [3/14/20 7:54 AM] connectString-----------------: oracle-db.wcsitesdb-ns.svc.cluster.local:1521/devpdb.k8s [replace] Replaced 1 occurrences in 1 files. [replace] Replaced 1 occurrences in 1 files. [replace] Replaced 1 occurrences in 1 files. [replace] Replaced 1 occurrences in 1 files. [replace] Replaced 1 occurrences in 1 files. [echo] [3/14/20 7:54 AM] Create schema using command: /u01/oracle/oracle_common/bin/rcu -silent -responseFile /u01/wcs-wls-docker-install/work/rcu.rsp -f \u0026lt; /u01/wcs-wls-docker-install/work/rcuPasswords8852085298596415722.txt \u0026gt;/u01/wcs-wls-docker-install/work/rcu_output.log [echo] [3/14/20 7:54 AM] RCU Create Schema -\u0026gt; Please wait ... may take several minutes [echo] [3/14/20 8:00 AM] [echo] RCU Logfile: /u01/wcs-wls-docker-install/work/rcu/RCU2020-03-14_07-54_2112542638/logs/rcu.log [echo] Processing command line .... [echo] Repository Creation Utility - Checking Prerequisites [echo] Checking Global Prerequisites [echo] Repository Creation Utility - Checking Prerequisites [echo] Checking Component Prerequisites [echo] Repository Creation Utility - Creating Tablespaces [echo] Validating and Creating Tablespaces [echo] Create tablespaces in the repository database [echo] Repository Creation Utility - Create [echo] Repository Create in progress. [echo] Executing pre create operations [echo] Percent Complete: 20 [echo] Percent Complete: 20 [echo] Percent Complete: 22 [echo] Percent Complete: 24 [echo] Percent Complete: 26 [echo] Percent Complete: 26 [echo] Percent Complete: 28 [echo] Percent Complete: 28 [echo] Creating Common Infrastructure Services(STB) [echo] Percent Complete: 36 [echo] Percent Complete: 36 [echo] Percent Complete: 46 [echo] Percent Complete: 46 [echo] Percent Complete: 46 [echo] Creating Audit Services Append(IAU_APPEND) [echo] Percent Complete: 54 [echo] Percent Complete: 54 [echo] Percent Complete: 64 [echo] Percent Complete: 64 [echo] Percent Complete: 64 [echo] Creating Audit Services Viewer(IAU_VIEWER) [echo] Percent Complete: 72 [echo] Percent Complete: 72 [echo] Percent Complete: 72 [echo] Percent Complete: 73 [echo] Percent Complete: 73 [echo] Percent Complete: 74 [echo] Percent Complete: 74 [echo] Percent Complete: 74 [echo] Creating WebLogic Services(WLS) [echo] Percent Complete: 79 [echo] Percent Complete: 79 [echo] Percent Complete: 83 [echo] Percent Complete: 83 [echo] Percent Complete: 92 [echo] Percent Complete: 99 [echo] Percent Complete: 99 [echo] Creating Audit Services(IAU) [echo] Percent Complete: 100 [echo] Creating Oracle Platform Security Services(OPSS) [echo] Creating WebCenter Sites(WCSITES) [echo] Executing post create operations [echo] Repository Creation Utility: Create - Completion Summary [echo] Database details: [echo] ----------------------------- [echo] Host Name : oracle-db.wcsitesdb-ns.svc.cluster.local:1521/devpdb.k8s [echo] Port : 1521 [echo] Service Name : devpdb.k8s [echo] Connected As : sys [echo] Prefix for (prefixable) Schema Owners : WCS1 [echo] RCU Logfile : /u01/wcs-wls-docker-install/work/rcu/RCU2020-03-14_07-54_2112542638/logs/rcu.log [echo] Component schemas created: [echo] ----------------------------- [echo] Component Status Logfile [echo] Common Infrastructure Services Success /u01/wcs-wls-docker-install/work/rcu/RCU2020-03-14_07-54_2112542638/logs/stb.log [echo] Oracle Platform Security Services Success /u01/wcs-wls-docker-install/work/rcu/RCU2020-03-14_07-54_2112542638/logs/opss.log [echo] WebCenter Sites Success /u01/wcs-wls-docker-install/work/rcu/RCU2020-03-14_07-54_2112542638/logs/wcsites.log [echo] Audit Services Success /u01/wcs-wls-docker-install/work/rcu/RCU2020-03-14_07-54_2112542638/logs/iau.log [echo] Audit Services Append Success /u01/wcs-wls-docker-install/work/rcu/RCU2020-03-14_07-54_2112542638/logs/iau_append.log [echo] Audit Services Viewer Success /u01/wcs-wls-docker-install/work/rcu/RCU2020-03-14_07-54_2112542638/logs/iau_viewer.log [echo] WebLogic Services Success /u01/wcs-wls-docker-install/work/rcu/RCU2020-03-14_07-54_2112542638/logs/wls.log [echo] Repository Creation Utility - Create : Operation Completed [echo] [3/14/20 8:00 AM] Successfully created schemas [echo] [3/14/20 8:00 AM] 2nd phase: WebCenter Sites RCU configuration completed successfully. [echo] [3/14/20 8:00 AM] Oracle WebCenter Sites Installation complete. You can connect to the WebCenter Sites instance at http://10.244.0.252:7002/sites/ Sites RCU Phase completed successfull!!! Sites Installation completed in 366 seconds. --------------------------------------------------------- The domain will be created using the script /u01/weblogic/create-domain-script.sh wlst.sh -skipWLSModuleScanning /u01/weblogic/createSitesDomain.py -oh /u01/oracle -jh /u01/jdk -parent /u01/oracle/user_projects/domains/wcsitesinfra/.. -name wcsitesinfra -user weblogic -password Welcome1 -rcuDb oracle-db.wcsitesdb-ns.svc.cluster.local:1521/devpdb.k8s -rcuPrefix WCS1 -rcuSchemaPwd Welcome1 -adminListenPort 7001 -adminName adminserver -managedNameBase wcsites_server -managedServerPort 8001 -prodMode true -managedServerCount 3 -clusterName wcsites_cluster -exposeAdminT3Channel false -t3ChannelPublicAddress 10.123.152.96 -t3ChannelPort 30012 -domainType wcsites -machineName wcsites_machine Initializing WebLogic Scripting Tool (WLST) ... Welcome to WebLogic Server Administration Scripting Shell Type help() for help on available commands Creating Admin Server... Creating cluster... Creating Node Managers... managed server name is wcsites_server1 managed server name is wcsites_server2 managed server name is wcsites_server3 ['wcsites_server1', 'wcsites_server2', 'wcsites_server3'] Will create Base domain at /u01/oracle/user_projects/domains/wcsitesinfra Writing base domain... Base domain created at /u01/oracle/user_projects/domains/wcsitesinfra Extending domain at /u01/oracle/user_projects/domains/wcsitesinfra Database oracle-db.wcsitesdb-ns.svc.cluster.local:1521/devpdb.k8s ExposeAdminT3Channel false with 10.123.152.96:30012 Applying JRF templates... Extension Templates added Applying Oracle WebCenter Sites templates... Extension Templates added Configuring the Service Table DataSource... fmwDb...jdbc:oracle:thin:@oracle-db.wcsitesdb-ns.svc.cluster.local:1521/devpdb.k8s Set user...WCS140_OPSS Set user...WCS140_IAU_APPEND Set user...WCS140_IAU_VIEWER Set user...WCS140_STB Set user...WCS140_WCSITES Getting Database Defaults... Targeting Server Groups... Targeting Server Groups... Set CoherenceClusterSystemResource to defaultCoherenceCluster for server:wcsites_server1 Set CoherenceClusterSystemResource to defaultCoherenceCluster for server:wcsites_server2 Set CoherenceClusterSystemResource to defaultCoherenceCluster for server:wcsites_server3 Targeting Cluster ... Set CoherenceClusterSystemResource to defaultCoherenceCluster for cluster:wcsites_cluster Set WLS clusters as target of defaultCoherenceCluster:[wcsites_cluster] Preparing to update domain... Mar 14, 2020 8:01:52 AM oracle.security.jps.az.internal.runtime.policy.AbstractPolicyImpl initializeReadStore INFO: Property for read store in parallel: oracle.security.jps.az.runtime.readstore.threads = null Domain updated successfully Copying /u01/weblogic/server-config-update.sh to PV /u01/oracle/user_projects/domains/wcsitesinfra Copying /u01/weblogic/unicast.py to PV /u01/oracle/user_projects/domains/wcsitesinfra replacing tokens in /u01/oracle/user_projects/domains/wcsitesinfra/server-config-update.sh Successfully Completed   Initialize the WebCenter Sites Domain To start the domain, apply the above domain.yaml:\n```bash $ kubectl apply -f kubernetes/create-wcsites-domain/output/weblogic-domains/wcsitesinfra/domain.yaml domain.weblogic.oracle/wcsitesinfra created ```  Verify the WebCenter Sites Domain Verify that the domain and servers pods and services are created and in the READY state:\nSample run below:\n-bash-4.2$ kubectl get pods -n wcsites-ns -w NAME READY STATUS RESTARTS\tAGE wcsitesinfra-create-fmw-infra-sample-domain-job-6l7zh 0/1 Completed 0 15m wcsitesinfra-introspect-domain-job-7tvdt 1/1 Running 0 15s wcsitesinfra-introspect-domain-job-7tvdt 0/1 Completed 0 25s wcsitesinfra-introspect-domain-job-7tvdt 0/1 Terminating 0 5s wcsitesinfra-adminserver 0/1 Pending 0 0s wcsitesinfra-adminserver 0/1 Init:0/1 0 0s wcsitesinfra-adminserver 0/1 PodInitializing 0 12s wcsitesinfra-adminserver 0/1 Running 0 13s wcsitesinfra-adminserver 1/1 Running 0 108s wcsitesinfra-wcsites-server1 0/1 Pending 0 0s wcsitesinfra-wcsites-server1 0/1 Init:0/1 0 1s wcsitesinfra-wcsites-server1 0/1 PodInitializing 0 13s wcsitesinfra-wcsites-server1 0/1 Running 0 14s wcsitesinfra-wcsites-server1 1/1 Running 0 96s -bash-4.2$ kubectl get all -n wcsites-ns NAME READY STATUS RESTARTS AGE pod/wcsitesinfra-adminserver 1/1 Running 0 7m5s pod/wcsitesinfra-create-fmw-infra-sample-domain-job-6l7zh 0/1 Completed 0 22m pod/wcsitesinfra-wcsites-server1 1/1 Running 0 5m17s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/wcsitesinfra-adminserver ClusterIP None \u0026lt;none\u0026gt; 7001/TCP 7m5s service/wcsitesinfra-cluster-wcsites-cluster ClusterIP 10.109.210.3 \u0026lt;none\u0026gt; 8001/TCP 5m17s service/wcsitesinfra-wcsites-server1 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP 5m17s NAME COMPLETIONS DURATION AGE job.batch/wcsitesinfra-create-fmw-infra-sample-domain-job 1/1 7m40s 22m To see the Admin and Managed Servers logs, you can check the pod logs:\n$ kubectl logs -f wcsitesinfra-adminserver -n wcsites-ns $ kubectl exec -it wcsitesinfra-adminserver -n wcsites-ns -- /bin/bash $ kubectl logs -f wcsitesinfra-wcsites-server1 -n wcsites-ns $ kubectl exec -it wcsitesinfra-wcsites-server1 -n wcsites-ns -- /bin/bash Verify the Pods Use the following command to see the pods running the servers:\n$ kubectl get pods -n NAMESPACE Here is an example of the output of this command:\n-bash-4.2$ kubectl get pods -n wcsites-ns NAME READY STATUS RESTARTS AGE wcsitesinfra-adminserver 1/1 Running 0 56m wcsitesinfra-create-fmw-infra-sample-domain-job-rq4xv 0/1 Completed 0 65m wcsitesinfra-wcsites-server1 1/1 Running 0 41m Verify the Services Use the following command to see the services for the domain:\n$ kubectl get services -n NAMESPACE Here is an example of the output of this command:\n-bash-4.2$ kubectl get services -n wcsites-ns NAME READY STATUS RESTARTS AGE wcsitesinfra-adminserver 1/1 Running 0 7m38s wcsitesinfra-create-fmw-infra-sample-domain-job-6l7zh 0/1 Completed 0 23m wcsitesinfra-wcsites-server1 1/1 Running 0 5m50s Expose WebCenter Sites Services Below are the default values for exposing services required for all the WebCenter Sites Managed Servers. Reset them if any values are modified.\nDetails on kubernetes/create-wcsites-domain/utils/wcs-services.yaml:\n name: wcsitesinfra-wcsites-server1-np namespace: wcsites-ns weblogic.domainUID: wcsitesinfra weblogic.serverName: wcsites_server1  Execute the below command for exposing the services: (If domain is configured for more than 3 Managed Servers then add the service yaml for additional servers.)\n$ kubectl apply -f kubernetes/create-wcsites-domain/utils/wcs-services.yaml service/wcsitesinfra-wcsites-server1-np created service/wcsitesinfra-wcsites-server1-svc created service/wcsitesinfra-wcsites-server2-svc created service/wcsitesinfra-wcsites-server3-svc created To verify the services created, here is an example of the output of this command:\n-bash-4.2$ kubectl get services -n wcsites-ns NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE wcsitesinfra-adminserver ClusterIP None \u0026lt;none\u0026gt; 7001/TCP 11m wcsitesinfra-cluster-wcsites-cluster ClusterIP 10.109.210.3 \u0026lt;none\u0026gt; 8001/TCP 9m14s wcsitesinfra-wcsites-server1 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP 9m14s wcsitesinfra-wcsites-server1-np NodePort 10.105.167.205 \u0026lt;none\u0026gt; 8001:30155/TCP 2m47s wcsitesinfra-wcsites-server1-svc ClusterIP None \u0026lt;none\u0026gt; 50000/TCP,50001/TCP,50002/TCP,50003/TCP,50004/TCP,50005/TCP,50006/TCP,50007/TCP,50008/TCP,50009/TCP 2m47s wcsitesinfra-wcsites-server2-svc ClusterIP None \u0026lt;none\u0026gt; 50000/TCP,50001/TCP,50002/TCP,50003/TCP,50004/TCP,50005/TCP,50006/TCP,50007/TCP,50008/TCP,50009/TCP 2m47s wcsitesinfra-wcsites-server3-svc ClusterIP None \u0026lt;none\u0026gt; 50000/TCP,50001/TCP,50002/TCP,50003/TCP,50004/TCP,50005/TCP,50006/TCP,50007/TCP,50008/TCP,50009/TCP 2m47s Load Balance With an Ingress Controller or A Web Server You can choose a load balancer provider for your WebLogic domains running in a Kubernetes cluster. Please refer to the WebLogic Kubernetes Operator Load Balancer Samples for information about the current capabilities and setup instructions for each of the supported load balancers.\nFor information on how to set up Loadbalancer for setting up WebCenter Sites domain on K8S:\nFor Traefik, see Setting Up Loadbalancer Traefik for the WebCenter Sites Domain on K8S\nFor Nginx, see Setting Up Loadbalancer Nginx for the WebCenter Sites Domain on K8S\nConfigure WebCenter Sites   Configure WebCenter Sites by hitting url http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-PORT}/sites/sitesconfigsetup\nWhen installing, select sample sites to be installed and enter the required passwords. Do not change the sites-config location. If you change the location, installation will fail.\n  After the configuration is complete, edit the domain, and restart the Managed Server.\n  To stop Managed Servers:\n$ kubectl patch domain wcsitesinfra -n wcsites-ns --type=\u0026#39;json\u0026#39; -p=\u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/clusters/0/replicas\u0026#34;, \u0026#34;value\u0026#34;: 0 }]\u0026#39; To start all configured Managed Servers:\n$ kubectl patch domain wcsitesinfra -n wcsites-ns --type=\u0026#39;json\u0026#39; -p=\u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/clusters/0/replicas\u0026#34;, \u0026#34;value\u0026#34;: 3 }]\u0026#39; Wait till the Managed Server pod is killed and then restart it. Monitor with below command: -bash-4.2$ kubectl get pods -n wcsites-ns -w NAME READY STATUS RESTARTS AGE wcsitesinfra-adminserver 1/1 Running 0 111m wcsitesinfra-create-fmw-infra-sample-domain-job-6l7zh 0/1 Completed 0 126m wcsitesinfra-wcsites-server1 1/1 Running 0 3m7s wcsitesinfra-wcsites-server2 1/1 Running 0 3m7s wcsitesinfra-wcsites-server3 1/1 Running 0 3m7s   Settings in WebCenter Sites Property Management Incase of Traefik Load Balancer: Use Property Management Tool and update cookieserver.validnames property with value JSESSIONID,sticky.\nIncase of Nginx Load Balancer: Use Property Management Tool and update cookieserver.validnames property with value JSESSIONID,stickyid.\nFor Publishing Setting in WebCenter Sites While configuring publishing destination use NodePort port of target cluster which can be found by executing below command:\n(In this example for publishihng the port 30155 has to be used.)\n-bash-4.2$ kubectl get service/wcsitesinfra-wcsites-server1-np -n wcsites-ns NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE wcsitesinfra-wcsites-server1-np NodePort 10.105.167.205 \u0026lt;none\u0026gt; 8001:30155/TCP 32h Customization A customer specific customizations (extend.sites.webapp-lib.war) has to be placed in sites-home directory inside your domain mount path.\n"
},
{
	"uri": "/fmw-kubernetes/soa-domains/adminguide/",
	"title": "Administration Guide",
	"tags": [],
	"description": "Describes how to use some of the common utility tools and configurations to administer Oracle SOA Suite domains.",
	"content": "Administer Oracle SOA Suite domains in Kubernetes.\n Set up a load balancer  Configure different load balancers for Oracle SOA Suite domains.\n Enable additional URL access  Extend an existing ingress to enable additional application URL access for Oracle SOA Suite domains.\n Configure SSL certificates  Create and configure custom SSL certificates for Oracle SOA Suite domains.\n Monitor a domain and publish logs  Monitor an Oracle SOA Suite domain and publish the WebLogic Server logs to Elasticsearch.\n Expose the T3/T3S protocol  Create a T3/T3S channel and the corresponding Kubernetes service to expose the T3/T3S protocol for the Administration Server and Managed Servers in an Oracle SOA Suite domain.\n Deploy composite applications  Deploy composite applications for Oracle SOA Suite and Oracle Service Bus domains.\n Persist adapter customizations  Persist the customizations done for Oracle SOA Suite adapters.\n Perform WLST operations  Perform WLST administration operations using a helper pod running in the same Kubernetes cluster as the Oracle SOA Suite domain.\n "
},
{
	"uri": "/fmw-kubernetes/wcsites-domains/adminguide/",
	"title": "Administration Guide",
	"tags": [],
	"description": "Describes how to use some of the common utility tools and configurations to administer Oracle WebCenter Sites domains.",
	"content": "Administer Oracle WebCenter Sites domains in Kubernetes.\n Set up a load balancer  Configure different load balancers for Oracle WebCenter Sites domains\n Monitor a domain and publish logs  Monitor an Oracle WebCenter Sites domain and publish the WebLogic Server logs to Elasticsearch.\n Elasticsearch integration for logs  Monitor an Oracle WebCenter Sites domain and publish the WebLogic Server logs to Elasticsearch.\n Publish logs to Elasticsearch  Monitor an Oracle WebCenter Sites domain and publish the WebLogic Server logs to Elasticsearch.\n "
},
{
	"uri": "/fmw-kubernetes/soa-domains/adminguide/configure-load-balancer/apache/",
	"title": "Apache web tier",
	"tags": [],
	"description": "Configure the Apache web tier load balancer for Oracle SOA Suite domains.",
	"content": "This section provides information about how to install and configure the Apache web tier to load balance Oracle SOA Suite domain clusters. You can configure Apache web tier for non-SSL and SSL termination access of the application URL.\nFollow these steps to set up the Apache web tier as a load balancer for an Oracle SOA Suite domain in a Kubernetes cluster:\n Build the Apache web tier image Create the Apache plugin configuration file Prepare the certificate and private key Install the Apache web tier Helm chart Verify domain application URL access Uninstall Apache web tier  Build the Apache web tier image Refer to the sample, to build the Apache web tier Docker image.\nCreate the Apache plugin configuration file   The configuration file named custom_mod_wl_apache.conf should have all the URL routing rules for the Oracle SOA Suite applications deployed in the domain that needs to be accessible externally. Update this file with values based on your environment. The file content is similar to below.\n  Click here to see the sample content of the configuration file custom_mod_wl_apache.conf for soa domain   # Copyright (c) 2020 Oracle and/or its affiliates. # # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # \u0026lt;IfModule mod_weblogic.c\u0026gt; WebLogicHost \u0026lt;WEBLOGIC_HOST\u0026gt; WebLogicPort 7001 \u0026lt;/IfModule\u0026gt; # Directive for weblogic admin Console deployed on WebLogic Admin Server \u0026lt;Location /console\u0026gt; SetHandler weblogic-handler WebLogicHost soainfra-adminserver WebLogicPort 7001 \u0026lt;/Location\u0026gt; \u0026lt;Location /em\u0026gt; SetHandler weblogic-handler WebLogicHost soainfra-adminserver WebLogicPort 7001 \u0026lt;/Location\u0026gt; \u0026lt;Location /servicebus\u0026gt; SetHandler weblogic-handler WebLogicHost soainfra-adminserver WebLogicPort 7001 \u0026lt;/Location\u0026gt; \u0026lt;Location /lwpfconsole\u0026gt; SetHandler weblogic-handler WebLogicHost soainfra-adminserver WebLogicPort 7001 \u0026lt;/Location\u0026gt; \u0026lt;Location /xbusrouting\u0026gt; SetHandler weblogic-handler WebLogicHost soainfra-adminserver WebLogicPort 7001 \u0026lt;/Location\u0026gt; \u0026lt;Location /xbustransform\u0026gt; SetHandler weblogic-handler WebLogicHost soainfra-adminserver WebLogicPort 7001 \u0026lt;/Location\u0026gt; \u0026lt;Location /weblogic/ready\u0026gt; SetHandler weblogic-handler WebLogicHost soainfra-adminserver WebLogicPort 7001 \u0026lt;/Location\u0026gt; # Directive for all applications deployed on weblogic cluster with a prepath defined by LOCATION variable. # For example, if the LOCATION is set to \u0026#39;/weblogic\u0026#39;, all applications deployed on the cluster can be accessed via # http://myhost:myport/weblogic/application_end_url # where \u0026#39;myhost\u0026#39; is the IP of the machine that runs the Apache web tier, and # \u0026#39;myport\u0026#39; is the port that the Apache web tier is publicly exposed to. # Note that LOCATION cannot be set to \u0026#39;/\u0026#39; unless this is the only Location module configured. \u0026lt;Location /soa-infra\u0026gt; WLSRequest On WebLogicCluster soainfra-cluster-soa-cluster:8001 PathTrim /weblogic1 \u0026lt;/Location\u0026gt; \u0026lt;Location /soa/composer\u0026gt; WLSRequest On WebLogicCluster soainfra-cluster-soa-cluster:8001 PathTrim /weblogic1 \u0026lt;/Location\u0026gt; \u0026lt;Location /integration/worklistapp\u0026gt; WLSRequest On WebLogicCluster soainfra-cluster-soa-cluster:8001 PathTrim /weblogic1 \u0026lt;/Location\u0026gt; \u0026lt;Location /ess\u0026gt; WLSRequest On WebLogicCluster soainfra-cluster-soa-cluster:8001 PathTrim /weblogic1 \u0026lt;/Location\u0026gt; \u0026lt;Location /EssHealthCheck\u0026gt; WLSRequest On WebLogicCluster soainfra-cluster-soa-cluster:8001 PathTrim /weblogic1 \u0026lt;/Location\u0026gt; # Directive for all application deployed on weblogic cluster with a prepath defined by LOCATION2 variable # For example, if the LOCATION2 is set to \u0026#39;/weblogic2\u0026#39;, all applications deployed on the cluster can be accessed via # http://myhost:myport/weblogic2/application_end_url # where \u0026#39;myhost\u0026#39; is the IP of the machine that runs the Apache web tier, and # \u0026#39;myport\u0026#39; is the port that the Apache webt ier is publicly exposed to. #\u0026lt;Location /weblogic2\u0026gt; #WLSRequest On #WebLogicCluster domain2-cluster-cluster-1:8021 #PathTrim /weblogic2 #\u0026lt;/Location\u0026gt;      Create a PV and PVC (pv-claim-name) that can be used to store the custom_mod_wl_apache.conf. Refer to the Sample for creating a PV or PVC.\n  Prepare the certificate and private key   (For the SSL termination configuration only) Run the following commands to generate your own certificate and private key using openssl.\n$ cd ${WORKDIR} $ cd charts/apache-samples/custom-sample $ export VIRTUAL_HOST_NAME=WEBLOGIC_HOST $ export SSL_CERT_FILE=WEBLOGIC_HOST.crt $ export SSL_CERT_KEY_FILE=WEBLOGIC_HOST.key $ sh certgen.sh  NOTE: Replace WEBLOGIC_HOST with the host name on which Apache web tier is to be installed.\n   Click here to see the output of the certifcate generation   $ls certgen.sh custom_mod_wl_apache.conf custom_mod_wl_apache.conf_orig input.yaml README.md $ sh certgen.sh Generating certs for WEBLOGIC_HOST Generating a 2048 bit RSA private key ........................+++ .......................................................................+++ unable to write \u0026#39;random state\u0026#39; writing new private key to \u0026#39;apache-sample.key\u0026#39; ----- $ ls certgen.sh custom_mod_wl_apache.conf_orig WEBLOGIC_HOST.info config.txt input.yaml WEBLOGIC_HOST.key custom_mod_wl_apache.conf WEBLOGIC_HOST.crt README.md      Prepare input values for the Apache web tier Helm chart.\nRun the following commands to prepare the input value file for the Apache web tier Helm chart.\n$ base64 -i ${SSL_CERT_FILE} | tr -d \u0026#39;\\n\u0026#39; $ base64 -i ${SSL_CERT_KEY_FILE} | tr -d \u0026#39;\\n\u0026#39; $ touch input.yaml Update the input parameters file, charts/apache-samples/custom-sample/input.yaml.\n  Click here to see the snapshot of the sample input.yaml file   $ cat apache-samples/custom-sample/input.yaml # Use this to provide your own Apache web tier configuration as needed; simply define this # Persistence Volume which contains your own custom_mod_wl_apache.conf file. persistentVolumeClaimName: \u0026lt;pv-claim-name\u0026gt; # The VirtualHostName of the Apache HTTP server. It is used to enable custom SSL configuration. virtualHostName: \u0026lt;WEBLOGIC_HOST\u0026gt; # The customer-supplied certificate to use for Apache web tier SSL configuration. # The value must be a string containing a base64 encoded certificate. Run following command to get it. # base64 -i ${SSL_CERT_FILE} | tr -d \u0026#39;\\n\u0026#39; customCert: \u0026lt;cert_data\u0026gt; # The customer-supplied private key to use for Apache web tier SSL configuration. # The value must be a string containing a base64 encoded key. Run following command to get it. # base64 -i ${SSL_KEY_FILE} | tr -d \u0026#39;\\n\u0026#39; customKey: \u0026lt;key_data\u0026gt;      Install the Apache web tier Helm chart   Install the Apache web tier Helm chart to the domain namespace (for example soans) with the specified input parameters:\n$ cd ${WORKDIR}/charts $ helm install apache-webtier --values apache-samples/custom-sample/input.yaml --namespace soans apache-webtier --set image=oracle/apache:12.2.1.3   Check the status of the Apache web tier:\n$ kubectl get all -n soans | grep apache Sample output of the status of the Apache web tier:\npod/apache-webtier-apache-webtier-65f69dc6bc-zg5pj 1/1 Running 0 22h service/apache-webtier-apache-webtier NodePort 10.108.29.98 \u0026lt;none\u0026gt; 80:30305/TCP,4433:30443/TCP 22h deployment.apps/apache-webtier-apache-webtier 1/1 1 1 22h replicaset.apps/apache-webtier-apache-webtier-65f69dc6bc 1 1 1 22h   Verify domain application URL access After the Apache web tier load balancer is running, verify that the domain applications are accessible through the load balancer port 30305/30443. The application URLs for domain of type soa are:\n Note: Port 30305 is the LOADBALANCER-Non-SSLPORT and port 30443 is LOADBALANCER-SSLPORT.\n NONSSL configuration http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/weblogic/ready http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/console http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/em http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/soa-infra http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/soa/composer http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/integration/worklistapp SSL configuration https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/weblogic/ready https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/console https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/em https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/soa-infra https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/soa/composer https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/integration/worklistapp Uninstall Apache web tier $ helm delete apache-webtier -n soans "
},
{
	"uri": "/fmw-kubernetes/soa-domains/adminguide/configuring-custom-ssl-certificates/",
	"title": "Configure SSL certificates",
	"tags": [],
	"description": "Create and configure custom SSL certificates for Oracle SOA Suite domains.",
	"content": "Secure Socket Layer (SSL) provides a secured communication for data sent over unsecured networks. In an SSL termination scenario, you can configure SSL between the client browser and the load balancer in your Oracle SOA Suite instance to ensure that applications are accessed securely. In an SSL end-to-end scenario, an Oracle SOA Suite domain is configured to use a self-signed SSL certificate that was generated during domain creation. Clients will typically receive a message indicating that the signing CA for the certificate is unknown and not trusted.\nThis section provides details on how to create and configure custom (CA-issued) SSL certificates for Oracle SOA Suite domains in both SSL end-to-end and SSL termination scenarios.\n Create custom SSL certificates in an SSL end-to-end scenario Create custom SSL certificates in an SSL termination at a load balancer  Create custom SSL certificates in an SSL end-to-end scenario These steps describe how to replace the identity and trust keystore of an Oracle SOA Suite domain with a custom identity and custom trust keystore and register with digital certificates procured from any third party authority.\nIn this documentation, the registered domain is mydomain.com and the CA signed certificates are taken from mydomain.\nCreate a custom identity and custom trust keystore and generate a certificate signing request (CSR) To create a custom identity and custom trust keystore and generate a CSR:\n  Log in to the Enterprise Manager (EM) Console and access the Keystores page by opening WebLogic Domain \u0026gt; Security \u0026gt; Keystore.\n  Under the system stripe, click Create Keystore to create a new keystore.\n  Provide the following details for custom identity:\nKeystore Name: custIdentity Protection: Select the Password option. Keystore Password: Enter the password. Confirm Password: Confirm the password.\n  Click Create Keystore to create another new keystore.\n  Provide the following details for custom trust:\n Keystore Name: custTrust Protection: Select the Password option. Keystore Password: Enter the password. Confirm Password: Confirm the password.    Click Manage on the custIdentity keystore name and provide the password that you specified previously.\n  Click Generate Keypair to create a new key pair, and provide the following details for custIdentity with alias as custIdentity and password:\n Alias Name: custIdentity Common Name: Common name, for example, soak8s.mydomain.com (Registered domain name) Organizational Unit: Name of the organizational unit Organization: Organization name Enter City, State, and Country names Key Type: RSA Key Size: 2048 Password: Enter the password    Click OK to generate the keypair.\n  Select the newly created keypair and click Generate CSR.\n  Export the created CSR, share it with Certificate Authority, such as digicert CA, and get root, intermediate, and signed certificates. The certificate is generated for the domain name you used in the Common Name field.\n  It is not mandatory to create identity and trust keystore under the system stripe that comes with default provisioning. You can create a new custom stripe and create identity and trust keystores under it.\nShare the CSR with CA to get CA-signed certificates   Select the new keypair under the custIdentity and click Generate CSR.\n  Export the created CSR and share it with the Certificate Authority and get root, intermediate, and signed certificates. The certificate is generated for the domain name you used in the Common Name field.\n  Download the certificates shared in the zip file from the CA. The zip file contains one of the following:\n the three certificates individually - root, intermediate, and signed certificates root and intermediate certificates in one chain and signed certificate separately    Double-click the certificate chain for root and intermediate certificates. You can see the full chain when you click on the certification path.\n  Extract the root and intermediate certificates individually by going to the certification path, select the certificate to be extracted (root or intermediate) and click View Certificate.\n  On the View Certificates pop-up, select the Details tab and click Copy to File.\n  In the Certificate Export wizard, click Next, then select Base 64 encoded X.509 (CER), and then click Next. Export the certificate.\n  Name the exported certificate as root and intermediate certificates respectively.\n  Import CA certificates Certificate Authority (CA) certificates must be imported in the following order: first the signed server certificate, then the intermediate certificate, and then the root certificate.\nTo import CA certificates:\n  Use WLST commands to import the certificate chain in the identity keystore (custIdentity):\na. Combine the three certificates into a single text file called chain.pem in the following order: signed server certificate, followed by intermediate certificate, followed by root certificate:\n-----BEGIN CERTIFICATE----- \u0026lt;signed server certificate\u0026gt; -----END CERTIFICATE----- -----BEGIN CERTIFICATE----- \u0026lt;intermediate certificate\u0026gt; -----END CERTIFICATE----- -----BEGIN CERTIFICATE----- \u0026lt;root certificate\u0026gt; -----END CERTIFICATE----- b. Place the chain.pem in /tmp from where you will be executing the kubectl commands (for example, on the master node).\nc. Enter the following command to change the file ownership to 1000:1000 user/group:\n$ sudo chown 1000:1000 /tmp/chain.pem d. Copy /tmp/chain.pem into the Administration Server pod (for example, soainfra-adminserver):\n$ kubectl cp /tmp/chain.pem soans/soainfra-adminserver:/tmp/chain.pem e. Exec into the Administration Server pod to perform all operations:\n$ kubectl exec -it soainfra-adminserver -n soans -- bash f. Start WLST and access the Oracle Platform Security Services (OPSS) key store service:\n$ cd /u01/oracle/oracle_common/common/bin/ $ ./wlst.sh : : wls:/offline\u0026gt; connect(\u0026quot;weblogic\u0026quot;,\u0026quot;Welcome1\u0026quot;,\u0026quot;t3://soainfra-adminserver:7001\u0026quot;) : : wls:/soainfra/serverConfig/\u0026gt; svc = getOpssService(name='KeyStoreService') g. Use the WLST importKeyStoreCertificate command to import chain.pem:\nsvc.importKeyStoreCertificate(appStripe='stripe', name='keystore', password='password', alias='alias', keypassword='keypassword', type='entrytype',filepath='absolute_file_path') For example:\nwls:/soainfra/serverConfig/\u0026gt; svc.importKeyStoreCertificate(appStripe='system', name='custIdentity', password=welcome1, alias='custIdentity', keypassword='welcome1', type='CertificateChain', filepath='/tmp/chain.pem') e. Exit WLST:\nexit()   Use Oracle Enterprise Manager to import the certificate chain into the trust keystore (custTrust):\na. Log in to the Enterprise Manager Console and access the Keystores page by opening WebLogic domain \u0026gt; Security \u0026gt; Keystore.\nb. Select the trust keystore (custTrust) and click Manage.\nc. Click Import Certificate and import the certificates in this order:\n  the signed server certificate as a trusted certificate (alias mySignedCert)\n  the intermediate certificate from CA as a trusted certificate (alias myInterCA)\n  the root certificate from CA as a trusted certificate (alias myRootCA)\n    Synchronize the local keystore with the security store Synchronize keystores to synchronize information between the domain home and the Oracle Platform Security Services (OPSS) store in the database.\nTo synchronize keystores:\n Exec into the Administration server pod (for example, soainfra-adminserver): $ kubectl exec -it soainfra-adminserver -n soans -- bash  Start WLST and access the Oracle Platform Security Services (OPSS) keystore service: $ cd /u01/oracle/oracle_common/common/bin/ $ ./wlst.sh : : wls:/offline\u0026gt; connect(\u0026quot;weblogic\u0026quot;,\u0026quot;Welcome1\u0026quot;,\u0026quot;t3://soainfra-adminserver:7001\u0026quot;) : : wls:/soainfra/serverConfig/\u0026gt; svc = getOpssService(name='KeyStoreService')  Enter the following commands to synchronize the custom identity and custom trust keystores:  Note: This step is necessary only if you are using the system stripe. You do not need to synchronize the keystores if you are using a custom stripe.\n wls:/soainfra/serverConfig/\u0026gt; svc.listKeyStoreAliases(appStripe=\u0026quot;system\u0026quot;, name=\u0026quot;custIdentity\u0026quot;, password=\u0026quot; ****\u0026quot;, type=\u0026quot;*\u0026quot;) wls:/soainfra/serverConfig/\u0026gt; syncKeyStores(appStripe='system',keystoreFormat='KSS') wls:/soainfra/serverConfig/\u0026gt; svc.listKeyStoreAliases (appStripe=\u0026quot;system\u0026quot;, name=\u0026quot;myKSSTrust\u0026quot;, password=\u0026quot;****\u0026quot;, type=\u0026quot;*\u0026quot;) wls:/soainfra/serverConfig/\u0026gt; syncKeyStores(appStripe='system',keystoreFormat='KSS')   Update the WebLogic keystores with custom identity and trust To update the WebLogic keystores with custom identity and custom trust:\n  In the WebLogic Server Administration Console, open Servers \u0026gt; AdminServer \u0026gt; Configurations \u0026gt; Keystores tab.\n  Change the Keystores to Custom Identity and Custom Trust and Save.\n  Provide the values for Custom Identity:\n Custom Identity Keystore: kss://system/custidentity Custom Identity KeyStore Type: KSS Custom Identity PassPhrase: enter password given while creating the custIdentity keystore. Confirm Custom Identity PassPhrase: reenter the password.    Provide the values for Custom Trust:\n Custom Trust Keystore: kss://system/custTrust Custom Trust KeyStore Type: KSS Custom Trust PassPhrase: enter password given while creating the custTrust keystore. Confirm Custom Trust PassPhrase: reenter the password.    Click Save and then Activate changes.\n  Open the SSL tab and provide the following details:\n Private Key Alias: custIdentity (this is the alias given while creating the key pair in the custIdentity keystore.) Private Key PassPhrase: enter password given while creating the key pair under the custIdentity keystore. Confirm Private Key PassPhrase: reenter the password.    In the Advanced section, change Hostname Verification to None. Click Save and Activate changes.\n  Repeat steps 1 to 7 for all Managed Servers.\n  Restart the domain.\n  Once the servers are up and running, you can check if the SSL URLs show the updated certificates.\n  For more details, refer to:\n Administering Oracle SOA Cloud Service Administering Oracle Fusion Middleware  Create custom SSL certificates in an SSL termination at a load balancer This section provides references to configure a custom SSL certificate at a load balancer.\nThere are multiple CA vendors in the marketplace today, each offering different levels of service at varying price points. Research and choose a CA vendor that meets your service-level and budget requirements.\nFor a CA vendor to issue you a CA-issued SSL certificate, you must provide the following information:\n Your custom domain name. Public information associated with the domain confirming you as the owner. Email address associated with the custom domain for verification.  Create a Certificate Signing Request (CSR) for your load balancer and submit the CSR to the CA vendor. After receiving the CA-issued certificate, refer to Administering Oracle SOA Cloud Service to import the CA-issued SSL certificate to the load balancer. If you are using openssl to create the certificates, you can refer to Manually Generate a Certificate Signing Request (CSR) Using OpenSSL to submit the CSR to the CA vendor.\n"
},
{
	"uri": "/fmw-kubernetes/wcsites-domains/adminguide/monitoring-wcsites-domains/",
	"title": "Monitor a domain and publish logs",
	"tags": [],
	"description": "Monitor an Oracle WebCenter Sites domain and publish the WebLogic Server logs to Elasticsearch.",
	"content": "After the Oracle WebCenter Sites domain is set up, you can:\n Monitor the Oracle WebCenter Sites instance using Prometheus and Grafana Publish WebLogic Server logs into Elasticsearch  Monitor the Oracle WebCenter Sites instance using Prometheus and Grafana Using the WebLogic Monitoring Exporter you can scrape runtime information from a running Oracle WebCenter Sites instance and monitor them using Prometheus and Grafana.\nPrerequisites This document assumes that the Prometheus Operator is deployed on the Kubernetes cluster. If it is not already deployed, follow the steps below for deploying the Prometheus Operator.\nClone the kube-prometheus project $ git clone https://github.com/coreos/kube-prometheus.git Label the nodes Kube-Prometheus requires all the exporter nodes to be labelled with kubernetes.io/os=linux. If a node is not labelled, then you must label it using the following command:\n$ kubectl label nodes --all kubernetes.io/os=linux Create Prometheus and Grafana resources Change to the kube-prometheus directory and execute the following commands to create the namespace and CRDs:\nNOTE: Wait for a minute for each command to process.\n$ cd kube-prometheus $ kubectl create -f manifests/setup $ until kubectl get servicemonitors --all-namespaces ; do date; sleep 1; echo \u0026#34;\u0026#34;; done $ kubectl create -f manifests/ Provide external access To provide external access for Grafana, Prometheus, and Alertmanager, execute the commands below:\n$ kubectl patch svc grafana -n monitoring --type=json -p \u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/type\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;NodePort\u0026#34; },{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/ports/0/nodePort\u0026#34;, \u0026#34;value\u0026#34;: 32100 }]\u0026#39; $ kubectl patch svc prometheus-k8s -n monitoring --type=json -p \u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/type\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;NodePort\u0026#34; },{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/ports/0/nodePort\u0026#34;, \u0026#34;value\u0026#34;: 32101 }]\u0026#39; $ kubectl patch svc alertmanager-main -n monitoring --type=json -p \u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/type\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;NodePort\u0026#34; },{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/ports/0/nodePort\u0026#34;, \u0026#34;value\u0026#34;: 32102 }]\u0026#39; NOTE:\n 32100 is the external port for Grafana 32101 is the external port for Prometheus 32102 is the external port for Alertmanager   Set Up the WebLogic Monitoring Exporter Set up the WebLogic Monitoring Exporter that will collect WebLogic Server metrics and monitor your WebCenter Sites domain.\nGenerate the WebLogic Monitoring Exporter Deployment Package Two packages are required as the listening ports are different for the Administration Server and Managed Servers. One binary required for the Admin Server (wls-exporter-as.war) and one for Managed Cluster (wls-exporter-ms.war). Set the required proxies and then run the script getX.X.X.sh to generate two binaries:\n$ cd kubernetes/create-wcsites-domain/utils/weblogic-monitoring-exporter $ sh get1.1.0.sh Output:\n % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 607 0 607 0 0 357 0 --:--:-- 0:00:01 --:--:-- 357 100 2016k 100 2016k 0 0 398k 0 0:00:05 0:00:05 --:--:-- 797k -------------------wls-exporter-ms start------------------- created /tmp/ci-GNysQzP1kv Copying completed /tmp/ci-GNysQzP1kv /kubernetes/create-wcsites-domain/utils/weblogic-monitoring-exporter in temp dir adding: WEB-INF/weblogic.xml (deflated 66%) adding: config.yml (deflated 63%) wls-exporter-ms.war is ready -------------------wls-exporter-ms end------------------- -------------------wls-exporter-as start------------------- Copying completed in temp dir adding: WEB-INF/weblogic.xml (deflated 66%) adding: config.yml (deflated 52%) wls-exporter-as.war is ready -------------------wls-exporter-as end------------------- zip completed kubernetes/3.3.0/create-wcsites-domain/utils/weblogic-monitoring-exporter Copy the WAR Files to the WebLogic Domain Home Copy the wls-exporter-as.war and wls-exporter-ms.war files to the domain home directory in the Administration Server pod.\n$ kubectl cp wls-exporter-as.war wcsites-ns/wcsitesinfra-adminserver:/u01/oracle/user_projects/domains/wcsitesinfra/ $ kubectl cp wls-exporter-ms.war wcsites-ns/wcsitesinfra-adminserver:/u01/oracle/user_projects/domains/wcsitesinfra/ Deploy the WebLogic Monitoring Exporter Follow these steps to deploy the package in the WebLogic Server instances:\n  In the Administration Server and Managed Servers, deploy the WebLogic Monitoring Exporter (wls-exporter-ms.war) separately using the Oracle Enterprise Manager.\n  Select the servers to which the Exporter WAR should be deployed:\n  Set the application name. The application name must be different if it is deployed separately in the Administration Server and Managed Servers. Make sure the context-root for both the deployments is wls-exporter:\n  Click Install and start application.\n  Then deploy the WebLogic Monitoring Exporter application (wls-exporter-ms.war).\n  Activate the changes to start the application. If the application is started and the port is exposed, then you can access the WebLogic Monitoring Exporter console using this URL: http://\u0026lt;server:port\u0026gt;/wls-exporter.\n  Repeat same steps for wls-exporter-as.war.\n  Configure Prometheus Operator Prometheus enables you to collect metrics from the WebLogic Monitoring Exporter. The Prometheus Operator identifies the targets using service discovery. To get the WebLogic Monitoring Exporter end point discovered as a target, you must create a service monitor pointing to the service.\nSee the following sample service monitor deployment YAML configuration file located at\nkubernetes/create-wcsites-domain/utils/weblogic-monitoring-exporter/wls-exporter.yaml.\nServiceMonitor for wls-exporter:\napiVersion: v1 kind: Secret metadata: name: basic-auth namespace: monitoring data: password: V2VsY29tZTE= # Welcome1 i.e.'WebLogic password' user: d2VibG9naWM= # weblogic i.e. 'WebLogic username' type: Opaque --- apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: wls-exporter-wcsitesinfra namespace: monitoring labels: k8s-app: wls-exporter spec: namespaceSelector: matchNames: - wcsites-ns selector: matchLabels: weblogic.domainName: wcsitesinfra endpoints: - basicAuth: password: name: basic-auth key: password username: name: basic-auth key: user port: default relabelings: - action: labelmap regex: __meta_kubernetes_service_label_(.+) interval: 10s honorLabels: true path: /wls-exporter/metrics The exporting of metrics from wls-exporter requires basicAuth so a Kubernetes Secret is created with the user name and password that are base64 encoded. This Secret will be used in the ServiceMonitor deployment.\nWhen generating the base64 encoded strings for the user name and password, observe if a new line character is appended in the encoded string. This line character causes an authentication failure. To avoid a new line string, use the following example:\n$ echo -n \u0026quot;Welcome1\u0026quot; | base64 V2VsY29tZTE= In the deployment YAML configuration for wls-exporter shown above, weblogic.domainName: wcsitesinfra is used as a label under spec.selector.matchLabels, so all the services will be selected for the service monitor. If you don\u0026rsquo;t use this label, you should create separate service monitors for each server\u0026ndash;if the server name is used as matching labels in spec.selector.matchLabels. Doing so will require you to relabel the configuration because Prometheus, by default, ignores the labels provided in the wls-exporter.\nBy default, Prometheus does not store all the labels provided by the target. In the service monitor deployment YAML configuration, you must mention the relabeling configuration (spec.endpoints.relabelings) so that certain labels provided by weblogic-monitoring-exporter (required for the Grafana dashboard) are stored in Prometheus. Do not delete the following section from the configuration YAML file:\nrelabelings: - action: labelmap regex: __meta_kubernetes_service_label_(.+) Add RoleBinding and Role for the WebLogic Domain Namespace The RoleBinding is required for Prometheus to access the endpoints provided by the WebLogic Monitoring Exporter. You need to add RoleBinding for the namespace under which the WebLogic Servers pods are running in the Kubernetes cluster. Edit the kube-prometheus/manifests/prometheus-roleBindingSpecificNamespaces.yaml file in the Prometheus Operator deployment manifests and add the RoleBinding for the namespace (wcsites-ns) similar to the following example:\n- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: prometheus-k8s namespace: wcsites-ns roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: prometheus-k8s subjects: - kind: ServiceAccount name: prometheus-k8s namespace: monitoring Similarly, add the Role for the namespace under which the WebLogic Servers pods are running in the Kubernetes cluster. Edit kube-prometheus/manifests/prometheus-roleSpecificNamespaces.yaml in the Prometheus Operator deployment manifests and add the Role for the namespace (wcsites-ns) similar to the following example:\n- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: prometheus-k8s namespace: wcsites-ns rules: - apiGroups: - \u0026quot;\u0026quot; resources: - services - endpoints - pods verbs: - get - list - watch Then apply prometheus-roleBindingSpecificNamespaces.yaml and prometheus-roleSpecificNamespaces.yaml for the RoleBinding and Role to take effect in the cluster.\n$ kubectl apply -f kube-prometheus/manifests/prometheus-roleBindingSpecificNamespaces.yaml $ kubectl apply -f kube-prometheus/manifests/prometheus-roleSpecificNamespaces.yaml Deploy the Service Monitor To deploy the service monitor, use the above wls-exporter.yaml deployment YAML and run the following command:\n$ kubectl create -f kubernetes/create-wcsites-domain/utils/weblogic-monitoring-exporter/wls-exporter.yaml Enable Prometheus to Discover the Service After the deployment of the service monitor, Prometheus should be able to discover wls-exporter and export metrics.\nYou can access the Prometheus dashboard at http://mycompany.com:32101/.\nDeploy Grafana Dashboard To view the domain metrics, deploy the Grafana dashboard provided in the WebLogic Monitoring Exporter.\nYou can access the Grafana dashboard at http://mycompany.com:32100/.\n  Log in to Grafana dashboard with admin/admin.\n  Go to Settings, then select DataSources, and then Add Data Source.\nHTTP URL: Prometheus URL http://mycompany.com:32101/\nAuth: Enable Basic Auth\nBasic Auth Details: WebLogic credentials provided in step Configure Prometheus Operator\n  Download the weblogic_dashboard.json file from here.\n  Click Add and then Import. Paste the modified JSON in the Paste JSON block, and then load it.\nThis displays the WebLogic Server Dashboard.\n  "
},
{
	"uri": "/fmw-kubernetes/soa-domains/appendix/docker-k8s-hardening/",
	"title": "Security hardening",
	"tags": [],
	"description": "Review resources for the Docker and Kubernetes cluster hardening.",
	"content": "Securing a Kubernetes cluster involves hardening on multiple fronts - securing the API servers, etcd, nodes, container images, container run-time, and the cluster network. Apply principles of defense in depth, principle of least privilege, and minimize the attack surface. Use security tools such as Kube-Bench to verify the cluster\u0026rsquo;s security posture. Since Kubernetes is evolving rapidly refer to Kubernetes Security Overview for the latest information on securing a Kubernetes cluster. Also ensure the deployed Docker containers follow the Docker Security guidance.\nThis section provides references on how to securely configure Docker and Kubernetes.\nReferences   Docker hardening\n https://docs.docker.com/engine/security/security/ https://blog.aquasec.com/docker-security-best-practices    Kubernetes hardening\n https://kubernetes.io/docs/concepts/security/overview/ https://kubernetes.io/docs/concepts/security/pod-security-standards/ https://blogs.oracle.com/developers/5-best-practices-for-kubernetes-security    Security best practices for Oracle WebLogic Server Running in Docker and Kubernetes\n https://blogs.oracle.com/weblogicserver/security-best-practices-for-weblogic-server-running-in-docker-and-kubernetes    "
},
{
	"uri": "/fmw-kubernetes/wcportal-domains/appendix/docker-k8s-hardening/",
	"title": "Security hardening",
	"tags": [],
	"description": "Review resources for the Docker and Kubernetes cluster hardening.",
	"content": "Securing a Kubernetes cluster involves hardening on multiple fronts - securing the API servers, etcd, nodes, container images, container run-time, and the cluster network. Apply principles of defense in depth, principle of least privilege, and minimize the attack surface. Use security tools such as Kube-Bench to verify the cluster\u0026rsquo;s security posture. Since Kubernetes is evolving rapidly refer to Kubernetes Security Overview for the latest information on securing a Kubernetes cluster. Also ensure the deployed Docker containers follow the Docker Security guidance.\nThis section provides references on how to securely configure Docker and Kubernetes.\nReferences   Docker hardening\n https://docs.docker.com/engine/security/security/ https://blog.aquasec.com/docker-security-best-practices    Kubernetes hardening\n https://kubernetes.io/docs/concepts/security/overview/ https://kubernetes.io/docs/concepts/security/pod-security-standards/ https://blogs.oracle.com/developers/5-best-practices-for-kubernetes-security    Security best practices for Oracle WebLogic Server Running in Docker and Kubernetes\n https://blogs.oracle.com/weblogicserver/security-best-practices-for-weblogic-server-running-in-docker-and-kubernetes    "
},
{
	"uri": "/fmw-kubernetes/wcsites-domains/appendix/docker-k8s-hardening/",
	"title": "Security hardening",
	"tags": [],
	"description": "Review resources for the Docker and Kubernetes cluster hardening.",
	"content": "Securing a Kubernetes cluster involves hardening on multiple fronts - securing the API servers, etcd, nodes, container images, container run-time, and the cluster network. Apply principles of defense in depth, principle of least privilege, and minimize the attack surface. Use security tools such as Kube-Bench to verify the cluster\u0026rsquo;s security posture. Since Kubernetes is evolving rapidly refer to Kubernetes Security Overview for the latest information on securing a Kubernetes cluster. Also ensure the deployed Docker containers follow the Docker Security guidance.\nThis section provides references on how to securely configure Docker and Kubernetes.\nReferences   Docker hardening\n https://docs.docker.com/engine/security/security/ https://blog.aquasec.com/docker-security-best-practices    Kubernetes hardening\n https://kubernetes.io/docs/concepts/security/overview/ https://kubernetes.io/docs/concepts/security/pod-security-standards/ https://blogs.oracle.com/developers/5-best-practices-for-kubernetes-security    Security best practices for Oracle WebLogic Server Running in Docker and Kubernetes\n https://blogs.oracle.com/weblogicserver/security-best-practices-for-weblogic-server-running-in-docker-and-kubernetes    "
},
{
	"uri": "/fmw-kubernetes/soa-domains/patch_and_upgrade/upgrade-k8s-cluster/",
	"title": "Upgrade a Kubernetes cluster",
	"tags": [],
	"description": "Upgrade the underlying Kubernetes cluster version in a running SOA Kubernetes environment.",
	"content": "These instructions describe how to upgrade a Kubernetes cluster created using kubeadm on which an Oracle SOA Suite domain is deployed. A rolling upgrade approach is used to upgrade nodes (master and worker) of the Kubernetes cluster.\nIt is expected that there will be a down time during the upgrade of the Kubernetes cluster as the nodes need to be drained as part of the upgrade process.\n Prerequisites  Review Prerequisites and ensure that your Kubernetes cluster is ready for upgrade. Make sure your environment meets all prerequisites. Make sure the database used for the SOA domain deployment is up and running during the upgrade process.  Upgrade the Kubernetes version An upgrade of Kubernetes is supported from one MINOR version to the next MINOR version, or between PATCH versions of the same MINOR. For example, you can upgrade from 1.x to 1.x+1, but not from 1.x to 1.x+2. To upgrade a Kubernetes version, first all the master nodes of the Kubernetes cluster must be upgraded sequentially, followed by the sequential upgrade of each worker node.\n See here for Kubernetes official documentation to upgrade from 1.19 to 1.20 See here for Kubernetes official documentation to upgrade from 1.20 to 1.21 See here for Kubernetes official documentation to upgrade from 1.21 to 1.22 See here for Kubernetes official documentation to upgrade from 1.22 to 1.23  "
},
{
	"uri": "/fmw-kubernetes/wcsites-domains/patch-and-upgrade/upgrade-k8s-cluster/",
	"title": "Upgrade a Kubernetes cluster",
	"tags": [],
	"description": "Upgrade the underlying Kubernetes cluster version in a running Oracle WebCenter Sites Kubernetes environment.",
	"content": "These instructions describe how to upgrade a Kubernetes cluster created using kubeadm on which an Oracle WebCenter Sites domain is deployed. A rolling upgrade approach is used to upgrade nodes (master and worker) of the Kubernetes cluster.\nIt is expected that there will be a down time during the upgrade of the Kubernetes cluster as the nodes need to be drained as part of the upgrade process.\n Prerequisites  Review Prerequisites and ensure that your Kubernetes cluster is ready for upgrade. Make sure your environment meets all prerequisites. Make sure the database used for the Oracle WebCenter Sites domain deployment is up and running during the upgrade process.  Upgrade the Kubernetes version An upgrade of Kubernetes is supported from one MINOR version to the next MINOR version, or between PATCH versions of the same MINOR. For example, you can upgrade from 1.x to 1.x+1, but not from 1.x to 1.x+2. To upgrade a Kubernetes version, first all the master nodes of the Kubernetes cluster must be upgraded sequentially, followed by the sequential upgrade of each worker node.\n See here for Kubernetes official documentation to upgrade.  "
},
{
	"uri": "/fmw-kubernetes/wcportal-domains/installguide/create-wcp-domain/",
	"title": "Create WebCenter Portal domain",
	"tags": [],
	"description": "Create an Oracle WebCenter Portal domain home on an existing PV or PVC, and create the domain resource YAML file for deploying the generated Oracle WebCenter Portal domain.",
	"content": "Contents  Introduction Prerequisites Prepare the WebCenter Portal Domain Creation Input File Create the WebCenter Portal Domain Initialize the WebCenter Portal Domain Verify the WebCenter Portal Domain Managing WebCenter Portal  Introduction You can use the sample scripts to create a WebCenter Portal domain home on an existing Kubernetes persistent volume (PV) and persistent volume claim (PVC).The scripts also generate the domain YAML file, which helps you start the Kubernetes artifacts of the corresponding domain.\nPrerequisites  Ensure that you have completed all of the steps under prepare-your-environment. Ensure that the database and the WebLogic Kubernetes operator is up.  Prepare the WebCenter Portal Domain Creation Input File If required, you can customize the parameters used for creating a domain in the create-domain-inputs.yaml file.\nPlease note that the sample scripts for the WebCenter Portal domain deployment are available from the previously downloaded repository at ${WORKDIR}/create-wcp-domain/domain-home-on-pv/.\nMake a copy of the create-domain-inputs.yaml file before updating the default values.\nThe default domain created by the script has the following characteristics:\n An Administration Server named AdminServer listening on port 7001. A configured cluster named wcp-cluster of size 5. Managed Server, named wcpserver, listening on port 8888. If configurePortletServer is set to true . It configures a cluster named wcportlet-cluster of size 5 and Managed Server, named wcportletserver, listening on port 8889. Log files that are located in /shared/logs/\u0026lt;domainUID\u0026gt;.  Configuration parameters The following parameters can be provided in the inputs file:\n   Parameter Definition Default     adminPort Port number for the Administration Server inside the Kubernetes cluster. 7001   sslEnabled SSL mode enabling flag false   configurePortletServer Configure portlet server cluster false   adminServerSSLPort SSL Port number for the Administration Server inside the Kubernetes cluster. 7002   adminServerName Name of the Administration Server. AdminServer   clusterName Name of the WebLogic cluster instance to generate for the domain. By default the cluster name is wcp-cluster for the WebCenter Portal domain. wcp-cluster   portletClusterName Name of the Portlet cluster instance to generate for the domain. By default the cluster name is wcportlet-cluster for the Portlet. wcportlet-cluster   configuredManagedServerCount Number of Managed Server instances for the domain. 5   createDomainFilesDir Directory on the host machine to locate all the files that you need to create a WebLogic domain, including the script that is specified in the createDomainScriptName property. By default, this directory is set to the relative path wlst, and the create script uses the built-in WLST offline scripts in the wlst directory to create the WebLogic domain. An absolute path is also supported to point to an arbitrary directory in the file system. The built-in scripts can be replaced by the user-provided scripts or model files as long as those files are in the specified directory. Files in this directory are put into a Kubernetes config map, which in turn is mounted to createDomainScriptsMountPath, so that the Kubernetes pod can use the scripts and supporting files to create a domain home. wlst   createDomainScriptsMountPath Mount path where the create domain scripts are located inside a pod. The create-domain.sh script creates a Kubernetes job to run the script (specified in the createDomainScriptName property) in a Kubernetes pod that creates a domain home. Files in the createDomainFilesDir directory are mounted to this location in the pod, so that the Kubernetes pod can use the scripts and supporting files to create a domain home. /u01/weblogic   createDomainScriptName Script that the create domain script uses to create a WebLogic domain. The create-domain.sh script creates a Kubernetes job to run this script that creates a domain home. The script is located in the in-pod directory that is specified in the createDomainScriptsMountPath property. If you need to provide your own scripts to create the domain home, instead of using the built-in scripts, you must use this property to set the name of the script to that which you want the create domain job to run. create-domain-job.sh   domainHome Home directory of the WebCenter Portal domain. This field cannot be modified. /u01/oracle/user_projects/domains/wcp-domain   domainPVMountPath Mount path of the domain persistent volume. This field cannot be modified. /u01/oracle/user_projects/domains   domainUID Unique ID that identifies this particular domain. Used as the name of the generated WebLogic domain as well as the name of the Kubernetes domain resource. This ID must be unique across all domains in a Kubernetes cluster. This ID cannot contain any character that is not valid in a Kubernetes service name. wcp-domain   exposeAdminNodePort Boolean indicating if the Administration Server is exposed outside of the Kubernetes cluster. false   exposeAdminT3Channel Boolean indicating if the T3 administrative channel is exposed outside the Kubernetes cluster. false   image WebCenter Portal Docker image. The WebLogic Kubernetes Operator requires WebCenter Portal release 12.2.1.4. Refer to WebCenter Portal Docker Image for details on how to obtain or create the image. oracle/wcportal:12.2.1.4   imagePullPolicy WebLogic Docker image pull policy. Legal values are IfNotPresent, Always, or Never IfNotPresent   imagePullSecretName Name of the Kubernetes secret to access the Docker Store to pull the WebLogic Server Docker image. The presence of the secret is validated when this parameter is specified.    includeServerOutInPodLog Boolean indicating whether to include server.out to the pod\u0026rsquo;s stdout. true   initialManagedServerReplicas Number of Managed Server to initially start for the domain. 2   javaOptions Java options for starting the Administration Server and Managed Servers. A Java option can include references to one or more of the following pre-defined variables to obtain WebLogic domain information: $(DOMAIN_NAME), $(DOMAIN_HOME), $(ADMIN_NAME), $(ADMIN_PORT), and $(SERVER_NAME). -Dweblogic.StdoutDebugEnabled=false   logHome The in-pod location for the domain log, server logs, server out, and Node Manager log files. This field cannot be modified. /u01/oracle/user_projects/logs/wcp-domain   managedServerNameBase Base string used to generate Managed Server names. wcpserver   portletServerNameBase Base string used to generate Portlet Server names. wcportletserver   managedServerPort Port number for each Managed Server. By default the managedServerPort is 8888 for the wcpserver and managedServerPort is 8889 for the wcportletserver. 8888   managedServerSSLPort SSL port number for each Managed Server. By default the managedServerPort is 8788 for the wcpserver and managedServerPort is 8789 for the wcportletserver. 8788   portletServerPort Port number for each Portlet Server. By default the portletServerPort is 8889 for the wcportletserver. 8888   portletServerSSLPort SSL port number for each Portlet Server. By default the portletServerSSLPort is 8789 for the wcportletserver. 8789   namespace Kubernetes namespace in which to create the domain. wcpns   persistentVolumeClaimName Name of the persistent volume claim created to host the domain home. If not specified, the value is derived from the domainUID as \u0026lt;domainUID\u0026gt;-weblogic-sample-pvc. wcp-domain-domain-pvc   productionModeEnabled Boolean indicating if production mode is enabled for the domain. true   serverStartPolicy Determines which WebLogic Server instances are to be started. Legal values are NEVER, IF_NEEDED, ADMIN_ONLY. IF_NEEDED   t3ChannelPort Port for the T3 channel of the NetworkAccessPoint. 30012   t3PublicAddress Public address for the T3 channel. This should be set to the public address of the Kubernetes cluster. This would typically be a load balancer address. For development environments only: In a single server (all-in-one) Kubernetes deployment, this may be set to the address of the master, or at the very least, it must be set to the address of one of the worker nodes. If not provided, the script will attempt to set it to the IP address of the Kubernetes cluster.   weblogicCredentialsSecretName Name of the Kubernetes secret for the Administration Server\u0026rsquo;s user name and password. If not specified, then the value is derived from the domainUID as \u0026lt;domainUID\u0026gt;-weblogic-credentials. wcp-domain-domain-credentials   weblogicImagePullSecretName Name of the Kubernetes secret for the Docker Store, used to pull the WebLogic Server image.    serverPodCpuRequest, serverPodMemoryRequest, serverPodCpuCLimit, serverPodMemoryLimit The maximum amount of compute resources allowed and minimum amount of compute resources required for each server pod. Please refer to the Kubernetes documentation on Managing Compute Resources for Containers for details. Resource requests and resource limits are not specified. Refer to WebCenter Portal Cluster Sizing Recommendations for more details.   rcuSchemaPrefix The schema prefix to use in the database, for example WCP1. You may wish to make this the same as the domainUID in order to simplify matching domain to their RCU schemas. WCP1   rcuDatabaseURL The database URL. dbhostname:dbport/servicename   rcuCredentialsSecret The Kubernetes secret containing the database credentials. wcp-domain-rcu-credentials   loadBalancerHostName Host name for the final url accessible outside K8S environment. abc.def.com   loadBalancerPortNumber Port for the final url accessible outside K8S environment. 30305   loadBalancerProtocol Protocol for the final url accessible outside K8S environment. http   loadBalancerType Loadbalancer name. Example: Traefik or \u0026quot;\u0026rdquo; traefik   unicastPort Starting range of unicast port that application will use. 50000    You can form the names of the Kubernetes resources in the generated YAML files with the value of these properties specified in the create-domain-inputs.yaml file: adminServerName, clusterName and managedServerNameBase. Characters that are invalid in a Kubernetes service name are converted to valid values in the generated YAML files. For example, an uppercase letter is converted to a lowercase letter and an underscore (\u0026quot;_\u0026quot;) is converted to a hyphen (\u0026quot;-\u0026quot;) .\nThe sample demonstrates how to create a WebCenter Portal domain home and associated Kubernetes resources for a domain that has one cluster only. In addition, the sample provides users with the capability to supply their own scripts to create the domain home for other use cases. You can modify the generated domain YAML file to include more use cases.\nCreate the WebCenter Portal Domain The syntax of the create-domain.sh script is as follows:\n $ ./create-domain.sh \\ -i create-domain-inputs.yaml \\ -o /\u0026lt;path to output-directory\u0026gt; The script performs the following functions:\n Creates a directory for the generated Kubernetes YAML files for this domain if it does not already exist. The path name is /\u0026lt;path to output-directory\u0026gt;/weblogic-domains/\u0026lt;domainUID\u0026gt;.If the directory already exists, remove its content before using this script. Creates a Kubernetes job to start the WebCenter Portal Container utility and run offline WLST scripts that create the domain on the shared storage. Runs and waits for the job to finish. Creates a Kubernetes domain YAML file, domain.yaml, in the directory that is created above. This YAML file can be used to create the Kubernetes resource using the kubectl create -f or kubectl apply -f command:  $ kubectl apply -f ../\u0026lt;path to output-directory\u0026gt;/weblogic-domains/\u0026lt;domainUID\u0026gt;/domain.yaml  Creates a convenient utility script, delete-domain-job.yaml, to clean up the domain home created by the create script.    Run the create-domain.sh sample script, pointing it at the create-domain-inputs.yaml inputs file and an output directory like below:\n$ cd ${WORKDIR}/create-wcp-domain/ $ sh create-domain.sh -i create-domain-inputs.yaml -o output Input parameters being used export version=\u0026#34;create-weblogic-sample-domain-inputs-v1\u0026#34; export sslEnabled=\u0026#34;false\u0026#34; export adminPort=\u0026#34;7001\u0026#34; export adminServerSSLPort=\u0026#34;7002\u0026#34; export adminServerName=\u0026#34;AdminServer\u0026#34; export domainUID=\u0026#34;wcp-domain\u0026#34; export domainHome=\u0026#34;/u01/oracle/user_projects/domains/$domainUID\u0026#34; export serverStartPolicy=\u0026#34;IF_NEEDED\u0026#34; export clusterName=\u0026#34;wcp-cluster\u0026#34; export configuredManagedServerCount=\u0026#34;5\u0026#34; export initialManagedServerReplicas=\u0026#34;2\u0026#34; export managedServerNameBase=\u0026#34;wcpserver\u0026#34; export managedServerPort=\u0026#34;8888\u0026#34; export managedServerSSLPort=\u0026#34;8788\u0026#34; export portletServerPort=\u0026#34;8889\u0026#34; export portletServerSSLPort=\u0026#34;8789\u0026#34; export image=\u0026#34;oracle/wcportal:12.2.1.4\u0026#34; export imagePullPolicy=\u0026#34;IfNotPresent\u0026#34; export productionModeEnabled=\u0026#34;true\u0026#34; export weblogicCredentialsSecretName=\u0026#34;wcp-domain-domain-credentials\u0026#34; export includeServerOutInPodLog=\u0026#34;true\u0026#34; export logHome=\u0026#34;/u01/oracle/user_projects/domains/logs/$domainUID\u0026#34; export httpAccessLogInLogHome=\u0026#34;true\u0026#34; export t3ChannelPort=\u0026#34;30012\u0026#34; export exposeAdminT3Channel=\u0026#34;false\u0026#34; export adminNodePort=\u0026#34;30701\u0026#34; export exposeAdminNodePort=\u0026#34;false\u0026#34; export namespace=\u0026#34;wcpns\u0026#34; javaOptions=-Dweblogic.StdoutDebugEnabled=false export persistentVolumeClaimName=\u0026#34;wcp-domain-domain-pvc\u0026#34; export domainPVMountPath=\u0026#34;/u01/oracle/user_projects/domains\u0026#34; export createDomainScriptsMountPath=\u0026#34;/u01/weblogic\u0026#34; export createDomainScriptName=\u0026#34;create-domain-job.sh\u0026#34; export createDomainFilesDir=\u0026#34;wlst\u0026#34; export rcuSchemaPrefix=\u0026#34;WCP1\u0026#34; export rcuDatabaseURL=\u0026#34;oracle-db.wcpns.svc.cluster.local:1521/devpdb.k8s\u0026#34; export rcuCredentialsSecret=\u0026#34;wcp-domain-rcu-credentials\u0026#34; export loadBalancerHostName=\u0026#34;abc.def.com\u0026#34; export loadBalancerPortNumber=\u0026#34;30305\u0026#34; export loadBalancerProtocol=\u0026#34;http\u0026#34; export loadBalancerType=\u0026#34;traefik\u0026#34; export unicastPort=\u0026#34;50000\u0026#34; Generating output/weblogic-domains/wcp-domain/create-domain-job.yaml Generating output/weblogic-domains/wcp-domain/delete-domain-job.yaml Generating output/weblogic-domains/wcp-domain/domain.yaml Checking to see if the secret wcp-domain-domain-credentials exists in namespace wcpns configmap/wcp-domain-create-wcp-infra-sample-domain-job-cm created Checking the configmap wcp-domain-create-wcp-infra-sample-domain-job-cm was created configmap/wcp-domain-create-wcp-infra-sample-domain-job-cm labeled Checking if object type job with name wcp-domain-create-wcp-infra-sample-domain-job exists Deleting wcp-domain-create-wcp-infra-sample-domain-job using output/weblogic-domains/wcp-domain/create-domain-job.yaml job.batch \u0026#34;wcp-domain-create-wcp-infra-sample-domain-job\u0026#34; deleted $loadBalancerType is NOT empty Creating the domain by creating the job output/weblogic-domains/wcp-domain/create-domain-job.yaml job.batch/wcp-domain-create-wcp-infra-sample-domain-job created Waiting for the job to complete... status on iteration 1 of 20 pod wcp-domain-create-wcp-infra-sample-domain-job-b5l6c status is Running status on iteration 2 of 20 pod wcp-domain-create-wcp-infra-sample-domain-job-b5l6c status is Running status on iteration 3 of 20 pod wcp-domain-create-wcp-infra-sample-domain-job-b5l6c status is Running status on iteration 4 of 20 pod wcp-domain-create-wcp-infra-sample-domain-job-b5l6c status is Running status on iteration 5 of 20 pod wcp-domain-create-wcp-infra-sample-domain-job-b5l6c status is Running status on iteration 6 of 20 pod wcp-domain-create-wcp-infra-sample-domain-job-b5l6c status is Running status on iteration 7 of 20 pod wcp-domain-create-wcp-infra-sample-domain-job-b5l6c status is Completed Domain wcp-domain was created and will be started by the WebLogic Kubernetes Operator The following files were generated: output/weblogic-domains/wcp-domain/create-domain-inputs.yaml output/weblogic-domains/wcp-domain/create-domain-job.yaml output/weblogic-domains/wcp-domain/domain.yaml Completed   To monitor the above domain creation logs:\n$ kubectl get pods -n wcpns |grep wcp-domain-create wcp-domain-create-fmw-infra-sample-domain-job-8jr6k 1/1 Running 0 6s $ kubectl get pods -n wcpns | grep wcp-domain-create | awk \u0026#39;{print $1}\u0026#39; | xargs kubectl -n wcpns logs -f SAMPLE OUTPUT:\nThe domain will be created using the script /u01/weblogic/create-domain-script.sh Initializing WebLogic Scripting Tool (WLST) ... Welcome to WebLogic Server Administration Scripting Shell Type help() for help on available commands ================================================================= WebCenter Portal Weblogic Operator Domain Creation Script 12.2.1.4.0 ================================================================= Creating Base Domain... Creating Admin Server... Creating cluster... managed server name is wcpserver1 managed server name is wcpserver2 managed server name is wcpserver3 managed server name is wcpserver4 managed server name is wcpserver5 ['wcpserver1', 'wcpserver2', 'wcpserver3', 'wcpserver4', 'wcpserver5'] Creating porlet cluster... managed server name is wcportletserver1 managed server name is wcportletserver2 managed server name is wcportletserver3 ['wcportletserver1', 'wcportletserver2', 'wcportletserver3', 'wcportletserver4', 'wcportletserver5'] Managed servers created... Creating Node Manager... Will create Base domain at /u01/oracle/user_projects/domains/wcp-domain Writing base domain... Base domain created at /u01/oracle/user_projects/domains/wcp-domain Extending Domain... Extending domain at /u01/oracle/user_projects/domains/wcp-domain Database oracle-db.wcpns.svc.cluster.local:1521/devpdb.k8s ExposeAdminT3Channel false with 100.111.157.155:30012 Applying JRF templates... Applying WCPortal templates... Extension Templates added... WC_Portal Managed server deleted... Configuring the Service Table DataSource... fmwDatabase jdbc:oracle:thin:@oracle-db.wcpns.svc.cluster.local:1521/devpdb.k8s Getting Database Defaults... Targeting Server Groups... Set CoherenceClusterSystemResource to defaultCoherenceCluster for server:wcpserver1 Set CoherenceClusterSystemResource to defaultCoherenceCluster for server:wcpserver2 Set CoherenceClusterSystemResource to defaultCoherenceCluster for server:wcpserver3 Set CoherenceClusterSystemResource to defaultCoherenceCluster for server:wcpserver4 Set CoherenceClusterSystemResource to defaultCoherenceCluster for server:wcpserver5 Set CoherenceClusterSystemResource to defaultCoherenceCluster for server:wcportletserver1 Set CoherenceClusterSystemResource to defaultCoherenceCluster for server:wcportletserver2 Set CoherenceClusterSystemResource to defaultCoherenceCluster for server:wcportletserver3 Targeting Cluster ... Set CoherenceClusterSystemResource to defaultCoherenceCluster for cluster:wcp-cluster Set WLS clusters as target of defaultCoherenceCluster:wcp-cluster Set CoherenceClusterSystemResource to defaultCoherenceCluster for cluster:wcportlet-cluster Set WLS clusters as target of defaultCoherenceCluster:wcportlet-cluster Preparing to update domain... Jan 12, 2021 10:30:09 AM oracle.security.jps.az.internal.runtime.policy.AbstractPolicyImpl initializeReadStore INFO: Property for read store in parallel: oracle.security.jps.az.runtime.readstore.threads = null Domain updated successfully Domain Creation is done... Successfully Completed   Initialize the WebCenter Portal Domain To start the domain, apply the above domain.yaml:\n$ kubectl apply -f output/weblogic-domains/wcp-domain/domain.yaml domain.weblogic.oracle/wcp-domain created Verify the WebCenter Portal Domain Verify that the domain and servers pods and services are created and in the READY state:\nSample run below:\n-bash-4.2$ kubectl get pods -n wcpns -w NAME READY STATUS RESTARTS\tAGE wcp-domain-create-fmw-infra-sample-domain-job-8jr6k 0/1 Completed 0 15m wcp-domain-adminserver 1/1 Running 0 8m9s wcp-domain-create-fmw-infra-sample-domain-job-8jr6k 0/1 Completed 0 3h6m wcp-domain-wcp-server1 0/1 Running 0 6m5s wcp-domain-wcp-server2 0/1 Running 0 6m4s wcp-domain-wcp-server2 1/1 Running 0 6m18s wcp-domain-wcp-server1 1/1 Running 0 6m54s -bash-4.2$ kubectl get all -n wcpns NAME READY STATUS RESTARTS AGE pod/wcp-domain-adminserver 1/1 Running 0 13m pod/wcp-domain-create-fmw-infra-sample-domain-job-8jr6k 0/1 Completed 0 3h12m pod/wcp-domain-wcp-server1 1/1 Running 0 11m pod/wcp-domain-wcp-server2 1/1 Running 0 11m pod/wcp-domain-wcportletserver1 1/1 Running 1 21h NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/wcp-domain-adminserver ClusterIP None \u0026lt;none\u0026gt; 7001/TCP 13m service/wcp-domain-cluster-wcp-cluster ClusterIP 10.98.145.173 \u0026lt;none\u0026gt; 8888/TCP 11m service/wcp-domain-wcp-server1 ClusterIP None \u0026lt;none\u0026gt; 8888/TCP 11m service/wcp-domain-wcp-server2 ClusterIP None \u0026lt;none\u0026gt; 8888/TCP 11m service/wcp-domain-cluster-wcportlet-cluster ClusterIP 10.98.145.173 \u0026lt;none\u0026gt; 8889/TCP 11m service/wcp-domain-wcportletserver1 ClusterIP None \u0026lt;none\u0026gt; 8889/TCP 11m NAME COMPLETIONS DURATION AGE job.batch/wcp-domain-create-fmw-infra-sample-domain-job 1/1 16m 3h12m To see the Admin and Managed Servers logs, you can check the pod logs:\n$ kubectl logs -f wcp-domain-adminserver -n wcpns $ kubectl logs -f wcp-domain-wcp-server1 -n wcpns Verify the Pods Use the following command to see the pods running the servers:\n$ kubectl get pods -n NAMESPACE Here is an example of the output of this command:\n-bash-4.2$ kubectl get pods -n wcpns NAME READY STATUS RESTARTS AGE rcu 1/1 Running 1 14d wcp-domain-adminserver 1/1 Running 0 16m wcp-domain-create-fmw-infra-sample-domain-job-8jr6k 0/1 Completed 0 3h14m wcp-domain-wcp-server1 1/1 Running 0 14m wcp-domain-wcp-server2 1/1 Running 0 14m wcp-domain-wcportletserver1 1/1 Running 1 14m Verify the Services Use the following command to see the services for the domain:\n$ kubectl get services -n NAMESPACE Here is an example of the output of this command:\n-bash-4.2$ kubectl get services -n wcpns NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE wcp-domain-adminserver ClusterIP None \u0026lt;none\u0026gt; 7001/TCP 17m wcp-domain-cluster-wcp-cluster ClusterIP 10.98.145.173 \u0026lt;none\u0026gt; 8888/TCP 14m wcp-domain-wcp-server1 ClusterIP None \u0026lt;none\u0026gt; 8888/TCP 14m wcp-domain-wcp-server2 ClusterIP None \u0026lt;none\u0026gt; 8888/TCP 14m wcp-domain-cluster-wcportlet-cluster ClusterIP 10.98.145.173 \u0026lt;none\u0026gt; 8889/TCP 14m wcp-domain-wcportletserver1 ClusterIP None \u0026lt;none\u0026gt; 8889/TCP 14m Managing WebCenter Portal To stop Managed Servers:\n$ kubectl patch domain wcp-domain -n wcpns --type=\u0026#39;json\u0026#39; -p=\u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/clusters/0/replicas\u0026#34;, \u0026#34;value\u0026#34;: 0 }]\u0026#39; To start all configured Managed Servers:\n$ kubectl patch domain wcp-domain -n wcpns --type=\u0026#39;json\u0026#39; -p=\u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/clusters/0/replicas\u0026#34;, \u0026#34;value\u0026#34;: 3 }]\u0026#39; -bash-4.2$ kubectl get pods -n wcpns -w NAME READY STATUS RESTARTS\tAGE wcp-domain-create-fmw-infra-sample-domain-job-8jr6k 0/1 Completed 0 15m wcp-domain-adminserver 1/1 Running 0 8m9s wcp-domain-create-fmw-infra-sample-domain-job-8jr6k 0/1 Completed 0 3h6m wcp-domain-wcp-server1 0/1 Running 0 6m5s wcp-domain-wcp-server2 0/1 Running 0 6m4s wcp-domain-wcp-server2 1/1 Running 0 6m18s wcp-domain-wcp-server1 1/1 Running 0 6m54s "
},
{
	"uri": "/fmw-kubernetes/wcportal-domains/manage-wcportal-domains/monitoring-and-publishing-logs/publishing-logs/logstash/",
	"title": "Logstash",
	"tags": [],
	"description": "Describes how to configure a WebCenter Portal domain to use logstash and publish the WebLogic Server logs to Elasticsearch.",
	"content": "Install Elasticsearch and Kibana To install Elasticsearch and Kibana, run the following command:\n$ cd ${WORKDIR}/elasticsearch-and-kibana $ kubectl create -f elasticsearch_and_kibana.yaml Publish to Elasticsearch The diagnostics or other logs can be pushed to Elasticsearch server using logstash pod. The logstash pod should have access to the shared domain home or the log location. In case of the Oracle WebCenter Portal domain, the persistent volume of the domain home can be used in the logstash pod. The steps to create the logstash pod are,\n  Get domain home persistence volume claim details of the Oracle WebCenter Portal domain. The following command will list the persistent volume claim details in the namespace - wcpns. In the example below the persistent volume claim is wcp-domain-domain-pvc.\n$ kubectl get pv -n wcpns NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE wcp-domain-domain-pv 10Gi RWX Retain Bound wcpns/wcp-domain-domain-pvc wcp-domain-domain-storage-class 175d   Create logstash configuration file logstash.conf. Below is a sample Logstash configuration file is located at ${WORKDIR}/logging-services/logstash. Below configuration pushes diagnostic and all domains logs.\ninput { file { path =\u0026gt; \u0026#34;/u01/oracle/user_projects/domains/wcp-domain/servers/**/logs/*-diagnostic.log\u0026#34; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026#34;/u01/oracle/user_projects/domains/logs/wcp-domain/*.log\u0026#34; start_position =\u0026gt; beginning } } filter { grok { match =\u0026gt; [ \u0026#34;message\u0026#34;, \u0026#34;\u0026lt;%{DATA:log_timestamp}\u0026gt; \u0026lt;%{WORD:log_level}\u0026gt; \u0026lt;%{WORD:thread}\u0026gt; \u0026lt;%{HOSTNAME:hostname}\u0026gt; \u0026lt;%{HOSTNAME:servername}\u0026gt; \u0026lt;%{DATA:timer}\u0026gt; \u0026lt;\u0026lt;%{DATA:kernel}\u0026gt;\u0026gt; \u0026lt;\u0026gt; \u0026lt;%{DATA:uuid}\u0026gt; \u0026lt;%{NUMBER:timestamp}\u0026gt; \u0026lt;%{DATA:misc}\u0026gt; \u0026lt;%{DATA:log_number}\u0026gt; \u0026lt;%{DATA:log_message}\u0026gt;\u0026#34; ] } } output { elasticsearch { hosts =\u0026gt; [\u0026#34;elasticsearch.default.svc.cluster.local:9200\u0026#34;] } }   Copy the logstash.conf into say /u01/oracle/user_projects/domains so that it can be used for logstash deployment, using Administration Server pod ( For example wcp-domain-adminserver pod in namespace wcpns):\n$ kubectl cp ${WORKDIR}/logging-services/logstash/logstash.conf wcpns/wcp-domain-adminserver:/u01/oracle/user_projects/domains -n wcpns   Create deployment YAML logstash.yaml for logstash pod using the domain home persistence volume claim. Make sure to point the logstash configuration file to correct location ( For example: we copied logstash.conf to /u01/oracle/user_projects/domains/logstash.conf) and also correct domain home persistence volume claim. Sample Logstash deployment is located at kubernetes/samples/scripts/create-wcp-domain/utils/logstash/logstash.yaml:\napiVersion: apps/v1\rkind: Deployment\rmetadata:\rname: logstash\rnamespace: wcpns\rspec:\rselector:\rmatchLabels:\rapp: logstash\rtemplate: metadata:\rlabels:\rapp: logstash\rspec:\rvolumes:\r- name: domain-storage-volume\rpersistentVolumeClaim:\rclaimName: wcp-domain-domain-pvc\r- name: shared-logs\remptyDir: {}\rcontainers:\r- name: logstash\rimage: logstash:6.6.0\rcommand: [\u0026quot;/bin/sh\u0026quot;]\rargs: [\u0026quot;/usr/share/logstash/bin/logstash\u0026quot;, \u0026quot;-f\u0026quot;, \u0026quot;/u01/oracle/user_projects/domains/logstash.conf\u0026quot;]\rimagePullPolicy: IfNotPresent\rvolumeMounts:\r- mountPath: /u01/oracle/user_projects/domains\rname: domain-storage-volume\r- name: shared-logs\rmountPath: /shared-logs\rports:\r- containerPort: 5044\rname: logstash\r  Deploy logstash to start publish logs to Elasticsearch\n$ kubectl create -f ${WORKDIR}/logging-services/logstash/logstash.yaml   Create an Index Pattern in Kibana Create an index pattern logstash* in Kibana \u0026gt; Management. After the servers are started, you will see the log data in the Kibana dashboard:\n"
},
{
	"uri": "/fmw-kubernetes/oid/prepare-your-environment/",
	"title": "Prepare Your Environment",
	"tags": [],
	"description": "Prepare your environment",
	"content": " Check the Kubernetes cluster is ready Obtain the OID container image Setup the code repository to deploy OID  Check the Kubernetes cluster is ready   Run the following command on the master node to check the cluster and worker nodes are running:\n$ kubectl get nodes,pods -n kube-system The output will look similar to the following:\nNAME STATUS ROLES AGE VERSION node/worker-node1 Ready \u0026lt;none\u0026gt; 17h v1.20.10 node/worker-node2 Ready \u0026lt;none\u0026gt; 17h v1.20.10 node/master-node Ready control-plane,master 23h v1.20.10 NAME READY STATUS RESTARTS AGE pod/coredns-66bff467f8-slxdq 1/1 Running 1 67d pod/coredns-66bff467f8-v77qt 1/1 Running 1 67d pod/etcd-10.89.73.42 1/1 Running 1 67d pod/kube-apiserver-10.89.73.42 1/1 Running 1 67d pod/kube-controller-manager-10.89.73.42 1/1 Running 27 67d pod/kube-flannel-ds-amd64-r2m8r 1/1 Running 2 48d pod/kube-flannel-ds-amd64-rdhrf 1/1 Running 2 6d1h pod/kube-flannel-ds-amd64-vpcbj 1/1 Running 3 66d pod/kube-proxy-jtcxm 1/1 Running 1 67d pod/kube-proxy-swfmm 1/1 Running 1 66d pod/kube-proxy-w6x6t 1/1 Running 1 66d pod/kube-scheduler-10.89.73.42 1/1 Running 29 67d   Obtain the OID container image The OID Kubernetes deployment requires access to an OID container image. The image can be obtained in the following ways:\n Prebuilt OID container image Build your own OID container image using WebLogic Image Tool  Prebuilt OID container image The latest prebuilt OID container image can be downloaded from Oracle Container Registry. This image is prebuilt by Oracle and includes Oracle Internet Directory 12.2.1.4.0 and the latest PSU.\nNote: Before using this image you must login to Oracle Container Registry, navigate to Middleware \u0026gt; oid_cpu and accept the license agreement.\nAlternatively the same image can also be downloaded from My Oracle Support by referring to the document ID 2723908.1.\nYou can use this image in the following ways:\n Pull the container image from the Oracle Container Registry automatically during the OID Kubernetes deployment. Manually pull the container image from the Oracle Container Registry or My Oracle Support, and then upload it to your own container registry. Manually pull the container image from the Oracle Container Registry or My Oracle Support and manually stage it on the master node and each worker node.  Build your own OID container image using WebLogic Image Tool You can build your own OID container image using the WebLogic Image Tool. This is recommended if you need to apply one off patches to a Prebuilt OID container image. For more information about building your own container image with WebLogic Image Tool, see Create or update an image.\nYou can use an image built with WebLogic Image Tool in the following ways:\n Manually upload them to your own container registry. Manually stage them on the master node and each worker node.  Note: This documentation does not tell you how to pull or push the above images into a private container registry, or stage them on the master and worker nodes. Details of this can be found in the Enterprise Deployment Guide.\nSetup the Code repository to deploy OID Oracle Internet Directory deployment on Kubernetes leverages deployment scripts provided by Oracle for creating Oracle Internet Directory containers using the Helm charts provided. To deploy Oracle Internet Directory on Kubernetes you should set up the deployment scripts on the master node as below:\n  Create a working directory to setup the source code.\n$ mkdir \u0026lt;workdir\u0026gt; For example:\n$ mkdir /scratch/OIDContainer   Download the latest OID deployment scripts from the OID repository:\n$ cd \u0026lt;workdir\u0026gt; $ git clone https://github.com/oracle/fmw-kubernetes.git For example:\n$ cd /scratch/OIDContainer $ git clone https://github.com/oracle/fmw-kubernetes.git   Set the $WORKDIR environment variable as follows:\n$ export WORKDIR=\u0026lt;workdir\u0026gt;/fmw-kubernetes/OracleInternetDirectory For example:\n$ export WORKDIR=/scratch/OIDContainer/fmw-kubernetes/OracleInternetDirectory You are now ready to create the OID deployment as per Create OID instances.\n  "
},
{
	"uri": "/fmw-kubernetes/oud/prepare-your-environment/",
	"title": "Prepare Your Environment",
	"tags": [],
	"description": "Prepare your environment",
	"content": " Check the Kubernetes cluster is ready Obtain the OUD container image Create a persistent volume directory Setup the code repository to deploy OUD  Check the Kubernetes cluster is ready As per the Prerequisites a Kubernetes cluster should have already been configured.\n  Run the following command on the master node to check the cluster and worker nodes are running:\n$ kubectl get nodes,pods -n kube-system The output will look similar to the following:\nNAME STATUS ROLES AGE VERSION node/worker-node1 Ready \u0026lt;none\u0026gt; 17h v1.20.10 node/worker-node2 Ready \u0026lt;none\u0026gt; 17h v1.20.10 node/master-node Ready control-plane,master 23h v1.20.10 NAME READY STATUS RESTARTS AGE pod/coredns-66bff467f8-slxdq 1/1 Running 1 67d pod/coredns-66bff467f8-v77qt 1/1 Running 1 67d pod/etcd-10.89.73.42 1/1 Running 1 67d pod/kube-apiserver-10.89.73.42 1/1 Running 1 67d pod/kube-controller-manager-10.89.73.42 1/1 Running 27 67d pod/kube-flannel-ds-amd64-r2m8r 1/1 Running 2 48d pod/kube-flannel-ds-amd64-rdhrf 1/1 Running 2 6d1h pod/kube-flannel-ds-amd64-vpcbj 1/1 Running 3 66d pod/kube-proxy-jtcxm 1/1 Running 1 67d pod/kube-proxy-swfmm 1/1 Running 1 66d pod/kube-proxy-w6x6t 1/1 Running 1 66d pod/kube-scheduler-10.89.73.42 1/1 Running 29 67d   Obtain the OUD container image The OUD Kubernetes deployment requires access to an OUD container image. The image can be obtained in the following ways:\n Prebuilt OUD container image Build your own OUD container image using WebLogic Image Tool  Prebuilt OUD container image The latest prebuilt OUD container image can be downloaded from Oracle Container Registry. This image is prebuilt by Oracle and includes Oracle Unified Directory 12.2.1.4.0 and the latest PSU.\nNote: Before using this image you must login to Oracle Container Registry, navigate to Middleware \u0026gt; oud_cpu and accept the license agreement.\nAlternatively the same image can also be downloaded from My Oracle Support by referring to the document ID 2723908.1.\nYou can use this image in the following ways:\n Pull the container image from the Oracle Container Registry automatically during the OUD Kubernetes deployment. Manually pull the container image from the Oracle Container Registry or My Oracle Support, and then upload it to your own container registry. Manually pull the container image from the Oracle Container Registry or My Oracle Support and manually stage it on the master node and each worker node.  Build your own OUD container image using WebLogic Image Tool You can build your own OUD container image using the WebLogic Image Tool. This is recommended if you need to apply one off patches to a Prebuilt OUD container image. For more information about building your own container image with WebLogic Image Tool, see Create or update image.\nYou can use an image built with WebLogic Image Tool in the following ways:\n Manually upload them to your own container registry. Manually stage them on the master node and each worker node.  Note: This documentation does not tell you how to pull or push the above images into a private container registry, or stage them on the master and worker nodes. Details of this can be found in the Enterprise Deployment Guide.\nCreate a persistent volume directory As referenced in Prerequisites the nodes in the Kubernetes cluster must have access to a persistent volume such as a Network File System (NFS) mount or a shared file system.\nMake sure the persistent volume path has full access permissions, and that the folder is empty. In this example /scratch/shared/ is a shared directory accessible from all nodes.\n  On the master node run the following command to create a user_projects directory:\n$ cd \u0026lt;persistent_volume\u0026gt; $ mkdir oud_user_projects $ chmod 777 oud_user_projects For example:\n$ cd /scratch/shared $ mkdir oud_user_projects $ chmod 777 oud_user_projects   On the master node run the following to ensure it is possible to read and write to the persistent volume:\n$ cd \u0026lt;persistent_volume\u0026gt;/oud_user_projects $ touch file.txt $ ls filemaster.txt For example:\n$ cd /scratch/shared/oud_user_projects $ touch filemaster.txt $ ls filemaster.txt On the first worker node run the following to ensure it is possible to read and write to the persistent volume:\n$ cd /scratch/shared/oud_user_projects $ ls filemaster.txt $ touch fileworker1.txt $ ls fileworker1.txt Repeat the above for any other worker nodes e.g fileworker2.txt etc. Once proven that it\u0026rsquo;s possible to read and write from each node to the persistent volume, delete the files created.\n  Setup the code repository to deploy OUD Oracle Unified Directory deployment on Kubernetes leverages deployment scripts provided by Oracle for creating Oracle Unified Directory containers using the Helm charts provided. To deploy Oracle Unified Directory on Kubernetes you should set up the deployment scripts on the persistent volume as below:\nNote: The work directory must be created on the persistent volume as access to the helm charts is required by a cron job created during OUD deployment.\n  Create a working directory on the persistent volume to setup the source code.\n$ mkdir \u0026lt;persistent_volume\u0026gt;/\u0026lt;workdir\u0026gt; For example:\n$ mkdir /scratch/shared/OUDContainer   Download the latest OUD deployment scripts from the OUD repository:\n$ cd \u0026lt;persistent_volume\u0026gt;/\u0026lt;workdir\u0026gt; $ git clone https://github.com/oracle/fmw-kubernetes.git For example:\n$ cd /scratch/shared/OUDContainer $ git clone https://github.com/oracle/fmw-kubernetes.git   Set the $WORKDIR environment variable as follows:\n$ export WORKDIR=\u0026lt;workdir\u0026gt;/fmw-kubernetes/OracleUnifiedDirectory For example:\n$ export WORKDIR=/scratch/shared/OUDContainer/fmw-kubernetes/OracleUnifiedDirectory You are now ready to create the OUD deployment as per Create OUD instances.\n  "
},
{
	"uri": "/fmw-kubernetes/oudsm/prepare-your-environment/",
	"title": "Prepare Your Environment",
	"tags": [],
	"description": "Prepare your environment",
	"content": " Check the Kubernetes cluster is ready Obtain the OUDSM container image Setup the code repository to deploy OUDSM  Check the Kubernetes cluster is ready As per the Prerequisites a Kubernetes cluster should have already been configured.\n  Run the following command on the master node to check the cluster and worker nodes are running:\n$ kubectl get nodes,pods -n kube-system The output will look similar to the following:\nNAME STATUS ROLES AGE VERSION node/worker-node1 Ready \u0026lt;none\u0026gt; 17h v1.20.10 node/worker-node2 Ready \u0026lt;none\u0026gt; 17h v1.20.10 node/master-node Ready control-plane,master 23h v1.20.10 NAME READY STATUS RESTARTS AGE pod/coredns-66bff467f8-slxdq 1/1 Running 1 67d pod/coredns-66bff467f8-v77qt 1/1 Running 1 67d pod/etcd-10.89.73.42 1/1 Running 1 67d pod/kube-apiserver-10.89.73.42 1/1 Running 1 67d pod/kube-controller-manager-10.89.73.42 1/1 Running 27 67d pod/kube-flannel-ds-amd64-r2m8r 1/1 Running 2 48d pod/kube-flannel-ds-amd64-rdhrf 1/1 Running 2 6d1h pod/kube-flannel-ds-amd64-vpcbj 1/1 Running 3 66d pod/kube-proxy-jtcxm 1/1 Running 1 67d pod/kube-proxy-swfmm 1/1 Running 1 66d pod/kube-proxy-w6x6t 1/1 Running 1 66d pod/kube-scheduler-10.89.73.42 1/1 Running 29 67d   Obtain the OUDSM container image The Oracle Unified Directory Services Manager (OUDSM) Kubernetes deployment requires access to an OUDSM container image. The image can be obtained in the following ways:\n Prebuilt OUDSM container image Build your own OUDSM container image using WebLogic Image Tool  Prebuilt OUDSM container image The latest prebuilt OUDSM container image can be downloaded from Oracle Container Registry. This image is prebuilt by Oracle and includes Oracle Unified Directory Services Manager 12.2.1.4.0 and the latest PSU.\nNote: Before using this image you must login to Oracle Container Registry, navigate to Middleware \u0026gt; oudsm_cpu and accept the license agreement.\nAlternatively the same image can also be downloaded from My Oracle Support by referring to the document ID 2723908.1.\nYou can use this image in the following ways:\n Pull the container image from the Oracle Container Registry automatically during the OUDSM Kubernetes deployment. Manually pull the container image from the Oracle Container Registry or My Oracle Support, and then upload it to your own container registry. Manually pull the container image from the Oracle Container Registry or My Oracle Support and manually stage it on the master node and each worker node.  Build your own OUDSM container image using WebLogic Image Tool You can build your own OUDSM container image using the WebLogic Image Tool. This is recommended if you need to apply one off patches to a Prebuilt OUDSM container image. For more information about building your own container image with WebLogic Image Tool, see Create or update image\nYou can use an image built with WebLogic Image Tool in the following ways:\n Manually upload them to your own container registry. Manually stage them on the master node and each worker node.  Note: This documentation does not tell you how to pull or push the above images into a private container registry, or stage them on the master and worker nodes. Details of this can be found in the Enterprise Deployment Guide.\nSetup the code repository to deploy OUDSM Oracle Unified Directory Services Manager deployment on Kubernetes leverages deployment scripts provided by Oracle for creating Oracle Unified Directory Services Manager containers using the Helm charts provided. To deploy Oracle Unified Directory Services Manager on Kubernetes you should set up the deployment scripts on the master node as below:\n  Create a working directory to setup the source code.\n$ mkdir \u0026lt;workdir\u0026gt; For example:\n$ mkdir /scratch/OUDSMContainer   Download the latest OUDSM deployment scripts from the OUDSM repository:\n$ cd \u0026lt;workdir\u0026gt; $ git clone https://github.com/oracle/fmw-kubernetes.git For example:\n$ cd /scratch/OUDSMContainer $ git clone https://github.com/oracle/fmw-kubernetes.git   Set the $WORKDIR environment variable as follows:\n$ export WORKDIR=\u0026lt;workdir\u0026gt;/fmw-kubernetes/OracleUnifiedDirectorySM For example:\n$ export WORKDIR=/scratch/OUDSMContainer/fmw-kubernetes/OracleUnifiedDirectorySM You are now ready to create the OUDSM deployment as per Create OUDSM instances.\n  "
},
{
	"uri": "/fmw-kubernetes/oam/prepare-your-environment/",
	"title": "Prepare your environment",
	"tags": [],
	"description": "Sample for creating an OAM domain home on an existing PV or PVC, and the domain resource YAML file for deploying the generated OAM domain.",
	"content": "To prepare for Oracle Access Management deployment in a Kubernetes environment, complete the following steps:\n  Check the Kubernetes cluster is ready\n  Obtain the OAM container image\n  Set up the code repository to deploy OAM domains\n  Install the WebLogic Kubernetes Operator\n  Create a namespace for Oracle Access Management\n  Create a Kubernetes secret for the container registry\n  RCU schema creation\n  Preparing the environment for domain creation\na. Creating Kubernetes secrets for the domain and RCU\nb. Create a Kubernetes persistent volume and persistent volume claim\n  Check the Kubernetes cluster is ready As per the Prerequisites a Kubernetes cluster should have already been configured.\nCheck that all the nodes in the Kubernetes cluster are running.\n  Run the following command on the master node to check the cluster and worker nodes are running:\n$ kubectl get nodes,pods -n kube-system The output will look similar to the following:\nNAME STATUS ROLES AGE VERSION node/worker-node1 Ready \u0026lt;none\u0026gt; 17h v1.20.10 node/worker-node2 Ready \u0026lt;none\u0026gt; 17h v1.20.10 node/master-node Ready control-plane,master 23h v1.20.10 NAME READY STATUS RESTARTS AGE pod/coredns-66bff467f8-fnhbq 1/1 Running 0 23h pod/coredns-66bff467f8-xtc8k 1/1 Running 0 23h pod/etcd-master 1/1 Running 0 21h pod/kube-apiserver-master-node 1/1 Running 0 21h pod/kube-controller-manager-master-node 1/1 Running 0 21h pod/kube-flannel-ds-amd64-lxsfw 1/1 Running 0 17h pod/kube-flannel-ds-amd64-pqrqr 1/1 Running 0 17h pod/kube-flannel-ds-amd64-wj5nh 1/1 Running 0 17h pod/kube-proxy-2kxv2 1/1 Running 0 17h pod/kube-proxy-82vvj 1/1 Running 0 17h pod/kube-proxy-nrgw9 1/1 Running 0 23h pod/kube-scheduler-master 1/1 Running 0 21   Obtain the OAM container image The OAM Kubernetes deployment requires access to an OAM container image. The image can be obtained in the following ways:\n Prebuilt OAM container image Build your own OAM container image using WebLogic Image Tool  Prebuilt OAM container image The latest prebuilt OAM container image can be downloaded from Oracle Container Registry. This image is prebuilt by Oracle and includes Oracle Access Management 12.2.1.4.0 and the latest PSU.\nNote: Before using this image you must login to Oracle Container Registry, navigate to Middleware \u0026gt; oam_cpu and accept the license agreement.\nAlternatively the same image can also be downloaded from My Oracle Support by referring to the document ID 2723908.1.\nYou can use this image in the following ways:\n Pull the container image from the Oracle Container Registry automatically during the OAM Kubernetes deployment. Manually pull the container image from the Oracle Container Registry or My Oracle Support, and then upload it to your own container registry. Manually pull the container image from the Oracle Container Registry or My Oracle Support and manually stage it on the master node and each worker node.  Build your own OAM container image using WebLogic Image Tool You can build your own OAM container image using the WebLogic Image Tool. This is recommended if you need to apply one off patches to a Prebuilt OAM container image. For more information about building your own container image with WebLogic Image Tool, see Create or update image.\nYou can use an image built with WebLogic Image Tool in the following ways:\n Manually upload them to your own container registry. Manually stage them on the master node and each worker node.  Note: This documentation does not tell you how to pull or push the above images into a private container registry, or stage them on the master and worker nodes. Details of this can be found in the Enterprise Deployment Guide.\nSet up the code repository to deploy OAM domains OAM domain deployment on Kubernetes leverages the WebLogic Kubernetes Operator infrastructure. For deploying the OAM domains, you need to set up the deployment scripts on the master node as below:\n  Create a working directory to setup the source code.\n$ mkdir \u0026lt;workdir\u0026gt; For example:\n$ mkdir /scratch/OAMK8S   Download the latest OAM deployment scripts from the OAM repository.\n$ cd \u0026lt;workdir\u0026gt; $ git clone https://github.com/oracle/fmw-kubernetes.git For example:\n$ cd /scratch/OAMK8S $ git clone https://github.com/oracle/fmw-kubernetes.git   Set the $WORKDIR environment variable as follows:\n$ export WORKDIR=\u0026lt;workdir\u0026gt;/fmw-kubernetes/OracleAccessManagement For example:\n$ export WORKDIR=/scratch/OAMK8S/fmw-kubernetes/OracleAccessManagement   Run the following command and see if the WebLogic custom resource definition name already exists:\n$ kubectl get crd In the output you should see:\nNo resources found If you see the following:\nNAME AGE domains.weblogic.oracle 5d then run the following command to delete the existing crd:\n$ kubectl delete crd domains.weblogic.oracle customresourcedefinition.apiextensions.k8s.io \u0026#34;domains.weblogic.oracle\u0026#34; deleted   Install the WebLogic Kubernetes Operator   On the master node run the following command to create a namespace for the operator:\n$ kubectl create namespace \u0026lt;sample-kubernetes-operator-ns\u0026gt; For example:\n$ kubectl create namespace opns The output will look similar to the following:\nnamespace/opns created   Create a service account for the operator in the operator\u0026rsquo;s namespace by running the following command:\n$ kubectl create serviceaccount -n \u0026lt;sample-kubernetes-operator-ns\u0026gt; \u0026lt;sample-kubernetes-operator-sa\u0026gt; For example:\n$ kubectl create serviceaccount -n opns op-sa The output will look similar to the following:\nserviceaccount/op-sa created   Run the following helm command to install and start the operator:\n$ cd $WORKDIR $ helm install weblogic-kubernetes-operator kubernetes/charts/weblogic-operator \\ --namespace \u0026lt;sample-kubernetes-operator-ns\u0026gt; \\ --set image=ghcr.io/oracle/weblogic-kubernetes-operator:3.3.0 \\ --set serviceAccount=\u0026lt;sample-kubernetes-operator-sa\u0026gt; \\ --set “enableClusterRoleBinding=true” \\ --set \u0026#34;domainNamespaceSelectionStrategy=LabelSelector\u0026#34; \\ --set \u0026#34;domainNamespaceLabelSelector=weblogic-operator\\=enabled\u0026#34; \\ --set \u0026#34;javaLoggingLevel=FINE\u0026#34; --wait For example:\n$ cd $WORKDIR $ helm install weblogic-kubernetes-operator kubernetes/charts/weblogic-operator \\ --namespace opns \\ --set image=ghcr.io/oracle/weblogic-kubernetes-operator:3.3.0 \\ --set serviceAccount=op-sa \\ --set \u0026#34;enableClusterRoleBinding=true\u0026#34; \\ --set \u0026#34;domainNamespaceSelectionStrategy=LabelSelector\u0026#34; \\ --set \u0026#34;domainNamespaceLabelSelector=weblogic-operator\\=enabled\u0026#34; \\ --set \u0026#34;javaLoggingLevel=FINE\u0026#34; --wait The output will look similar to the following:\nNAME: weblogic-kubernetes-operator LAST DEPLOYED: Mon Mar 06 10:25:39 NAMESPACE: opns STATUS: deployed REVISION: 1 TEST SUITE: None   Verify that the operator\u0026rsquo;s pod and services are running by executing the following command:\n$ kubectl get all -n \u0026lt;sample-kubernetes-operator-ns\u0026gt; For example:\n$ kubectl get all -n opns The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE pod/weblogic-operator-676d5cc6f4-wct7b 2/2 Running 0 40s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/internal-weblogic-operator-svc ClusterIP 10.101.1.198 \u0026lt;none\u0026gt; 8082/TCP 40s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/weblogic-operator 1/1 1 1 40s NAME DESIRED CURRENT READY AGE replicaset.apps/weblogic-operator-676d5cc6f4 1 1 1 40s   Verify the operator pod\u0026rsquo;s log:\n$ kubectl logs -n \u0026lt;sample-kubernetes-operator-ns\u0026gt; -c weblogic-operator deployments/weblogic-operator For example:\n$ kubectl logs -n opns -c weblogic-operator deployments/weblogic-operator The output will look similar to the following:\n... {\u0026quot;timestamp\u0026quot;:\u0026quot;2022-03-06T10:26:10.917829423Z\u0026quot;,\u0026quot;thread\u0026quot;:13,\u0026quot;fiber\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;namespace\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;domainUID\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;level\u0026quot;:\u0026quot;CONFIG\u0026quot;,\u0026quot;class\u0026quot;:\u0026quot;oracle.kubernetes.operator.TuningParametersImpl\u0026quot;,\u0026quot;method\u0026quot;:\u0026quot;update\u0026quot;,\u0026quot;timeInMillis\u0026quot;:1635762370917,\u0026quot;message\u0026quot;:\u0026quot;Reloading tuning parameters from Operator's config map\u0026quot;,\u0026quot;exception\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;code\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;headers\u0026quot;:{},\u0026quot;body\u0026quot;:\u0026quot;\u0026quot;} {\u0026quot;timestamp\u0026quot;:\u0026quot;2022-03-06T10:26:20.920145876Z\u0026quot;,\u0026quot;thread\u0026quot;:13,\u0026quot;fiber\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;namespace\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;domainUID\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;level\u0026quot;:\u0026quot;CONFIG\u0026quot;,\u0026quot;class\u0026quot;:\u0026quot;oracle.kubernetes.operator.TuningParametersImpl\u0026quot;,\u0026quot;method\u0026quot;:\u0026quot;update\u0026quot;,\u0026quot;timeInMillis\u0026quot;:1635762380920,\u0026quot;message\u0026quot;:\u0026quot;Reloading tuning parameters from Operator's config map\u0026quot;,\u0026quot;exception\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;code\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;headers\u0026quot;:{},\u0026quot;body\u0026quot;:\u0026quot;\u0026quot;} {\u0026quot;timestamp\u0026quot;:\u0026quot;2022-03-06T10:26:30.922360564Z\u0026quot;,\u0026quot;thread\u0026quot;:19,\u0026quot;fiber\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;namespace\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;domainUID\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;level\u0026quot;:\u0026quot;CONFIG\u0026quot;,\u0026quot;class\u0026quot;:\u0026quot;oracle.kubernetes.operator.TuningParametersImpl\u0026quot;,\u0026quot;method\u0026quot;:\u0026quot;update\u0026quot;,\u0026quot;timeInMillis\u0026quot;:1635762390922,\u0026quot;message\u0026quot;:\u0026quot;Reloading tuning parameters from Operator's config map\u0026quot;,\u0026quot;exception\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;code\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;headers\u0026quot;:{},\u0026quot;body\u0026quot;:\u0026quot;\u0026quot;} {\u0026quot;timestamp\u0026quot;:\u0026quot;2022-03-06T10:26:40.924847211Z\u0026quot;,\u0026quot;thread\u0026quot;:29,\u0026quot;fiber\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;namespace\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;domainUID\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;level\u0026quot;:\u0026quot;CONFIG\u0026quot;,\u0026quot;class\u0026quot;:\u0026quot;oracle.kubernetes.operator.TuningParametersImpl\u0026quot;,\u0026quot;method\u0026quot;:\u0026quot;update\u0026quot;,\u0026quot;timeInMillis\u0026quot;:1635762400924,\u0026quot;message\u0026quot;:\u0026quot;Reloading tuning parameters from Operator's config map\u0026quot;,\u0026quot;exception\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;code\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;headers\u0026quot;:{},\u0026quot;body\u0026quot;:\u0026quot;\u0026quot;}   Create a namespace for Oracle Access Management   Run the following command to create a namespace for the domain:\n$ kubectl create namespace \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl create namespace oamns The output will look similar to the following:\nnamespace/oamns created   Run the following command to tag the namespace so the WebLogic Kubernetes Operator can manage it:\n$ kubectl label namespaces \u0026lt;domain_namespace\u0026gt; weblogic-operator=enabled For example:\n$ kubectl label namespaces oamns weblogic-operator=enabled The output will look similar to the following:\nnamespace/oamns labeled   Run the following command to check the label was created:\n$ kubectl describe namespace \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl describe namespace oamns The output will look similar to the following:\nName: oamns Labels: weblogic-operator=enabled Annotations: \u0026lt;none\u0026gt; Status: Active No resource quota. No LimitRange resource.   Create a Kubernetes secret for the container registry In this section you create a secret that stores the credentials for the container registry where the OAM image is stored. This step must be followed if using Oracle Container Registry or your own private registry. If you are not using a container registry and have loaded the images on each of the master and worker nodes, you can skip this step.\n  Run the following command to create the secret:\nkubectl create secret docker-registry \u0026#34;orclcred\u0026#34; --docker-server=\u0026lt;CONTAINER_REGISTRY\u0026gt; \\ --docker-username=\u0026#34;\u0026lt;USER_NAME\u0026gt;\u0026#34; \\ --docker-password=\u0026lt;PASSWORD\u0026gt; --docker-email=\u0026lt;EMAIL_ID\u0026gt; \\ --namespace=\u0026lt;domain_namespace\u0026gt; For example, if using Oracle Container Registry:\nkubectl create secret docker-registry \u0026#34;orclcred\u0026#34; --docker-server=container-registry.oracle.com \\ --docker-username=\u0026#34;user@example.com\u0026#34; \\ --docker-password=password --docker-email=user@example.com \\ --namespace=oamns Replace \u0026lt;USER_NAME\u0026gt; and \u0026lt;PASSWORD\u0026gt; with the credentials for the registry with the following caveats:\n  If using Oracle Container Registry to pull the OAM container image, this is the username and password used to login to Oracle Container Registry. Before you can use this image you must login to Oracle Container Registry, navigate to Middleware \u0026gt; oam_cpu and accept the license agreement.\n  If using your own container registry to store the OAM container image, this is the username and password (or token) for your container registry.\n  The output will look similar to the following:\nsecret/orclcred created   RCU schema creation In this section you create the RCU schemas in the Oracle Database.\nBefore following the steps in this section, make sure that the database and listener are up and running and you can connect to the database via SQL*Plus or other client tool.\n  If using Oracle Container Registry or your own container registry for your OAM container image, run the following command to create a helper pod to run RCU:\n$ kubectl run --image=\u0026lt;image_name-from-registry\u0026gt;:\u0026lt;tag\u0026gt; --image-pull-policy=\u0026#34;IfNotPresent\u0026#34; --overrides=\u0026#39;{\u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;spec\u0026#34;:{\u0026#34;imagePullSecrets\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;orclcred\u0026#34;}]}}\u0026#39; helper -n \u0026lt;domain_namespace\u0026gt; -- sleep infinity For example:\n$ kubectl run --image=container-registry.oracle.com/middleware/oam_cpu:12.2.1.4-jdk8-ol7-220119.2059 --image-pull-policy=\u0026#34;IfNotPresent\u0026#34; --overrides=\u0026#39;{\u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;,\u0026#34;spec\u0026#34;:{\u0026#34;imagePullSecrets\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;orclcred\u0026#34;}]}}\u0026#39; helper -n oamns -- sleep infinity If you are not using a container registry and have loaded the image on each of the master and worker nodes, run the following command:\n$ kubectl run helper --image \u0026lt;image\u0026gt;:\u0026lt;tag\u0026gt; -n oamns -- sleep infinity For example:\n$ kubectl run helper --image oracle/oam:12.2.1.4-jdk8-ol7-220119.2059 -n oamns -- sleep infinity The output will look similar to the following:\npod/helper created   Run the following command to check the pod is running:\n$ kubectl get pods -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get pods -n oamns The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE helper 1/1 Running 0 3m Note: If you are pulling the image from a container registry it may take several minutes before the pod has a STATUS of 1\\1. While the pod is starting you can check the status of the pod, by running the following command:\n$ kubectl describe pod helper -n oamns   Run the following command to start a bash shell in the helper pod:\n$ kubectl exec -it helper -n \u0026lt;domain_namespace\u0026gt; -- /bin/bash For example:\n$ kubectl exec -it helper -n oamns -- /bin/bash This will take you into a bash shell in the running helper pod:\n[oracle@helper ~]$   In the helper bash shell run the following commands to set the environment:\n[oracle@helper ~]$ export CONNECTION_STRING=\u0026lt;db_host.domain\u0026gt;:\u0026lt;db_port\u0026gt;/\u0026lt;service_name\u0026gt; [oracle@helper ~]$ export RCUPREFIX=\u0026lt;rcu_schema_prefix\u0026gt; [oracle@helper ~]$ echo -e \u0026lt;db_pwd\u0026gt;\u0026#34;\\n\u0026#34;\u0026lt;rcu_schema_pwd\u0026gt; \u0026gt; /tmp/pwd.txt [oracle@helper ~]$ cat /tmp/pwd.txt where:\n\u0026lt;db_host.domain\u0026gt;:\u0026lt;db_port\u0026gt;/\u0026lt;service_name\u0026gt;\tis your database connect string\n\u0026lt;rcu_schema_prefix\u0026gt; is the RCU schema prefix you want to set\n\u0026lt;db_pwd\u0026gt; is the SYS password for the database\n\u0026lt;rcu_schema_pwd\u0026gt; is the password you want to set for the \u0026lt;rcu_schema_prefix\u0026gt;\nFor example:\n[oracle@helper ~]$ export CONNECTION_STRING=mydatabasehost.example.com:1521/orcl.example.com [oracle@helper ~]$ export RCUPREFIX=OAMK8S [oracle@helper ~]$ echo -e \u0026lt;password\u0026gt;\u0026#34;\\n\u0026#34;\u0026lt;password\u0026gt; \u0026gt; /tmp/pwd.txt [oracle@helper ~]$ cat /tmp/pwd.txt \u0026lt;password\u0026gt; \u0026lt;password\u0026gt;   In the helper bash shell run the following command to create the RCU schemas in the database:\n$ [oracle@helper ~]$ /u01/oracle/oracle_common/bin/rcu -silent -createRepository -databaseType ORACLE -connectString \\ $CONNECTION_STRING -dbUser sys -dbRole sysdba -useSamePasswordForAllSchemaUsers true \\ -selectDependentsForComponents true -schemaPrefix $RCUPREFIX -component MDS -component IAU \\ -component IAU_APPEND -component IAU_VIEWER -component OPSS -component WLS -component STB -component OAM -f \u0026lt; /tmp/pwd.txt The output will look similar to the following:\nRCU Logfile: /tmp/RCU2022-03-06_10-29_561898106/logs/rcu.log Processing command line .... Repository Creation Utility - Checking Prerequisites Checking Global Prerequisites Repository Creation Utility - Checking Prerequisites Checking Component Prerequisites Repository Creation Utility - Creating Tablespaces Validating and Creating Tablespaces Create tablespaces in the repository database Repository Creation Utility - Create Repository Create in progress. Executing pre create operations Percent Complete: 18 Percent Complete: 18 Percent Complete: 19 Percent Complete: 20 Percent Complete: 21 Percent Complete: 21 Percent Complete: 22 Percent Complete: 22 Creating Common Infrastructure Services(STB) Percent Complete: 30 Percent Complete: 30 Percent Complete: 39 Percent Complete: 39 Percent Complete: 39 Creating Audit Services Append(IAU_APPEND) Percent Complete: 46 Percent Complete: 46 Percent Complete: 55 Percent Complete: 55 Percent Complete: 55 Creating Audit Services Viewer(IAU_VIEWER) Percent Complete: 62 Percent Complete: 62 Percent Complete: 63 Percent Complete: 63 Percent Complete: 64 Percent Complete: 64 Creating Metadata Services(MDS) Percent Complete: 73 Percent Complete: 73 Percent Complete: 73 Percent Complete: 74 Percent Complete: 74 Percent Complete: 75 Percent Complete: 75 Percent Complete: 75 Creating Weblogic Services(WLS) Percent Complete: 80 Percent Complete: 80 Percent Complete: 83 Percent Complete: 83 Percent Complete: 91 Percent Complete: 98 Percent Complete: 98 Creating Audit Services(IAU) Percent Complete: 100 Creating Oracle Platform Security Services(OPSS) Creating Oracle Access Manager(OAM) Executing post create operations Repository Creation Utility: Create - Completion Summary Database details: ----------------------------- Host Name : mydatabasehost.example.com Port : 1521 Service Name : ORCL.EXAMPLE.COM Connected As : sys Prefix for (prefixable) Schema Owners : OAMK8S RCU Logfile : /tmp/RCU2022-03-06_10-29_561898106/logs/rcu.log Component schemas created: ----------------------------- Component Status Logfile Common Infrastructure Services Success /tmp/RCU2022-03-06_10-29_561898106/logs/stb.log Oracle Platform Security Services Success /tmp/RCU2022-03-06_10-29_561898106/logs/opss.log Oracle Access Manager Success /tmp/RCU2022-03-06_10-29_561898106/logs/oam.log Audit Services Success /tmp/RCU2022-03-06_10-29_561898106/logs/iau.log Audit Services Append Success /tmp/RCU2022-03-06_10-29_561898106/logs/iau_append.log Audit Services Viewer Success /tmp/RCU2022-03-06_10-29_561898106/logs/iau_viewer.log Metadata Services Success /tmp/RCU2022-03-06_10-29_561898106/logs/mds.log WebLogic Services Success /tmp/RCU2022-03-06_10-29_561898106/logs/wls.log Repository Creation Utility - Create : Operation Completed [oracle@helper ~]$   Exit the helper bash shell by issuing the command exit.\n  Preparing the environment for domain creation In this section you prepare the environment for the OAM domain creation. This involves the following steps:\na. Creating Kubernetes secrets for the domain and RCU\nb. Create a Kubernetes persistent volume and persistent volume claim\nCreating Kubernetes secrets for the domain and RCU   Create a Kubernetes secret for the domain using the create-weblogic-credentials script in the same Kubernetes namespace as the domain:\n$ cd $WORKDIR/kubernetes/create-weblogic-domain-credentials $ ./create-weblogic-credentials.sh -u weblogic -p \u0026lt;pwd\u0026gt; -n \u0026lt;domain_namespace\u0026gt; -d \u0026lt;domain_uid\u0026gt; -s \u0026lt;kubernetes_domain_secret\u0026gt; where:\n-u weblogic is the WebLogic username\n-p \u0026lt;pwd\u0026gt; is the password for the weblogic user\n-n \u0026lt;domain_namespace\u0026gt; is the domain namespace\n-d \u0026lt;domain_uid\u0026gt; is the domain UID to be created. The default is domain1 if not specified\n-s \u0026lt;kubernetes_domain_secret\u0026gt; is the name you want to create for the secret for this namespace. The default is to use the domainUID if not specified\nFor example:\n$ cd $WORKDIR/kubernetes/create-weblogic-domain-credentials $ ./create-weblogic-credentials.sh -u weblogic -p \u0026lt;password\u0026gt; -n oamns -d accessdomain -s accessdomain-credentials The output will look similar to the following:\nsecret/accessdomain-credentials created secret/accessdomain-credentials labeled The secret accessdomain-credentials has been successfully created in the oamns namespace.   Verify the secret is created using the following command:\n$ kubectl get secret \u0026lt;kubernetes_domain_secret\u0026gt; -o yaml -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get secret accessdomain-credentials -o yaml -n oamns The output will look similar to the following:\napiVersion: v1 data: password: V2VsY29tZTE= username: d2VibG9naWM= kind: Secret metadata: creationTimestamp: \u0026quot;2022-03-06T10:41:11Z\u0026quot; labels: weblogic.domainName: accessdomain weblogic.domainUID: accessdomain name: accessdomain-credentials namespace: oamns resourceVersion: \u0026quot;2913144\u0026quot; uid: 5f8d9874-9cd7-42be-af4b-54f787e71ac2 type: Opaque   Create a Kubernetes secret for RCU using the create-weblogic-credentials script in the same Kubernetes namespace as the domain:\n$ cd $WORKDIR/kubernetes/create-rcu-credentials $ ./create-rcu-credentials.sh -u \u0026lt;rcu_prefix\u0026gt; -p \u0026lt;rcu_schema_pwd\u0026gt; -a sys -q \u0026lt;sys_db_pwd\u0026gt; -d \u0026lt;domain_uid\u0026gt; -n \u0026lt;domain_namespace\u0026gt; -s \u0026lt;kubernetes_rcu_secret\u0026gt; where:\n-u \u0026lt;rcu_prefix\u0026gt; is the name of the RCU schema prefix created previously\n-p \u0026lt;rcu_schema_pwd\u0026gt; is the password for the RCU schema prefix\n-q \u0026lt;sys_db_pwd\u0026gt; is the sys database password\n-d \u0026lt;domain_uid\u0026gt; is the domain_uid that you created earlier\n-n \u0026lt;domain_namespace\u0026gt; is the domain namespace\n-s \u0026lt;kubernetes_rcu_secret\u0026gt; is the name of the rcu secret to create\nFor example:\n$ cd $WORKDIR/kubernetes/create-rcu-credentials $ ./create-rcu-credentials.sh -u OAMK8S -p \u0026lt;password\u0026gt; -a sys -q \u0026lt;password\u0026gt; -d accessdomain -n oamns -s accessdomain-rcu-credentials The output will look similar to the following:\nsecret/accessdomain-rcu-credentials created secret/accessdomain-rcu-credentials labeled The secret accessdomain-rcu-credentials has been successfully created in the oamns namespace.   Verify the secret is created using the following command:\n$ kubectl get secret \u0026lt;kubernetes_rcu_secret\u0026gt; -o yaml -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get secret accessdomain-rcu-credentials -o yaml -n oamns The output will look similar to the following:\napiVersion: v1 data: password: T3JhY2xlXzEyMw== sys_password: T3JhY2xlXzEyMw== sys_username: c3lz username: T0FNSzhT kind: Secret metadata: creationTimestamp: \u0026quot;2022-03-06T10:50:34Z\u0026quot; labels: weblogic.domainName: accessdomain weblogic.domainUID: accessdomain name: accessdomain-rcu-credentials namespace: oamns resourceVersion: \u0026quot;2913938\u0026quot; uid: 3798af1b-2783-415f-aea8-31e0610220a7 type: Opaque   Create a Kubernetes persistent volume and persistent volume claim As referenced in Prerequisites the nodes in the Kubernetes cluster must have access to a persistent volume such as a Network File System (NFS) mount or a shared file system.\nA persistent volume is the same as a disk mount but is inside a container. A Kubernetes persistent volume is an arbitrary name (determined in this case, by Oracle) that is mapped to a physical volume on a disk.\nWhen a container is started, it needs to mount that volume. The physical volume should be on a shared disk accessible by all the Kubernetes worker nodes because it is not known on which worker node the container will be started. In the case of Identity and Access Management, the persistent volume does not get erased when a container stops. This enables persistent configurations.\nThe example below uses an NFS mounted volume (\u0026lt;persistent_volume\u0026gt;/accessdomainpv). Other volume types can also be used. See the official Kubernetes documentation for Volumes.\nNote: The persistent volume directory needs to be accessible to both the master and worker node(s). Make sure this path has full access permissions, and that the folder is empty. In this example /scratch/shared/accessdomainpv is accessible from all nodes via NFS.\nTo create a Kubernetes persistent volume, perform the following steps:\n  Make a backup copy of the create-pv-pvc-inputs.yaml file and create required directories:\n$ cd $WORKDIR/kubernetes/create-weblogic-domain-pv-pvc $ cp create-pv-pvc-inputs.yaml create-pv-pvc-inputs.yaml.orig $ mkdir output $ mkdir -p \u0026lt;persistent_volume\u0026gt;/accessdomainpv $ chmod -R 777 \u0026lt;persistent_volume\u0026gt;/accessdomainpv For example:\n$ cd $WORKDIR/kubernetes/create-weblogic-domain-pv-pvc $ cp create-pv-pvc-inputs.yaml create-pv-pvc-inputs.yaml.orig $ mkdir output $ mkdir -p /scratch/shared/accessdomainpv $ chmod -R 777 /scratch/shared/accessdomainpv   On the master node run the following command to ensure it is possible to read and write to the persistent volume:\ncd \u0026lt;persistent_volume\u0026gt;/accessdomainpv touch filemaster.txt ls filemaster.txt For example:\ncd /scratch/shared/accessdomainpv touch filemaster.txt ls filemaster.txt On the first worker node run the following to ensure it is possible to read and write to the persistent volume:\ncd /scratch/shared/accessdomainpv ls filemaster.txt touch fileworker1.txt ls fileworker1.txt Repeat the above for any other worker nodes e.g fileworker2.txt etc. Once proven that it\u0026rsquo;s possible to read and write from each node to the persistent volume, delete the files created.\n  Navigate to $WORKDIR/kubernetes/create-weblogic-domain-pv-pvc:\n$ cd $WORKDIR/kubernetes/create-weblogic-domain-pv-pvc and edit the create-pv-pvc-inputs.yaml file and update the following parameters to reflect your settings. Save the file when complete:\nbaseName: \u0026lt;domain\u0026gt; domainUID: \u0026lt;domain_uid\u0026gt; namespace: \u0026lt;domain_namespace\u0026gt; weblogicDomainStorageType: NFS weblogicDomainStorageNFSServer: \u0026lt;nfs_server\u0026gt; weblogicDomainStoragePath: \u0026lt;physical_path_of_persistent_storage\u0026gt; weblogicDomainStorageSize: 10Gi For example:\n\t# The base name of the pv and pvc baseName: domain # Unique ID identifying a domain. # If left empty, the generated pv can be shared by multiple domains # This ID must not contain an underscope (\u0026quot;_\u0026quot;), and must be lowercase and unique across all domains in a Kubernetes cluster. domainUID: accessdomain # Name of the namespace for the persistent volume claim namespace: oamns ... # Persistent volume type for the persistent storage. # The value must be 'HOST_PATH' or 'NFS'. # If using 'NFS', weblogicDomainStorageNFSServer must be specified. weblogicDomainStorageType: NFS # The server name or ip address of the NFS server to use for the persistent storage. # The following line must be uncomment and customized if weblogicDomainStorateType is NFS: weblogicDomainStorageNFSServer: mynfsserver # Physical path of the persistent storage. # When weblogicDomainStorageType is set to HOST_PATH, this value should be set the to path to the # domain storage on the Kubernetes host. # When weblogicDomainStorageType is set to NFS, then weblogicDomainStorageNFSServer should be set # to the IP address or name of the DNS server, and this value should be set to the exported path # on that server. # Note that the path where the domain is mounted in the WebLogic containers is not affected by this # setting, that is determined when you create your domain. # The following line must be uncomment and customized: weblogicDomainStoragePath: /scratch/shared/accessdomainpv # Reclaim policy of the persistent storage # The valid values are: 'Retain', 'Delete', and 'Recycle' weblogicDomainStorageReclaimPolicy: Retain # Total storage allocated to the persistent storage. weblogicDomainStorageSize: 10Gi   Execute the create-pv-pvc.sh script to create the PV and PVC configuration files:\n$ ./create-pv-pvc.sh -i create-pv-pvc-inputs.yaml -o output The output will be similar to the following:\nInput parameters being used export version=\u0026quot;create-weblogic-sample-domain-pv-pvc-inputs-v1\u0026quot; export baseName=\u0026quot;domain\u0026quot; export domainUID=\u0026quot;accessdomain\u0026quot; export namespace=\u0026quot;oamns\u0026quot; export weblogicDomainStorageType=\u0026quot;NFS\u0026quot; export weblogicDomainStorageNFSServer=\u0026quot;mynfsserver\u0026quot; export weblogicDomainStoragePath=\u0026quot;/scratch/shared/accessdomainpv\u0026quot; export weblogicDomainStorageReclaimPolicy=\u0026quot;Retain\u0026quot; export weblogicDomainStorageSize=\u0026quot;10Gi\u0026quot; Generating output/pv-pvcs/accessdomain-domain-pv.yaml Generating output/pv-pvcs/accessdomain-domain-pvc.yaml The following files were generated: output/pv-pvcs/accessdomain-domain-pv.yaml.yaml output/pv-pvcs/accessdomain-domain-pvc.yaml   Run the following to show the files are created:\n$ ls output/pv-pvcs accessdomain-domain-pv.yaml accessdomain-domain-pvc.yaml create-pv-pvc-inputs.yaml   Run the following kubectl command to create the PV and PVC in the domain namespace:\n$ kubectl create -f output/pv-pvcs/accessdomain-domain-pv.yaml -n \u0026lt;domain_namespace\u0026gt; $ kubectl create -f output/pv-pvcs/accessdomain-domain-pvc.yaml -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl create -f output/pv-pvcs/accessdomain-domain-pv.yaml -n oamns $ kubectl create -f output/pv-pvcs/accessdomain-domain-pvc.yaml -n oamns The output will look similar to the following:\npersistentvolume/accessdomain-domain-pv created persistentvolumeclaim/accessdomain-domain-pvc created   Run the following commands to verify the PV and PVC were created successfully:\n$ kubectl describe pv \u0026lt;pv_name\u0026gt; $ kubectl describe pvc \u0026lt;pvc_name\u0026gt; -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl describe pv accessdomain-domain-pv $ kubectl describe pvc accessdomain-domain-pvc -n oamns The output will look similar to the following:\n$ kubectl describe pv accessdomain-domain-pv Name: accessdomain-domain-pv Labels: weblogic.domainUID=accessdomain Annotations: pv.kubernetes.io/bound-by-controller: yes Finalizers: [kubernetes.io/pv-protection] StorageClass: accessdomain-domain-storage-class Status: Bound Claim: oamns/accessdomain-domain-pvc Reclaim Policy: Retain Access Modes: RWX VolumeMode: Filesystem Capacity: 10Gi Node Affinity: \u0026lt;none\u0026gt; Message: Source: Type: NFS (an NFS mount that lasts the lifetime of a pod) Server: mynfsserver Path: /scratch/shared/accessdomainpv ReadOnly: false Events: \u0026lt;none\u0026gt; $ kubectl describe pvc accessdomain-domain-pvc -n oamns Name: accessdomain-domain-pvc Namespace: oamns StorageClass: accessdomain-domain-storage-class Status: Bound Volume: accessdomain-domain-pv Labels: weblogic.domainUID=accessdomain Annotations: pv.kubernetes.io/bind-completed: yes pv.kubernetes.io/bound-by-controller: yes Finalizers: [kubernetes.io/pvc-protection] Capacity: 10Gi Access Modes: RWX VolumeMode: Filesystem Events: \u0026lt;none\u0026gt; Mounted By: \u0026lt;none\u0026gt; You are now ready to create the OAM domain as per Create OAM Domains\n  "
},
{
	"uri": "/fmw-kubernetes/oig/prepare-your-environment/",
	"title": "Prepare your environment",
	"tags": [],
	"description": "Preparation to deploy OIG on Kubernetes",
	"content": "To prepare for Oracle Identity Governance deployment in a Kubernetes environment, complete the following steps:\n  Check the Kubernetes cluster is ready\n  Obtain the OIG container image\n  Setup the code repository to deploy OIG domains\n  Install the WebLogic Kubernetes Operator\n  Create a namespace for Oracle Identity Governance\n  Create a Kubernetes secret for the container registry\n  RCU schema creation\n  Preparing the environment for domain creation\na. Creating Kubernetes secrets for the domain and RCU\nb. Create a Kubernetes persistent volume and persistent volume claim\n  Check the Kubernetes cluster is ready As per the Prerequisites a Kubernetes cluster should have already been configured.\n  Run the following command on the master node to check the cluster and worker nodes are running:\n$ kubectl get nodes,pods -n kube-system The output will look similar to the following:\nNAME STATUS ROLES AGE VERSION node/worker-node1 Ready \u0026lt;none\u0026gt; 17h v1.20.10 node/worker-node2 Ready \u0026lt;none\u0026gt; 17h v1.20.10 node/master-node Ready master 23h v1.20.10 NAME READY STATUS RESTARTS AGE pod/coredns-66bff467f8-fnhbq 1/1 Running 0 23h pod/coredns-66bff467f8-xtc8k 1/1 Running 0 23h pod/etcd-master 1/1 Running 0 21h pod/kube-apiserver-master-node 1/1 Running 0 21h pod/kube-controller-manager-master-node 1/1 Running 0 21h pod/kube-flannel-ds-amd64-lxsfw 1/1 Running 0 17h pod/kube-flannel-ds-amd64-pqrqr 1/1 Running 0 17h pod/kube-flannel-ds-amd64-wj5nh 1/1 Running 0 17h pod/kube-proxy-2kxv2 1/1 Running 0 17h pod/kube-proxy-82vvj 1/1 Running 0 17h pod/kube-proxy-nrgw9 1/1 Running 0 23h pod/kube-scheduler-master 1/1 Running 0 21$   Obtain the OIG container image The OIG Kubernetes deployment requires access to an OIG container image. The image can be obtained in the following ways:\n Prebuilt OIG container image Build your own OIG container image using WebLogic Image Tool  Prebuilt OIG container image The latest prebuilt OIG container image can be downloaded from Oracle Container Registry. This image is prebuilt by Oracle and includes Oracle Identity Governance 12.2.1.4.0 and the latest PSU.\nNote: Before using this image you must login to Oracle Container Registry, navigate to Middleware \u0026gt; oig_cpu and accept the license agreement.\nAlternatively the same image can also be downloaded from My Oracle Support by referring to the document ID 2723908.1.\nYou can use this image in the following ways:\n Pull the container image from the Oracle Container Registry automatically during the OIG Kubernetes deployment. Manually pull the container image from the Oracle Container Registry or My Oracle Support, and then upload it to your own container registry. Manually pull the container image from the Oracle Container Registry or My Oracle Support and manually stage it on the master node and each worker node.  Build your own OIG container image using WebLogic Image Tool You can build your own OIG container image using the WebLogic Image Tool. This is recommended if you need to apply one off patches to a Prebuilt OIG container image. For more information about building your own container image with WebLogic Image Tool, see Create or update image.\nYou can use an image built with WebLogic Image Tool in the following ways:\n Manually upload them to your own container registry. Manually stage them on the master node and each worker node.  Note: This documentation does not tell you how to pull or push the above images into a private container registry, or stage them on the master and worker nodes. Details of this can be found in the Enterprise Deployment Guide.\nSetup the code repository to deploy OIG domains Oracle Identity Governance domain deployment on Kubernetes leverages the WebLogic Kubernetes Operator infrastructure. For deploying the OIG domains, you need to set up the deployment scripts on the master node as below:\n  Create a working directory to setup the source code.\n$ mkdir \u0026lt;workdir\u0026gt; For example:\n$ mkdir /scratch/OIGK8S   Download the latest OIG deployment scripts from the OIG repository.\n$ cd \u0026lt;workdir\u0026gt; $ git clone https://github.com/oracle/fmw-kubernetes.git For example:\n$ cd /scratch/OIGK8S $ git clone https://github.com/oracle/fmw-kubernetes.git   Set the $WORKDIR environment variable as follows:\n$ export WORKDIR=\u0026lt;workdir\u0026gt;/fmw-kubernetes/OracleIdentityGovernance For example:\n$ export WORKDIR=/scratch/OIGK8S/fmw-kubernetes/OracleIdentityGovernance   Run the following command and see if the WebLogic custom resource definition name already exists:\n$ kubectl get crd In the output you should see:\nNo resources found in default namespace. If you see the following:\nNAME AGE domains.weblogic.oracle 5d then run the following command to delete the existing crd:\n$ kubectl delete crd domains.weblogic.oracle customresourcedefinition.apiextensions.k8s.io \u0026#34;domains.weblogic.oracle\u0026#34; deleted   Install the WebLogic Kubernetes Operator   On the master node run the following command to create a namespace for the operator:\n$ kubectl create namespace \u0026lt;sample-kubernetes-operator-ns\u0026gt; For example:\n$ kubectl create namespace opns The output will look similar to the following:\nnamespace/opns created   Create a service account for the operator in the operator\u0026rsquo;s namespace by running the following command:\n$ kubectl create serviceaccount -n \u0026lt;sample-kubernetes-operator-ns\u0026gt; \u0026lt;sample-kubernetes-operator-sa\u0026gt; For example:\n$ kubectl create serviceaccount -n opns op-sa The output will look similar to the following:\nserviceaccount/op-sa created   Run the following helm command to install and start the operator:\n$ cd $WORKDIR $ helm install weblogic-kubernetes-operator kubernetes/charts/weblogic-operator \\ --namespace \u0026lt;sample-kubernetes-operator-ns\u0026gt; \\ --set image=ghcr.io/oracle/weblogic-kubernetes-operator:3.3.0 \\ --set serviceAccount=\u0026lt;sample-kubernetes-operator-sa\u0026gt; \\ --set “enableClusterRoleBinding=true” \\ --set \u0026#34;domainNamespaceSelectionStrategy=LabelSelector\u0026#34; \\ --set \u0026#34;domainNamespaceLabelSelector=weblogic-operator\\=enabled\u0026#34; \\ --set \u0026#34;javaLoggingLevel=FINE\u0026#34; --wait For example:\n$ cd $WORKDIR $ helm install weblogic-kubernetes-operator kubernetes/charts/weblogic-operator \\ --namespace opns \\ --set image=ghcr.io/oracle/weblogic-kubernetes-operator:3.3.0 \\ --set serviceAccount=op-sa \\ --set \u0026#34;enableClusterRoleBinding=true\u0026#34; \\ --set \u0026#34;domainNamespaceSelectionStrategy=LabelSelector\u0026#34; \\ --set \u0026#34;domainNamespaceLabelSelector=weblogic-operator\\=enabled\u0026#34; \\ --set \u0026#34;javaLoggingLevel=FINE\u0026#34; --wait The output will look similar to the following:\nNAME: weblogic-kubernetes-operator LAST DEPLOYED: Wed Mar 9 11:51:37 2022 NAMESPACE: opns STATUS: deployed REVISION: 1 TEST SUITE: None   Verify that the operator\u0026rsquo;s pod and services are running by executing the following command:\n$ kubectl get all -n \u0026lt;sample-kubernetes-operator-ns\u0026gt; For example:\n$ kubectl get all -n opns The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE pod/weblogic-operator-676d5cc6f4-rwzxf 2/2 Running 0 59s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/internal-weblogic-operator-svc ClusterIP 10.102.7.232 \u0026lt;none\u0026gt; 8082/TCP 59s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/weblogic-operator 1/1 1 1 59s NAME DESIRED CURRENT READY AGE replicaset.apps/weblogic-operator-676d5cc6f4 1 1 1 59s   Verify the operator pod\u0026rsquo;s log:\n$ kubectl logs -n \u0026lt;sample-kubernetes-operator-ns\u0026gt; -c weblogic-operator deployments/weblogic-operator For example:\n$ kubectl logs -n opns -c weblogic-operator deployments/weblogic-operator The output will look similar to the following:\n{\u0026quot;timestamp\u0026quot;:\u0026quot;2022-03-09T11:52:53.167756673Z\u0026quot;,\u0026quot;thread\u0026quot;:23,\u0026quot;fiber\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;namespace\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;domainUID\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;level\u0026quot;:\u0026quot;CONFIG\u0026quot;,\u0026quot;class\u0026quot;:\u0026quot;oracle.kubernetes.operator.TuningParametersImpl\u0026quot;,\u0026quot;method\u0026quot;:\u0026quot;update\u0026quot;,\u0026quot;timeInMillis\u0026quot;:1636650293167,\u0026quot;message\u0026quot;:\u0026quot;Reloading tuning parameters from Operator's config map\u0026quot;,\u0026quot;exception\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;code\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;headers\u0026quot;:{},\u0026quot;body\u0026quot;:\u0026quot;\u0026quot;} {\u0026quot;timestamp\u0026quot;:\u0026quot;2022-03-09T11:53:03.170083172Z\u0026quot;,\u0026quot;thread\u0026quot;:30,\u0026quot;fiber\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;namespace\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;domainUID\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;level\u0026quot;:\u0026quot;CONFIG\u0026quot;,\u0026quot;class\u0026quot;:\u0026quot;oracle.kubernetes.operator.TuningParametersImpl\u0026quot;,\u0026quot;method\u0026quot;:\u0026quot;update\u0026quot;,\u0026quot;timeInMillis\u0026quot;:1636650303170,\u0026quot;message\u0026quot;:\u0026quot;Reloading tuning parameters from Operator's config map\u0026quot;,\u0026quot;exception\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;code\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;headers\u0026quot;:{},\u0026quot;body\u0026quot;:\u0026quot;\u0026quot;} {\u0026quot;timestamp\u0026quot;:\u0026quot;2022-03-09T11:52:13.172302644Z\u0026quot;,\u0026quot;thread\u0026quot;:29,\u0026quot;fiber\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;namespace\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;domainUID\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;level\u0026quot;:\u0026quot;CONFIG\u0026quot;,\u0026quot;class\u0026quot;:\u0026quot;oracle.kubernetes.operator.TuningParametersImpl\u0026quot;,\u0026quot;method\u0026quot;:\u0026quot;update\u0026quot;,\u0026quot;timeInMillis\u0026quot;:1636650313172,\u0026quot;message\u0026quot;:\u0026quot;Reloading tuning parameters from Operator's config map\u0026quot;,\u0026quot;exception\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;code\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;headers\u0026quot;:{},\u0026quot;body\u0026quot;:\u0026quot;\u0026quot;}   Create a namespace for Oracle Identity Governance   Run the following command to create a namespace for the domain:\n$ kubectl create namespace \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl create namespace oigns The output will look similar to the following:\nnamespace/oigns created   Run the following command to tag the namespace so the WebLogic Kubernetes Operator can manage it:\n$ kubectl label namespaces \u0026lt;domain_namespace\u0026gt; weblogic-operator=enabled For example:\n$ kubectl label namespaces oigns weblogic-operator=enabled The output will look similar to the following:\nnamespace/oigns labeled   Run the following command to check the label was created:\n$ kubectl describe namespace \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl describe namespace oigns The output will look similar to the following:\nName: oigns Labels: weblogic-operator=enabled Annotations: \u0026lt;none\u0026gt; Status: Active No resource quota. No LimitRange resource.   Create a Kubernetes secret for the container registry In this section you create a secret that stores the credentials for the container registry where the OIG image is stored. This step must be followed if using Oracle Container Registry or your own private registry. If you are not using a container registry and have loaded the images on each of the master and worker nodes, you can skip this step.\n  Run the following command to create the secret:\nkubectl create secret docker-registry \u0026#34;orclcred\u0026#34; --docker-server=\u0026lt;CONTAINER_REGISTRY\u0026gt; \\ --docker-username=\u0026#34;\u0026lt;USER_NAME\u0026gt;\u0026#34; \\ --docker-password=\u0026lt;PASSWORD\u0026gt; --docker-email=\u0026lt;EMAIL_ID\u0026gt; \\ --namespace=\u0026lt;domain_namespace\u0026gt; For example, if using Oracle Container Registry:\nkubectl create secret docker-registry \u0026#34;orclcred\u0026#34; --docker-server=container-registry.oracle.com \\ --docker-username=\u0026#34;user@example.com\u0026#34; \\ --docker-password=password --docker-email=user@example.com \\ --namespace=oigns Replace \u0026lt;USER_NAME\u0026gt; and \u0026lt;PASSWORD\u0026gt; with the credentials for the registry with the following caveats:\n  If using Oracle Container Registry to pull the OIG container image, this is the username and password used to login to Oracle Container Registry. Before you can use this image you must login to Oracle Container Registry, navigate to Middleware \u0026gt; oig_cpu and accept the license agreement.\n  If using your own container registry to store the OIG container image, this is the username and password (or token) for your container registry.\n  The output will look similar to the following:\nsecret/orclcred created   RCU schema creation In this section you create the RCU schemas in the Oracle Database.\nBefore following the steps in this section, make sure that the database and listener are up and running and you can connect to the database via SQL*Plus or other client tool.\n  If using Oracle Container Registry or your own container registry for your OIG container image, run the following command to create a helper pod to run RCU:\n$ kubectl run --image=\u0026lt;image_name-from-registry\u0026gt; --image-pull-policy=\u0026#34;IfNotPresent\u0026#34; --overrides=\u0026#39;{\u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;spec\u0026#34;:{\u0026#34;imagePullSecrets\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;orclcred\u0026#34;}]}}\u0026#39; helper -n \u0026lt;domain_namespace\u0026gt; -- sleep infinity For example:\n$ kubectl run --image=container-registry.oracle.com/middleware/oig_cpu:12.2.1.4-jdk8-ol7-220120.1359 --image-pull-policy=\u0026#34;IfNotPresent\u0026#34; --overrides=\u0026#39;{\u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;,\u0026#34;spec\u0026#34;:{\u0026#34;imagePullSecrets\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;orclcred\u0026#34;}]}}\u0026#39; helper -n oigns -- sleep infinity If you are not using a container registry and have loaded the image on each of the master and worker nodes, run the following command:\n$ kubectl run helper --image \u0026lt;image\u0026gt; -n oigns -- sleep infinity For example:\n$ kubectl run helper --image oracle/oig:12.2.1.4-jdk8-ol7-220120.1359 -n oigns -- sleep infinity The output will look similar to the following:\npod/helper created   Run the following command to check the pod is running:\n$ kubectl get pods -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get pods -n oigns The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE helper 1/1 Running 0 3m Note: If you are pulling the image from a container registry it may take several minutes before the pod has a STATUS of 1\\1. While the pod is starting you can check the status of the pod, by running the following command:\n$ kubectl describe pod helper -n oigns   Run the following command to start a bash shell in the helper pod:\n$ kubectl exec -it helper -n \u0026lt;domain_namespace\u0026gt; -- /bin/bash For example:\n$ kubectl exec -it helper -n oigns -- /bin/bash This will take you into a bash shell in the running helper pod:\n[oracle@helper oracle]$   In the helper bash shell run the following commands to set the environment:\n[oracle@helper oracle]$ export DB_HOST=\u0026lt;db_host.domain\u0026gt; [oracle@helper oracle]$ export DB_PORT=\u0026lt;db_port\u0026gt; [oracle@helper oracle]$ export DB_SERVICE=\u0026lt;service_name\u0026gt; [oracle@helper oracle]$ export RCUPREFIX=\u0026lt;rcu_schema_prefix\u0026gt; [oracle@helper oracle]$ export RCU_SCHEMA_PWD=\u0026lt;rcu_schema_pwd\u0026gt; [oracle@helper oracle]$ echo -e \u0026lt;db_pwd\u0026gt;\u0026#34;\\n\u0026#34;\u0026lt;rcu_schema_pwd\u0026gt; \u0026gt; /tmp/pwd.txt [oracle@helper oracle]$ cat /tmp/pwd.txt where:\n\u0026lt;db_host.domain\u0026gt; is the database server hostname\n\u0026lt;db_port\u0026gt; is the database listener port\n\u0026lt;service_name\u0026gt; is the database service name\n\u0026lt;rcu_schema_prefix\u0026gt; is the RCU schema prefix you want to set\n\u0026lt;rcu_schema_pwd\u0026gt; is the password you want to set for the \u0026lt;rcu_schema_prefix\u0026gt;\n\u0026lt;db_pwd\u0026gt; is the SYS password for the database\nFor example:\n[oracle@helper oracle]$ export DB_HOST=mydatabasehost.example.com [oracle@helper oracle]$ export DB_PORT=1521 [oracle@helper oracle]$ export DB_SERVICE=orcl.example.com [oracle@helper oracle]$ export RCUPREFIX=OIGK8S [oracle@helper oracle]$ export RCU_SCHEMA_PWD=\u0026lt;password\u0026gt; [oracle@helper oracle]$ echo -e \u0026lt;password\u0026gt;\u0026#34;\\n\u0026#34;\u0026lt;password\u0026gt; \u0026gt; /tmp/pwd.txt [oracle@helper oracle]$ cat /tmp/pwd.txt \u0026lt;password\u0026gt; \u0026lt;password\u0026gt;   In the helper bash shell run the following commands to create the RCU schemas in the database:\n[oracle@helper oracle]$ /u01/oracle/oracle_common/bin/rcu -silent -createRepository -databaseType ORACLE -connectString \\ $DB_HOST:$DB_PORT/$DB_SERVICE -dbUser sys -dbRole sysdba -useSamePasswordForAllSchemaUsers true \\ -selectDependentsForComponents true -schemaPrefix $RCUPREFIX -component OIM -component MDS -component SOAINFRA -component OPSS \\ -f \u0026lt; /tmp/pwd.txt The output will look similar to the following:\nRCU Logfile: /tmp/RCU2022-03-09_17-09_964981565/logs/rcu.log Processing command line .... Repository Creation Utility - Checking Prerequisites Checking Global Prerequisites Repository Creation Utility - Checking Prerequisites Checking Component Prerequisites Repository Creation Utility - Creating Tablespaces Validating and Creating Tablespaces Create tablespaces in the repository database Repository Creation Utility - Create Repository Create in progress. Percent Complete: 10 Executing pre create operations Percent Complete: 25 Percent Complete: 25 Percent Complete: 26 Percent Complete: 27 Percent Complete: 28 Percent Complete: 28 Percent Complete: 29 Percent Complete: 29 Creating Common Infrastructure Services(STB) Percent Complete: 36 Percent Complete: 36 Percent Complete: 44 Percent Complete: 44 Percent Complete: 44 Creating Audit Services Append(IAU_APPEND) Percent Complete: 51 Percent Complete: 51 Percent Complete: 59 Percent Complete: 59 Percent Complete: 59 Creating Audit Services Viewer(IAU_VIEWER) Percent Complete: 66 Percent Complete: 66 Percent Complete: 67 Percent Complete: 67 Percent Complete: 68 Percent Complete: 68 Creating Metadata Services(MDS) Percent Complete: 76 Percent Complete: 76 Percent Complete: 76 Percent Complete: 77 Percent Complete: 77 Percent Complete: 78 Percent Complete: 78 Percent Complete: 78 Creating Weblogic Services(WLS) Percent Complete: 82 Percent Complete: 82 Percent Complete: 83 Percent Complete: 84 Percent Complete: 86 Percent Complete: 88 Percent Complete: 88 Percent Complete: 88 Creating User Messaging Service(UCSUMS) Percent Complete: 92 Percent Complete: 92 Percent Complete: 95 Percent Complete: 95 Percent Complete: 100 Creating Audit Services(IAU) Creating Oracle Platform Security Services(OPSS) Creating SOA Infrastructure(SOAINFRA) Creating Oracle Identity Manager(OIM) Executing post create operations Repository Creation Utility: Create - Completion Summary Database details: ----------------------------- Host Name : mydatabasehost.example.com Port : 1521 Service Name : ORCL.EXAMPLE.COM Connected As : sys Prefix for (prefixable) Schema Owners : OIGK8S RCU Logfile : /tmp/RCU2022-03-09_17-09_964981565/logs/rcu.log Component schemas created: ----------------------------- Component Status Logfile Common Infrastructure Services Success /tmp/RCU2022-03-09_17-09_964981565/logs/stb.log Oracle Platform Security Services Success /tmp/RCU2022-03-09_17-09_964981565/logs/opss.log SOA Infrastructure Success /tmp/RCU2022-03-09_17-09_964981565/logs/soainfra.log Oracle Identity Manager Success /tmp/RCU2022-03-09_17-09_964981565/logs/oim.log User Messaging Service Success /tmp/RCU2022-03-09_17-09_964981565/logs/ucsums.log Audit Services Success /tmp/RCU2022-03-09_17-09_964981565/logs/iau.log Audit Services Append Success /tmp/RCU2022-03-09_17-09_964981565/logs/iau_append.log Audit Services Viewer Success /tmp/RCU2022-03-09_17-09_964981565/logs/iau_viewer.log Metadata Services Success /tmp/RCU2022-03-09_17-09_964981565/logs/mds.log WebLogic Services Success /tmp/RCU2022-03-09_17-09_964981565/logs/wls.log Repository Creation Utility - Create : Operation Completed [oracle@helper oracle]$   Run the following command to patch schemas in the database:\nThis command should be run if you are using an OIG image that contains OIG bundle patches. If using an OIG image without OIG bundle patches, then you can skip this step.\n [oracle@helper oracle]$ /u01/oracle/oracle_common/modules/thirdparty/org.apache.ant/1.10.5.0.0/apache-ant-1.10.5/bin/ant \\ -f /u01/oracle/idm/server/setup/deploy-files/automation.xml \\ run-patched-sql-files \\ -logger org.apache.tools.ant.NoBannerLogger \\ -logfile /u01/oracle/idm/server/bin/patch_oim_wls.log \\ -DoperationsDB.host=$DB_HOST \\ -DoperationsDB.port=$DB_PORT \\ -DoperationsDB.serviceName=$DB_SERVICE \\ -DoperationsDB.user=${RCUPREFIX}_OIM \\ -DOIM.DBPassword=$RCU_SCHEMA_PWD \\ -Dojdbc=/u01/oracle/oracle_common/modules/oracle.jdbc/ojdbc8.jar The output will look similar to the following:\nBuildfile: /u01/oracle/idm/server/setup/deploy-files/automation.xml   Verify the database was patched successfully by viewing the patch_oim_wls.log:\n[oracle@helper oracle]$ cat /u01/oracle/idm/server/bin/patch_oim_wls.log The output should look similar to below:\n... [sql] Executing resource: /u01/oracle/idm/server/db/oim/oracle/StoredProcedures/OfflineDataPurge/oim_pkg_offline_datapurge_pkg_body.sql [sql] Executing resource: /u01/oracle/idm/server/db/oim/oracle/Upgrade/oim12cps4/list/oim12cps4_dml_pty_insert_sysprop_RequestJustificationLocale.sql [sql] Executing resource: /u01/oracle/idm/server/db/oim/oracle/Upgrade/oim12cps4/list/oim12cps4_dml_pty_insert_sysprop_reportee_chain_for_mgr.sql [sql] 36 of 36 SQL statements executed successfully BUILD SUCCESSFUL Total time: 5 second   Exit the helper bash shell by issuing the command exit.\n  Preparing the environment for domain creation In this section you prepare the environment for the OIG domain creation. This involves the following steps:\na. Creating Kubernetes secrets for the domain and RCU\nb. Create a Kubernetes persistent volume and persistent volume claim\nCreating Kubernetes secrets for the domain and RCU   Create a Kubernetes secret for the domain using the create-weblogic-credentials script in the same Kubernetes namespace as the domain:\n$ cd $WORKDIR/kubernetes/create-weblogic-domain-credentials $ ./create-weblogic-credentials.sh -u weblogic -p \u0026lt;pwd\u0026gt; -n \u0026lt;domain_namespace\u0026gt; -d \u0026lt;domain_uid\u0026gt; -s \u0026lt;kubernetes_domain_secret\u0026gt; where:\n-u weblogic is the WebLogic username\n-p \u0026lt;pwd\u0026gt; is the password for the WebLogic user\n-n \u0026lt;domain_namespace\u0026gt; is the domain namespace\n-d \u0026lt;domain_uid\u0026gt; is the domain UID to be created. The default is domain1 if not specified\n-s \u0026lt;kubernetes_domain_secret\u0026gt; is the name you want to create for the secret for this namespace. The default is to use the domainUID if not specified\nFor example:\n$ cd $WORKDIR/kubernetes/create-weblogic-domain-credentials $ ./create-weblogic-credentials.sh -u weblogic -p \u0026lt;password\u0026gt; -n oigns -d governancedomain -s oig-domain-credentials The output will look similar to the following:\nsecret/oig-domain-credentials created secret/oig-domain-credentials labeled The secret oig-domain-credentials has been successfully created in the oigns namespace.   Verify the secret is created using the following command:\n$ kubectl get secret \u0026lt;kubernetes_domain_secret\u0026gt; -o yaml -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get secret oig-domain-credentials -o yaml -n oigns The output will look similar to the following:\n$ kubectl get secret oig-domain-credentials -o yaml -n oigns apiVersion: v1 data: password: V2VsY29tZTE= username: d2VibG9naWM= kind: Secret metadata: creationTimestamp: \u0026quot;2022-03-09T17:47:29Z\u0026quot; labels: weblogic.domainName: governancedomain weblogic.domainUID: governancedomain name: oig-domain-credentials namespace: oigns resourceVersion: \u0026quot;3216738\u0026quot; uid: c2ec07e0-0135-458d-bceb-c648d2a9ac54 type: Opaque   Create a Kubernetes secret for RCU in the same Kubernetes namespace as the domain, using the create-weblogic-credentials.sh script:\n$ cd $WORKDIR/kubernetes/create-rcu-credentials $ ./create-rcu-credentials.sh -u \u0026lt;rcu_prefix\u0026gt; -p \u0026lt;rcu_schema_pwd\u0026gt; -a sys -q \u0026lt;sys_db_pwd\u0026gt; -d \u0026lt;domain_uid\u0026gt; -n \u0026lt;domain_namespace\u0026gt; -s \u0026lt;kubernetes_rcu_secret\u0026gt; where:\n-u \u0026lt;rcu_prefix\u0026gt; is the name of the RCU schema prefix created previously\n-p \u0026lt;rcu_schema_pwd\u0026gt; is the password for the RCU schema prefix\n-a \u0026lt;sys_db_user\u0026gt; is the database user with sys dba privilege\n-q \u0026lt;sys_db_pwd\u0026gt; is the sys database password\n-d \u0026lt;domain_uid\u0026gt; is the domain_uid that you created earlier\n-n \u0026lt;domain_namespace\u0026gt; is the domain namespace\n-s \u0026lt;kubernetes_rcu_secret\u0026gt; is the name of the rcu secret to create\nFor example:\n$ cd $WORKDIR/kubernetes/create-rcu-credentials $ ./create-rcu-credentials.sh -u OIGK8S -p \u0026lt;password\u0026gt; -a sys -q \u0026lt;password\u0026gt; -d governancedomain -n oigns -s oig-rcu-credentials The output will look similar to the following:\nsecret/oig-rcu-credentials created secret/oig-rcu-credentials labeled The secret oig-rcu-credentials has been successfully created in the oigns namespace.   Verify the secret is created using the following command:\n$ kubectl get secret \u0026lt;kubernetes_rcu_secret\u0026gt; -o yaml -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get secret oig-rcu-credentials -o yaml -n oigns The output will look similar to the following:\napiVersion: v1 data: password: V2VsY29tZTE= sys_password: V2VsY29tZTE= sys_username: c3lz username: T0lHSzhT kind: Secret metadata: creationTimestamp: \u0026quot;2022-03-09T17:50:50Z\u0026quot; labels: weblogic.domainName: governancedomain weblogic.domainUID: governancedomain name: oig-rcu-credentials namespace: oigns resourceVersion: \u0026quot;3217023\u0026quot; uid: ce70b91a-fbbc-4839-9616-4cc2c1adeb4f type: Opaque   Create a Kubernetes persistent volume and persistent volume claim As referenced in Prerequisites the nodes in the Kubernetes cluster must have access to a persistent volume such as a Network File System (NFS) mount or a shared file system.\nA persistent volume is the same as a disk mount but is inside a container. A Kubernetes persistent volume is an arbitrary name (determined in this case, by Oracle) that is mapped to a physical volume on a disk.\nWhen a container is started, it needs to mount that volume. The physical volume should be on a shared disk accessible by all the Kubernetes worker nodes because it is not known on which worker node the container will be started. In the case of Identity and Access Management, the persistent volume does not get erased when a container stops. This enables persistent configurations.\nThe example below uses an NFS mounted volume (\u0026lt;persistent_volume\u0026gt;/governancedomainpv). Other volume types can also be used. See the official Kubernetes documentation for Volumes.\nNote: The persistent volume directory needs to be accessible to both the master and worker node(s). Make sure this path has full access permissions, and that the folder is empty. In this example /scratch/shared/governancedomainpv is accessible from all nodes via NFS.\n  Make a backup copy of the create-pv-pvc-inputs.yaml file and create required directories:\n$ cd $WORKDIR/kubernetes/create-weblogic-domain-pv-pvc $ cp create-pv-pvc-inputs.yaml create-pv-pvc-inputs.yaml.orig $ mkdir output $ mkdir -p \u0026lt;persistent_volume\u0026gt;/governancedomainpv $ chmod -R 777 \u0026lt;persistent_volume\u0026gt;/governancedomainpv For example:\n$ cd $WORKDIR/kubernetes/create-weblogic-domain-pv-pvc $ cp create-pv-pvc-inputs.yaml create-pv-pvc-inputs.yaml.orig $ mkdir output $ mkdir -p /scratch/shared/governancedomainpv $ chmod -R 777 /scratch/shared/governancedomainpv   On the master node run the following command to ensure it is possible to read and write to the persistent volume:\ncd \u0026lt;persistent_volume\u0026gt;/governancedomainpv touch file.txt ls filemaster.txt For example:\ncd /scratch/shared/governancedomainpv touch filemaster.txt ls filemaster.txt On the first worker node run the following to ensure it is possible to read and write to the persistent volume:\ncd /scratch/shared/governancedomainpv ls filemaster.txt touch fileworker1.txt ls fileworker1.txt Repeat the above for any other worker nodes e.g fileworker2.txt etc. Once proven that it\u0026rsquo;s possible to read and write from each node to the persistent volume, delete the files created.\n  Navigate to $WORKDIR/kubernetes/create-weblogic-domain-pv-pvc:\n$ cd $WORKDIR/kubernetes/create-weblogic-domain-pv-pvc and edit the create-pv-pvc-inputs.yaml file and update the following parameters to reflect your settings. Save the file when complete:\nbaseName: \u0026lt;domain\u0026gt; domainUID: \u0026lt;domain_uid\u0026gt; namespace: \u0026lt;domain_namespace\u0026gt; weblogicDomainStorageType: NFS weblogicDomainStorageNFSServer: \u0026lt;nfs_server\u0026gt; weblogicDomainStoragePath: \u0026lt;physical_path_of_persistent_storage\u0026gt; weblogicDomainStorageSize: 10Gi For example:\n# The base name of the pv and pvc baseName: domain # Unique ID identifying a domain. # If left empty, the generated pv can be shared by multiple domains # This ID must not contain an underscope (\u0026quot;_\u0026quot;), and must be lowercase and unique across all domains in a Kubernetes cluster. domainUID: governancedomain # Name of the namespace for the persistent volume claim namespace: oigns # Persistent volume type for the persistent storage. # The value must be 'HOST_PATH' or 'NFS'. # If using 'NFS', weblogicDomainStorageNFSServer must be specified. weblogicDomainStorageType: NFS # The server name or ip address of the NFS server to use for the persistent storage. # The following line must be uncomment and customized if weblogicDomainStorateType is NFS: weblogicDomainStorageNFSServer: mynfsserver # Physical path of the persistent storage. # When weblogicDomainStorageType is set to HOST_PATH, this value should be set the to path to the # domain storage on the Kubernetes host. # When weblogicDomainStorageType is set to NFS, then weblogicDomainStorageNFSServer should be set # to the IP address or name of the DNS server, and this value should be set to the exported path # on that server. # Note that the path where the domain is mounted in the WebLogic containers is not affected by this # setting, that is determined when you create your domain. # The following line must be uncomment and customized: weblogicDomainStoragePath: /scratch/shared/governancedomainpv # Reclaim policy of the persistent storage # The valid values are: 'Retain', 'Delete', and 'Recycle' weblogicDomainStorageReclaimPolicy: Retain # Total storage allocated to the persistent storage. weblogicDomainStorageSize: 10Gi   Execute the create-pv-pvc.sh script to create the PV and PVC configuration files:\n$ ./create-pv-pvc.sh -i create-pv-pvc-inputs.yaml -o output The output will be similar to the following:\nInput parameters being used export version=\u0026quot;create-weblogic-sample-domain-pv-pvc-inputs-v1\u0026quot; export baseName=\u0026quot;domain\u0026quot; export domainUID=\u0026quot;governancedomain\u0026quot; export namespace=\u0026quot;oigns\u0026quot; export weblogicDomainStorageType=\u0026quot;NFS\u0026quot; export weblogicDomainStorageNFSServer=\u0026quot;mynfsserver\u0026quot; export weblogicDomainStoragePath=\u0026quot;/scratch/shared/governancedomainpv\u0026quot; export weblogicDomainStorageReclaimPolicy=\u0026quot;Retain\u0026quot; export weblogicDomainStorageSize=\u0026quot;10Gi\u0026quot; Generating output/pv-pvcs/governancedomain-domain-pv.yaml Generating output/pv-pvcs/governancedomain-domain-pvc.yaml The following files were generated: output/pv-pvcs/governancedomain-domain-pv.yaml output/pv-pvcs/governancedomain-domain-pvc.yaml Completed   Run the following to show the files are created:\n$ ls output/pv-pvcs create-pv-pvc-inputs.yaml governancedomain-domain-pv.yaml governancedomain-domain-pvc.yaml   Run the following kubectl command to create the PV and PVC in the domain namespace:\n$ kubectl create -f output/pv-pvcs/governancedomain-domain-pv.yaml -n \u0026lt;domain_namespace\u0026gt; $ kubectl create -f output/pv-pvcs/governancedomain-domain-pvc.yaml -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl create -f output/pv-pvcs/governancedomain-domain-pv.yaml -n oigns $ kubectl create -f output/pv-pvcs/governancedomain-domain-pvc.yaml -n oigns The output will look similar to the following:\npersistentvolume/governancedomain-domain-pv created persistentvolumeclaim/governancedomain-domain-pvc created   Run the following commands to verify the PV and PVC were created successfully:\n$ kubectl describe pv \u0026lt;pv_name\u0026gt; $ kubectl describe pvc \u0026lt;pvc_name\u0026gt; -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl describe pv governancedomain-domain-pv $ kubectl describe pvc governancedomain-domain-pvc -n oigns The output will look similar to the following:\n$ kubectl describe pv governancedomain-domain-pv Name: governancedomain-domain-pv Labels: weblogic.domainUID=governancedomain Annotations: pv.kubernetes.io/bound-by-controller: yes Finalizers: [kubernetes.io/pv-protection] StorageClass: governancedomain-domain-storage-class Status: Bound Claim: oigns/governancedomain-domain-pvc Reclaim Policy: Retain Access Modes: RWX VolumeMode: Filesystem Capacity: 10Gi Node Affinity: \u0026lt;none\u0026gt; Message: Source: Type: NFS (an NFS mount that lasts the lifetime of a pod) Server: mynfsserver Path: /scratch/shared/governancedomainpv ReadOnly: false Events: \u0026lt;none\u0026gt; $ kubectl describe pvc governancedomain-domain-pvc -n oigns Name: governancedomain-domain-pvc Namespace: oigns StorageClass: governancedomain-domain-storage-class Status: Bound Volume: governancedomain-domain-pv Labels: weblogic.domainUID=governancedomain Annotations: pv.kubernetes.io/bind-completed: yes pv.kubernetes.io/bound-by-controller: yes Finalizers: [kubernetes.io/pvc-protection] Capacity: 10Gi Access Modes: RWX VolumeMode: Filesystem Mounted By: \u0026lt;none\u0026gt; Events: \u0026lt;none\u0026gt; You are now ready to create the OIG domain as per Create OIG Domains\n  "
},
{
	"uri": "/fmw-kubernetes/oig/manage-oig-domains/running-oig-utilities/",
	"title": "Runnning OIG utilities",
	"tags": [],
	"description": "Describes the steps for running OIG utilities in Kubernetes.",
	"content": "Run OIG utlities inside the OIG Kubernetes cluster.\nRun utilities in an interactive bash shell   Access a bash shell inside the \u0026lt;domain_uid\u0026gt;-oim-server1 pod:\n$ kubectl -n oigns exec -it \u0026lt;domain_uid\u0026gt;-oim-server1 -- bash For example:\n$ kubectl -n oigns exec -it governancedomain-oim-server1 -- bash This will take you into a bash shell in the running \u0026lt;domain_uid\u0026gt;-oim-server1 pod:\n[oracle@governancedomain-oim-server1 oracle]$   Navigate to the /u01/oracle/idm/server/bin directory and execute the utility as required. For example:\n[oracle@governancedomain-oim-server1 oracle] cd /u01/oracle/idm/server/bin [oracle@governancedomain-oim-server1 bin]$ ./\u0026lt;filename\u0026gt;.sh Note: Some utilties such as PurgeCache.sh, GenerateSnapshot.sh etc, may prompt to enter the t3 URL, for example:\n[oracle@governancedomain-oim-server1 bin]$ sh GenerateSnapshot.sh For running the Utilities the following environment variables need to be set APP_SERVER is weblogic OIM_ORACLE_HOME is /u01/oracle/idm/ JAVA_HOME is /u01/jdk MW_HOME is /u01/oracle WL_HOME is /u01/oracle/wlserver DOMAIN_HOME is /u01/oracle/user_projects/domains/governancedomain Executing -Dweblogic.security.SSL.trustedCAKeyStore= in IPv4 mode [Enter Xellerate admin username :]xelsysadm [Enter password for xelsysadm :] [Threads to use [ 8 ]] [Enter serverURL :[t3://oimhostname:oimportno ]] To find the t3 URL run:\n$ kubectl get services -n oigns | grep oim-cluster The output will look similar to the following:\ngovernancedomain-cluster-oim-cluster ClusterIP 10.110.161.82 \u0026lt;none\u0026gt; 14002/TCP,14000/TCP 4d In this case the t3 URL is: t3://governancedomain-cluster-oim-cluster:14000.\n  Passing inputs as a jar/xml file   Copy the input file to pass to a directory of your choice.\n  Run the following command to copy the input file to the running governancedomain-oim-server1 pod.\n$ kubectl -n oigns cp /\u0026lt;path\u0026gt;/\u0026lt;inputFile\u0026gt; governancedomain-oim-server1:/u01/oracle/idm/server/bin/   Access a bash shell inside the governancedomain-oim-server1 pod:\n$ kubectl -n oigns exec -it governancedomain-oim-server1 -- bash This will take you into a bash shell in the running governancedomain-oim-server1 pod:\n[oracle@governancedomain-oim-server1 oracle]$   Navigate to the /u01/oracle/idm/server/bin directory and execute the utility as required, passing the input file. For example:\n[oracle@governancedomain-oim-server1 oracle] cd /u01/oracle/idm/server/bin [oracle@governancedomain-oim-server1 bin]$ ./\u0026lt;filename\u0026gt;.sh -inputFile \u0026lt;inputFile\u0026gt; Note As pods are stateless the copied input file will remain until the pod restarts.\n  Editing property/profile files To edit a property/profile file in the Kubernetes cluster:\n  Copy the input file from the pod to a on the local system, for example:\n$ kubectl -n oigns cp governancedomain-oim-server1:/u01/oracle/idm/server/bin/\u0026lt;file.properties_profile\u0026gt; /\u0026lt;path\u0026gt;/\u0026lt;file.properties_profile\u0026gt; Note: If you see the message tar: Removing leading '/' from member names this can be ignored.\n  Edit the \u0026lt;/path\u0026gt;/\u0026lt;file.properties_profile\u0026gt; in an editor of your choice.\n  Copy the file back to the pod:\n$ kubectl -n oigns cp /\u0026lt;path\u0026gt;/\u0026lt;file.properties_profile\u0026gt; governancedomain-oim-server1:/u01/oracle/idm/server/bin/ Note: As pods are stateless the copied input file will remain until the pod restarts. Preserve a local copy in case you need to copy files back after pod restart.\n  "
},
{
	"uri": "/fmw-kubernetes/wccontent-domains/oracle-cloud/configure-load-balancer/",
	"title": "Set up a load balancer",
	"tags": [],
	"description": "Configure different load balancers for Oracle WebCenter Content domains.",
	"content": "WebLogic Kubernetes Operator supports ingress-based load balancers such as Traefik.\n Traefik  Configure the ingress-based Traefik load balancer for Oracle WebCenter Content domains.\n "
},
{
	"uri": "/fmw-kubernetes/wccontent-domains/patch_and_upgrade/upgrade-k8s-cluster/",
	"title": "Upgrade a Kubernetes cluster",
	"tags": [],
	"description": "Upgrade the underlying Kubernetes cluster version in a running WebCenter Content Kubernetes environment.",
	"content": "These instructions describe how to upgrade a Kubernetes cluster created using kubeadm on which an Oracle WebCenter Content domain is deployed. A rolling upgrade approach is used to upgrade nodes (master and worker) of the Kubernetes cluster.\nIt is expected that there will be a down time during the upgrade of the Kubernetes cluster as the nodes need to be drained as part of the upgrade process.\n Prerequisites  Review Prerequisites and ensure that your Kubernetes cluster is ready for upgrade. Make sure your environment meets all prerequisites. Make sure the database used for the WebCenter Content domain deployment is up and running during the upgrade process.  Upgrade the Kubernetes version An upgrade of Kubernetes is supported from one MINOR version to the next MINOR version, or between PATCH versions of the same MINOR. For example, you can upgrade from 1.x to 1.x+1, but not from 1.x to 1.x+2. To upgrade a Kubernetes version, first all the master nodes of the Kubernetes cluster must be upgraded sequentially, followed by the sequential upgrade of each worker node.\n See here for Kubernetes official documentation to upgrade from v1.14.x to v1.15.x. See here for Kubernetes official documentation to upgrade from v1.15.x to v1.16.x. See here for Kubernetes official documentation to upgrade from v1.16.x to v1.17.x. See here for Kubernetes official documentation to upgrade from v1.17.x to v1.18.x.  "
},
{
	"uri": "/fmw-kubernetes/wccontent-domains/oracle-cloud/create-wccontent-domains/",
	"title": "Create Oracle WebCenter Content domain",
	"tags": [],
	"description": "Create Oracle WebCenter Content domain on Oracle Kubernetes Engine (OKE).",
	"content": "Contents  Run the create domain script Create Container Clusters (OKE) Verify the results Verify the pods Verify the services Expose service for IBR intradoc port  Run the create domain script Run the create domain script, specifying your inputs file and an output directory to store the generated artifacts:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-wcc-domain/domain-home-on-pv/ $ ./create-domain.sh \\ -i create-domain-inputs.yaml \\ -o \u0026lt;path to output-directory\u0026gt; The script will perform the following steps:\n  Create a directory for the generated Kubernetes YAML files for this domain if it does not already exist. The path name is \u0026lt;path to output-directory\u0026gt;/weblogic-domains/\u0026lt;domainUID\u0026gt;. If the directory already exists, its contents must be removed before using this script.\n  Create a Kubernetes job that will start up a utility Oracle WebCenter Content container and run offline WLST scripts to create the domain on the shared storage.\n  Run and wait for the job to finish.\n  Create a Kubernetes domain YAML file, domain.yaml, in the \u0026ldquo;output\u0026rdquo; directory that was created above. This YAML file can be used to create the Kubernetes resource using the kubectl create -f or kubectl apply -f command.\n  Run oke-start-managed-server-wrapper.sh script, which intrenally applies the domain YAML. This script also applies initial configurations for Managed Server containers and readies Managed Servers for future inter-container communications.\n$ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-wcc-domain/domain-home-on-pv/ $ ./oke-start-managed-servers-wrapper.sh -o \u0026lt;path_to_output_directory\u0026gt; -l \u0026lt;load_balancer_external_ip\u0026gt; -p \u0026lt;load_balancer_port\u0026gt;   Verify the results The create domain script will verify that the domain was created, and will report failure if there was any error. However, it may be desirable to manually verify the domain, even if just to gain familiarity with the various Kubernetes objects that were created by the script.\nGenerated YAML files with the default inputs   Click here to see sample content of the generated `domain.yaml`.   $ cat output/weblogic-domains/wccinfra/domain.yaml # Copyright (c) 2017, 2021, Oracle and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # # This is an example of how to define a Domain resource. # apiVersion: \u0026quot;weblogic.oracle/v8\u0026quot; kind: Domain metadata: name: wccinfra namespace: wccns labels: weblogic.domainUID: wccinfra spec: # The WebLogic Domain Home domainHome: /u01/oracle/user_projects/domains/wccinfra maxClusterConcurrentStartup: 1 # The domain home source type # Set to PersistentVolume for domain-in-pv, Image for domain-in-image, or FromModel for model-in-image domainHomeSourceType: PersistentVolume # The WebLogic Server image that the WebLogic Kubernetes Operator uses to start the domain image: \u0026quot;phx.ocir.io/xxxxxxxxxx/oracle/wccontent/oracle/wccontent:x.x.x.x\u0026quot; # imagePullPolicy defaults to \u0026quot;Always\u0026quot; if image version is :latest imagePullPolicy: \u0026quot;IfNotPresent\u0026quot; # Identify which Secret contains the credentials for pulling an image imagePullSecrets: - name: image-secret # Identify which Secret contains the WebLogic Admin credentials (note that there is an example of # how to create that Secret at the end of this file) webLogicCredentialsSecret: name: wccinfra-domain-credentials # Whether to include the server out file into the pod's stdout, default is true includeServerOutInPodLog: true # Whether to enable log home logHomeEnabled: true # Whether to write HTTP access log file to log home httpAccessLogInLogHome: true # The in-pod location for domain log, server logs, server out, introspector out, and Node Manager log files logHome: /u01/oracle/user_projects/domains/logs/wccinfra # An (optional) in-pod location for data storage of default and custom file stores. # If not specified or the value is either not set or empty (e.g. dataHome: \u0026quot;\u0026quot;) then the # data storage directories are determined from the WebLogic domain home configuration. dataHome: \u0026quot;\u0026quot; # serverStartPolicy legal values are \u0026quot;NEVER\u0026quot;, \u0026quot;IF_NEEDED\u0026quot;, or \u0026quot;ADMIN_ONLY\u0026quot; # This determines which WebLogic Servers the WebLogic Kubernetes Operator will start up when it discovers this Domain # - \u0026quot;NEVER\u0026quot; will not start any server in the domain # - \u0026quot;ADMIN_ONLY\u0026quot; will start up only the administration server (no managed servers will be started) # - \u0026quot;IF_NEEDED\u0026quot; will start all non-clustered servers, including the administration server and clustered servers up to the replica count serverStartPolicy: \u0026quot;IF_NEEDED\u0026quot; serverPod: # an (optional) list of environment variable to be set on the servers env: - name: JAVA_OPTIONS value: \u0026quot;-Dweblogic.StdoutDebugEnabled=false\u0026quot; - name: USER_MEM_ARGS value: \u0026quot;-Djava.security.egd=file:/dev/./urandom -Xms256m -Xmx1024m \u0026quot; volumes: - name: weblogic-domain-storage-volume persistentVolumeClaim: claimName: wccinfra-domain-pvc volumeMounts: - mountPath: /u01/oracle/user_projects/domains name: weblogic-domain-storage-volume # adminServer is used to configure the desired behavior for starting the administration server. adminServer: # serverStartState legal values are \u0026quot;RUNNING\u0026quot; or \u0026quot;ADMIN\u0026quot; # \u0026quot;RUNNING\u0026quot; means the listed server will be started up to \u0026quot;RUNNING\u0026quot; mode # \u0026quot;ADMIN\u0026quot; means the listed server will be start up to \u0026quot;ADMIN\u0026quot; mode serverStartState: \u0026quot;RUNNING\u0026quot; # adminService: # channels: # The Admin Server's NodePort # - channelName: default # nodePort: 30701 # Uncomment to export the T3Channel as a service # - channelName: T3Channel # clusters is used to configure the desired behavior for starting member servers of a cluster. # If you use this entry, then the rules will be applied to ALL servers that are members of the named clusters. clusters: - clusterName: ibr_cluster serverService: precreateService: true serverStartState: \u0026quot;RUNNING\u0026quot; serverPod: # Instructs Kubernetes scheduler to prefer nodes for new cluster members where there are not # already members of the same cluster. affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: \u0026quot;weblogic.clusterName\u0026quot; operator: In values: - $(CLUSTER_NAME) topologyKey: \u0026quot;kubernetes.io/hostname\u0026quot; replicas: 1 # The number of managed servers to start for unlisted clusters # replicas: 1 # Istio # configuration: # istio: # enabled: # readinessPort: - clusterName: ucm_cluster clusterService: annotations: traefik.ingress.kubernetes.io/affinity: \u0026quot;true\u0026quot; traefik.ingress.kubernetes.io/service.sticky.cookie: \u0026quot;true\u0026quot; traefik.ingress.kubernetes.io/session-cookie-name: JSESSIONID serverService: precreateService: true serverStartState: \u0026quot;RUNNING\u0026quot; serverPod: # Instructs Kubernetes scheduler to prefer nodes for new cluster members where there are not # already members of the same cluster. affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: \u0026quot;weblogic.clusterName\u0026quot; operator: In values: - $(CLUSTER_NAME) topologyKey: \u0026quot;kubernetes.io/hostname\u0026quot; replicas: 3 # The number of managed servers to start for unlisted clusters # replicas: 1    Verify the domain To confirm that the domain was created, enter the following command:\n$ kubectl describe domain DOMAINUID -n NAMESPACE Replace DOMAINUID with the domainUID and NAMESPACE with the actual namespace.\n  Click here to see a sample domain description.   [opc@bastionhost domain-home-on-pv]$ kubectl describe domain wccinfra -n wccns Name: wccinfra Namespace: wccns Labels: weblogic.domainUID=wccinfra Annotations: kubectl.kubernetes.io/last-applied-configuration: {\u0026quot;apiVersion\u0026quot;:\u0026quot;weblogic.oracle/v8\u0026quot;,\u0026quot;kind\u0026quot;:\u0026quot;Domain\u0026quot;,\u0026quot;metadata\u0026quot;:{\u0026quot;annotations\u0026quot;:{},\u0026quot;labels\u0026quot;:{\u0026quot;weblogic.domainUID\u0026quot;:\u0026quot;wccinfra\u0026quot;},\u0026quot;name\u0026quot;:\u0026quot;wccinfr... API Version: weblogic.oracle/v8 Kind: Domain Metadata: Creation Timestamp: 2021-08-24T12:26:19Z Generation: 33 Managed Fields: API Version: weblogic.oracle/v8 Fields Type: FieldsV1 fieldsV1: f:metadata: f:annotations: .: f:kubectl.kubernetes.io/last-applied-configuration: f:labels: .: f:weblogic.domainUID: Manager: kubectl Operation: Update Time: 2021-09-30T10:56:07Z API Version: weblogic.oracle/v8 Fields Type: FieldsV1 fieldsV1: f:status: .: f:clusters: f:conditions: f:introspectJobFailureCount: f:servers: f:startTime: Manager: Kubernetes Java Client Operation: Update Time: 2021-10-04T20:06:17Z Resource Version: 115422662 Self Link: /apis/weblogic.oracle/v8/namespaces/wccns/domains/wccinfra UID: e283c968-b80b-404b-aa1e-711080d7cc38 Spec: Admin Server: Server Start State: RUNNING Clusters: Cluster Name: ibr_cluster Replicas: 1 Server Pod: Affinity: Pod Anti Affinity: Preferred During Scheduling Ignored During Execution: Pod Affinity Term: Label Selector: Match Expressions: Key: weblogic.clusterName Operator: In Values: $(CLUSTER_NAME) Topology Key: kubernetes.io/hostname Weight: 100 Server Service: Precreate Service: true Server Start State: RUNNING Cluster Name: ucm_cluster Cluster Service: Annotations: traefik.ingress.kubernetes.io/affinity: true traefik.ingress.kubernetes.io/service.sticky.cookie: true traefik.ingress.kubernetes.io/session-cookie-name: JSESSIONID Replicas: 3 Server Pod: Affinity: Pod Anti Affinity: Preferred During Scheduling Ignored During Execution: Pod Affinity Term: Label Selector: Match Expressions: Key: weblogic.clusterName Operator: In Values: $(CLUSTER_NAME) Topology Key: kubernetes.io/hostname Weight: 100 Server Service: Precreate Service: true Server Start State: RUNNING Data Home: Domain Home: /u01/oracle/user_projects/domains/wccinfra Domain Home Source Type: PersistentVolume Http Access Log In Log Home: true Image: phx.ocir.io/xxxxxxxxxx/oracle/wccontent:x.x.x.x Image Pull Policy: IfNotPresent Image Pull Secrets: Name: image-secret Include Server Out In Pod Log: true Log Home: /u01/oracle/user_projects/domains/logs/wccinfra Log Home Enabled: true Max Cluster Concurrent Startup: 1 Server Pod: Env: Name: JAVA_OPTIONS Value: -Dweblogic.StdoutDebugEnabled=false Name: USER_MEM_ARGS Value: -Djava.security.egd=file:/dev/./urandom -Xms256m -Xmx1024m Volume Mounts: Mount Path: /u01/oracle/user_projects/domains Name: weblogic-domain-storage-volume Volumes: Name: weblogic-domain-storage-volume Persistent Volume Claim: Claim Name: wccinfra-domain-pvc Server Start Policy: IF_NEEDED Web Logic Credentials Secret: Name: wccinfra-domain-credentials Status: Clusters: Cluster Name: ibr_cluster Maximum Replicas: 5 Minimum Replicas: 0 Ready Replicas: 1 Replicas: 1 Replicas Goal: 1 Cluster Name: ucm_cluster Maximum Replicas: 5 Minimum Replicas: 0 Ready Replicas: 3 Replicas: 3 Replicas Goal: 3 Conditions: Last Transition Time: 2021-09-30T11:04:35.889547Z Reason: ServersReady Status: True Type: Available Introspect Job Failure Count: 0 Servers: Desired State: RUNNING Health: Activation Time: 2021-09-30T10:58:38.381000Z Overall Health: ok Subsystems: Subsystem Name: ServerRuntime Symptoms: Node Name: 10.0.10.135 Server Name: adminserver State: RUNNING Cluster Name: ibr_cluster Desired State: RUNNING Health: Activation Time: 2021-09-30T11:01:09.987000Z Overall Health: ok Subsystems: Subsystem Name: ServerRuntime Symptoms: Node Name: 10.0.10.135 Server Name: ibr_server1 State: RUNNING Cluster Name: ibr_cluster Desired State: SHUTDOWN Server Name: ibr_server2 Cluster Name: ibr_cluster Desired State: SHUTDOWN Server Name: ibr_server3 Cluster Name: ibr_cluster Desired State: SHUTDOWN Server Name: ibr_server4 Cluster Name: ibr_cluster Desired State: SHUTDOWN Server Name: ibr_server5 Cluster Name: ucm_cluster Desired State: RUNNING Health: Activation Time: 2021-09-30T11:00:36.369000Z Overall Health: ok Subsystems: Subsystem Name: ServerRuntime Symptoms: Node Name: 10.0.10.142 Server Name: ucm-server1 State: RUNNING Cluster Name: ucm_cluster Desired State: RUNNING Health: Activation Time: 2021-09-30T11:02:35.448000Z Overall Health: ok Subsystems: Subsystem Name: ServerRuntime Symptoms: Node Name: 10.0.10.135 Server Name: ucm-server2 State: RUNNING Cluster Name: ucm_cluster Desired State: RUNNING Health: Activation Time: 2021-09-30T11:04:32.314000Z Overall Health: ok Subsystems: Subsystem Name: ServerRuntime Symptoms: Node Name: 10.0.10.142 Server Name: ucm-server3 State: RUNNING Cluster Name: ucm_cluster Desired State: SHUTDOWN Server Name: ucm-server4 Cluster Name: ucm_cluster Desired State: SHUTDOWN Server Name: ucm-server5 Start Time: 2021-08-24T12:26:20.033714Z Events: \u0026lt;none\u0026gt;    In the Status section of the output, the available servers and clusters are listed. Note that if this command is issued soon after the script finishes, there may be no servers available yet, or perhaps only the Administration Server but no Managed Servers. The WebLogic Kubernetes Operator will start up the Administration Server first and wait for it to become ready before starting the Managed Servers.\nVerify the pods Enter the following command to see the pods running the servers:\n$ kubectl get pods -n NAMESPACE Here is an example of the output of this command. You can verify that an Administration Server and Managed Servers for ucm and ibr cluster are running.\n$ kubectl get pod -n wccns NAME READY STATUS RESTARTS AGE rcu 1/1 Running 0 54d wccinfra-adminserver 1/1 Running 0 18d wccinfra-create-fmw-infra-sample-domain-job-xqnn4 0/1 Completed 0 54d wccinfra-ibr-server1 1/1 Running 0 18d wccinfra-ucm-server1 1/1 Running 0 18d wccinfra-ucm-server2 1/1 Running 0 18d wccinfra-ucm-server3 1/1 Running 0 18d Verify the services Enter the following command to see the services for the domain:\n$ kubectl get services -n NAMESPACE Here is an example of the output of this command.\n  Click here to see a sample list of services.   $ kubectl get services -n wccns NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE oracle-db LoadBalancer 10.96.74.187 123.45.xxx.xxx 1521:30011/TCP 80d wccinfra-adminserver ClusterIP None \u0026lt;none\u0026gt; 7001/TCP 18d wccinfra-cluster-ibr-cluster ClusterIP 10.96.206.89 \u0026lt;none\u0026gt; 16250/TCP 119s wccinfra-cluster-ucm-cluster ClusterIP 10.96.180.150 \u0026lt;none\u0026gt; 16200/TCP 54d wccinfra-ibr-server1 ClusterIP None \u0026lt;none\u0026gt; 16250/TCP 18d wccinfra-ibr-server2 ClusterIP 10.96.185.209 \u0026lt;none\u0026gt; 16250/TCP 18d wccinfra-ibr-server3 ClusterIP 10.96.43.99 \u0026lt;none\u0026gt; 16250/TCP 18d wccinfra-ibr-server4 ClusterIP 10.96.77.52 \u0026lt;none\u0026gt; 16250/TCP 18d wccinfra-ibr-server5 ClusterIP 10.96.63.174 \u0026lt;none\u0026gt; 16250/TCP 18d wccinfra-ucm-server1 ClusterIP None \u0026lt;none\u0026gt; 16200/TCP 18d wccinfra-ucm-server2 ClusterIP None \u0026lt;none\u0026gt; 16200/TCP 18d wccinfra-ucm-server3 ClusterIP None \u0026lt;none\u0026gt; 16200/TCP 18d wccinfra-ucm-server4 ClusterIP 10.96.141.251 \u0026lt;none\u0026gt; 16200/TCP 18d wccinfra-ucm-server5 ClusterIP 10.96.85.52 \u0026lt;none\u0026gt; 16200/TCP 18d    Expose service for IBR intradoc port  Get the IP address for the node, hosting ibr managed server pod. In this sample, node running wccinfra-ibr-server1 pod has ip \u0026lsquo;10.0.10.xx\u0026rsquo; $ kubectl get pods -n wccns -o wide #output NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES wccinfra-adminserver 1/1 Running 0 4h50m 10.244.0.150 10.0.10.xxx \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; wccinfra-create-fmw-infra-sample-domain-job-zbsxr 0/1 Completed 0 7d22h 10.244.1.25 10.0.10.xx \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; wccinfra-ibr-server1 1/1 Running 0 4h48m 10.244.1.38 10.0.10.xx \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; wccinfra-ucm-server1 1/1 Running 0 4h48m 10.244.1.39 10.0.10.xx \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; wccinfra-ucm-server2 1/1 Running 0 4h46m 10.244.0.151 10.0.10.xxx \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; wccinfra-ucm-server3 1/1 Running 0 4h44m 10.244.1.40 10.0.10.xx \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;  Expose service for IBR intradoc port $ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-wcc-domain/domain-home-on-pv/ $ kubectl expose service/wccinfra-cluster-ibr-cluster --name wccinfra-cluster-ibr-cluster-ext --port=5555 --target-port=5555 --external-ip=\u0026lt;your-ibr-managed-server-node-ip\u0026gt; -n wccns #sample $ kubectl expose service/wccinfra-cluster-ibr-cluster --name wccinfra-cluster-ibr-cluster-ext --port=5555 --target-port=5555 --external-ip=10.0.10.xx -n wccns $ kubectl get service/wccinfra-cluster-ibr-cluster-ext -n wccns -o yaml \u0026gt; wccinfra-cluster-ibr-cluster-ext.yaml $ sed -i \u0026#34;0,/5555/s//16250/\u0026#34; wccinfra-cluster-ibr-cluster-ext.yaml $ kubectl -n wccns apply -f wccinfra-cluster-ibr-cluster-ext.yaml  Verify ibr service name \u0026lsquo;wccinfra-cluster-ibr-cluster-ext\u0026rsquo; $ kubectl get svc -n wccns NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE oracle-db LoadBalancer 10.96.74.187 123.45.xxx.xxx 1521:30011/TCP 13d wccinfra-adminserver ClusterIP None \u0026lt;none\u0026gt; 7001/TCP 5h10m wccinfra-cluster-ibr-cluster ClusterIP 10.96.155.21 \u0026lt;none\u0026gt; 16250/TCP 20s wccinfra-cluster-ibr-cluster-ext ClusterIP 10.96.152.184 10.0.10.xx 5555/TCP 7d3h wccinfra-cluster-ucm-cluster ClusterIP 10.96.136.224 \u0026lt;none\u0026gt; 16200/TCP 7d4h   "
},
{
	"uri": "/fmw-kubernetes/wccontent-domains/patch_and_upgrade/",
	"title": "Patch and upgrade",
	"tags": [],
	"description": "",
	"content": "Patch an existing Oracle WebCenter Content image or upgrade the infrastructure, such as upgrading the underlying Kubernetes cluster to a new release and upgrading the WebLogic Kubernetes Operator release.\n Patch an image  Create a patched Oracle WebCenter Content image using the WebLogic Image Tool.\n Upgrade an WebLogic Kubernetes Operator release  Upgrade the WebLogic Kubernetes Operator release to a newer version.\n Upgrade a Kubernetes cluster  Upgrade the underlying Kubernetes cluster version in a running WebCenter Content Kubernetes environment.\n "
},
{
	"uri": "/fmw-kubernetes/wccontent-domains/adminguide/weblogic-logging-exporter-setup/",
	"title": "Publish logs to Elasticsearch",
	"tags": [],
	"description": "Use the WebLogic Logging Exporter to publish the WebLogic Server logs to Elasticsearch.",
	"content": "The WebLogic Logging Exporter adds a log event handler to WebLogic Server. WebLogic Server logs can be pushed to Elasticsearch in Kubernetes directly by using the Elasticsearch REST API. For more details, see to the WebLogic Logging Exporter project.\nThis sample shows you how to publish WebLogic Server logs to Elasticsearch and view them in Kibana. For publishing WebLogic Kubernetes Operator logs, see this sample.\nPrerequisites This document assumes that you have already set up Elasticsearch and Kibana for logs collection. If you have not, please see this document.\n Download the WebLogic Logging Exporter binaries The pre-built binaries are available on the WebLogic Logging Exporter Releases page.\nDownload:\n weblogic-logging-exporter-1.0.0.jar from the Releases page. snakeyaml-1.25.jar from Maven Central.  These identifiers are used in the sample commands in this document.\n wccns: WebCenter Content domain namespace wccinfra: domainUID wccinfra-adminserver: Administration Server pod name   Copy the JAR Files to the WebLogic Domain Home Copy the weblogic-logging-exporter-1.0.0.jar and snakeyaml-1.25.jar files to the domain home directory in the Administration Server pod.\n$ kubectl cp \u0026lt;file-to-copy\u0026gt; \u0026lt;namespace\u0026gt;/\u0026lt;Administration-Server-pod\u0026gt;:\u0026lt;domainhome\u0026gt; $ kubectl cp weblogic-logging-exporter-1.0.0.jar wccns/wccinfra-adminserver:/u01/oracle/user_projects/domains/wccinfra/ $ kubectl cp snakeyaml-1.25.jar wccns/wccinfra-adminserver:/u01/oracle/user_projects/domains/wccinfra/ Add a Startup Class to the Domain Configuration In this step, we configure weblogic-logging-exporter JAR as a startup class in the WebLogic servers where we intend to collect the logs.\n  In the WebLogic Server Administration Console, in the left navigation pane, expand Environment, and then select Startup and Shutdown Classes.\n  Add a new startup class. You may choose any descriptive name, however, the class name must be weblogic.logging.exporter.Startup.\n  Target the startup class to each server from which you want to export logs.\n  You can verify this by checking for the update in your config.xml file(/u01/oracle/user_projects/domains/wccinfra/config/config.xml) which should be similar to this example:\n$ kubectl exec -n wccns -it wccinfra-adminserver cat /u01/oracle/user_projects/domains/wccinfra/config/config.xml \u0026lt;startup-class\u0026gt; \u0026lt;name\u0026gt;weblogic-logging-exporter\u0026lt;/name\u0026gt; \u0026lt;target\u0026gt;AdminServer,ucm_cluster,ibr_cluster,ipm_cluster,capture_cluster,wccadf_cluster\u0026lt;/target\u0026gt; \u0026lt;class-name\u0026gt;weblogic.logging.exporter.Startup\u0026lt;/class-name\u0026gt; \u0026lt;/startup-class\u0026gt;   Update the WebLogic Server CLASSPATH   Copy the setDomainEnv.sh file from the pod to a local folder:\n$ kubectl cp wccns/wccinfra-adminserver:/u01/oracle/user_projects/domains/wccinfra/bin/setDomainEnv.sh $PWD/setDomainEnv.sh tar: Removing leading `/' from member names Ignore exception: tar: Removing leading '/' from member names\n  Modify setDomainEnv.sh to update the Server Class path, add below code at the end of file:\nCLASSPATH=/u01/oracle/user_projects/domains/wccinfra/weblogic-logging-exporter-1.0.0.jar:/u01/oracle/user_projects/domains/wccinfra/snakeyaml-1.25.jar:${CLASSPATH} export CLASSPATH   Copy back the modified setDomainEnv.sh file to the pod:\n$ kubectl cp setDomainEnv.sh wccns/wccinfra-adminserver:/u01/oracle/user_projects/domains/wccinfra/bin/setDomainEnv.sh ``\n  Create a Configuration File for the WebLogic Logging Exporter In this step, we will be creating the configuration file for weblogic-logging-exporter.\n  Specify the Elasticsearch server host and port number in file kubernetes/samples/scripts/create-wcc-domain/utils/weblogic-logging-exporter/WebLogicLoggingExporter.yaml:\nExample:\nweblogicLoggingIndexName: wls publishHost: elasticsearch.default.svc.cluster.local publishPort: 9200 domainUID: wccinfra weblogicLoggingExporterEnabled: true weblogicLoggingExporterSeverity: Notice weblogicLoggingExporterBulkSize: 2 weblogicLoggingExporterFilters: - FilterExpression: NOT(MSGID = 'BEA-000449')   Copy the WebLogicLoggingExporter.yaml file to the domain home directory in the WebLogic Administration Server pod:\n$ kubectl cp kubernetes/samples/scripts/create-wcc-domain/utils/weblogic-logging-exporter/WebLogicLoggingExporter.yaml wccns/wccinfra-adminserver:/u01/oracle/user_projects/domains/wccinfra/config/   Restart All the Servers in the Domain To restart the servers, stop and then start them using the following commands:\nTo STOP the servers: $ kubectl patch domain wccinfra -n wccns --type='json' -p='[{\u0026quot;op\u0026quot;: \u0026quot;replace\u0026quot;, \u0026quot;path\u0026quot;: \u0026quot;/spec/serverStartPolicy\u0026quot;, \u0026quot;value\u0026quot;: \u0026quot;NEVER\u0026quot; }]' To START the servers: $ kubectl patch domain wccinfra -n wccns --type='json' -p='[{\u0026quot;op\u0026quot;: \u0026quot;replace\u0026quot;, \u0026quot;path\u0026quot;: \u0026quot;/spec/serverStartPolicy\u0026quot;, \u0026quot;value\u0026quot;: \u0026quot;IF_NEEDED\u0026quot; }]' After all the servers are restarted, see their server logs to check that the weblogic-logging-exporter class is called, as shown below:\n======================= Weblogic Logging Exporter Startup class called ================== Reading configuration from file name: /u01/oracle/user_projects/domains/wccinfra/config/WebLogicLoggingExporter.yaml Config{weblogicLoggingIndexName='wls', publishHost='elasticsearch.default.svc.cluster.local', publishPort=9200, weblogicLoggingExporterSeverity='Notice', weblogicLoggingExporterBulkSize='1', enabled=true, weblogicLoggingExporterFilters=[ FilterConfig{expression='NOT(MSGID = 'BEA-000449')', servers=[]}], domainUID='wccinfra'} ====================== WebLogic Logging Exporter is ebled ================= publishHost in initialize: elasticsearch.default.svc.cluster.local ================= publishPort in initialize: 9200 ================= url in executePutOrPostOnUrl: http://elasticsearch.default.svc.cluster.local:9200/wls Create an Index Pattern in Kibana Create an appropriate index pattern in Kibana \u0026gt; Management. After the servers are started, you will see the log data in the Kibana dashboard.\n"
},
{
	"uri": "/fmw-kubernetes/soa-domains/patch_and_upgrade/",
	"title": "Patch and upgrade",
	"tags": [],
	"description": "",
	"content": "Patch an existing Oracle SOA Suite image or upgrade the infrastructure, such as upgrading the underlying Kubernetes cluster to a new release and upgrading the WebLogic Kubernetes Operator release.\n Patch an image  Create a patched Oracle SOA Suite image using the WebLogic Image Tool.\n Upgrade an operator release  Upgrade the WebLogic Kubernetes Operator release to a newer version.\n Upgrade a Kubernetes cluster  Upgrade the underlying Kubernetes cluster version in a running SOA Kubernetes environment.\n "
},
{
	"uri": "/fmw-kubernetes/wcsites-domains/oracle-cloud/ocir/",
	"title": "Creating an OCIR",
	"tags": [],
	"description": "Running WebLogic Kubernetes Operator managed Oracle WebCenter Sites domains on OKE",
	"content": "Creation of OCIR Push all the required images to OCIR and use from OCIR. Follow the below steps before pushing the images to OCIR\nCreate an \u0026ldquo;Auth token\u0026rdquo; Create an \u0026ldquo;Auth token\u0026rdquo; which will be used as docker password to push/pull images from OCIR Login to Console and navigate to User Settings, which is in the drop down under your OCI username in the top nav  On User Details page, select \u0026ldquo;Auth Tokens\u0026rdquo; in the left nav and then Click the \u0026ldquo;Generate Token\u0026rdquo; button: Enter a Name and Click \u0026ldquo;Generate Token\u0026rdquo;  Token will get generated  Copy the generated token. NOTE: It will only be displayed this one time, and you will need to copy it to a secure place for further use. NOTE: It will only be displayed this one time, and you will need to copy it to a secure place for further use.  Get the OCIR name Get the OCIR Repo Name by Log in to Oracle Cloud Infrastructure Console. In he OCI Console, open the Navigation menu. Under Solutions and Platform, go to Developer Services and click Registry (OCIR). Using the OCIR Using the Docker CLI to login to OCIR ( for phoenix : phx.ocir.io , ashburn: iad.ocir.io etc) a. docker login phx.ocir.io b. When promoted for username enter docker username as OCIR RepoName/oci username ( eg., axcmmdmzqtqb/oracleidentitycloudservice/myemailid@oracle.com) c. When prompted for your password, enter the generated Auth Token i.e p[3k;pYePDSTD:-(LlAS Now you can tag the images and push to OCIR.\n$ docker login phx.ocir.io $ username - axcmmdmzqtqb/oracleidentitycloudservice/myemailid@oracle.com $ password - p[3k;pYePDSTD:-(LlAS (Token Generated for OCIR using user setting) This has to be done on Bastion Node for all the images.\n"
},
{
	"uri": "/fmw-kubernetes/wcsites-domains/patch-and-upgrade/",
	"title": "Upgrade and Patch ",
	"tags": [],
	"description": "Pre-requisites for setting up WebCenter Sites domains with WebLogic Kubernetes Operator",
	"content": "Patch an existing Oracle WebCenter Sites image or upgrade the infrastructure, such as upgrading the underlying Kubernetes cluster to a new release and upgrading the WebLogic Kubernetes Operator release.\n Patch a Oracle WebCenter Sites product Docker image  Upgrade the underlying Oracle WebCenter Sites product image in a running Oracle WebCenter Sites Kubernetes environment.\n Upgrade an operator release  Upgrade the WebLogic Kubernetes Operator release to a newer version.\n Upgrade a Kubernetes cluster  Upgrade the underlying Kubernetes cluster version in a running Oracle WebCenter Sites Kubernetes environment.\n "
},
{
	"uri": "/fmw-kubernetes/wcportal-domains/manage-wcportal-domains/",
	"title": "Administration Guide",
	"tags": [],
	"description": "Describes how to use some common utility tools and configurations to administer  WebCenter Portal domain.",
	"content": "Administer Oracle WebCenter Portal domain in Kubernetes.\n Set up a load balancer  Configure different load balancers for the Oracle WebCenter Portal domain.\n Monitor a domain and publish logs  Monitor Oracle WebCenter Portal and publishing logs to Elasticsearch.\n "
},
{
	"uri": "/fmw-kubernetes/wcportal-domains/create-or-update-image/",
	"title": "Create or update an image",
	"tags": [],
	"description": "Create or update an Oracle WebCenter Portal Docker image used for deploying Oracle WebCenter Portal domains. An Oracle WebCenter Portal Docker image can be created using the WebLogic Image Tool or using the Dockerfile approach.",
	"content": "You can build an Oracle WebCenter Portal image for production deployments with patches (bundle or interim) using the WebLogic Image Tool, you must have access to the My Oracle Support (MOS) to download (bundle or interim) patches.\n Create or update an Oracle WebCenter Portal Docker image using the WebLogic Image Tool  Set up the WebLogic Image Tool Create an image Update an image   Create an Oracle WebCenter Portal Docker image using Dockerfile  Create or update an Oracle WebCenter Portal Docker image using the WebLogic Image Tool Using the WebLogic Image Tool, you can create a new Oracle WebCenter Portal Docker image (can include patches as well) or update an existing image with one or more patches (bundle patch and interim patches).\n Recommendations:\n Use create for creating a new Oracle WebCenter Portal Docker image:  without any patches or, containing the Oracle WebCenter Portal binaries, bundle , and interim patches. This is the recommended approach if you have access to the Oracle WebCenter Portal patches because it optimizes the size of the image.   Use update for patching an existing Oracle WebCenter Portal Docker image with a single interim patch. Note that the patched image size may increase considerably due to additional image layers introduced by the patch application tool.    Prerequisites Set up the WebLogic Image Tool Validate the setup WebLogic Image Tool build directory WebLogic Image Tool cache Set up additional build scripts  Prerequisites Verify that your environment meets the following prerequisites:\n Docker client and daemon on the build machine, with minimum Docker version 18.03.1.ce. Bash version 4.0 or later, to enable the command complete feature. JAVA_HOME environment variable set to the appropriate JDK location.  Set up the WebLogic Image Tool To set up the WebLogic Image Tool:\n  Create a working directory and change to it. In these steps, this directory is imagetool-setup.\n$ mkdir imagetool-setup $ cd imagetool-setup   Download the latest version of the WebLogic Image Tool from the releases page.\n  Unzip the release ZIP file to the imagetool-setup directory.\n  Execute the following commands to set up the WebLogic Image Tool on a Linux environment:\n$ cd imagetool-setup/imagetool/bin $ source setup.sh   Validate the setup To validate the setup of the WebLogic Image Tool:\n  Enter the following command to retrieve the version of the WebLogic Image Tool:\n$ imagetool --version   Enter imagetool then press the Tab key to display the available imagetool commands:\n$ imagetool \u0026lt;TAB\u0026gt; cache create help rebase update   WebLogic Image Tool build directory The WebLogic Image Tool creates a temporary Docker context directory, prefixed by wlsimgbuilder_temp, every time the tool runs. Under normal circumstances, this context directory is deleted. However, if the process is aborted or the tool is unable to remove the directory, it is safe for you to delete it manually. By default, the WebLogic Image Tool creates the Docker context directory under the user\u0026rsquo;s home directory. If you prefer to use a different directory for the temporary context, set the environment variable WLSIMG_BLDDIR:\n$ export WLSIMG_BLDDIR=\u0026#34;/path/to/buid/dir\u0026#34; WebLogic Image Tool cache The WebLogic Image Tool maintains a local file cache store. This store is used to look up where the Java, WebLogic Server installers, and WebLogic Server patches reside in the local file system. By default, the cache store is located in the user\u0026rsquo;s $HOME/cache directory. Under this directory, the lookup information is stored in the .metadata file. All automatically downloaded patches also reside in this directory. You can change the default cache store location by setting the environment variable WLSIMG_CACHEDIR:\n$ export WLSIMG_CACHEDIR=\u0026#34;/path/to/cachedir\u0026#34; Set up additional build scripts To create an Oracle WebCenter Portal Docker image using the WebLogic Image Tool, additional container scripts for Oracle WebCenter Portal domains are required.\n  Clone the docker-images repository to set up those scripts. In these steps, this directory is DOCKER_REPO:\n$ cd imagetool-setup $ git clone https://github.com/oracle/docker-images.git   Copy the additional WebLogic Image Tool build files from the operator source repository to the imagetool-setup location:\n$ mkdir -p imagetool-setup/docker-images/OracleWebCenterPortal/imagetool/12.2.1.4.0 $ cd imagetool-setup/docker-images/OracleWebCenterPortal/imagetool/12.2.1.4.0 $ cp -rf ${WORKDIR}/imagetool-scripts/* .    Note: To create the image, continue with the following steps. To update the image, see update an image.\n Create an image After setting up the WebLogic Image Tool and configuring the required build scripts, create a new Oracle WebCenter Portal Docker image using the WebLogic Image Tool as described ahead.\nDownload the Oracle WebCenter Portal installation binaries and patches You must download the required Oracle WebCenter Portal installation binaries and patches listed below from the Oracle Software Delivery Cloud and save them in a directory of your choice. In these steps, the directory is download location.\nThe installation binaries and patches required for release 21.2.3 are:\n  JDK:\n jdk-8u281-linux-x64.tar.gz    Fusion Middleware Infrastructure installer:\n fmw_12.2.1.4.0_infrastructure.jar    WCP installers:\n fmw_12.2.1.4.0_wcportal.jar    Fusion Middleware Infrastructure patches:\n p28186730_139425_Generic.zip (OPatch) p32253037_122140_Generic.zip(WLS) p31544353_122140_Linux-x86-64.zip(WLS ADR Patch) p32124456_122140_Generic.zip(Bundle patch for Oracle Coherence Version 12.2.1.4.7) p31666198_122140_Generic.zip(OPSS Bundle Patch 12.2.1.4.200724) p32357288_122140_Generic.zip(ADF BUNDLE PATCH 12.2.1.4.210107)    WCP patches:\n p32224021_122140_Generic.zip(WCP BUNDLE PATCH 12.2.1.4.201126) p31852495_122140_Generic.zip(WEBCENTER CORE BUNDLE PATCH 12.2.1.4.200905))    Update required build files The following files in the code repository location \u0026lt;imagetool-setup-location\u0026gt;/docker-images/OracleWebCenterPortal/imagetool/12.2.1.4.0 are used for creating the image:\n additionalBuildCmds.txt buildArgs    In the buildArgs file, update all occurrences of %DOCKER_REPO% with the docker-images repository location, which is the complete path of \u0026lt;imagetool-setup-location\u0026gt;/docker-images.\nFor example, update:\n%DOCKER_REPO%/OracleWebCenterPortal/imagetool/12.2.1.4.0/\nto:\n\u0026lt;imagetool-setup-location\u0026gt;/docker-images/OracleWebCenterPortal/imagetool/12.2.1.4.0/\n  Similarly, update the placeholders %JDK_VERSION% and %BUILDTAG% with appropriate values.\n  Update the response file \u0026lt;imagetool-setup-location\u0026gt;/docker-images/OracleFMWInfrastructure/dockerfiles/12.2.1.4/install.file to add the parameter INSTALL_TYPE=\u0026quot;Fusion Middleware Infrastructure\u0026quot; in the [GENERIC] section.\n  Create the image   Add a JDK package to the WebLogic Image Tool cache:\n$ imagetool cache addInstaller --type jdk --version 8u281 --path \u0026lt;download location\u0026gt;/jdk-8u281-linux-x64.tar.gz   Add the downloaded installation binaries to the WebLogic Image Tool cache:\n$ imagetool cache addInstaller --type fmw --version 12.2.1.4.0 --path \u0026lt;download location\u0026gt;/fmw_12.2.1.4.0_infrastructure.jar $ imagetool cache addInstaller --type wcp --version 12.2.1.4.0 --path \u0026lt;download location\u0026gt;/fmw_12.2.1.4.0_wcportal.jar   Add the downloaded OPatch patch to the WebLogic Image Tool cache:\n$ imagetool cache addEntry --key 28186730_13.9.4.2.5 --value \u0026lt;download location\u0026gt;/p28186730_139425_Generic.zip   Append the --opatchBugNumber flag and the OPatch patch key to the create command in the buildArgs file:\n--opatchBugNumber 28186730_13.9.4.2.5   Add the downloaded product patches to the WebLogic Image Tool cache:\n$ imagetool cache addEntry --key 32253037_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p32253037_122140_Generic.zip $ imagetool cache addEntry --key 32124456_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p32124456_122140_Generic.zip $ imagetool cache addEntry --key 32357288_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p32357288_122140_Generic.zip $ imagetool cache addEntry --key 32224021_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p32224021_122140_Generic.zip $ imagetool cache addEntry --key 31666198_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p31666198_122140_Generic.zip $ imagetool cache addEntry --key 31544353_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p31544353_122140_Linux-x86-64.zip $ imagetool cache addEntry --key 31852495_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p31852495_122140_Generic.zip   Append the --patches flag and the product patch keys to the create command in the buildArgs file. The --patches list must be a comma-separated collection of patch --key values used in the imagetool cache addEntry commands above.\nSample --patches list for the product patches added in to the cache:\n--patches 32253037_12.2.1.4.0,32124456_12.2.1.4.0,32357288_12.2.1.4.0,32224021_12.2.1.4.0 Example buildArgs file after appending the OPatch patch and product patches:\ncreate --jdkVersion=8u281 --type wcp --version=12.2.1.4.0 --tag=oracle/wcportal:12.2.1.4 --pull --fromImage ghcr.io/oracle/oraclelinux:7-slim --additionalBuildCommands \u0026lt;imagetool-setup-location\u0026gt;/docker-images/OracleWebCenterPortal/imagetool/12.2.1.4.0/additionalBuildCmds.txt --additionalBuildFiles \u0026lt;imagetool-setup-location\u0026gt;/docker-images/OracleWebCenterPortal/dockerfiles/12.2.1.4/container-scripts --opatchBugNumber 28186730_13.9.4.2.5 --patches 32253037_12.2.1.4.0,32124456_12.2.1.4.0,32357288_12.2.1.4.0,32224021_12.2.1.4.0,31666198_12.2.1.4.0,31544353_12.2.1.4.0,31852495_12.2.1.4.0  Note: In the buildArgs file:\n --jdkVersion value must match the --version value used in the imagetool cache addInstaller command for --type jdk. --version value must match the --version value used in the imagetool cache addInstaller command for --type wcp. --pull always pulls the latest base Linux image oraclelinux:7-slim from the Docker registry. This flag can be removed if you want to use the Linux image oraclelinux:7-slim, which is already available on the host where the WCP image is created.   Refer to this page for the complete list of options available with the WebLogic Image Tool create command.\n  Create the Oracle WebCenter Portal image:\n$ imagetool @\u0026lt;absolute path to buildargs file\u0026gt;  Note: Make sure that the absolute path to the buildargs file is prepended with a @ character, as shown in the example above.\n For example:\n$ imagetool @\u0026lt;imagetool-setup-location\u0026gt;/docker-images/OracleWebCenterPortal/imagetool/12.2.1.4.0/buildArgs    Click here to see the sample Dockerfile generated with the imagetool command.    ########## BEGIN DOCKERFILE ########## # # Copyright (c) 2019, 2021, Oracle and/or its affiliates. # # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # # FROM ghcr.io/oracle/oraclelinux:7-slim as os_update LABEL com.oracle.weblogic.imagetool.buildid=\u0026quot;dabe3ff7-ec35-4b8d-b62a-c3c02fed5571\u0026quot; USER root RUN yum -y --downloaddir=/tmp/imagetool install gzip tar unzip libaio jq hostname procps sudo zip \\ \u0026amp;\u0026amp; yum -y --downloaddir=/tmp/imagetool clean all \\ \u0026amp;\u0026amp; rm -rf /var/cache/yum/* \\ \u0026amp;\u0026amp; rm -rf /tmp/imagetool ## Create user and group RUN if [ -z \u0026quot;$(getent group oracle)\u0026quot; ]; then hash groupadd \u0026amp;\u0026gt; /dev/null \u0026amp;\u0026amp; groupadd oracle || exit -1 ; fi \\ \u0026amp;\u0026amp; if [ -z \u0026quot;$(getent passwd oracle)\u0026quot; ]; then hash useradd \u0026amp;\u0026gt; /dev/null \u0026amp;\u0026amp; useradd -g oracle oracle || exit -1; fi \\ \u0026amp;\u0026amp; mkdir -p /u01 \\ \u0026amp;\u0026amp; chown oracle:oracle /u01 \\ \u0026amp;\u0026amp; chmod 775 /u01 # Install Java FROM os_update as jdk_build LABEL com.oracle.weblogic.imagetool.buildid=\u0026quot;dabe3ff7-ec35-4b8d-b62a-c3c02fed5571\u0026quot; ENV JAVA_HOME=/u01/jdk COPY --chown=oracle:oracle jdk-8u251-linux-x64.tar.gz /tmp/imagetool/ USER oracle RUN tar xzf /tmp/imagetool/jdk-8u251-linux-x64.tar.gz -C /u01 \\ \u0026amp;\u0026amp; $(test -d /u01/jdk* \u0026amp;\u0026amp; mv /u01/jdk* /u01/jdk || mv /u01/graal* /u01/jdk) \\ \u0026amp;\u0026amp; rm -rf /tmp/imagetool \\ \u0026amp;\u0026amp; rm -f /u01/jdk/javafx-src.zip /u01/jdk/src.zip # Install Middleware FROM os_update as wls_build LABEL com.oracle.weblogic.imagetool.buildid=\u0026quot;dabe3ff7-ec35-4b8d-b62a-c3c02fed5571\u0026quot; ENV JAVA_HOME=/u01/jdk \\ ORACLE_HOME=/u01/oracle \\ OPATCH_NO_FUSER=true RUN mkdir -p /u01/oracle \\ \u0026amp;\u0026amp; mkdir -p /u01/oracle/oraInventory \\ \u0026amp;\u0026amp; chown oracle:oracle /u01/oracle/oraInventory \\ \u0026amp;\u0026amp; chown oracle:oracle /u01/oracle COPY --from=jdk_build --chown=oracle:oracle /u01/jdk /u01/jdk/ COPY --chown=oracle:oracle fmw_12.2.1.4.0_infrastructure.jar fmw.rsp /tmp/imagetool/ COPY --chown=oracle:oracle fmw_12.2.1.4.0_wcportal.jar wcp.rsp /tmp/imagetool/ COPY --chown=oracle:oracle oraInst.loc /u01/oracle/ COPY --chown=oracle:oracle p28186730_139425_Generic.zip /tmp/imagetool/opatch/ COPY --chown=oracle:oracle patches/* /tmp/imagetool/patches/ USER oracle RUN echo \u0026quot;INSTALLING MIDDLEWARE\u0026quot; \\ \u0026amp;\u0026amp; echo \u0026quot;INSTALLING fmw\u0026quot; \\ \u0026amp;\u0026amp; \\ /u01/jdk/bin/java -Xmx1024m -jar /tmp/imagetool/fmw_12.2.1.4.0_infrastructure.jar -silent ORACLE_HOME=/u01/oracle \\ -responseFile /tmp/imagetool/fmw.rsp -invPtrLoc /u01/oracle/oraInst.loc -ignoreSysPrereqs -force -novalidation \\ \u0026amp;\u0026amp; echo \u0026quot;INSTALLING wcp\u0026quot; \\ \u0026amp;\u0026amp; \\ /u01/jdk/bin/java -Xmx1024m -jar /tmp/imagetool/fmw_12.2.1.4.0_wcportal.jar -silent ORACLE_HOME=/u01/oracle \\ -responseFile /tmp/imagetool/wcp.rsp -invPtrLoc /u01/oracle/oraInst.loc -ignoreSysPrereqs -force -novalidation \\ \u0026amp;\u0026amp; chmod -R g+r /u01/oracle RUN cd /tmp/imagetool/opatch \\ \u0026amp;\u0026amp; /u01/jdk/bin/jar -xf /tmp/imagetool/opatch/p28186730_139425_Generic.zip \\ \u0026amp;\u0026amp; /u01/jdk/bin/java -jar /tmp/imagetool/opatch/6880880/opatch_generic.jar -silent -ignoreSysPrereqs -force -novalidation oracle_home=/u01/oracle # Apply all patches provided at the same time RUN /u01/oracle/OPatch/opatch napply -silent -oh /u01/oracle -phBaseDir /tmp/imagetool/patches \\ \u0026amp;\u0026amp; test $? -eq 0 \\ \u0026amp;\u0026amp; /u01/oracle/OPatch/opatch util cleanup -silent -oh /u01/oracle \\ || (cat /u01/oracle/cfgtoollogs/opatch/opatch*.log \u0026amp;\u0026amp; exit 1) FROM os_update as final_build ARG ADMIN_NAME ARG ADMIN_HOST ARG ADMIN_PORT ARG MANAGED_SERVER_PORT ENV ORACLE_HOME=/u01/oracle \\ JAVA_HOME=/u01/jdk \\ PATH=${PATH}:/u01/jdk/bin:/u01/oracle/oracle_common/common/bin:/u01/oracle/wlserver/common/bin:/u01/oracle LABEL com.oracle.weblogic.imagetool.buildid=\u0026quot;dabe3ff7-ec35-4b8d-b62a-c3c02fed5571\u0026quot; COPY --from=jdk_build --chown=oracle:oracle /u01/jdk /u01/jdk/ COPY --from=wls_build --chown=oracle:oracle /u01/oracle /u01/oracle/ USER oracle WORKDIR /u01/oracle #ENTRYPOINT /bin/bash ENV ORACLE_HOME=/u01/oracle \\ SCRIPT_FILE=/u01/oracle/container-scripts/* \\ USER_MEM_ARGS=\u0026quot;-Djava.security.egd=file:/dev/./urandom\u0026quot; \\ PATH=$PATH:/usr/java/default/bin:/u01/oracle/oracle_common/common/bin:/u01/oracle/wlserver/common/bin:/u01/oracle/container-scripts USER root RUN env \u0026amp;\u0026amp; \\ mkdir -p /u01/oracle/container-scripts \u0026amp;\u0026amp; \\ mkdir -p /u01/oracle/logs \u0026amp;\u0026amp; \\ mkdir -p /u01/esHome/esNode \u0026amp;\u0026amp; \\ chown oracle:oracle -R /u01 $VOLUME_DIR \u0026amp;\u0026amp; \\ chmod a+xr /u01 COPY --chown=oracle:oracle files/container-scripts/ /u01/oracle/container-scripts/ RUN chmod +xr $SCRIPT_FILE \u0026amp;\u0026amp; \\ rm /u01/oracle/oracle_common/lib/ons.jar /u01/oracle/oracle_common/modules/oracle.jdbc/simplefan.jar USER oracle EXPOSE $WCPORTAL_PORT $ADMIN_PORT WORKDIR ${ORACLE_HOME} CMD [\u0026quot;/u01/oracle/container-scripts/configureOrStartAdminServer.sh\u0026quot;] ########## END DOCKERFILE ##########      Check the created image using the docker images command:\n$ docker images | grep wcportal   Update an image After setting up the WebLogic Image Tool and configuring the build scripts, use the WebLogic Image Tool to update an existing Oracle WebCenter Portal Docker image:\n  Enter the following command to add the OPatch patch to the WebLogic Image Tool cache:\n$ imagetool cache addEntry --key 28186730_13.9.4.2.5 --value \u0026lt;downloaded-patches-location\u0026gt;/p28186730_139425_Generic.zip   Execute the imagetool cache addEntry command for each patch to add the required patch(es) to the WebLogic Image Tool cache. For example, to add patch p30761841_122140_Generic.zip:\n$ imagetool cache addEntry --key=32224021_12.2.1.4.0 --value \u0026lt;downloaded-patches-location\u0026gt;/p32224021_122140_Generic.zip   Provide the following arguments to the WebLogic Image Tool update command:\n –-fromImage - Identify the image that needs to be updated. In the example below, the image to be updated is oracle/wcportal:12.2.1.4. –-patches - Multiple patches can be specified as a comma-separated list. --tag - Specify the new tag to be applied for the image being built.  Refer here for the complete list of options available with the WebLogic Image Tool update command.\n Note: The WebLogic Image Tool cache should have the latest OPatch zip. The WebLogic Image Tool updates the OPatch if it is not already updated in the image.\n Examples   Click here to see the example of update command:    $ imagetool update --fromImage oracle/wcportal:12.2.1.4 --tag=wcportal:12.2.1.4-32224021 --patches=32224021_12.2.1.4.0 [INFO ] Image Tool build ID: 50f9b9aa-596c-4bae-bdff-c47c16b4c928 [INFO ] Temporary directory used for docker build context: /scratch/imagetoolcache/builddir/wlsimgbuilder_temp5130105621506307568 [INFO ] Using patch 28186730_13.9.4.2.5 from cache: /home/imagetool-setup/jars/p28186730_139425_Generic.zip [INFO ] Updating OPatch in final image from version 13.9.4.2.1 to version 13.9.4.2.5 [WARNING] Skipping patch conflict check, no support credentials provided [WARNING] No credentials provided, skipping validation of patches [INFO ] Using patch 32224021_12.2.1.4 from cache: /home/imagetool-setup/jars/p32224021_122140_Generic.zip [INFO ] docker cmd = docker build --no-cache --force-rm --tag wcportal:12.2.1.4-32224021 --build-arg http_proxy=http://\u0026lt;YOUR-COMPANY-PROXY\u0026gt; --build-arg https_proxy=http://\u0026lt;YOUR-COMPANY-PROXY\u0026gt; --build-arg no_proxy=\u0026lt;IP addresses and Domain address for no_proxy\u0026gt;,/var/run/docker.sock \u0026lt;work-directory\u0026gt;/wlstmp/wlsimgbuilder_temp5130105621506307568 Sending build context to Docker daemon 192.4MB Step 1/9 : FROM oracle/wcportal:12.2.1.4 as final_build ---\u0026gt; 5592ff7e5a02 Step 2/9 : USER root ---\u0026gt; Running in 0b3ff2600f11 Removing intermediate container 0b3ff2600f11 ---\u0026gt; faad3a32f39c Step 3/9 : ENV OPATCH_NO_FUSER=true ---\u0026gt; Running in 2beab0bfe88b Removing intermediate container 2beab0bfe88b ---\u0026gt; 6fd9e1664818 Step 4/9 : LABEL com.oracle.weblogic.imagetool.buildid=\u0026quot;50f9b9aa-596c-4bae-bdff-c47c16b4c928\u0026quot; ---\u0026gt; Running in 9a5f8fc172c9 Removing intermediate container 9a5f8fc172c9 ---\u0026gt; 499620a1f857 Step 5/9 : USER oracle ---\u0026gt; Running in fe28af056858 Removing intermediate container fe28af056858 ---\u0026gt; 3507971c35d5 Step 6/9 : COPY --chown=oracle:oracle p28186730_139425_Generic.zip /tmp/imagetool/opatch/ ---\u0026gt; c44c3c7b17f7 Step 7/9 : RUN cd /tmp/imagetool/opatch \u0026amp;\u0026amp; /u01/jdk/bin/jar -xf /tmp/imagetool/opatch/p28186730_139425_Generic.zip \u0026amp;\u0026amp; /u01/jdk/bin/java -jar /tmp/imagetool/opatch/6880880/opatch_generic.jar -silent -ignoreSysPrereqs -force -novalidation oracle_home=/u01/oracle \u0026amp;\u0026amp; rm -rf /tmp/imagetool ---\u0026gt; Running in 8380260fe62d Launcher log file is /tmp/OraInstall2021-04-08_05-18-14AM/launcher2021-04-08_05-18-14AM.log. Extracting the installer . . . . Done Checking if CPU speed is above 300 MHz. Actual 2195.098 MHz Passed Checking swap space: must be greater than 512 MB. Actual 14999 MB Passed Checking if this platform requires a 64-bit JVM. Actual 64 Passed (64-bit not required) Checking temp space: must be greater than 300 MB. Actual 152772 MB Passed Preparing to launch the Oracle Universal Installer from /tmp/OraInstall2021-04-08_05-18-14AM Installation Summary Disk Space : Required 34 MB, Available 152,736 MB Feature Sets to Install: Next Generation Install Core 13.9.4.0.1 OPatch 13.9.4.2.5 OPatch Auto OPlan 13.9.4.2.5 Session log file is /tmp/OraInstall2021-04-08_05-18-14AM/install2021-04-08_05-18-14AM.log Loading products list. Please wait. 1% 40% Loading products. Please wait. 98% 99% Updating Libraries Starting Installations 1% 94% 95% 96% Install pending Installation in progress Component : oracle.glcm.logging 1.6.4.0.0 Copying files for oracle.glcm.logging 1.6.4.0.0 Component : oracle.glcm.comdev 7.8.4.0.0 Copying files for oracle.glcm.comdev 7.8.4.0.0 Component : oracle.glcm.dependency 1.8.4.0.0 Copying files for oracle.glcm.dependency 1.8.4.0.0 Component : oracle.glcm.xmldh 3.4.4.0.0 Copying files for oracle.glcm.xmldh 3.4.4.0.0 Component : oracle.glcm.wizard 7.8.4.0.0 Copying files for oracle.glcm.wizard 7.8.4.0.0 Component : oracle.glcm.opatch.common.api 13.9.4.0.0 Copying files for oracle.glcm.opatch.common.api 13.9.4.0.0 Component : oracle.nginst.common 13.9.4.0.0 Copying files for oracle.nginst.common 13.9.4.0.0 Component : oracle.nginst.core 13.9.4.0.0 Copying files for oracle.nginst.core 13.9.4.0.0 Component : oracle.glcm.encryption 2.7.4.0.0 Copying files for oracle.glcm.encryption 2.7.4.0.0 Component : oracle.swd.opatch 13.9.4.2.5 Copying files for oracle.swd.opatch 13.9.4.2.5 Component : oracle.glcm.osys.core 13.9.1.0.0 Copying files for oracle.glcm.osys.core 13.9.1.0.0 Component : oracle.glcm.oplan.core 13.9.4.2.0 Copying files for oracle.glcm.oplan.core 13.9.4.2.0 Install successful Post feature install pending Post Feature installing Feature Set : glcm_common_lib Feature Set : glcm_common_logging_lib Post Feature installing glcm_common_lib Post Feature installing glcm_common_logging_lib Feature Set : commons-cli_1.3.1.0.0 Post Feature installing commons-cli_1.3.1.0.0 Feature Set : oracle.glcm.opatch.common.api.classpath Post Feature installing oracle.glcm.opatch.common.api.classpath Feature Set : glcm_encryption_lib Post Feature installing glcm_encryption_lib Feature Set : oracle.glcm.osys.core.classpath Post Feature installing oracle.glcm.osys.core.classpath Feature Set : oracle.glcm.oplan.core.classpath Post Feature installing oracle.glcm.oplan.core.classpath Feature Set : oracle.glcm.opatchauto.core.classpath Post Feature installing oracle.glcm.opatchauto.core.classpath Feature Set : oracle.glcm.opatchauto.core.binary.classpath Post Feature installing oracle.glcm.opatchauto.core.binary.classpath Feature Set : oracle.glcm.opatchauto.core.actions.classpath Post Feature installing oracle.glcm.opatchauto.core.actions.classpath Feature Set : oracle.glcm.opatchauto.core.wallet.classpath Post Feature installing oracle.glcm.opatchauto.core.wallet.classpath Post feature install complete String substitutions pending String substituting Component : oracle.glcm.logging 1.6.4.0.0 String substituting oracle.glcm.logging 1.6.4.0.0 Component : oracle.glcm.comdev 7.8.4.0.0 String substituting oracle.glcm.comdev 7.8.4.0.0 Component : oracle.glcm.dependency 1.8.4.0.0 String substituting oracle.glcm.dependency 1.8.4.0.0 Component : oracle.glcm.xmldh 3.4.4.0.0 String substituting oracle.glcm.xmldh 3.4.4.0.0 Component : oracle.glcm.wizard 7.8.4.0.0 String substituting oracle.glcm.wizard 7.8.4.0.0 Component : oracle.glcm.opatch.common.api 13.9.4.0.0 String substituting oracle.glcm.opatch.common.api 13.9.4.0.0 Component : oracle.nginst.common 13.9.4.0.0 String substituting oracle.nginst.common 13.9.4.0.0 Component : oracle.nginst.core 13.9.4.0.0 String substituting oracle.nginst.core 13.9.4.0.0 Component : oracle.glcm.encryption 2.7.4.0.0 String substituting oracle.glcm.encryption 2.7.4.0.0 Component : oracle.swd.opatch 13.9.4.2.5 String substituting oracle.swd.opatch 13.9.4.2.5 Component : oracle.glcm.osys.core 13.9.1.0.0 String substituting oracle.glcm.osys.core 13.9.1.0.0 Component : oracle.glcm.oplan.core 13.9.4.2.0 String substituting oracle.glcm.oplan.core 13.9.4.2.0 String substitutions complete Link pending Linking in progress Component : oracle.glcm.logging 1.6.4.0.0 Linking oracle.glcm.logging 1.6.4.0.0 Component : oracle.glcm.comdev 7.8.4.0.0 Linking oracle.glcm.comdev 7.8.4.0.0 Component : oracle.glcm.dependency 1.8.4.0.0 Linking oracle.glcm.dependency 1.8.4.0.0 Component : oracle.glcm.xmldh 3.4.4.0.0 Linking oracle.glcm.xmldh 3.4.4.0.0 Component : oracle.glcm.wizard 7.8.4.0.0 Linking oracle.glcm.wizard 7.8.4.0.0 Component : oracle.glcm.opatch.common.api 13.9.4.0.0 Linking oracle.glcm.opatch.common.api 13.9.4.0.0 Component : oracle.nginst.common 13.9.4.0.0 Linking oracle.nginst.common 13.9.4.0.0 Component : oracle.nginst.core 13.9.4.0.0 Linking oracle.nginst.core 13.9.4.0.0 Component : oracle.glcm.encryption 2.7.4.0.0 Linking oracle.glcm.encryption 2.7.4.0.0 Component : oracle.swd.opatch 13.9.4.2.5 Linking oracle.swd.opatch 13.9.4.2.5 Component : oracle.glcm.osys.core 13.9.1.0.0 Linking oracle.glcm.osys.core 13.9.1.0.0 Component : oracle.glcm.oplan.core 13.9.4.2.0 Linking oracle.glcm.oplan.core 13.9.4.2.0 Linking in progress Link successful Setup pending Setup in progress Component : oracle.glcm.logging 1.6.4.0.0 Setting up oracle.glcm.logging 1.6.4.0.0 Component : oracle.glcm.comdev 7.8.4.0.0 Setting up oracle.glcm.comdev 7.8.4.0.0 Component : oracle.glcm.dependency 1.8.4.0.0 Setting up oracle.glcm.dependency 1.8.4.0.0 Component : oracle.glcm.xmldh 3.4.4.0.0 Setting up oracle.glcm.xmldh 3.4.4.0.0 Component : oracle.glcm.wizard 7.8.4.0.0 Setting up oracle.glcm.wizard 7.8.4.0.0 Component : oracle.glcm.opatch.common.api 13.9.4.0.0 Setting up oracle.glcm.opatch.common.api 13.9.4.0.0 Component : oracle.nginst.common 13.9.4.0.0 Setting up oracle.nginst.common 13.9.4.0.0 Component : oracle.nginst.core 13.9.4.0.0 Setting up oracle.nginst.core 13.9.4.0.0 Component : oracle.glcm.encryption 2.7.4.0.0 Setting up oracle.glcm.encryption 2.7.4.0.0 Component : oracle.swd.opatch 13.9.4.2.5 Setting up oracle.swd.opatch 13.9.4.2.5 Component : oracle.glcm.osys.core 13.9.1.0.0 Setting up oracle.glcm.osys.core 13.9.1.0.0 Component : oracle.glcm.oplan.core 13.9.4.2.0 Setting up oracle.glcm.oplan.core 13.9.4.2.0 Setup successful Save inventory pending Saving inventory 97% Saving inventory complete 98% Configuration complete Component : glcm_common_logging_lib Saving the inventory glcm_common_logging_lib Component : glcm_encryption_lib Component : oracle.glcm.opatch.common.api.classpath Saving the inventory oracle.glcm.opatch.common.api.classpath Saving the inventory glcm_encryption_lib Component : cieCfg_common_rcu_lib Component : glcm_common_lib Saving the inventory cieCfg_common_rcu_lib Saving the inventory glcm_common_lib Component : oracle.glcm.logging Saving the inventory oracle.glcm.logging Component : cieCfg_common_lib Saving the inventory cieCfg_common_lib Component : svctbl_lib Saving the inventory svctbl_lib Component : com.bea.core.binxml_dependencies Saving the inventory com.bea.core.binxml_dependencies Component : svctbl_jmx_client Saving the inventory svctbl_jmx_client Component : cieCfg_wls_shared_lib Saving the inventory cieCfg_wls_shared_lib Component : rcuapi_lib Saving the inventory rcuapi_lib Component : rcu_core_lib Saving the inventory rcu_core_lib Component : cieCfg_wls_lib Saving the inventory cieCfg_wls_lib Component : cieCfg_wls_external_lib Saving the inventory cieCfg_wls_external_lib Component : cieCfg_wls_impl_lib Saving the inventory cieCfg_wls_impl_lib Component : rcu_dependencies_lib Saving the inventory rcu_dependencies_lib Component : oracle.fmwplatform.fmwprov_lib Saving the inventory oracle.fmwplatform.fmwprov_lib Component : fmwplatform-wlst-dependencies Saving the inventory fmwplatform-wlst-dependencies Component : oracle.fmwplatform.ocp_lib Saving the inventory oracle.fmwplatform.ocp_lib Component : oracle.fmwplatform.ocp_plugin_lib Saving the inventory oracle.fmwplatform.ocp_plugin_lib Component : wlst.wls.classpath Saving the inventory wlst.wls.classpath Component : maven.wls.classpath Saving the inventory maven.wls.classpath Component : com.oracle.webservices.fmw.ws-assembler Saving the inventory com.oracle.webservices.fmw.ws-assembler Component : sdpmessaging_dependencies Saving the inventory sdpmessaging_dependencies Component : sdpclient_dependencies Saving the inventory sdpclient_dependencies Component : com.oracle.jersey.fmw.client Saving the inventory com.oracle.jersey.fmw.client Component : com.oracle.webservices.fmw.client Saving the inventory com.oracle.webservices.fmw.client Component : oracle.jrf.wls.classpath Saving the inventory oracle.jrf.wls.classpath Component : oracle.jrf.wlst Saving the inventory oracle.jrf.wlst Component : fmwshare-wlst-dependencies Saving the inventory fmwshare-wlst-dependencies Component : oracle.fmwshare.pyjar Saving the inventory oracle.fmwshare.pyjar Component : com.oracle.webservices.wls.jaxws-owsm-client Saving the inventory com.oracle.webservices.wls.jaxws-owsm-client Component : glcm_common_logging_lib Component : glcm_common_lib Saving the inventory glcm_common_lib Component : glcm_encryption_lib Saving the inventory glcm_encryption_lib Component : oracle.glcm.opatch.common.api.classpath Saving the inventory oracle.glcm.opatch.common.api.classpath Component : cieCfg_common_rcu_lib Saving the inventory cieCfg_common_rcu_lib Saving the inventory glcm_common_logging_lib Component : oracle.glcm.logging Saving the inventory oracle.glcm.logging Component : cieCfg_common_lib Saving the inventory cieCfg_common_lib Component : svctbl_lib Saving the inventory svctbl_lib Component : com.bea.core.binxml_dependencies Saving the inventory com.bea.core.binxml_dependencies Component : svctbl_jmx_client Saving the inventory svctbl_jmx_client Component : cieCfg_wls_shared_lib Saving the inventory cieCfg_wls_shared_lib Component : rcuapi_lib Saving the inventory rcuapi_lib Component : rcu_core_lib Saving the inventory rcu_core_lib Component : cieCfg_wls_lib Saving the inventory cieCfg_wls_lib Component : cieCfg_wls_external_lib Saving the inventory cieCfg_wls_external_lib Component : cieCfg_wls_impl_lib Saving the inventory cieCfg_wls_impl_lib Component : soa_com.bea.core.binxml_dependencies Saving the inventory soa_com.bea.core.binxml_dependencies Component : glcm_common_logging_lib Saving the inventory glcm_common_logging_lib Component : glcm_common_lib Saving the inventory glcm_common_lib Component : glcm_encryption_lib Saving the inventory glcm_encryption_lib Component : oracle.glcm.opatch.common.api.classpath Component : oracle.glcm.oplan.core.classpath Saving the inventory oracle.glcm.oplan.core.classpath Saving the inventory oracle.glcm.opatch.common.api.classpath The install operation completed successfully. Logs successfully copied to /u01/oracle/.inventory/logs. Removing intermediate container 8380260fe62d ---\u0026gt; d57be7ffa162 Step 8/9 : COPY --chown=oracle:oracle patches/* /tmp/imagetool/patches/ ---\u0026gt; dd421aae5aaf Step 9/9 : RUN /u01/oracle/OPatch/opatch napply -silent -oh /u01/oracle -phBaseDir /tmp/imagetool/patches \u0026amp;\u0026amp; test $? -eq 0 \u0026amp;\u0026amp; /u01/oracle/OPatch/opatch util cleanup -silent -oh /u01/oracle || (cat /u01/oracle/cfgtoollogs/opatch/opatch*.log \u0026amp;\u0026amp; exit 1) ---\u0026gt; Running in 323e7ae70339 Oracle Interim Patch Installer version 13.9.4.2.5 Copyright (c) 2021, Oracle Corporation. All rights reserved. Oracle Home : /u01/oracle Central Inventory : /u01/oracle/.inventory from : /u01/oracle/oraInst.loc OPatch version : 13.9.4.2.5 OUI version : 13.9.4.0.0 Log file location : /u01/oracle/cfgtoollogs/opatch/opatch2021-04-08_05-20-25AM_1.log OPatch detects the Middleware Home as \u0026quot;/u01/oracle\u0026quot; Verifying environment and performing prerequisite checks... OPatch continues with these patches: 32224021 Do you want to proceed? [y|n] Y (auto-answered by -silent) User Responded with: Y All checks passed. Please shutdown Oracle instances running out of this ORACLE_HOME on the local system. (Oracle Home = '/u01/oracle') Is the local system ready for patching? [y|n] Y (auto-answered by -silent) User Responded with: Y Backing up files... Applying interim patch '32224021' to OH '/u01/oracle' ApplySession: Optional component(s) [ oracle.webcenter.sca, 12.2.1.4.0 ] , [ oracle.webcenter.sca, 12.2.1.4.0 ] , [ oracle.webcenter.ucm, 12.2.1.4.0 ] , [ oracle.webcenter.ucm, 12.2.1.4.0 ] not present in the Oracle Home or a higher version is found. Patching component oracle.webcenter.portal, 12.2.1.4... Patching component oracle.webcenter.portal, 12.2.1.4... Patching component oracle.rcu.webcenter.portal, 12.2.1.0... Patching component oracle.rcu.webcenter.portal, 12.2.1.0... Patch 32224021 successfully applied. Log file location: /u01/oracle/cfgtoollogs/opatch/opatch2021-04-08_05-20-25AM_1.log OPatch succeeded. Oracle Interim Patch Installer version 13.9.4.2.5 Copyright (c) 2021, Oracle Corporation. All rights reserved. Oracle Home : /u01/oracle Central Inventory : /u01/oracle/.inventory from : /u01/oracle/oraInst.loc OPatch version : 13.9.4.2.5 OUI version : 13.9.4.0.0 Log file location : /u01/oracle/cfgtoollogs/opatch/opatch2021-04-08_05-27-11AM_1.log OPatch detects the Middleware Home as \u0026quot;/u01/oracle\u0026quot; Invoking utility \u0026quot;cleanup\u0026quot; OPatch will clean up 'restore.sh,make.txt' files and 'scratch,backup' directories. You will be still able to rollback patches after this cleanup. Do you want to proceed? [y|n] Y (auto-answered by -silent) User Responded with: Y Backup area for restore has been cleaned up. For a complete list of files/directories deleted, Please refer log file. OPatch succeeded. Removing intermediate container 323e7ae70339 ---\u0026gt; 0e7c514dcf7b Successfully built 0e7c514dcf7b Successfully tagged wcportal:12.2.1.4-32224021 [INFO ] Build successful. Build time=645s. Image tag=wcportal:12.2.1.4-32224021      Click here to see the example Dockerfile generated by the WebLogic Image Tool with the --dryRun option:    $ imagetool update --fromImage oracle/wcportal:12.2.1.4 --tag=wcportal:12.2.1.4-30761841 --patches=30761841_12.2.1.4.0 --dryRun [INFO ] Image Tool build ID: a473ba32-84b6-4374-9425-9e92ac90ee87 [INFO ] Temporary directory used for docker build context: /scratch/imagetoolcache/builddir/wlsimgbuilder_temp874401188519547557 [INFO ] Using patch 28186730_13.9.4.2.5 from cache: /home/imagetool-setup/jars/p28186730_139425_Generic.zip [INFO ] Updating OPatch in final image from version 13.9.4.2.1 to version 13.9.4.2.5 [WARNING] Skipping patch conflict check, no support credentials provided [WARNING] No credentials provided, skipping validation of patches [INFO ] Using patch 32224021_12.2.1.4 from cache: /home/imagetool-setup/jars/p32224021_122140_Generic.zip [INFO ] docker cmd = docker build --no-cache --force-rm --tag wcportal:12.2.1.4-32224021 --build-arg http_proxy=http://\u0026lt;YOUR-COMPANY-PROXY\u0026gt; --build-arg https_proxy=http://\u0026lt;YOUR-COMPANY-PROXY\u0026gt; --build-arg no_proxy=\u0026lt;IP addresses and Domain address for no_proxy\u0026gt;,/var/run/docker.sock \u0026lt;work-directory\u0026gt;/wlstmp/wlsimgbuilder_temp874401188519547557 ########## BEGIN DOCKERFILE ########## # # Copyright (c) 2019, 2021, Oracle and/or its affiliates. # # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # # FROM oracle/wcportal:12.2.1.4 as final_build USER root ENV OPATCH_NO_FUSER=true LABEL com.oracle.weblogic.imagetool.buildid=\u0026quot;a473ba32-84b6-4374-9425-9e92ac90ee87\u0026quot; USER oracle COPY --chown=oracle:oracle p28186730_139425_Generic.zip /tmp/imagetool/opatch/ RUN cd /tmp/imagetool/opatch \\ \u0026amp;\u0026amp; /u01/jdk/bin/jar -xf /tmp/imagetool/opatch/p28186730_139425_Generic.zip \\ \u0026amp;\u0026amp; /u01/jdk/bin/java -jar /tmp/imagetool/opatch/6880880/opatch_generic.jar -silent -ignoreSysPrereqs -force -novalidation oracle_home=/u01/oracle \\ \u0026amp;\u0026amp; rm -rf /tmp/imagetool COPY --chown=oracle:oracle patches/* /tmp/imagetool/patches/ # Apply all patches provided at the same time RUN /u01/oracle/OPatch/opatch napply -silent -oh /u01/oracle -phBaseDir /tmp/imagetool/patches \\ \u0026amp;\u0026amp; test $? -eq 0 \\ \u0026amp;\u0026amp; /u01/oracle/OPatch/opatch util cleanup -silent -oh /u01/oracle \\ || (cat /u01/oracle/cfgtoollogs/opatch/opatch*.log \u0026amp;\u0026amp; exit 1) ########## END DOCKERFILE ##########      Check the built image using the docker images command:\n$ docker images | grep wcportal wcportal 12.2.1.4-30761841 2ef2a67a685b About a minute ago 3.58GB $   Create an Oracle WebCenter Portal Docker image using Dockerfile For test and development purposes, you can create an Oracle WebCenter Portal image using the Dockerfile. Consult the README file for important prerequisite steps, such as building or pulling the Server JRE Docker image, Oracle Fusion Middleware Infrastructure Docker image and downloading the Oracle WebCenter Portal installer and bundle patch binaries.\nA prebuilt Oracle Fusion Middleware Infrastructure image, container-registry.oracle.com/middleware/fmw-infrastructure:12.2.1.4, is available at container-registry.oracle.com. We recommend that you pull and rename this image to build the Oracle WebCenter Portal image.\n$ docker pull container-registry.oracle.com/middleware/fmw-infrastructure:12.2.1.4 $ docker tag container-registry.oracle.com/middleware/fmw-infrastructure:12.2.1.4 oracle/fmw-infrastructure:12.2.1.4 To build an Oracle Fusion Middleware Infrastructure image and on top of that the Oracle WebCenter Portal image as a layer, follow these steps:\n  Make a local clone of the sample repository:\n$ git clone https://github.com/oracle/docker-images   Download the Oracle WebCenter Portal installer from the Oracle Technology Network or e-delivery.\n Note: Copy the installer binaries to the same location as the Dockerfile.\n   Create the Oracle WebCenter Portal image by running the provided script:\n$ cd docker-images/OracleWebCenterPortal/dockerfiles $ ./buildDockerImage.sh -v 12.2.1.4 -s The image produced is named oracle/wcportal:12.2.1.4. The samples and instructions assume the Oracle WebCenter Portal image is named oracle/wcportal:12.2.1.4. You must rename your image to match this name, or update the samples to refer to the image you created.\n  "
},
{
	"uri": "/fmw-kubernetes/soa-domains/",
	"title": "Oracle SOA Suite",
	"tags": [],
	"description": "The Oracle WebLogic Kubernetes Operator (the “operator”) supports deployment of Oracle SOA Suite components such as Oracle Service-Oriented Architecture (SOA), Oracle Service Bus, and Oracle Enterprise Scheduler (ESS). Follow the instructions in this guide to set up these Oracle SOA Suite domains on Kubernetes.",
	"content": "The WebLogic Kubernetes Operator (the “operator”) supports deployment of Oracle SOA Suite components such as Oracle Service-Oriented Architecture (SOA), Oracle Service Bus, and Oracle Enterprise Scheduler (ESS). Currently the operator supports these domain types:\n soa : Deploys a SOA domain with Oracle Enterprise Scheduler (ESS) osb : Deploys an Oracle Service Bus domain soaosb : Deploys a domain with SOA, Oracle Service Bus, and Oracle Enterprise Scheduler (ESS)  In this release, Oracle SOA Suite domains are supported using the “domain on a persistent volume” model only, where the domain home is located in a persistent volume (PV).\nThe operator has several key features to assist you with deploying and managing Oracle SOA Suite domains in a Kubernetes environment. You can:\n Create Oracle SOA Suite instances in a Kubernetes persistent volume (PV). This PV can reside in an NFS file system or other Kubernetes volume types. Start servers based on declarative startup parameters and desired states. Expose the Oracle SOA Suite services and composites for external access. Scale Oracle SOA Suite domains by starting and stopping Managed Servers on demand, or by integrating with a REST API. Publish operator and WebLogic Server logs to Elasticsearch and interact with them in Kibana. Monitor the Oracle SOA Suite instance using Prometheus and Grafana.  Current production release The current production release for the Oracle SOA Suite domains deployment on Kubernetes is 22.2.2. This release uses the WebLogic Kubernetes Operator version 3.4.0.\nRecent changes and known issues See the Release Notes for recent changes and known issues for Oracle SOA Suite domains deployment on Kubernetes.\nLimitations See here for limitations in this release.\nAbout this documentation This documentation includes sections targeted to different audiences. To help you find what you are looking for more easily, please consult this table of contents:\n  Quick Start explains how to quickly get an Oracle SOA Suite domain instance running using default settings. Note that this is only for development and test purposes.\n  Install Guide and Administration Guide provide detailed information about all aspects of using the Kubernetes operator including:\n Installing and configuring the operator. Using the operator to create and manage Oracle SOA Suite domains. Configuring Kubernetes load balancers. Configuring custom SSL certificates. Configuring Elasticsearch and Kibana to access the operator and WebLogic Server log files. Deploying composite applications for Oracle SOA Suite and Oracle Service Bus. Patching an Oracle SOA Suite Docker image. Removing domains. And much more!    Documentation for earlier releases To view documentation for an earlier release, see:\n Version 22.1.2 Version 21.4.2 Version 21.3.2 Version 21.2.2 Version 21.1.2 Version 20.4.2 Version 20.3.3  Additional reading Oracle SOA Suite domains deployment on Kubernetes leverages the WebLogic Kubernetes Operator framework.\n To develop an understanding of the operator, including design, architecture, domain life cycle management, and configuration overrides, review the operator documentation. To learn more about the Oracle SOA Suite architecture and components, see Understanding Oracle SOA Suite. To review the known issues and common questions for Oracle SOA Suite domains deployment on Kubernetes, see the frequently asked questions.  "
},
{
	"uri": "/fmw-kubernetes/wcportal-domains/manage-wcportal-domains/configure-load-balancer/apachewebtier/",
	"title": "Apache webtier",
	"tags": [],
	"description": "Configure the Apache webtier load balancer for an Oracle WebCenter Portal domain.",
	"content": "To load balance Oracle WebCenter Portal domain clusters, you can install Apache webtier and configure it for non-SSL and SSL termination access of the application URL. Follow these steps to set up Apache webtier as a load balancer for an Oracle WebCenter Portal domain in a Kubernetes cluster:\n Build the Apache webtier image Create the Apache plugin configuration file Prepare the certificate and private key Install the Apache webtier Helm chart Verify domain application URL access Uninstall Apache webtier  Build the Apache webtier image To build the Apache webtier Docker image, refer to the sample.\nCreate the Apache plugin configuration file  The configuration file named custom_mod_wl_apache.conf should have all the URL routing rules for the Oracle WebCenter Portal applications deployed in the domain that needs to be accessible externally. Update this file with values based on your environment. The file content is similar to below.    Click here to see the sample content of the configuration file custom_mod_wl_apache.conf for wcp-domain domain   $ cat ${WORKDIR}/charts/apache-samples/custom-sample/custom_mod_wl_apache.conf #Copyright (c) 2018 Oracle and/or its affiliates. All rights reserved. # # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # \u0026lt;IfModule mod_weblogic.c\u0026gt; WebLogicHost \u0026lt;WEBLOGIC_HOST\u0026gt; WebLogicPort 7001 \u0026lt;/IfModule\u0026gt; # Directive for weblogic admin Console deployed on Weblogic Admin Server \u0026lt;Location /console\u0026gt; SetHandler weblogic-handler WebLogicHost wcp-domain-adminserver WebLogicPort 7001 \u0026lt;/Location\u0026gt; \u0026lt;Location /em\u0026gt; SetHandler weblogic-handler WebLogicHost wcp-domain-adminserver WebLogicPort 7001 \u0026lt;/Location\u0026gt; \u0026lt;Location /webcenter\u0026gt; WLSRequest On WebLogicCluster wcp-domain-cluster-wcp-cluster:8888 PathTrim /weblogic1 \u0026lt;/Location\u0026gt; \u0026lt;Location /rsscrawl\u0026gt; WLSRequest On WebLogicCluster wcp-domain-cluster-wcp-cluster:8888 PathTrim /weblogic1 \u0026lt;/Location\u0026gt; \u0026lt;Location /rest\u0026gt; WLSRequest On WebLogicCluster wcp-domain-cluster-wcp-cluster:8888 PathTrim /weblogic1 \u0026lt;/Location\u0026gt; \u0026lt;Location /webcenterhelp\u0026gt; WLSRequest On WebLogicCluster wcp-domain-cluster-wcp-cluster:8888 PathTrim /weblogic1 \u0026lt;/Location\u0026gt; \u0026lt;Location /wsrp-tools\u0026gt; WLSRequest On WebLogicCluster wcp-domain-cluster-wcportlet-cluster:8889 PathTrim /weblogic1 \u0026lt;/Location\u0026gt; \u0026lt;Location /portalTools\u0026gt; WLSRequest On WebLogicCluster wcp-domain-cluster-wcportlet-cluster:8889 PathTrim /weblogic1 \u0026lt;/Location\u0026gt;     Update persistentVolumeClaimName in ${WORKDIR}/charts/apache-samples/custom-sample/input.yamlwith Persistence Volume which contains your own custom_mod_wl_apache.conf file. Use the PV/PVC created at the time of preparing environment, Copy the custom_mod_wl_apache.conf file to existing PersistantVolume.  Prepare the certificate and private key   (For the SSL termination configuration only) Run the following commands to generate your own certificate and private key using openssl.\n$ cd ${WORKDIR}/charts/apache-samples/custom-sample $ export VIRTUAL_HOST_NAME=WEBLOGIC_HOST $ export SSL_CERT_FILE=WEBLOGIC_HOST.crt $ export SSL_CERT_KEY_FILE=WEBLOGIC_HOST.key $ sh certgen.sh  NOTE: Replace WEBLOGIC_HOST with the name of the host on which Apache webtier is to be installed.\n   Click here to see the output of the certifcate generation   $ls certgen.sh custom_mod_wl_apache.conf custom_mod_wl_apache.conf_orig input.yaml README.md $ sh certgen.sh Generating certs for WEBLOGIC_HOST Generating a 2048 bit RSA private key ........................+++ .......................................................................+++ unable to write \u0026#39;random state\u0026#39; writing new private key to \u0026#39;apache-sample.key\u0026#39; ----- $ ls certgen.sh custom_mod_wl_apache.conf_orig WEBLOGIC_HOST.info config.txt input.yaml WEBLOGIC_HOST.key custom_mod_wl_apache.conf WEBLOGIC_HOST.crt README.md      Prepare input values for the Apache webtier Helm chart.\nRun the following commands to prepare the input value file for the Apache webtier Helm chart.\n$ base64 -i ${SSL_CERT_FILE} | tr -d \u0026#39;\\n\u0026#39; $ base64 -i ${SSL_CERT_KEY_FILE} | tr -d \u0026#39;\\n\u0026#39; $ touch input.yaml Update virtualHostName with the value of the WEBLOGIC_HOST in file ${WORKDIR}/charts/apache-samples/custom-sample/input.yaml\n  Click here to see the snapshot of the sample input.yaml file   $ cat apache-samples/custom-sample/input.yaml # Use this to provide your own Apache webtier configuration as needed; simply define this # path and put your own custom_mod_wl_apache.conf file under this path. persistentVolumeClaimName: wcp-domain-domain-pvc # The VirtualHostName of the Apache HTTP server. It is used to enable custom SSL configuration. virtualHostName: \u0026lt;WEBLOGIC_HOST\u0026gt;      Install the Apache webtier Helm chart   Install the Apache webtier Helm chart to the domain wcpns namespace with the specified input parameters:\n$ cd ${WORKDIR}/charts $ kubectl create namespace apache-webtier $ helm install apache-webtier --values apache-samples/custom-sample/input.yaml --namespace wcpns apache-webtier --set image=oracle/apache:12.2.1.3   Check the status of the Apache webtier:\n$ kubectl get all -n wcpns | grep apache Sample output of the status of the apache webtier:\npod/apache-webtier-apache-webtier-65f69dc6bc-zg5pj 1/1 Running 0 22h service/apache-webtier-apache-webtier NodePort 10.108.29.98 \u0026lt;none\u0026gt; 80:30305/TCP,4433:30443/TCP 22h deployment.apps/apache-webtier-apache-webtier 1/1 1 1 22h replicaset.apps/apache-webtier-apache-webtier-65f69dc6bc 1 1 1 22h   Verify domain application URL access Once the Apache webtier load balancer is up, verify that the domain applications are accessible through the load balancer port 30305/30443. The application URLs for domain of type wcp are:\n Note: Port 30305 is the LOADBALANCER-Non-SSLPORT and Port 30443 is LOADBALANCER-SSLPORT.\n Non-SSL configuration http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/console http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/em http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/webcenter http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/webcenterhelp http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/rest http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/rsscrawl SSL configuration https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/webcenter https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/console https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/em https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/rsscrawl https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/webcenterhelp https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/rest Uninstall Apache webtier $ helm delete apache-webtier -n wcpns "
},
{
	"uri": "/fmw-kubernetes/wcsites-domains/adminguide/elasticsearch-integration/",
	"title": "Elasticsearch integration for logs",
	"tags": [],
	"description": "Monitor an Oracle WebCenter Sites domain and publish the WebLogic Server logs to Elasticsearch.",
	"content": "1. Integrate Elasticsearch to WebLogic Kubernetes Operator For reference information, see Elasticsearch integration for the WebLogic Kubernetes Operator.\nTo enable elasticsearch integration, you must edit file kubernetes/charts/weblogic-operator/values.yaml before deploying the WebLogic Kubernetes Operator.\n# elkIntegrationEnabled specifies whether or not ELK integration is enabled. elkIntegrationEnabled: true # logStashImage specifies the Docker image containing logstash. # This parameter is ignored if 'elkIntegrationEnabled' is false. logStashImage: \u0026quot;logstash:6.6.0\u0026quot; # elasticSearchHost specifies the hostname of where Elasticsearch is running. # This parameter is ignored if 'elkIntegrationEnabled' is false. elasticSearchHost: \u0026quot;elasticsearch.default.svc.cluster.local\u0026quot; # elasticSearchPort specifies the port number of where Elasticsearch is running. # This parameter is ignored if 'elkIntegrationEnabled' is false. elasticSearchPort: 9200\rAfter you\u0026rsquo;ve deployed WebLogic Kubernetes Operator and made the above changes, the weblogic-operator pod will have additional Logstash container. The Logstash container will push the weblogic-operator logs to the configured Elasticsearch server.\n2. Publish WebLogic Server and WebCenter Sites Logs using Logstash Pod You can publish the WebLogic Server logs to Elasticsearch Server using Logstash pod. This Logstash pod must have access to the shared domain home. For the WebCenter Sites wcsitesinfra, you can use the persistent volume of the domain home in the Logstash pod. The steps to create the Logstash pod are as follows:\nSample Logstash configuration file is located at kubernetes/create-wcsites-domain/utils/logstash/logstash.conf\n$ vi kubernetes/create-wcsites-domain/utils/logstash/logstash.conf input {\rfile {\rpath =\u0026gt; \u0026quot;/u01/oracle/user_projects/logs/wcsitesinfra/adminserver.log\u0026quot;\rstart_position =\u0026gt; beginning\r}\rfile {\rpath =\u0026gt; \u0026quot;/u01/oracle/user_projects/logs/wcsitesinfra/wcsites-server*.log\u0026quot;\rstart_position =\u0026gt; beginning\r}\rfile {\rpath =\u0026gt; \u0026quot;/u01/oracle/user_projects/logs/wcsitesinfra/adminserver.out\u0026quot;\rstart_position =\u0026gt; beginning\r}\rfile {\rpath =\u0026gt; \u0026quot;/u01/oracle/user_projects/logs/wcsitesinfra/wcsites-server*.out\u0026quot;\rstart_position =\u0026gt; beginning\r}\rfile {\rpath =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/wcsitesinfra/servers/**/logs/sites.log\u0026quot;\rstart_position =\u0026gt; beginning\r}\rfile {\rpath =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/wcsitesinfra/servers/**/logs/cas.log\u0026quot;\rstart_position =\u0026gt; beginning\r}\r}\rfilter {\rgrok {\rmatch =\u0026gt; [ \u0026quot;message\u0026quot;, \u0026quot;\u0026lt;%{DATA:log_timestamp}\u0026gt; \u0026lt;%{WORD:log_level}\u0026gt; \u0026lt;%{WORD:thread}\u0026gt; \u0026lt;%{HOSTNAME:hostname}\u0026gt; \u0026lt;%{HOSTNAME:servername}\u0026gt; \u0026lt;%{DATA:timer}\u0026gt; \u0026lt;\u0026lt;%{DATA:kernel}\u0026gt;\u0026gt; \u0026lt;\u0026gt; \u0026lt;%{DATA:uuid}\u0026gt; \u0026lt;%{NUMBER:timestamp}\u0026gt; \u0026lt;%{DATA:misc}\u0026gt; \u0026lt;%{DATA:log_number}\u0026gt; \u0026lt;%{DATA:log_message}\u0026gt;\u0026quot; ]\r}\r}\routput {\relasticsearch {\rhosts =\u0026gt; [\u0026quot;elasticsearch.default.svc.cluster.local:9200\u0026quot;]\r}\r}\rHere ** means that all sites.log and cas.log from any servers under wcsitesinfra will be pushed to Logstash.\n$ kubectl cp kubernetes/create-wcsites-domain/utils/logstash/logstash.conf wcsites-ns/wcsitesinfra-adminserver:/u01/oracle/user_projects/logs/logstash.conf Get the persistent volume details of the domain home of the WebLogic Server(s). The following command will list the persistent volume details in the namespace - \u0026ldquo;wcsites-ns\u0026rdquo;:\n$ kubectl get pv -n wcsites-ns NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE wcsitesinfra-domain-pv 10Gi RWX Retain Bound wcsites-ns/wcsitesinfra-domain-pvc wcsitesinfra-domain-storage-class 5d21h Sample Logstash deployment is located at kubernetes/create-wcsites-domain/utils/logstash/logstash.yaml for Logstash pod. The mounted persistent volume of the domain home will provide access to the WebLogic Server logs to Logstash pod.\napiVersion: apps/v1beta1\rkind: Deployment\rmetadata:\rname: logstash-wls\rnamespace: wcsites-ns\rspec:\rtemplate: # create pods using pod definition in this template\rmetadata:\rlabels:\rk8s-app: logstash-wls\rspec:\rvolumes:\r- name: weblogic-domain-storage-volume\rpersistentVolumeClaim:\rclaimName: wcsitesinfra-domain-pvc\r- name: shared-logs\remptyDir: {}\rcontainers:\r- name: logstash\rimage: logstash:6.6.0\rcommand: [\u0026quot;/bin/sh\u0026quot;]\rargs: [\u0026quot;/usr/share/logstash/bin/logstash\u0026quot;, \u0026quot;-f\u0026quot;, \u0026quot;/u01/oracle/user_projects/logs/logstash.conf\u0026quot;]\rimagePullPolicy: IfNotPresent\rvolumeMounts:\r- mountPath: /u01/oracle/user_projects\rname: weblogic-domain-storage-volume\r- name: shared-logs\rmountPath: /shared-logs\rports:\r- containerPort: 5044\rname: logstash\rAfter you have created the Logstash deployment yaml and Logstash configuration file, deploy Logstash using following command:\n$ kubectl create -f kubernetes/create-wcsites-domain/utils/logstash/logstash.yaml 3. Test the Deployment of Elasticsearch and Kibana The WebLogic Operator also provides a sample deployment of Elasticsearch and Kibana for testing purpose. You can deploy Elasticsearch and Kibana on the Kubernetes cluster as shown below:\n$ kubectl create -f kubernetes/elasticsearch-and-kibana/elasticsearch_and_kibana.yaml Get the Kibana dashboard port information as shown below: Wait for pods to start:\n-bash-4.2$ kubectl get pods -w NAME READY STATUS RESTARTS AGE elasticsearch-8bdb7cf54-mjs6s 1/1 Running 0 4m3s kibana-dbf8964b6-n8rcj 1/1 Running 0 4m3s -bash-4.2$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE elasticsearch ClusterIP 10.100.11.154 \u0026lt;none\u0026gt; 9200/TCP,9300/TCP 4m32s kibana NodePort 10.97.205.0 \u0026lt;none\u0026gt; 5601:31884/TCP 4m32s kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 71d You can access the Kibana dashboard at http://mycompany.com:kibana-nodeport/. In our example, the node port would be 31884.\nCreate an Index Pattern in Kibana Create an index pattern logstash* in Kibana \u0026gt; Management. After the servers are started, you will see the log data in the Kibana dashboard:\n"
},
{
	"uri": "/fmw-kubernetes/soa-domains/adminguide/monitoring-soa-domains/",
	"title": "Monitor a domain and publish logs",
	"tags": [],
	"description": "Monitor an Oracle SOA Suite domain and publish the WebLogic Server logs to Elasticsearch.",
	"content": "After the Oracle SOA Suite domain is set up, you can:\n Monitor the Oracle SOA Suite instance using Prometheus and Grafana Publish WebLogic Server logs into Elasticsearch Publish SOA server diagnostics logs into Elasticsearch  Monitor the Oracle SOA Suite instance using Prometheus and Grafana Using the WebLogic Monitoring Exporter you can scrape runtime information from a running Oracle SOA Suite instance and monitor them using Prometheus and Grafana.\nSet up monitoring Follow these steps to set up monitoring for an Oracle SOA Suite instance. For more details on WebLogic Monitoring Exporter, see here.\nPublish WebLogic Server logs into Elasticsearch You can publish the WebLogic Server logs to Elasticsearch using the WebLogic Logging exporter and interact with them in Kibana. See Publish logs to Elasticsearch.\nWebLogic Server logs can also be published to Elasticsearch using Fluentd. See Fluentd configuration steps.\nPublish SOA server diagnostics logs into Elasticsearch This section shows you how to publish diagnostics logs to Elasticsearch and view them in Kibana. For publishing operator logs, see this sample.\nPrerequisites If you have not already set up Elasticsearch and Kibana for logs collection, refer this document and complete the setup.\nPublish to Elasticsearch The Diagnostics or other logs can be pushed to Elasticsearch server using logstash pod. The logstash pod should have access to the shared domain home or the log location. In case of the Oracle SOA Suite domain, the persistent volume of the domain home can be used in the logstash pod. The steps to create the logstash pod are,\n  Get Domain home persistence volume claim details of the domain home of the Oracle SOA Suite domain. The following command will list the persistent volume claim details in the namespace - soans. In the example below the persistent volume claim is soainfra-domain-pvc:\n$ kubectl get pvc -n soans Sample output:\nNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE soainfra-domain-pvc Bound soainfra-domain-pv 10Gi RWX soainfra-domain-storage-class xxd   Create logstash configuration file (logstash.conf). Below is a sample logstash configuration to push diagnostic logs of all servers available at DOMAIN_HOME/servers/\u0026lt;server_name\u0026gt;/logs/-diagnostic.log:\ninput { file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/soainfra/servers/**/logs/*-diagnostic.log\u0026quot; start_position =\u0026gt; beginning } } filter { grok { match =\u0026gt; [ \u0026quot;message\u0026quot;, \u0026quot;\u0026lt;%{DATA:log_timestamp}\u0026gt; \u0026lt;%{WORD:log_level}\u0026gt; \u0026lt;%{WORD:thread}\u0026gt; \u0026lt;%{HOSTNAME:hostname}\u0026gt; \u0026lt;%{HOSTNAME:servername}\u0026gt; \u0026lt;%{DATA:timer}\u0026gt; \u0026lt;\u0026lt;%{DATA:kernel}\u0026gt;\u0026gt; \u0026lt;\u0026gt; \u0026lt;%{DATA:uuid}\u0026gt; \u0026lt;%{NUMBER:timestamp}\u0026gt; \u0026lt;%{DATA:misc}\u0026gt; \u0026lt;%{DATA:log_number}\u0026gt; \u0026lt;%{DATA:log_message}\u0026gt;\u0026quot; ] } } output { elasticsearch { hosts =\u0026gt; [\u0026quot;elasticsearch.default.svc.cluster.local:9200\u0026quot;] } }   Copy the logstash.conf into say /u01/oracle/user_projects/domains so that it can be used for logstash deployment, using Administration Server pod ( For example soainfra-adminserver pod in namespace soans):\n$ kubectl cp logstash.conf soans/soainfra-adminserver:/u01/oracle/user_projects/domains --namespace soans   Create deployment YAML (logstash.yaml) for logstash pod using the domain home persistence volume claim. Make sure to point the logstash configuration file to correct location ( For example: we copied logstash.conf to /u01/oracle/user_projects/domains/logstash.conf) and also correct domain home persistence volume claim. Below is a sample logstash deployment YAML:\napiVersion: apps/v1 kind: Deployment metadata: name: logstash-soa namespace: soans spec: selector: matchLabels: app: logstash-soa template: # create pods using pod definition in this template metadata: labels: app: logstash-soa spec: volumes: - name: soainfra-domain-storage-volume persistentVolumeClaim: claimName: soainfra-domain-pvc - name: shared-logs emptyDir: {} containers: - name: logstash image: logstash:6.6.0 command: [\u0026quot;/bin/sh\u0026quot;] args: [\u0026quot;/usr/share/logstash/bin/logstash\u0026quot;, \u0026quot;-f\u0026quot;, \u0026quot;/u01/oracle/user_projects/domains/logstash.conf\u0026quot;] imagePullPolicy: IfNotPresent volumeMounts: - mountPath: /u01/oracle/user_projects name: soainfra-domain-storage-volume - name: shared-logs mountPath: /shared-logs ports: - containerPort: 5044 name: logstash   Deploy logstash to start publish logs to Elasticsearch:\n$ kubectl create -f logstash.yaml   Now, you can view the diagnostics logs using Kibana with index pattern \u0026ldquo;logstash-*\u0026rdquo;.\n  "
},
{
	"uri": "/fmw-kubernetes/wcsites-domains/adminguide/weblogic-logging-exporter-setup/",
	"title": "Publish logs to Elasticsearch",
	"tags": [],
	"description": "Monitor an Oracle WebCenter Sites domain and publish the WebLogic Server logs to Elasticsearch.",
	"content": "The WebLogic Logging Exporter adds a log event handler to WebLogic Server. WebLogic Server logs can be pushed to Elasticsearch in Kubernetes directly by using the Elasticsearch REST API. For more details, see to the WebLogic Logging Exporter project.\nThis sample shows you how to publish WebLogic Server logs to Elasticsearch and view them in Kibana. For publishing operator logs, see this sample.\nPrerequisites This document assumes that you have already set up Elasticsearch and Kibana for logs collection. If you have not, please see this document.\n Download the WebLogic Logging Exporter binaries The pre-built binaries are available on the WebLogic Logging Exporter Releases page.\nDownload:\n weblogic-logging-exporter-1.0.0.jar from the Releases page. snakeyaml-1.25.jar from Maven Central.  These identifiers are used in the sample commands in this document.\n wcsites-ns: WebCenter Sites domain namespace wcsitesinfra: domainUID wcsitesinfra-adminserver: Administration Server pod name   Copy the JAR Files to the WebLogic Domain Home Copy the weblogic-logging-exporter-1.0.0.jar and snakeyaml-1.25.jar files to the domain home directory in the Administration Server pod.\n$ kubectl cp \u0026lt;file-to-copy\u0026gt; \u0026lt;namespace\u0026gt;/\u0026lt;Administration-Server-pod\u0026gt;:\u0026lt;domainhome\u0026gt; $ kubectl cp snakeyaml-1.25.jar wcsites-ns/wcsitesinfra-adminserver:/u01/oracle/user_projects/domains/wcsitesinfra/ $ kubectl cp weblogic-logging-exporter-1.0.0.jar wcsites-ns/wcsitesinfra-adminserver:/u01/oracle/user_projects/domains/wcsitesinfra/ Add a Startup Class to the Domain Configuration   In the WebLogic Server Administration Console, in the left navigation pane, expand Environment, and then select Startup and Shutdown Classes.\n  Add a new startup class. You may choose any descriptive name, however, the class name must be weblogic.logging.exporter.Startup.\n  Target the startup class to each server from which you want to export logs.\n  In your /u01/oracle/user_projects/domains/wcsitesinfra/config/config.xml file, this update should look similar to the following example:\n$ kubectl exec -it wcsitesinfra-adminserver -n wcsites-ns cat /u01/oracle/user_projects/domains/wcsitesinfra/config/config.xml \u0026lt;startup-class\u0026gt; \u0026lt;name\u0026gt;weblogic-logging-exporter\u0026lt;/name\u0026gt; \u0026lt;target\u0026gt;AdminServer,wcsites_cluster\u0026lt;/target\u0026gt; \u0026lt;class-name\u0026gt;weblogic.logging.exporter.Startup\u0026lt;/class-name\u0026gt; \u0026lt;/startup-class\u0026gt;   Update the WebLogic Server CLASSPATH   Copy the setDomainEnv.sh file from the pod to a local folder:\n$ kubectl cp wcsites-ns/wcsitesinfra-adminserver:/u01/oracle/user_projects/domains/wcsitesinfra/bin/setDomainEnv.sh $PWD/setDomainEnv.sh tar: Removing leading `/' from member names Ignore exception: tar: Removing leading '/' from member names\n  Update the server class path in setDomainEnv.sh:\nCLASSPATH=/u01/oracle/user_projects/domains/wcsitesinfra/weblogic-logging-exporter-1.0.0.jar:/u01/oracle/user_projects/domains/wcsitesinfra/snakeyaml-1.25.jar:${CLASSPATH} export CLASSPATH   Copy back the modified setDomainEnv.sh file to the pod:\n$ kubectl cp setDomainEnv.sh wcsites-ns/wcsitesinfra-adminserver:/u01/oracle/user_projects/domains/wcsitesinfra/bin/setDomainEnv.sh ``\n  Create a Configuration File for the WebLogic Logging Exporter   Specify the Elasticsearch server host and port number in file kubernetes/create-wcsites-domain/utils/weblogic-logging-exporter/WebLogicLoggingExporter.yaml:\nExample:\nweblogicLoggingIndexName: wls publishHost: elasticsearch.default.svc.cluster.local publishPort: 9200 domainUID: wcsitesinfra weblogicLoggingExporterEnabled: true weblogicLoggingExporterSeverity: TRACE weblogicLoggingExporterBulkSize: 1   Copy the WebLogicLoggingExporter.yaml file to the domain home directory in the WebLogic Administration Server pod:\n$ kubectl cp kubernetes/create-wcsites-domain/utils/weblogic-logging-exporter/WebLogicLoggingExporter.yaml wcsites-ns/wcsitesinfra-adminserver:/u01/oracle/user_projects/domains/wcsitesinfra/config/   Restart All the Servers in the Domain To restart the servers, stop and then start them using the following commands:\nTo stop the servers:\n$ kubectl patch domain wcsitesinfra -n wcsites-ns --type='json' -p='[{\u0026quot;op\u0026quot;: \u0026quot;replace\u0026quot;, \u0026quot;path\u0026quot;: \u0026quot;/spec/serverStartPolicy\u0026quot;, \u0026quot;value\u0026quot;: \u0026quot;NEVER\u0026quot; }]' To start the servers:\n$ kubectl patch domain wcsitesinfra -n wcsites-ns --type='json' -p='[{\u0026quot;op\u0026quot;: \u0026quot;replace\u0026quot;, \u0026quot;path\u0026quot;: \u0026quot;/spec/serverStartPolicy\u0026quot;, \u0026quot;value\u0026quot;: \u0026quot;IF_NEEDED\u0026quot; }]' After all the servers are restarted, see their server logs to check that the weblogic-logging-exporter class is called, as shown below:\n======================= WebLogic Logging Exporter Startup class called Reading configuration from file name: /u01/oracle/user_projects/domains/wcsitesinfra/config/WebLogicLoggingExporter.yaml Config{weblogicLoggingIndexName='wls', publishHost='domain.host.com', publishPort=9200, weblogicLoggingExporterSeverity='Notice', weblogicLoggingExporterBulkSize='2', enabled=true, weblogicLoggingExporterFilters=FilterConfig{expression='NOT(MSGID = 'BEA-000449')', servers=[]}], domainUID='wcsitesinfra'} Create an Index Pattern in Kibana Create an index pattern wls* in Kibana \u0026gt; Management. After the servers are started, you will see the log data in the Kibana dashboard:\n"
},
{
	"uri": "/fmw-kubernetes/wcportal-domains/appendix/additional-configuration/",
	"title": "Additional Configuration",
	"tags": [],
	"description": "Describes how to create connections to Oracle WebCenter Content Server to enable content integration within Oracle WebCenter Portal.",
	"content": "Creating a Connection to Oracle WebCenter Content Server To enable content integration within Oracle WebCenter Portal create a connection to Oracle WebCenter Content Server using JAX-WS. Follow the steps in the documentation link to create the connection.\n Note: If the Oracle WebCenter Content Server is configured with SSL, before creating the connection, the SSL certificate should be imported into any location under mount path of domain persistent volume to avoid loss of certificate due pod restart.\n Import SSL Certificate Import the certificate using below sample command, update the keystore location to a directory under mount path of the domain persistent volume :\n$ kubectl exec -it wcp-domain-adminserver -n wcpns /bin/bash $ cd $JAVA_HOME/bin $ ./keytool -importcert -alias collab_cert -file /filepath/sslcertificate/contentcert.crt -keystore /u01/oracle/user_projects/domains/wcp-domain/DemoTrust.jks Update the TrustStore To update the truststore location edit domain.yaml file, append -Djavax.net.ssl.trustStore to the spec.serverPod.env.JAVA_OPTIONS environment variable value. The truststore location used in -Djavax.net.ssl.trustStore option should be same as keystore location where the SSL certificate has been imported.\nserverPod: # an (optional) list of environment variable to be set on the servers env: - name: JAVA_OPTIONS value: \u0026#34;-Dweblogic.StdoutDebugEnabled=true -Dweblogic.ssl.Enabled=true -Dweblogic.security.SSL.ignoreHostnameVerification=true -Djavax.net.ssl.trustStore=/u01/oracle/user_projects/domains/wcp-domain/DemoTrust.jks\u0026#34; - name: USER_MEM_ARGS value: \u0026#34;-Djava.security.egd=file:/dev/./urandom -Xms256m -Xmx1024m \u0026#34; volumes: - name: weblogic-domain-storage-volume persistentVolumeClaim: claimName: wcp-domain-domains-pvc volumeMounts: - mountPath: /u01/oracle/user_projects/domains name: weblogic-domain-storage-volume Apply the domain.yaml file to restart the Oracle WebCenter Portal domain.\n$ kubectl apply -f domain.yaml "
},
{
	"uri": "/fmw-kubernetes/wccontent-domains/adminguide/configure-load-balancer/apache/",
	"title": "Apache webtier",
	"tags": [],
	"description": "Configure the Apache webtier load balancer for Oracle WebCenter Content domain.",
	"content": "This section provides information about how to install and configure Apache webtier to load balance Oracle WebCenter Content domain clusters. You can configure Apache webtier for non-SSL and SSL termination access of the application URL.\nFollow these steps to set up Apache webtier as a load balancer for an Oracle WebCenter Content domain in a Kubernetes cluster:\n Build the Apache webtier image Create the Apache plugin configuration file Prepare the certificate and private key Install the Apache webtier Helm chart Verify domain application URL access Uninstall Apache webtier  Build the Apache webtier image Refer to the sample, to build the Apache webtier Docker image.\nCreate the Apache plugin configuration file  The configuration file named custom_mod_wl_apache.conf should have all the URL routing rules for the Oracle WebCenter Content application deployed in the domain that needs to be accessible externally. Update this file with values based on your environment. The file content is similar to below mentioned sample.    Click here to see the sample content of the configuration file custom_mod_wl_apache.conf for Oracle WebCenter Content domain   # Copyright (c) 2018, 2020, Oracle Corporation and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. \u0026lt;IfModule mod_weblogic.c\u0026gt; WebLogicHost \u0026lt;WEBLOGIC_HOST\u0026gt; WebLogicPort 7001 \u0026lt;/IfModule\u0026gt; # Directive for weblogic admin Console deployed on Weblogic Admin Server \u0026lt;Location /console\u0026gt; SetHandler weblogic-handler WebLogicHost wccinfra-adminserver WebLogicPort 7001 \u0026lt;/Location\u0026gt; \u0026lt;Location /em\u0026gt; SetHandler weblogic-handler WebLogicHost wccinfra-adminserver WebLogicPort 7001 \u0026lt;/Location\u0026gt; \u0026lt;Location /weblogic/ready\u0026gt; SetHandler weblogic-handler WebLogicHost wccinfra-adminserver WebLogicPort 7001 \u0026lt;/Location\u0026gt; # Directive for all application deployed on weblogic cluster with a prepath defined by LOCATION variable # For example, if the LOCAITON is set to \u0026#39;/weblogic\u0026#39;, all applications deployed on the cluster can be accessed via # http://myhost:myport/weblogic/application_end_url # where \u0026#39;myhost\u0026#39; is the IP of the machine that runs the Apache web tier, and # \u0026#39;myport\u0026#39; is the port that the Apache web tier is publicly exposed to. # Note that LOCATION cannot be set to \u0026#39;/\u0026#39; unless this is the only Location module configured. \u0026lt;Location /cs\u0026gt; WLSRequest On WebLogicCluster wccinfra-cluster-ucm-cluster:16200 PathTrim /weblogic1 \u0026lt;/Location\u0026gt; \u0026lt;Location /adfAuthentication\u0026gt; WLSRequest On WebLogicCluster wccinfra-cluster-ucm-cluster:16200 PathTrim /weblogic1 \u0026lt;/Location\u0026gt; \u0026lt;Location /ibr\u0026gt; WLSRequest On WebLogicCluster wccinfra-cluster-ibr-cluster:16250 PathTrim /weblogic1 \u0026lt;/Location\u0026gt; \u0026lt;Location /ibr/adfAuthentication\u0026gt; WLSRequest On WebLogicCluster wccinfra-cluster-ibr-cluster:16250 PathTrim /weblogic1 \u0026lt;/Location\u0026gt; \u0026lt;Location /imaging\u0026gt; WLSRequest On WebLogicCluster wccinfra-cluster-ipm-cluster:16000 PathTrim /weblogic1 \u0026lt;/Location\u0026gt; \u0026lt;Location /dc-console\u0026gt; WLSRequest On WebLogicCluster wccinfra-cluster-capture-cluster:16400 PathTrim /weblogic1 \u0026lt;/Location\u0026gt; \u0026lt;Location /dc-client\u0026gt; WLSRequest On WebLogicCluster wccinfra-cluster-capture-cluster:16400 PathTrim /weblogic1 \u0026lt;/Location\u0026gt; \u0026lt;Location /wcc\u0026gt; WLSRequest On WebLogicCluster wccinfra-cluster-wccadf-cluster:16225 PathTrim /weblogic1 \u0026lt;/Location\u0026gt; # Directive for all application deployed on weblogic cluster with a prepath defined by LOCATION2 variable # For example, if the LOCAITON2 is set to \u0026#39;/weblogic2\u0026#39;, all applications deployed on the cluster can be accessed via # http://myhost:myport/weblogic2/application_end_url # where \u0026#39;myhost\u0026#39; is the IP of the machine that runs the Apache web tier, and # \u0026#39;myport\u0026#39; is the port that the Apache webt ier is publicly exposed to. #\u0026lt;Location /weblogic2\u0026gt; #WLSRequest On #WebLogicCluster domain2-cluster-cluster-1:8021 #PathTrim /weblogic2 #\u0026lt;/Location\u0026gt;     Update persistentVolumeClaimName with your PV-claim-name which contains your custom_mod_wl_apache.conf in file kubernetes/samples/charts/apache-samples/custom-sample/input.yaml.  Prepare the certificate and private key   (For the SSL termination configuration only) Run the following commands to generate your own certificate and private key using openssl.\n$ cd ${WORKDIR}/weblogic-kubernetes-operator $ cd kubernetes/samples/charts/apache-samples/custom-sample $ export VIRTUAL_HOST_NAME=WEBLOGIC_HOST $ export SSL_CERT_FILE=WEBLOGIC_HOST.crt $ export SSL_CERT_KEY_FILE=WEBLOGIC_HOST.key $ sh certgen.sh  NOTE: Replace WEBLOGIC_HOST with the host name on which Apache webtier is to be installed.\n   Click here to see the output of the certifcate generation   $ls certgen.sh custom_mod_wl_apache.conf custom_mod_wl_apache.conf_orig input.yaml README.md $ sh certgen.sh Generating certs for WEBLOGIC_HOST Generating a 2048 bit RSA private key ........................+++ .......................................................................+++ unable to write \u0026#39;random state\u0026#39; writing new private key to \u0026#39;apache-sample.key\u0026#39; ----- $ ls certgen.sh custom_mod_wl_apache.conf_orig WEBLOGIC_HOST.info config.txt input.yaml WEBLOGIC_HOST.key custom_mod_wl_apache.conf WEBLOGIC_HOST.crt README.md      Prepare input values for the Apache webtier Helm chart.\nRun the following commands to prepare the input value file for the Apache webtier Helm chart.\n$ base64 -i ${SSL_CERT_FILE} | tr -d \u0026#39;\\n\u0026#39; $ base64 -i ${SSL_CERT_KEY_FILE} | tr -d \u0026#39;\\n\u0026#39; $ touch input.yaml Update virtualHostName with the value of the WEBLOGIC_HOST in file kubernetes/samples/charts/apache-samples/custom-sample/input.yaml\n  Click here to see the snapshot of the sample input.yaml file   $ cat apache-samples/custom-sample/input.yaml # Use this to provide your own Apache webtier configuration as needed; simply define this # path and put your own custom_mod_wl_apache.conf file under this path. persistentVolumeClaimName: \u0026lt;pv-claim-name\u0026gt; # The VirtualHostName of the Apache HTTP server. It is used to enable custom SSL configuration. virtualHostName: \u0026lt;WEBLOGIC_HOST\u0026gt;      Install the Apache webtier Helm chart   Install the Apache webtier Helm chart to the domain wccns namespace with the specified input parameters:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/charts $ kubectl create namespace apache-webtier $ helm install apache-webtier --values apache-samples/custom-sample/input.yaml --namespace wccns apache-webtier --set image=oracle/apache:12.2.1.3   Check the status of the Apache webtier:\n$ kubectl get all -n wccns | grep apache Sample output of the status of the apache webtier:\n  pod/apache-webtier-new-apache-webtier-65d8d7c59f-k27wf 1/1 Running 0 9d service/apache-webtier-new-apache-webtier NodePort 10.108.12.143 \u0026lt;none\u0026gt; 80:30505/TCP,4433:30453/TCP 9d deployment.apps/apache-webtier-new-apache-webtier 1/1 1 1 9d replicaset.apps/apache-webtier-new-apache-webtier-65d8d7c59f 1 1 1 9d Verify domain application URL access Post the Apache webtier load balancer is up, verify that the domain applications are accessible through the load balancer port 30505/30453. The application URLs for domain of type wcc are:\n Note: Port 30505 is the LOADBALANCER-Non-SSLPORT and Port 30453 is LOADBALANCER-SSLPORT.\n Non-SSL configuration http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/weblogic/ready http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/console http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/em http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/cs http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/ibr http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/imaging http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/dc-console http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/wcc SSL configuration https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/weblogic/ready https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/console https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/em https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/cs https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/ibr https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/imaging https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/dc-console https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/wcc Uninstall Apache webtier $ helm delete apache-webtier -n wccns "
},
{
	"uri": "/fmw-kubernetes/wcportal-domains/installguide/configure-wcp-search/",
	"title": "Configure WebCenter Portal For Search",
	"tags": [],
	"description": "Set up search functionality in Oracle WebCenter Portal using Elasticsearch.",
	"content": " Introduction Set Up Persistent Volume and Persistent Volume Claim Create a Secret Headless Service LoadBalancer LoadBalancer Validation Elasticsearch Cluster Deployment Validation  Introduction Elasticsearch is a highly scalable search engine. It allows you to store, search, and analyze big volumes of data quickly and provides a distributed, multitenant-capable full-text search engine with an HTTP web interface and schema-free JSON document.\nSet Up Persistent Volume and Persistent Volume Claim Create a Kubernetes PV and PVC (Persistent Volume and Persistent Volume Claim) to store Elasticsearch data. To create PV and PVC, use the deployment YAML configuration file located at ${WORKDIR}/create-wcp-es-cluster/es-pvpvc.yaml.\napiVersion: v1 kind: PersistentVolume metadata: name: es-data-pv namespace: wcpns spec: storageClassName: es-data-pv-storage-class capacity: storage: 10Gi accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Retain hostPath: path: \u0026#34;/scratch/esdata\u0026#34; --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: es-data-pvc namespace: wcpns spec: storageClassName: es-data-pv-storage-class accessModes: - ReadWriteMany resources: requests: storage: 10Gi To create PV \u0026amp; PVC run the below command:\n$ kubectl apply -f es-pvpvc.yaml Create a Secret To grant access to Oracle WebCenter Portal, create a Kubernetes secret using the deployment YAML configuration file located at ${WORKDIR}/create-wcp-es-cluster/es-secret.yaml\napiVersion: v1 kind: Secret metadata: name: es-secret namespace: wcpns data: # base64 encoded strings wls-admin: d2VibG9naWM= wls-admin-pwd: d2VsY29tZTE= search-admin: d2NjcmF3bGFkbWlu search-admin-pwd: d2VsY29tZTE= Where: wls-admin :Oracle WebCenter Admin UserName wls-admin-pwd :Oracle WebCenter Admin Password search-admin :ElasticSearch Username search-admin-pwd : ElasticSearch Password  To create Kubernetes Secret run the below command:\n$ kubectl apply -f es-secret.yaml Headless Service Each node in Elasticsearch cluster can communicate using a headless service. Create a headless service using the deployment YAML configuration file located at ${WORKDIR}/create-wcp-es-cluster/es-service.yaml to establish cluster communication.\napiVersion: v1 kind: Service metadata: name: es-svc namespace: wcpns labels: service: elasticsearch spec: # headless service clusterIP: None ports: - port: 9200 name: http - port: 9300 name: transport selector: service: elasticsearch To create Headless Service run below command:\n$ kubectl apply -f es-service.yaml LoadBalancer To access the Elasticsearch service outside of the Kubernetes cluster, create an external loadbalancer. Then access the Elasticsearch service by using the external IP of loadbalancer, create a loadbalancer using the deployment YAML configuration file located at ${WORKDIR}/create-wcp-es-cluster/es-loadbalancer.yaml.\napiVersion: v1 kind: Service metadata: name: es-loadbalancer namespace: wcpns labels: type: external spec: type: LoadBalancer selector: service: elasticsearch ports: - name: http port: 9200 targetPort: 9200 To create a loadbalancer run below command:\n$ kubectl apply -f es-loadbalancer.yaml LoadBalancer Validation Once the loadbalancer is successfully deployed, validate it by running the following command:\n$ kubectl get svc -n wcpns -l type=external Make a note of the external IP from the above command and use this below sample URL to access Elasticsearch cluster health : http://externalIP:9200/_cluster/health\nElasticsearch Cluster Using the Kubernetes StatefulSet controller create an Elasticsearch Cluster comprising of three node using the deployment YAML configuration file located at ${WORKDIR}/create-wcp-es-cluster/es-statefulset.yaml\napiVersion: apps/v1 kind: StatefulSet metadata: name: es-statefulset namespace: wcpns labels: service: elasticsearch spec: serviceName: es-svc replicas: 1 selector: matchLabels: service: elasticsearch template: metadata: labels: service: elasticsearch spec: initContainers: - name: increase-the-vm-max-map-count image: busybox command: - sysctl - -w - vm.max_map_count=262144 securityContext: privileged: true - name: increase-the-ulimit image: busybox command: - sh - -c - ulimit -n 65536 securityContext: privileged: true volumes: - name: es-node persistentVolumeClaim: claimName: es-data-pvc - name: wcp-domain persistentVolumeClaim: claimName: wcp-domain-domain-pvc containers: - name: es-container image: oracle/wcportal:12.2.1.4 imagePullPolicy: IfNotPresent command: [ \u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;/u01/oracle/container-scripts/configureOrStartElasticsearch.sh\u0026#34; ] readinessProbe: httpGet: path: / port: 9200 httpHeaders: - name: Authorization value: Basic d2NjcmF3bGFkbWluOndlbGNvbWUx initialDelaySeconds: 150 periodSeconds: 30 timeoutSeconds: 10 successThreshold: 1 failureThreshold: 10 lifecycle: preStop: exec: command: [ \u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;/u01/oracle/container-scripts/elasticsearchPreStopHandler.sh\u0026#34; ] ports: - containerPort: 9200 name: http - containerPort: 9300 name: tcp env: - name: NODE_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: UNICAST_HOST_LIST value: \u0026#34;es-svc\u0026#34; - name: ADMIN_USERNAME valueFrom: secretKeyRef: name: es-secret key: wls-admin - name: ADMIN_PASSWORD valueFrom: secretKeyRef: name: es-secret key: wls-admin-pwd - name: SEARCH_APP_USERNAME valueFrom: secretKeyRef: name: es-secret key: search-admin - name: SEARCH_APP_USER_PASSWORD valueFrom: secretKeyRef: name: es-secret key: search-admin-pwd - name: ADMIN_SERVER_CONTAINER_NAME value: wcp-domain-adminserver - name: ADMIN_PORT value: \u0026#34;7001\u0026#34; - name: ES_CLUSTER_NAME value: es-cluster - name: DOMAIN_NAME value: wcp-domain - name: CONFIGURE_ES_CONNECTION value: \u0026#34;true\u0026#34; - name: LOAD_BALANCER_IP value: \u0026#34;es-loadbalancer.wcpns.svc.cluster.local\u0026#34; volumeMounts: - name: es-node mountPath: /u01/esHome/esNode - name: wcp-domain mountPath: /u01/oracle/user_projects/domains  Note: The values used for ADMIN_PORT and Image name should be same as values passed to create-domain.sh job while creating domain.\n To create a es-statefulset run below command:\n$ kubectl apply -f es-statefulset.yaml  Note: After setting up Elasticsearch cluster restart all the instance of Oracle WebCenter Portal server.\n Deployment Validation Validate the deployment by running the following command:\n$ kubectl get pods -n wcpns -l service=elasticsearch "
},
{
	"uri": "/fmw-kubernetes/oam/create-oam-domains/",
	"title": "Create OAM domains",
	"tags": [],
	"description": "Sample for creating an OAM domain home on an existing PV or PVC, and the domain resource YAML file for deploying the generated OAM domain.",
	"content": "  Introduction\n  Prerequisites\n  Prepare the create domain script\n  Run the create domain script\n  Set the OAM server memory parameters\n  Initializing the domain\n  Verify the results\na. Verify the domain, pods and services\nb. Verify the domain\nc. Verify the pods\n  Introduction The OAM deployment scripts demonstrate the creation of an OAM domain home on an existing Kubernetes persistent volume (PV) and persistent volume claim (PVC). The scripts also generate the domain YAML file, which can then be used to start the Kubernetes artifacts of the corresponding domain.\nPrerequisites Before you begin, perform the following steps:\n Review the Domain resource documentation. Ensure that you have executed all the preliminary steps documented in Prepare your environment. Ensure that the database is up and running.  Prepare the create domain script The sample scripts for Oracle Access Management domain deployment are available at $WORKDIR/kubernetes/create-access-domain.\n  Make a copy of the create-domain-inputs.yaml file:\n$ cd $WORKDIR/kubernetes/create-access-domain/domain-home-on-pv $ cp create-domain-inputs.yaml create-domain-inputs.yaml.orig   Edit the create-domain-inputs.yaml and modify the following parameters. Save the file when complete:\ndomainUID: \u0026lt;domain_uid\u0026gt; domainHome: /u01/oracle/user_projects/domains/\u0026lt;domain_uid\u0026gt; image: \u0026lt;image_name\u0026gt;:\u0026lt;tag\u0026gt; imagePullSecretName: \u0026lt;container_registry_secret\u0026gt; weblogicCredentialsSecretName: \u0026lt;kubernetes_domain_secret\u0026gt; logHome: /u01/oracle/user_projects/domains/logs/\u0026lt;domain_uid\u0026gt; namespace: \u0026lt;domain_namespace\u0026gt; persistentVolumeClaimName: \u0026lt;pvc_name\u0026gt; rcuSchemaPrefix: \u0026lt;rcu_prefix\u0026gt; rcuDatabaseURL: \u0026lt;rcu_db_host\u0026gt;:\u0026lt;rcu_db_port\u0026gt;/\u0026lt;rcu_db_service_name\u0026gt; rcuCredentialsSecret: \u0026lt;kubernetes_rcu_secret\u0026gt; Note : imagePullSecretName is not required if you are not using a container registry.\nFor example:\ndomainUID: accessdomain domainHome: /u01/oracle/user_projects/domains/accessdomain image: container-registry.oracle.com/middleware/oam_cpu:12.2.1.4-jdk8-ol7-220119.2059 imagePullSecretName: orclcred weblogicCredentialsSecretName: accessdomain-credentials logHome: /u01/oracle/user_projects/domains/logs/accessdomain namespace: oamns persistentVolumeClaimName: accessdomain-domain-pvc rcuSchemaPrefix: OAMK8S rcuDatabaseURL: mydatabasehost.example.com:1521/orcl.example.com rcuCredentialsSecret: accessdomain-rcu-credentials   A full list of parameters in the create-domain-inputs.yaml file are shown below:\n   Parameter Definition Default     adminPort Port number for the Administration Server inside the Kubernetes cluster. 7001   adminNodePort Port number of the Administration Server outside the Kubernetes cluster. 30701   adminServerName Name of the Administration Server. AdminServer   clusterName Name of the WebLogic cluster instance to generate for the domain. By default the cluster name is oam_cluster for the OAM domain. oam_cluster   configuredManagedServerCount Number of Managed Server instances to generate for the domain. 5   createDomainFilesDir Directory on the host machine to locate all the files to create a WebLogic domain, including the script that is specified in the createDomainScriptName property. By default, this directory is set to the relative path wlst, and the create script will use the built-in WLST offline scripts in the wlst directory to create the WebLogic domain. It can also be set to the relative path wdt, and then the built-in WDT scripts will be used instead. An absolute path is also supported to point to an arbitrary directory in the file system. The built-in scripts can be replaced by the user-provided scripts or model files as long as those files are in the specified directory. Files in this directory are put into a Kubernetes config map, which in turn is mounted to the createDomainScriptsMountPath, so that the Kubernetes pod can use the scripts and supporting files to create a domain home. wlst   createDomainScriptsMountPath Mount path where the create domain scripts are located inside a pod. The create-domain.sh script creates a Kubernetes job to run the script (specified in the createDomainScriptName property) in a Kubernetes pod to create a domain home. Files in the createDomainFilesDir directory are mounted to this location in the pod, so that the Kubernetes pod can use the scripts and supporting files to create a domain home. /u01/weblogic   createDomainScriptName Script that the create domain script uses to create a WebLogic domain. The create-domain.sh script creates a Kubernetes job to run this script to create a domain home. The script is located in the in-pod directory that is specified in the createDomainScriptsMountPath property. If you need to provide your own scripts to create the domain home, instead of using the built-it scripts, you must use this property to set the name of the script that you want the create domain job to run. create-domain-job.sh   domainHome Home directory of the OAM domain. If not specified, the value is derived from the domainUID as /shared/domains/\u0026lt;domainUID\u0026gt;. /u01/oracle/user_projects/domains/accessinfra   domainPVMountPath Mount path of the domain persistent volume. /u01/oracle/user_projects   domainUID Unique ID that will be used to identify this particular domain. Used as the name of the generated WebLogic domain as well as the name of the Kubernetes domain resource. This ID must be unique across all domains in a Kubernetes cluster. This ID cannot contain any character that is not valid in a Kubernetes service name. accessinfra   domainType Type of the domain. Mandatory input for OAM domains. You must provide one of the supported domain type value: oam (deploys an OAM domain) oam   exposeAdminNodePort Boolean indicating if the Administration Server is exposed outside of the Kubernetes cluster. false   exposeAdminT3Channel Boolean indicating if the T3 administrative channel is exposed outside the Kubernetes cluster. true   image OAM container image. The operator requires OAM 12.2.1.4. Refer to Obtain the OAM container image for details on how to obtain or create the image. oracle/oam:12.2.1.4.0   imagePullPolicy WebLogic container image pull policy. Legal values are IfNotPresent, Always, or Never IfNotPresent   imagePullSecretName Name of the Kubernetes secret to access the container registry to pull the OAM container image. The presence of the secret will be validated when this parameter is specified.    includeServerOutInPodLog Boolean indicating whether to include the server .out to the pod\u0026rsquo;s stdout. true   initialManagedServerReplicas Number of Managed Servers to initially start for the domain. 2   javaOptions Java options for starting the Administration Server and Managed Servers. A Java option can have references to one or more of the following pre-defined variables to obtain WebLogic domain information: $(DOMAIN_NAME), $(DOMAIN_HOME), $(ADMIN_NAME), $(ADMIN_PORT), and $(SERVER_NAME). -Dweblogic.StdoutDebugEnabled=false   logHome The in-pod location for the domain log, server logs, server out, and Node Manager log files. If not specified, the value is derived from the domainUID as /shared/logs/\u0026lt;domainUID\u0026gt;. /u01/oracle/user_projects/domains/logs/accessinfra   managedServerNameBase Base string used to generate Managed Server names. oam_server   managedServerPort Port number for each Managed Server. 8001   namespace Kubernetes namespace in which to create the domain. accessns   persistentVolumeClaimName Name of the persistent volume claim created to host the domain home. If not specified, the value is derived from the domainUID as \u0026lt;domainUID\u0026gt;-weblogic-sample-pvc. accessinfra-domain-pvc   productionModeEnabled Boolean indicating if production mode is enabled for the domain. true   serverStartPolicy Determines which WebLogic Server instances will be started. Legal values are NEVER, IF_NEEDED, ADMIN_ONLY. IF_NEEDED   t3ChannelPort Port for the T3 channel of the NetworkAccessPoint. 30012   t3PublicAddress Public address for the T3 channel. This should be set to the public address of the Kubernetes cluster. This would typically be a load balancer address. For development environments only: In a single server (all-in-one) Kubernetes deployment, this may be set to the address of the master, or at the very least, it must be set to the address of one of the worker nodes. If not provided, the script will attempt to set it to the IP address of the Kubernetes cluster   weblogicCredentialsSecretName Name of the Kubernetes secret for the Administration Server\u0026rsquo;s user name and password. If not specified, then the value is derived from the domainUID as \u0026lt;domainUID\u0026gt;-weblogic-credentials. accessinfra-domain-credentials   weblogicImagePullSecretName Name of the Kubernetes secret for the container registry, used to pull the WebLogic Server image.    serverPodCpuRequest, serverPodMemoryRequest, serverPodCpuCLimit, serverPodMemoryLimit The maximum amount of compute resources allowed, and minimum amount of compute resources required, for each server pod. Please refer to the Kubernetes documentation on Managing Compute Resources for Containers for details. Resource requests and resource limits are not specified.   rcuSchemaPrefix The schema prefix to use in the database, for example OAM1. You may wish to make this the same as the domainUID in order to simplify matching domains to their RCU schemas. OAM1   rcuDatabaseURL The database URL. oracle-db.default.svc.cluster.local:1521/devpdb.k8s   rcuCredentialsSecret The Kubernetes secret containing the database credentials. accessinfra-rcu-credentials    Note that the names of the Kubernetes resources in the generated YAML files may be formed with the value of some of the properties specified in the create-inputs.yaml file. Those properties include the adminServerName, clusterName and managedServerNameBase. If those values contain any characters that are invalid in a Kubernetes service name, those characters are converted to valid values in the generated YAML files. For example, an uppercase letter is converted to a lowercase letter and an underscore (\u0026quot;_\u0026quot;) is converted to a hyphen (\u0026quot;-\u0026quot;).\nThe sample demonstrates how to create an OAM domain home and associated Kubernetes resources for a domain that has one cluster only. In addition, the sample provides the capability for users to supply their own scripts to create the domain home for other use cases. The generated domain YAML file could also be modified to cover more use cases.\nRun the create domain script   Run the create domain script, specifying your inputs file and an output directory to store the generated artifacts:\n$ cd $WORKDIR/kubernetes/create-access-domain/domain-home-on-pv $ ./create-domain.sh -i create-domain-inputs.yaml -o /\u0026lt;path to output-directory\u0026gt; For example:\n$ cd $WORKDIR/kubernetes/create-access-domain/domain-home-on-pv $ ./create-domain.sh -i create-domain-inputs.yaml -o output The output will look similar to the following:\nInput parameters being used export version=\u0026quot;create-weblogic-sample-domain-inputs-v1\u0026quot; export adminPort=\u0026quot;7001\u0026quot; export adminServerName=\u0026quot;AdminServer\u0026quot; export domainUID=\u0026quot;accessdomain\u0026quot; export domainType=\u0026quot;oam\u0026quot; export domainHome=\u0026quot;/u01/oracle/user_projects/domains/accessdomain\u0026quot; export serverStartPolicy=\u0026quot;IF_NEEDED\u0026quot; export clusterName=\u0026quot;oam_cluster\u0026quot; export configuredManagedServerCount=\u0026quot;5\u0026quot; export initialManagedServerReplicas=\u0026quot;2\u0026quot; export managedServerNameBase=\u0026quot;oam_server\u0026quot; export managedServerPort=\u0026quot;14100\u0026quot; export image=\u0026quot;container-registry.oracle.com/middleware/oam_cpu:12.2.1.4-jdk8-ol7-220119.2059\u0026quot; export imagePullPolicy=\u0026quot;IfNotPresent\u0026quot; export imagePullSecretName=\u0026quot;orclcred\u0026quot; export productionModeEnabled=\u0026quot;true\u0026quot; export weblogicCredentialsSecretName=\u0026quot;accessdomain-credentials\u0026quot; export includeServerOutInPodLog=\u0026quot;true\u0026quot; export logHome=\u0026quot;/u01/oracle/user_projects/domains/logs/accessdomain\u0026quot; export httpAccessLogInLogHome=\u0026quot;true\u0026quot; export t3ChannelPort=\u0026quot;30012\u0026quot; export exposeAdminT3Channel=\u0026quot;false\u0026quot; export adminNodePort=\u0026quot;30701\u0026quot; export exposeAdminNodePort=\u0026quot;false\u0026quot; export namespace=\u0026quot;oamns\u0026quot; javaOptions=-Dweblogic.StdoutDebugEnabled=false export persistentVolumeClaimName=\u0026quot;accessdomain-domain-pvc\u0026quot; export domainPVMountPath=\u0026quot;/u01/oracle/user_projects/domains\u0026quot; export createDomainScriptsMountPath=\u0026quot;/u01/weblogic\u0026quot; export createDomainScriptName=\u0026quot;create-domain-job.sh\u0026quot; export createDomainFilesDir=\u0026quot;wlst\u0026quot; export rcuSchemaPrefix=\u0026quot;OAMK8S\u0026quot; export rcuDatabaseURL=\u0026quot;mydatabasehost.example.com:1521/orcl.example.com\u0026quot; export rcuCredentialsSecret=\u0026quot;accessdomain-rcu-credentials\u0026quot; createFiles - valuesInputFile is create-domain-inputs.yaml createDomainScriptName is create-domain-job.sh Generating output/weblogic-domains/accessdomain/create-domain-job.yaml Generating output/weblogic-domains/accessdomain/delete-domain-job.yaml Generating output/weblogic-domains/accessdomain/domain.yaml Checking to see if the secret accessdomain-credentials exists in namespace oamns configmap/accessdomain-create-oam-infra-domain-job-cm created Checking the configmap accessdomain-create-oam-infra-domain-job-cm was created configmap/accessdomain-create-oam-infra-domain-job-cm labeled Checking if object type job with name accessdomain-create-oam-infra-domain-job exists No resources found in oamns namespace. Creating the domain by creating the job output/weblogic-domains/accessdomain/create-domain-job.yaml job.batch/accessdomain-create-oam-infra-domain-job created Waiting for the job to complete... status on iteration 1 of 20 pod accessdomain-create-oam-infra-domain-job-6tgw4 status is Running status on iteration 2 of 20 pod accessdomain-create-oam-infra-domain-job-6tgw4 status is Running status on iteration 3 of 20 pod accessdomain-create-oam-infra-domain-job-6tgw4 status is Running status on iteration 4 of 20 pod accessdomain-create-oam-infra-domain-job-6tgw4 status is Running status on iteration 5 of 20 pod accessdomain-create-oam-infra-domain-job-6tgw4 status is Running status on iteration 6 of 20 pod accessdomain-create-oam-infra-domain-job-6tgw4 status is Completed Domain accessdomain was created and will be started by the WebLogic Kubernetes Operator The following files were generated: output/weblogic-domains/accessdomain/create-domain-inputs.yaml output/weblogic-domains/accessdomain/create-domain-job.yaml output/weblogic-domains/accessdomain/domain.yaml Note: If the domain creation fails, refer to the Troubleshooting section.\nThe command creates a domain.yaml file required for domain creation.\n  Set the OAM server memory parameters By default, the java memory parameters assigned to the oam_server cluster are very small. The minimum recommended values are -Xms4096m -Xmx8192m. However, Oracle recommends you to set these to -Xms8192m -Xmx8192m in a production environment.\n  Navigate to the /output/weblogic-domains/\u0026lt;domain_uid\u0026gt; directory:\n$ cd $WORKDIR/kubernetes/create-access-domain/domain-home-on-pv/output/weblogic-domains/\u0026lt;domain_uid\u0026gt; For example:\n$ cd $WORKDIR/kubernetes/create-access-domain/domain-home-on-pv/output/weblogic-domains/accessdomain   Edit the domain.yaml file and locate the section of the file starting with: - clusterName: oam_cluster. Immediately after the line: topologyKey: \u0026quot;kubernetes.io/hostname\u0026quot;, add the following lines:\nenv: - name: USER_MEM_ARGS value: \u0026quot;-XX:+UseContainerSupport -Djava.security.egd=file:/dev/./urandom -Xms8192m -Xmx8192m\u0026quot; For example:\n - clusterName: oam_cluster serverService: precreateService: true serverStartState: \u0026quot;RUNNING\u0026quot; serverPod: # Instructs Kubernetes scheduler to prefer nodes for new cluster members where there are not # already members of the same cluster. affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: \u0026quot;weblogic.clusterName\u0026quot; operator: In values: - $(CLUSTER_NAME) topologyKey: \u0026quot;kubernetes.io/hostname\u0026quot; env:\t- name: USER_MEM_ARGS value: \u0026quot;-XX:+UseContainerSupport -Djava.security.egd=file:/dev/./urandom -Xms8192m -Xmx8192m\u0026quot; replicas: 2   In the domain.yaml locate the section of the file starting with adminServer:. Under the env: tag add the following CLASSPATH entries. This is required for running the idmconfigtool from the Administration Server.\n- name: CLASSPATH value: \u0026quot;/u01/oracle/wlserver/server/lib/weblogic.jar\u0026quot; For example:\n adminServer: # serverStartState legal values are \u0026quot;RUNNING\u0026quot; or \u0026quot;ADMIN\u0026quot; # \u0026quot;RUNNING\u0026quot; means the listed server will be started up to \u0026quot;RUNNING\u0026quot; mode # \u0026quot;ADMIN\u0026quot; means the listed server will be start up to \u0026quot;ADMIN\u0026quot; mode serverStartState: \u0026quot;RUNNING\u0026quot; adminService: channels: # The Admin Server's NodePort - channelName: default nodePort: 30701 # Uncomment to export the T3Channel as a service - channelName: T3Channel serverPod: # an (optional) list of environment variable to be set on the admin servers env: - name: USER_MEM_ARGS value: \u0026quot;-Djava.security.egd=file:/dev/./urandom -Xms512m -Xmx1024m \u0026quot; - name: CLASSPATH value: \u0026quot;/u01/oracle/wlserver/server/lib/weblogic.jar\u0026quot;   If required, you can add the optional parameter maxClusterConcurrentStartup to the spec section of the domain.yaml. This parameter specifies the number of managed servers to be started in sequence per cluster. For example if you updated the initialManagedServerReplicas to 4 in create-domain-inputs.yaml and only had 2 nodes, then setting maxClusterConcurrentStartup: 1 will start one managed server at a time on each node, rather than starting them all at once. This can be useful to take the strain off individual nodes at startup. Below is an example with the parameter added:\napiVersion: \u0026quot;weblogic.oracle/v8\u0026quot; kind: Domain metadata: name: accessdomain namespace: oamns labels: weblogic.domainUID: accessdomain spec: # The WebLogic Domain Home domainHome: /u01/oracle/user_projects/domains/accessdomain maxClusterConcurrentStartup: 1 # The domain home source type # Set to PersistentVolume for domain-in-pv, Image for domain-in-image, or FromModel for model-in-image domainHomeSourceType: PersistentVolume # The WebLogic Server container image that the Operator uses to start the domain image: \u0026quot;oracle/oam:12.2.1.4.0\u0026quot; ....   Save the changes to domain.yaml\n  Initializing the domain   Create the Kubernetes resource using the following command:\n$ kubectl apply -f $WORKDIR/kubernetes/create-access-domain/domain-home-on-pv/output/weblogic-domains/\u0026lt;domain_uid\u0026gt;/domain.yaml For example:\n$ kubectl apply -f $WORKDIR/kubernetes/create-access-domain/domain-home-on-pv/output/weblogic-domains/accessdomain/domain.yaml The output will look similar to the following:\ndomain.weblogic.oracle/accessdomain created   Verify the results Verify the domain, pods and services   Verify the domain, servers pods and services are created and in the READY state with a status of 1/1, by running the following command:\n$ kubectl get all,domains -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get all,domains -n oamns The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE pod/accessdomain-adminserver 1/1 Running 0 11m pod/accessdomain-create-oam-infra-domain-job-7c9r9 0/1 Completed 0 18m pod/accessdomain-oam-policy-mgr1 1/1 Running 0 3m31s pod/accessdomain-oam-policy-mgr2 1/1 Running 0 3m31s pod/accessdomain-oam-server1 1/1 Running 0 3m31s pod/accessdomain-oam-server2 1/1 Running 0 3m31s pod/helper 1/1 Running 0 33m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/accessdomain-adminserver ClusterIP None \u0026lt;none\u0026gt; 7001/TCP 11m service/accessdomain-cluster-oam-cluster ClusterIP 10.101.59.154 \u0026lt;none\u0026gt; 14100/TCP 3m31s service/accessdomain-cluster-policy-cluster ClusterIP 10.98.236.51 \u0026lt;none\u0026gt; 15100/TCP 3m31s service/accessdomain-oam-policy-mgr1 ClusterIP None \u0026lt;none\u0026gt; 15100/TCP 3m31s service/accessdomain-oam-policy-mgr2 ClusterIP None \u0026lt;none\u0026gt; 15100/TCP 3m31s service/accessdomain-oam-policy-mgr3 ClusterIP 10.96.244.37 \u0026lt;none\u0026gt; 15100/TCP 3m31s service/accessdomain-oam-policy-mgr4 ClusterIP 10.105.201.23 \u0026lt;none\u0026gt; 15100/TCP 3m31s service/accessdomain-oam-policy-mgr5 ClusterIP 10.110.12.227 \u0026lt;none\u0026gt; 15100/TCP 3m31s service/accessdomain-oam-server1 ClusterIP None \u0026lt;none\u0026gt; 14100/TCP 3m31s service/accessdomain-oam-server2 ClusterIP None \u0026lt;none\u0026gt; 14100/TCP 3m31s service/accessdomain-oam-server3 ClusterIP 10.103.178.35 \u0026lt;none\u0026gt; 14100/TCP 3m31s service/accessdomain-oam-server4 ClusterIP 10.97.254.78 \u0026lt;none\u0026gt; 14100/TCP 3m31s service/accessdomain-oam-server5 ClusterIP 10.105.65.104 \u0026lt;none\u0026gt; 14100/TCP 3m31s NAME COMPLETIONS DURATION AGE job.batch/accessdomain-create-oam-infra-domain-job 1/1 2m6s 18m NAME AGE domain.weblogic.oracle/accessdomain 12m Note: It will take several minutes before all the services listed above show. When a pod has a STATUS of 0/1 the pod is started but the OAM server associated with it is currently starting. While the pods are starting you can check the startup status in the pod logs, by running the following command:\n$ kubectl logs accessdomain-adminserver -n oamns $ kubectl logs accessdomain-oam-policy-mgr1 -n oamns $ kubectl logs accessdomain-oam-server1 -n oamns etc.. The default domain created by the script has the following characteristics:\n An Administration Server named AdminServer listening on port 7001. A configured OAM cluster named oam_cluster of size 5. A configured Policy Manager cluster named policy_cluster of size 5. Two started OAM managed Servers, named oam_server1 and oam_server2, listening on port 14100. Two started Policy Manager managed servers named oam-policy-mgr1 and oam-policy-mgr2, listening on port 15100. Log files that are located in \u0026lt;persistent_volume\u0026gt;/logs/\u0026lt;domainUID\u0026gt;.    Verify the domain   Run the following command to describe the domain:\n$ kubectl describe domain \u0026lt;domain_uid\u0026gt; -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl describe domain accessdomain -n oamns The output will look similar to the following:\nName: accessdomain Namespace: oamns Labels: weblogic.domainUID=accessdomain Annotations: \u0026lt;none\u0026gt; API Version: weblogic.oracle/v8 Kind: Domain Metadata: Creation Timestamp: 2022-03-07T11:59:51Z Generation: 1 Managed Fields: API Version: weblogic.oracle/v8 Fields Type: FieldsV1 fieldsV1: f:status: .: f:clusters: f:conditions: f:introspectJobFailureCount: f:servers: f:startTime: Manager: Kubernetes Java Client Operation: Update Time: 2022-03-07T11:59:51Z API Version: weblogic.oracle/v8 Fields Type: FieldsV1 fieldsV1: f:metadata: f:annotations: .: f:kubectl.kubernetes.io/last-applied-configuration: f:labels: .: f:weblogic.domainUID: Manager: kubectl-client-side-apply Operation: Update Time: 2022-03-07T11:59:51Z Resource Version: 1495179 UID: a90107d5-dbaf-4d86-9439-d5369faabd35 Spec: Admin Server: Server Pod: Env: Name: USER_MEM_ARGS Value: -Djava.security.egd=file:/dev/./urandom -Xms512m -Xmx1024m Name: CLASSPATH Value: /u01/oracle/wlserver/server/lib/weblogic.jar Server Start State: RUNNING Clusters: Cluster Name: policy_cluster Replicas: 2 Server Pod: Affinity: Pod Anti Affinity: Preferred During Scheduling Ignored During Execution: Pod Affinity Term: Label Selector: Match Expressions: Key: weblogic.clusterName Operator: In Values: $(CLUSTER_NAME) Topology Key: kubernetes.io/hostname Weight: 100 Server Service: Precreate Service: true Server Start State: RUNNING Cluster Name: oam_cluster Replicas: 2 Server Pod: Affinity: Pod Anti Affinity: Preferred During Scheduling Ignored During Execution: Pod Affinity Term: Label Selector: Match Expressions: Key: weblogic.clusterName Operator: In Values: $(CLUSTER_NAME) Topology Key: kubernetes.io/hostname Weight: 100 Env: Name: USER_MEM_ARGS Value: -XX:+UseContainerSupport -Djava.security.egd=file:/dev/./urandom -Xms8192m -Xmx8192m Server Service: Precreate Service: true Server Start State: RUNNING Data Home: Domain Home: /u01/oracle/user_projects/domains/accessdomain Domain Home Source Type: PersistentVolume Http Access Log In Log Home: true Image: container-registry.oracle.com/middleware/oam_cpu:12.2.1.4-jdk8-ol7-220119.2059 Image Pull Policy: IfNotPresent Image Pull Secrets: Name: orclcred Include Server Out In Pod Log: true Log Home: /u01/oracle/user_projects/domains/logs/accessdomain Log Home Enabled: true Server Pod: Env: Name: JAVA_OPTIONS Value: -Dweblogic.StdoutDebugEnabled=false Name: USER_MEM_ARGS Value: -Djava.security.egd=file:/dev/./urandom -Xms256m -Xmx1024m Volume Mounts: Mount Path: /u01/oracle/user_projects/domains Name: weblogic-domain-storage-volume Volumes: Name: weblogic-domain-storage-volume Persistent Volume Claim: Claim Name: accessdomain-domain-pvc Server Start Policy: IF_NEEDED Web Logic Credentials Secret: Name: accessdomain-credentials Status: Clusters: Cluster Name: oam_cluster Maximum Replicas: 5 Minimum Replicas: 0 Ready Replicas: 2 Replicas: 2 Replicas Goal: 2 Cluster Name: policy_cluster Maximum Replicas: 5 Minimum Replicas: 0 Ready Replicas: 2 Replicas: 2 Replicas Goal: 2 Conditions: Last Transition Time: 2022-03-07T12:11:52.623959Z Reason: ServersReady Status: True Type: Available Introspect Job Failure Count: 0 Servers: Desired State: RUNNING Health: Activation Time: 2022-03-07T12:08:29.271000Z Overall Health: ok Subsystems: Subsystem Name: ServerRuntime Symptoms: Node Name: 10.250.42.252 Server Name: AdminServer State: RUNNING Cluster Name: oam_cluster Desired State: RUNNING Health: Activation Time: 2022-03-07T12:11:02.696000Z Overall Health: ok Subsystems: Subsystem Name: ServerRuntime Symptoms: Node Name: 10.250.42.255 Server Name: oam_server1 State: RUNNING Cluster Name: oam_cluster Desired State: RUNNING Health: Activation Time: 2022-03-07T12:11:46.175000Z Overall Health: ok Subsystems: Subsystem Name: ServerRuntime Symptoms: Node Name: 10.250.42.252 Server Name: oam_server2 State: RUNNING Cluster Name: oam_cluster Desired State: SHUTDOWN Server Name: oam_server3 Cluster Name: oam_cluster Desired State: SHUTDOWN Server Name: oam_server4 Cluster Name: oam_cluster Desired State: SHUTDOWN Server Name: oam_server5 Cluster Name: policy_cluster Desired State: RUNNING Health: Activation Time: 2022-03-07T12:11:20.404000Z Overall Health: ok Subsystems: Subsystem Name: ServerRuntime Symptoms: Node Name: 10.250.42.255 Server Name: oam_policy_mgr1 State: RUNNING Cluster Name: policy_cluster Desired State: RUNNING Health: Activation Time: 2022-03-07T12:11:09.719000Z Overall Health: ok Subsystems: Subsystem Name: ServerRuntime Symptoms: Node Name: 10.250.42.252 Server Name: oam_policy_mgr2 State: RUNNING Cluster Name: policy_cluster Desired State: SHUTDOWN Server Name: oam_policy_mgr3 Cluster Name: policy_cluster Desired State: SHUTDOWN Server Name: oam_policy_mgr4 Cluster Name: policy_cluster Desired State: SHUTDOWN Server Name: oam_policy_mgr5 Start Time: 2022-03-07T11:59:51.682687Z Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal DomainCreated 13m weblogic.operator Domain resource accessdomain was created Normal DomainProcessingStarting 5m9s (x2 over 13m) weblogic.operator Creating or updating Kubernetes presence for WebLogic Domain with UID accessdomain Normal DomainProcessingCompleted 114s weblogic.operator Successfully completed processing domain resource accessdomain In the Status section of the output, the available servers and clusters are listed.\n  Verify the pods   Run the following command to see the pods running the servers and which nodes they are running on:\n$ kubectl get pods -n \u0026lt;domain_namespace\u0026gt; -o wide For example:\n$ kubectl get pods -n oamns -o wide The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES accessdomain-adminserver 1/1 Running 0 18m 10.244.6.63 10.250.42.252 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; accessdomain-create-oam-infra-domain-job-7c9r9 0/1 Completed 0 25m 10.244.6.61 10.250.42.252 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; accessdomain-oam-policy-mgr1 1/1 Running 0 10m 10.244.5.13 10.250.42.255 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; accessdomain-oam-policy-mgr2 1/1 Running 0 10m 10.244.6.65 10.250.42.252 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; accessdomain-oam-server1 1/1 Running 0 10m 10.244.5.12 10.250.42.255 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; accessdomain-oam-server2 1/1 Running 0 10m 10.244.6.64 10.250.42.252 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; helper 1/1 Running 0 40m 10.244.6.60 10.250.42.252 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; You are now ready to configure an Ingress to direct traffic for your OAM domain as per Configure an Ingress for an OAM domain.\n  "
},
{
	"uri": "/fmw-kubernetes/oig/create-oig-domains/",
	"title": "Create OIG domains",
	"tags": [],
	"description": "Sample for creating an OIG domain home on an existing PV or PVC, and the domain resource YAML file for deploying the generated OIG domain.",
	"content": "  Introduction\n  Prerequisites\n  Prepare the create domain script\n  Run the create domain script\na. Generate the create domain script\nb. Setting the OIM server memory parameters\nc. Run the create domain scripts\n  Verify the results\na. Verify the domain, pods and services\nb. Verify the domain\nc. Verify the pods\n  Introduction The OIG deployment scripts demonstrate the creation of an OIG domain home on an existing Kubernetes persistent volume (PV) and persistent volume claim (PVC). The scripts also generate the domain YAML file, which can then be used to start the Kubernetes artifacts of the corresponding domain.\nPrerequisites Before you begin, perform the following steps:\n Review the Domain resource documentation. Ensure that you have executed all the preliminary steps documented in Prepare your environment. Ensure that the database is up and running.  Prepare the create domain script The sample scripts for Oracle Identity Governance domain deployment are available at $WORKDIR/kubernetes/create-oim-domain.\n  Make a copy of the create-domain-inputs.yaml file:\n$ cd $WORKDIR/kubernetes/create-oim-domain/domain-home-on-pv $ cp create-domain-inputs.yaml create-domain-inputs.yaml.orig   Edit the create-domain-inputs.yaml and modify the following parameters. Save the file when complete:\ndomainUID: \u0026lt;domain_uid\u0026gt; domainHome: /u01/oracle/user_projects/domains/\u0026lt;domain_uid\u0026gt; image: \u0026lt;image_name\u0026gt; imagePullSecretName: \u0026lt;container_registry_secret\u0026gt; weblogicCredentialsSecretName: \u0026lt;kubernetes_domain_secret\u0026gt; logHome: /u01/oracle/user_projects/domains/logs/\u0026lt;domain_id\u0026gt; namespace: \u0026lt;domain_namespace\u0026gt; persistentVolumeClaimName: \u0026lt;pvc_name\u0026gt; rcuSchemaPrefix: \u0026lt;rcu_prefix\u0026gt; rcuDatabaseURL: \u0026lt;rcu_db_host\u0026gt;:\u0026lt;rcu_db_port\u0026gt;/\u0026lt;rcu_db_service_name\u0026gt; rcuCredentialsSecret: \u0026lt;kubernetes_rcu_secret\u0026gt; frontEndHost: \u0026lt;front_end_hostname\u0026gt; frontEndPort: \u0026lt;front_end_port\u0026gt; Note : imagePullSecretName is not required if you are not using a container registry.\nFor example:\ndomainUID: governancedomain domainHome: /u01/oracle/user_projects/domains/governancedomain image: container-registry.oracle.com/middleware/oig_cpu:12.2.1.4-jdk8-ol7-220120.1359 imagePullSecretName: orclcred weblogicCredentialsSecretName: oig-domain-credentials logHome: /u01/oracle/user_projects/domains/logs/governancedomain namespace: oigns persistentVolumeClaimName: governancedomain-domain-pvc rcuSchemaPrefix: OIGK8S rcuDatabaseURL: mydatabasehost.example.com:1521/orcl.example.com rcuCredentialsSecret: oig-rcu-credentials frontEndHost: example.com frontEndPort: 14100 Note: For now frontEndHost and front_end_port should be set to example.com and 14100 respectively. These values will be changed to the correct values in post installation tasks in Set OIMFrontendURL using MBeans.\n  A full list of parameters in the create-domain-inputs.yaml file are shown below:\n   Parameter Definition Default     adminPort Port number for the Administration Server inside the Kubernetes cluster. 7001   adminNodePort Port number of the Administration Server outside the Kubernetes cluster. 30701   adminServerName Name of the Administration Server. AdminServer   clusterName Name of the WebLogic cluster instance to generate for the domain. By default the cluster name is oimcluster for the OIG domain. oimcluster   configuredManagedServerCount Number of Managed Server instances to generate for the domain. 5   createDomainFilesDir Directory on the host machine to locate all the files to create a WebLogic domain, including the script that is specified in the createDomainScriptName property. By default, this directory is set to the relative path wlst, and the create script will use the built-in WLST offline scripts in the wlst directory to create the WebLogic domain. It can also be set to the relative path wdt, and then the built-in WDT scripts will be used instead. An absolute path is also supported to point to an arbitrary directory in the file system. The built-in scripts can be replaced by the user-provided scripts or model files as long as those files are in the specified directory. Files in this directory are put into a Kubernetes config map, which in turn is mounted to the createDomainScriptsMountPath, so that the Kubernetes pod can use the scripts and supporting files to create a domain home. wlst   createDomainScriptsMountPath Mount path where the create domain scripts are located inside a pod. The create-domain.sh script creates a Kubernetes job to run the script (specified in the createDomainScriptName property) in a Kubernetes pod to create a domain home. Files in the createDomainFilesDir directory are mounted to this location in the pod, so that the Kubernetes pod can use the scripts and supporting files to create a domain home. /u01/weblogic   createDomainScriptName Script that the create domain script uses to create a WebLogic domain. The create-domain.sh script creates a Kubernetes job to run this script to create a domain home. The script is located in the in-pod directory that is specified in the createDomainScriptsMountPath property. If you need to provide your own scripts to create the domain home, instead of using the built-it scripts, you must use this property to set the name of the script that you want the create domain job to run. create-domain-job.sh   domainHome Home directory of the OIG domain. If not specified, the value is derived from the domainUID as /shared/domains/\u0026lt;domainUID\u0026gt;. /u01/oracle/user_projects/domains/oimcluster   domainPVMountPath Mount path of the domain persistent volume. /u01/oracle/user_projects/domains   domainUID Unique ID that will be used to identify this particular domain. Used as the name of the generated WebLogic domain as well as the name of the Kubernetes domain resource. This ID must be unique across all domains in a Kubernetes cluster. This ID cannot contain any character that is not valid in a Kubernetes service name. oimcluster   exposeAdminNodePort Boolean indicating if the Administration Server is exposed outside of the Kubernetes cluster. false   exposeAdminT3Channel Boolean indicating if the T3 administrative channel is exposed outside the Kubernetes cluster. true   image OIG container image. The operator requires OIG 12.2.1.4. Refer to OIG domains for details on how to obtain or create the image. oracle/oig:12.2.1.4.0   imagePullPolicy WebLogic container image pull policy. Legal values are IfNotPresent, Always, or Never IfNotPresent   imagePullSecretName Name of the Kubernetes secret to access the container registry to pull the OIG container image. The presence of the secret will be validated when this parameter is specified.    includeServerOutInPodLog Boolean indicating whether to include the server .out to the pod\u0026rsquo;s stdout. true   initialManagedServerReplicas Number of Managed Servers to initially start for the domain. 2   javaOptions Java options for starting the Administration Server and Managed Servers. A Java option can have references to one or more of the following pre-defined variables to obtain WebLogic domain information: $(DOMAIN_NAME), $(DOMAIN_HOME), $(ADMIN_NAME), $(ADMIN_PORT), and $(SERVER_NAME). -Dweblogic.StdoutDebugEnabled=false   logHome The in-pod location for the domain log, server logs, server out, and Node Manager log files. If not specified, the value is derived from the domainUID as /shared/logs/\u0026lt;domainUID\u0026gt;. /u01/oracle/user_projects/domains/logs/oimcluster   managedServerNameBase Base string used to generate Managed Server names. oim_server   managedServerPort Port number for each Managed Server. 8001   namespace Kubernetes namespace in which to create the domain. oimcluster   persistentVolumeClaimName Name of the persistent volume claim created to host the domain home. If not specified, the value is derived from the domainUID as \u0026lt;domainUID\u0026gt;-weblogic-sample-pvc. oimcluster-domain-pvc   productionModeEnabled Boolean indicating if production mode is enabled for the domain. true   serverStartPolicy Determines which WebLogic Server instances will be started. Legal values are NEVER, IF_NEEDED, ADMIN_ONLY. IF_NEEDED   t3ChannelPort Port for the T3 channel of the NetworkAccessPoint. 30012   t3PublicAddress Public address for the T3 channel. This should be set to the public address of the Kubernetes cluster. This would typically be a load balancer address. For development environments only: In a single server (all-in-one) Kubernetes deployment, this may be set to the address of the master, or at the very least, it must be set to the address of one of the worker nodes. If not provided, the script will attempt to set it to the IP address of the Kubernetes cluster   weblogicCredentialsSecretName Name of the Kubernetes secret for the Administration Server\u0026rsquo;s user name and password. If not specified, then the value is derived from the domainUID as \u0026lt;domainUID\u0026gt;-weblogic-credentials. oimcluster-domain-credentials   weblogicImagePullSecretName Name of the Kubernetes secret for the container registry, used to pull the WebLogic Server image.    serverPodCpuRequest, serverPodMemoryRequest, serverPodCpuCLimit, serverPodMemoryLimit The maximum amount of compute resources allowed, and minimum amount of compute resources required, for each server pod. Please refer to the Kubernetes documentation on Managing Compute Resources for Containers for details. Resource requests and resource limits are not specified.   rcuSchemaPrefix The schema prefix to use in the database, for example OIGK8S. You may wish to make this the same as the domainUID in order to simplify matching domains to their RCU schemas. OIGK8S   rcuDatabaseURL The database URL. oracle-db.default.svc.cluster.local:1521/devpdb.k8s   rcuCredentialsSecret The Kubernetes secret containing the database credentials. oimcluster-rcu-credentials   frontEndHost The entry point URL for the OIM. Not set   frontEndPort The entry point port for the OIM. Not set    Note that the names of the Kubernetes resources in the generated YAML files may be formed with the value of some of the properties specified in the create-inputs.yaml file. Those properties include the adminServerName, clusterName and managedServerNameBase. If those values contain any characters that are invalid in a Kubernetes service name, those characters are converted to valid values in the generated YAML files. For example, an uppercase letter is converted to a lowercase letter and an underscore (\u0026quot;_\u0026quot;) is converted to a hyphen (\u0026quot;-\u0026quot;).\nThe sample demonstrates how to create an OIG domain home and associated Kubernetes resources for a domain that has one cluster only. In addition, the sample provides the capability for users to supply their own scripts to create the domain home for other use cases. The generated domain YAML file could also be modified to cover more use cases.\nRun the create domain script Generate the create domain script   Run the create domain script, specifying your inputs file and an output directory to store the generated artifacts:\n$ cd $WORKDIR/kubernetes/create-oim-domain/domain-home-on-pv $ mkdir output $ ./create-domain.sh -i create-domain-inputs.yaml -o /\u0026lt;path to output-directory\u0026gt; For example:\n$ cd $WORKDIR/kubernetes/create-oim-domain/domain-home-on-pv $ mkdir output $ ./create-domain.sh -i create-domain-inputs.yaml -o output The output will look similar to the following:\nInput parameters being used export version=\u0026quot;create-weblogic-sample-domain-inputs-v1\u0026quot; export adminPort=\u0026quot;7001\u0026quot; export adminServerName=\u0026quot;AdminServer\u0026quot; export domainUID=\u0026quot;governancedomain\u0026quot; export domainHome=\u0026quot;/u01/oracle/user_projects/domains/governancedomain\u0026quot; export serverStartPolicy=\u0026quot;IF_NEEDED\u0026quot; export clusterName=\u0026quot;oim_cluster\u0026quot; export configuredManagedServerCount=\u0026quot;5\u0026quot; export initialManagedServerReplicas=\u0026quot;1\u0026quot; export managedServerNameBase=\u0026quot;oim_server\u0026quot; export managedServerPort=\u0026quot;14000\u0026quot; export image=\u0026quot;container-registry.oracle.com/middleware/oig_cpu:12.2.1.4-jdk8-ol7-220120.1359\u0026quot; export imagePullPolicy=\u0026quot;IfNotPresent\u0026quot; export imagePullSecretName=\u0026quot;orclcred\u0026quot; export productionModeEnabled=\u0026quot;true\u0026quot; export weblogicCredentialsSecretName=\u0026quot;oig-domain-credentials\u0026quot; export includeServerOutInPodLog=\u0026quot;true\u0026quot; export logHome=\u0026quot;/u01/oracle/user_projects/domains/logs/governancedomain\u0026quot; export t3ChannelPort=\u0026quot;30012\u0026quot; export exposeAdminT3Channel=\u0026quot;false\u0026quot; export adminNodePort=\u0026quot;30701\u0026quot; export exposeAdminNodePort=\u0026quot;false\u0026quot; export namespace=\u0026quot;oigns\u0026quot; javaOptions=-Dweblogic.StdoutDebugEnabled=false export persistentVolumeClaimName=\u0026quot;governancedomain-domain-pvc\u0026quot; export domainPVMountPath=\u0026quot;/u01/oracle/user_projects/domains\u0026quot; export createDomainScriptsMountPath=\u0026quot;/u01/weblogic\u0026quot; export createDomainScriptName=\u0026quot;create-domain-job.sh\u0026quot; export createDomainFilesDir=\u0026quot;wlst\u0026quot; export rcuSchemaPrefix=\u0026quot;OIGK8S\u0026quot; export rcuDatabaseURL=\u0026quot;mydatabasehost.example.com:1521/orcl.example.com\u0026quot; export rcuCredentialsSecret=\u0026quot;oig-rcu-credentials\u0026quot; export frontEndHost=\u0026quot;example.com\u0026quot; export frontEndPort=\u0026quot;14100\u0026quot; Generating output/weblogic-domains/governancedomain/create-domain-job.yaml Generating output/weblogic-domains/governancedomain/delete-domain-job.yaml Generating output/weblogic-domains/governancedomain/domain.yaml Checking to see if the secret governancedomain-domain-credentials exists in namespace oigns configmap/governancedomain-create-fmw-infra-sample-domain-job-cm created Checking the configmap governancedomain-create-fmw-infra-sample-domain-job-cm was created configmap/governancedomain-create-fmw-infra-sample-domain-job-cm labeled Checking if object type job with name governancedomain-create-fmw-infra-sample-domain-job exists No resources found in oigns namespace. Creating the domain by creating the job output/weblogic-domains/governancedomain/create-domain-job.yaml job.batch/governancedomain-create-fmw-infra-sample-domain-job created Waiting for the job to complete... status on iteration 1 of 40 pod governancedomain-create-fmw-infra-sample-domain-job-8cww8 status is Running status on iteration 2 of 40 pod governancedomain-create-fmw-infra-sample-domain-job-8cww8 status is Running status on iteration 3 of 40 pod governancedomain-create-fmw-infra-sample-domain-job-8cww8 status is Running status on iteration 4 of 40 pod governancedomain-create-fmw-infra-sample-domain-job-8cww8 status is Running status on iteration 5 of 40 pod governancedomain-create-fmw-infra-sample-domain-job-8cww8 status is Running status on iteration 6 of 40 pod governancedomain-create-fmw-infra-sample-domain-job-8cww8 status is Running status on iteration 7 of 40 pod governancedomain-create-fmw-infra-sample-domain-job-8cww8 status is Running status on iteration 8 of 40 pod governancedomain-create-fmw-infra-sample-domain-job-8cww8 status is Running status on iteration 9 of 40 pod governancedomain-create-fmw-infra-sample-domain-job-8cww8 status is Running status on iteration 10 of 40 pod governancedomain-create-fmw-infra-sample-domain-job-8cww8 status is Running status on iteration 11 of 40 pod governancedomain-create-fmw-infra-sample-domain-job-8cww8 status is Completed Domain governancedomain was created and will be started by the WebLogic Kubernetes Operator The following files were generated: output/weblogic-domains/governancedomain/create-domain-inputs.yaml output/weblogic-domains/governancedomain/create-domain-job.yaml output/weblogic-domains/governancedomain/domain.yaml sed Completed $ Note: If the create domain script creation fails, refer to the Troubleshooting section.\n  Setting the OIM server memory parameters   Navigate to the /output/weblogic-domains/\u0026lt;domain_uid\u0026gt; directory:\n$ cd $WORKDIR/kubernetes/create-oim-domain/domain-home-on-pv/output/weblogic-domains/\u0026lt;domain_uid\u0026gt; For example:\n$ cd $WORKDIR/kubernetes/create-oim-domain/domain-home-on-pv/output/weblogic-domains/governancedomain   Edit the domain_oim_soa.yaml and locate the section of the file starting with: - clusterName: oim_cluster. Immediately after the line: topologyKey: \u0026quot;kubernetes.io/hostname\u0026quot;, add the following lines:\nenv: - name: USER_MEM_ARGS value: \u0026quot;-Djava.security.egd=file:/dev/./urandom -Xms2408m -Xmx8192m\u0026quot; The file should looks as follows:\n- clusterName: oim_cluster serverService: precreateService: true serverStartState: \u0026quot;RUNNING\u0026quot; serverPod: # Instructs Kubernetes scheduler to prefer nodes for new cluster members where there are not # already members of the same cluster. affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: \u0026quot;weblogic.clusterName\u0026quot; operator: In values: - $(CLUSTER_NAME) topologyKey: \u0026quot;kubernetes.io/hostname\u0026quot; env: - name: USER_MEM_ARGS value: \u0026quot;-Djava.security.egd=file:/dev/./urandom -Xms2408m -Xmx8192m\u0026quot; replicas: 1 ...   Run the create domain scripts   Create the Kubernetes resource using the following command:\n$ cd $WORKDIR/kubernetes/create-oim-domain/domain-home-on-pv/output/weblogic-domains/\u0026lt;domain_uid\u0026gt; $ kubectl apply -f domain.yaml For example:\n$ cd $WORKDIR/kubernetes/create-oim-domain/domain-home-on-pv/output/weblogic-domains/governancedomain $ kubectl apply -f domain.yaml The output will look similar to the following:\ndomain.weblogic.oracle/governancedomain created   Run the following command to view the status of the OIG pods:\n$ kubectl get pods -n oigns The output will initially look similar to the following:\nNAME READY STATUS RESTARTS AGE helper 1/1 Running 0 3h30m governancedomain-create-fmw-infra-sample-domain-job-8cww8 0/1 Completed 0 27m governancedomain-introspect-domain-job-p4brt 1/1 Running 0 6s The introspect-domain-job pod will be displayed first. Run the command again after several minutes and check to see that the Administration Server and SOA Server are both started. When started they should have STATUS = Running and READY = 1/1.\nNAME READY STATUS RESTARTS AGE helper 1/1 Running 0 3h38m governancedomain-adminserver 1/1 Running 0 7m30s governancedomain-create-fmw-infra-sample-domain-job-8cww8 0/1 Completed 0 35m governancedomain-soa-server1 1/1 Running 0 4m Note: It will take several minutes before all the pods listed above show. When a pod has a STATUS of 0/1 the pod is started but the OIG server associated with it is currently starting. While the pods are starting you can check the startup status in the pod logs, by running the following command:\n$ kubectl logs governancedomain-adminserver -n oigns $ kubectl logs governancedomain-soa-server1 -n oigns   Once both pods are running, start the OIM Server using the following command:\n$ cd $WORKDIR/kubernetes/create-oim-domain/domain-home-on-pv/output/weblogic-domains/governancedomain/ $ kubectl apply -f domain_oim_soa.yaml For example:\n$ cd $WORKDIR/kubernetes/create-oim-domain/domain-home-on-pv/output/weblogic-domains/governancedomain/ $ kubectl apply -f domain_oim_soa.yaml The output will look similar to the following:\ndomain.weblogic.oracle/governancedomain configured   Verify the results Verify the domain, pods and services   Verify the domain, servers pods and services are created and in the READY state with a STATUS of 1/1, by running the following command:\n$ kubectl get all,domains -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get all,domains -n oigns The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE pod/governancedomain-adminserver 1/1 Running 0 16m pod/governancedomain-create-fmw-infra-sample-domain-job-8cww8 0/1 Completed 0 36m pod/governancedomain-oim-server1 1/1 Running 0 5m57s pod/governancedomain-soa-server1 1/1 Running 0 13m pod/helper 1/1 Running 0 3h40m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/governancedomain-adminserver ClusterIP None \u0026lt;none\u0026gt; 7001/TCP 16m service/governancedomain-cluster-oim-cluster ClusterIP 10.97.121.159 \u0026lt;none\u0026gt; 14000/TCP 13m service/governancedomain-cluster-soa-cluster ClusterIP 10.111.231.242 \u0026lt;none\u0026gt; 8001/TCP 13m service/governancedomain-oim-server1 ClusterIP None \u0026lt;none\u0026gt; 14000/TCP 5m57s service/governancedomain-oim-server2 ClusterIP 10.108.139.30 \u0026lt;none\u0026gt; 14000/TCP 5m57s service/governancedomain-oim-server3 ClusterIP 10.97.170.104 \u0026lt;none\u0026gt; 14000/TCP 5m57s service/governancedomain-oim-server4 ClusterIP 10.99.82.214 \u0026lt;none\u0026gt; 14000/TCP 5m57s service/governancedomain-oim-server5 ClusterIP 10.98.75.228 \u0026lt;none\u0026gt; 14000/TCP 5m57s service/governancedomain-soa-server1 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP 13m service/governancedomain-soa-server2 ClusterIP 10.107.232.220 \u0026lt;none\u0026gt; 8001/TCP 13m service/governancedomain-soa-server3 ClusterIP 10.108.203.6 \u0026lt;none\u0026gt; 8001/TCP 13m service/governancedomain-soa-server4 ClusterIP 10.96.178.0 \u0026lt;none\u0026gt; 8001/TCP 13m service/governancedomain-soa-server5 ClusterIP 10.107.83.62 \u0026lt;none\u0026gt; 8001/TCP 13m NAME COMPLETIONS DURATION AGE job.batch/governancedomain-create-fmw-infra-sample-domain-job 1/1 5m30s 36m NAME AGE domain.weblogic.oracle/governancedomain 17m Note: It will take several minutes before all the services listed above show. While the governancedomain-oim-server1 pod has a STATUS of 0/1 the pod is started but the OIG server associated with it is currently starting. While the pod is starting you can check the startup status in the pod logs, by running the following command:\n$ kubectl logs governancedomain-oim-server1 -n oigns   The default domain created by the script has the following characteristics:\n An Administration Server named AdminServer listening on port 7001. A configured OIG cluster named oig_cluster of size 5. A configured SOA cluster named soa_cluster of size 5. One started OIG managed Server, named oim_server1, listening on port 14000. One started SOA managed Server, named soa_server1, listening on port 8001. Log files that are located in \u0026lt;persistent_volume\u0026gt;/logs/\u0026lt;domainUID\u0026gt;  Verify the domain   Run the following command to describe the domain:\n$ kubectl describe domain \u0026lt;domain_uid\u0026gt; -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl describe domain governancedomain -n oigns The output will look similar to the following:\nName: governancedomain Namespace: oigns Labels: weblogic.domainUID=governancedomain Annotations: \u0026lt;none\u0026gt; API Version: weblogic.oracle/v8 Kind: Domain Metadata: Creation Timestamp: 2022-03-10T11:44:17Z Generation: 2 Managed Fields: API Version: weblogic.oracle/v8 Fields Type: FieldsV1 fieldsV1: f:metadata: f:annotations: .: f:kubectl.kubernetes.io/last-applied-configuration: f:labels: .: f:weblogic.domainUID: Manager: kubectl-client-side-apply Operation: Update Time: 2022-03-10T14:59:44Z API Version: weblogic.oracle/v8 Fields Type: FieldsV1 fieldsV1: f:status: .: f:clusters: f:conditions: f:introspectJobFailureCount: f:servers: f:startTime: Manager: Kubernetes Java Client Operation: Update Time: 2022-03-10T11:51:12Z Resource Version: 383381 UID: ea95c549-c414-42a6-8de4-beaf1204872e Spec: Admin Server: Server Pod: Env: Name: USER_MEM_ARGS Value: -Djava.security.egd=file:/dev/./urandom -Xms512m -Xmx1024m Server Start State: RUNNING Clusters: Cluster Name: soa_cluster Replicas: 1 Server Pod: Affinity: Pod Anti Affinity: Preferred During Scheduling Ignored During Execution: Pod Affinity Term: Label Selector: Match Expressions: Key: weblogic.clusterName Operator: In Values: $(CLUSTER_NAME) Topology Key: kubernetes.io/hostname Weight: 100 Server Service: Precreate Service: true Server Start State: RUNNING Cluster Name: oim_cluster Replicas: 1 Server Pod: Affinity: Pod Anti Affinity: Preferred During Scheduling Ignored During Execution: Pod Affinity Term: Label Selector: Match Expressions: Key: weblogic.clusterName Operator: In Values: $(CLUSTER_NAME) Topology Key: kubernetes.io/hostname Weight: 100 Env: Name: USER_MEM_ARGS Value: -Djava.security.egd=file:/dev/./urandom -Xms2408m -Xmx8192m Server Service: Precreate Service: true Server Start State: RUNNING Data Home: Domain Home: /u01/oracle/user_projects/domains/governancedomain Domain Home Source Type: PersistentVolume Http Access Log In Log Home: true Image: container-registry.oracle.com/middleware/oig_cpu:12.2.1.4-jdk8-ol7-220120.1359 Image Pull Policy: IfNotPresent Image Pull Secrets: Name: orclcred Include Server Out In Pod Log: true Log Home: /u01/oracle/user_projects/domains/logs/governancedomain Log Home Enabled: true Server Pod: Env: Name: JAVA_OPTIONS Value: -Dweblogic.StdoutDebugEnabled=false Name: USER_MEM_ARGS Value: -Djava.security.egd=file:/dev/./urandom -Xms256m -Xmx1024m Volume Mounts: Mount Path: /u01/oracle/user_projects/domains Name: weblogic-domain-storage-volume Volumes: Name: weblogic-domain-storage-volume Persistent Volume Claim: Claim Name: governancedomain-domain-pvc Server Start Policy: IF_NEEDED Web Logic Credentials Secret: Name: oig-domain-credentials Status: Clusters: Cluster Name: oim_cluster Maximum Replicas: 5 Minimum Replicas: 0 Ready Replicas: 1 Replicas: 1 Replicas Goal: 1 Cluster Name: soa_cluster Maximum Replicas: 5 Minimum Replicas: 0 Ready Replicas: 1 Replicas: 1 Replicas Goal: 1 Conditions: Last Transition Time: 2022-03-10T11:59:53.249700Z Reason: ServersReady Status: True Type: Available Introspect Job Failure Count: 0 Servers: Desired State: RUNNING Health: Activation Time: 2022-03-10T11:46:49.874000Z Overall Health: ok Subsystems: Subsystem Name: ServerRuntime Symptoms: Node Name: 10.250.40.59 Server Name: AdminServer State: RUNNING Cluster Name: oim_cluster Desired State: RUNNING Health: Activation Time: 2022-03-10T15:06:21.693000Z Overall Health: ok Subsystems: Subsystem Name: ServerRuntime Symptoms: Node Name: 10.250.40.59 Server Name: oim_server1 State: RUNNING Cluster Name: oim_cluster Desired State: SHUTDOWN Server Name: oim_server2 Cluster Name: oim_cluster Desired State: SHUTDOWN Server Name: oim_server3 Cluster Name: oim_cluster Desired State: SHUTDOWN Server Name: oim_server4 Cluster Name: oim_cluster Desired State: SHUTDOWN Server Name: oim_server5 Cluster Name: soa_cluster Desired State: RUNNING Health: Activation Time: 2022-03-10T11:49:26.340000Z Overall Health: ok Subsystems: Subsystem Name: ServerRuntime Symptoms: Node Name: 10.250.40.59 Server Name: soa_server1 State: RUNNING Cluster Name: soa_cluster Desired State: SHUTDOWN Server Name: soa_server2 Cluster Name: soa_cluster Desired State: SHUTDOWN Server Name: soa_server3 Cluster Name: soa_cluster Desired State: SHUTDOWN Server Name: soa_server4 Cluster Name: soa_cluster Desired State: SHUTDOWN Server Name: soa_server5 Start Time: 2022-03-10T14:50:19.148541Z Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal DomainCreated 19m weblogic.operator Domain resource governancedomain was created Normal DomainProcessingCompleted 12m weblogic.operator Successfully completed processing domain resource governancedomain Normal DomainChanged 10m weblogic.operator Domain resource governancedomain was changed Normal DomainProcessingStarting 10m (x2 over 19m) weblogic.operator Creating or updating Kubernetes presence for WebLogic Domain with UID governancedomai In the Status section of the output, the available servers and clusters are listed.\n  Verify the pods   Run the following command to see the pods running the servers and which nodes they are running on:\n$ kubectl get pods -n \u0026lt;namespace\u0026gt; -o wide For example:\n$ kubectl get pods -n oigns -o wide The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES helper 1/1 Running 0 3h50m 10.244.1.39 worker-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; governancedomain-adminserver 1/1 Running 0 27m 10.244.1.42 worker-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; governancedomain-create-fmw-infra-sample-domain-job-8cww8 0/1 Completed 0 47m 10.244.1.40 worker-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; governancedomain-oim-server1 1/1 Running 0 16m 10.244.1.44 worker-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; governancedomain-soa-server1 1/1 Running 0 24m 10.244.1.43 worker-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; You are now ready to configure an Ingress to direct traffic for your OIG domain as per Configure an ingress for an OIG domain.\n  "
},
{
	"uri": "/fmw-kubernetes/oid/create-oid-instances/",
	"title": "Create Oracle Internet Directory Instances",
	"tags": [],
	"description": "This document provides details of the oid Helm chart.",
	"content": " Introduction Create a Kubernetes namespace Create a Kubernetes secret for the container registry Create a persistent volume directory The oid Helm chart Create OID instances Helm command output Verify the OID deployment Undeploy an OID deployment Appendix: Configuration parameters  Introduction This chapter demonstrates how to deploy Oracle Internet Directory (OID) 12c instance(s) using the Helm package manager for Kubernetes.\nBased on the configuration, this chart deploys the following objects in the specified namespace of a Kubernetes cluster.\n Service Account Secret Persistent Volume and Persistent Volume Claim Pod(s)/Container(s) for Oracle Internet Directory Instances Services for interfaces exposed through Oracle Internet Directory Instances Ingress configuration  Create a Kubernetes namespace Create a Kubernetes namespace for the OID deployment by running the following command:\n$ kubectl create namespace \u0026lt;namespace\u0026gt; For example:\n$ kubectl create namespace oidns The output will look similar to the following:\nnamespace/oidns created Create a Kubernetes secret for the container registry In this section you create a secret that stores the credentials for the container registry where the OID image is stored. This step must be followed if using Oracle Container Registry or your own private registry. If you are not using a container registry and have loaded the images on each of the master and worker nodes, you can skip this step.\n  Run the following command to create the secret:\nkubectl create secret docker-registry \u0026#34;orclcred\u0026#34; --docker-server=\u0026lt;CONTAINER_REGISTRY\u0026gt; \\ --docker-username=\u0026#34;\u0026lt;USER_NAME\u0026gt;\u0026#34; \\ --docker-password=\u0026lt;PASSWORD\u0026gt; --docker-email=\u0026lt;EMAIL_ID\u0026gt; \\ --namespace=\u0026lt;domain_namespace\u0026gt; For example, if using Oracle Container Registry:\nkubectl create secret docker-registry \u0026#34;orclcred\u0026#34; --docker-server=container-registry.oracle.com \\ --docker-username=\u0026#34;user@example.com\u0026#34; \\ --docker-password=password --docker-email=user@example.com \\ --namespace=oudns Replace \u0026lt;USER_NAME\u0026gt; and \u0026lt;PASSWORD\u0026gt; with the credentials for the registry with the following caveats:\n  If using Oracle Container Registry to pull the OID container image, this is the username and password used to login to Oracle Container Registry. Before you can use this image you must login to Oracle Container Registry, navigate to Middleware \u0026gt; oid_cpu and accept the license agreement.\n  If using your own container registry to store the OID container image, this is the username and password (or token) for your container registry.\n  The output will look similar to the following:\nsecret/orclcred created   Create a persistent volume directory As referenced in Prerequisites the nodes in the Kubernetes cluster must have access to a persistent volume such as a Network File System (NFS) mount or a shared file system.\nMake sure the persistent volume path has full access permissions, and that the folder is empty. In this example /scratch/shared/ is a shared directory accessible from all nodes.\n  On the master node run the following command to create a user_projects directory:\n$ cd \u0026lt;persistent_volume\u0026gt; $ mkdir oid_user_projects $ chmod 777 oid_user_projects For example:\n$ cd /scratch/shared $ mkdir oid_user_projects $ chmod 777 oid_user_projects   On the master node run the following to ensure it is possible to read and write to the persistent volume:\n$ cd \u0026lt;persistent_volume\u0026gt;/oid_user_projects $ touch file.txt $ ls filemaster.txt For example:\n$ cd /scratch/shared/oid_user_projects $ touch filemaster.txt $ ls filemaster.txt On the first worker node run the following to ensure it is possible to read and write to the persistent volume:\n$ cd /scratch/shared/oid_user_projects $ ls filemaster.txt $ touch fileworker1.txt $ ls fileworker1.txt Repeat the above for any other worker nodes e.g fileworker2.txt etc. Once proven that it\u0026rsquo;s possible to read and write from each node to the persistent volume, delete the files created.\n  The oid Helm chart The \u0026lsquo;oid\u0026rsquo; Helm chart allows you to create Oracle Internet Directory instances along with Kubernetes objects in a specified namespace using the oid Helm Chart.\nThe deployment can be initiated by running the following Helm command with reference to the oid Helm chart, along with configuration parameters according to your environment.\ncd $WORKDIR/kubernetes/helm $ helm install --namespace \u0026lt;namespace\u0026gt; \\ \u0026lt;Configuration Parameters\u0026gt; \\ \u0026lt;deployment/release name\u0026gt; \\ \u0026lt;Helm Chart Path/Name\u0026gt; Configuration Parameters (override values in chart) can be passed on with --set arguments on the command line and/or with -f / --values arguments when referring to files.\nNote: The examples in Create OID instances below provide values which allow the user to override the default values provided by the Helm chart. A full list of configuration parameters and their default values is shown in Appendix: Configuration parameters.\nFor more details about the helm command and parameters, please execute helm --help and helm install --help.\nCreate OID instances You can create OID instances using one of the following methods:\n Using a YAML file Using --set argument  Note: It is not possible to install sample data or load an ldif file during the OID deployment. In order to load data in OID, create the OID deployment and then use ldapmodify post the ingress deployment. See Using LDAP utilities.\nUsing a YAML file   Navigate to the $WORKDIR/kubernetes/helm directory:\n$ cd $WORKDIR/kubernetes/helm   Create an oid-values-override.yaml as follows:\nimage: repository: \u0026lt;image\u0026gt; tag: \u0026lt;tag\u0026gt; pullPolicy: IfNotPresent imagePullSecrets: - name: orclcred oidConfig: realmDN: \u0026lt;baseDN\u0026gt; domainName: \u0026lt;domainName\u0026gt; orcladminPassword: \u0026lt;password\u0026gt; dbUser: sys dbPassword: \u0026lt;sys_password\u0026gt; dbschemaPassword: \u0026lt;password\u0026gt; rcuSchemaPrefix: \u0026lt;rcu_prefix\u0026gt; rcuDatabaseURL: \u0026lt;db_hostname\u0026gt;:\u0026lt;dp_port\u0026gt;/\u0026lt;db_service\u0026gt; sslwalletPassword: \u0026lt;password\u0026gt; persistence: type: filesystem filesystem: hostPath:: path: \u0026lt;persistent_volume\u0026gt;/oid_user_projects odsm: adminUser: weblogic adminPassword: \u0026lt;password\u0026gt; For example:\nimage: repository: container-registry.oracle.com/middleware/oid_cpu tag: 12.2.1.4-jdk8-ol7-220223.1744 pullPolicy: IfNotPresent imagePullSecrets: - name: orclcred oidConfig: realmDN: dc=oid,dc=example,dc=com domainName: oid_domain orcladminPassword: \u0026lt;password\u0026gt; dbUser: sys dbPassword: \u0026lt;password\u0026gt; dbschemaPassword: \u0026lt;password\u0026gt; rcuSchemaPrefix: OIDK8S rcuDatabaseURL: oiddb.example.com:1521/oiddb.example.com sslwalletPassword: \u0026lt;password\u0026gt; persistence: type: filesystem filesystem: hostPath: path: /scratch/shared/oid_user_projects odsm: adminUser: weblogic adminPassword: \u0026lt;password\u0026gt; The following caveats exist:\n  \u0026lt;baseDN\u0026gt; should be set to the value for the base DN to be created.\n  \u0026lt;domainName\u0026gt; should be set to the value for the domain name to be created.\n  rcuDatabaseURL, dbUser and dbPassword should be set to the relevant values for the database created as per Prerequisites.\n  rcuSchemaPrefix and dbschemaPassword should be set to a value of your choice. This creates the OID schema in the database.\n  Replace \u0026lt;password\u0026gt; with a the relevant passwords.\n  If you are not using Oracle Container Registry or your own container registry for your OID container image, then you can remove the following:\nimagePullSecrets: - name: orclcred   If using NFS for your persistent volume the change the persistence section as follows:\npersistence: type: networkstorage networkstorage: nfs: path: \u0026lt;persistent_volume\u0026gt;/oud_user_projects server: \u0026lt;NFS IP address\u0026gt;     Run the following to create the OID instances:\nhelm install --namespace \u0026lt;namespace\u0026gt; --values oid-values-override.yaml release name\u0026gt; oid For example:\nhelm install --namespace oidns --values oid-values-override.yaml oid oid   Check the OID deployment as per Verify the OID deployment.\n  Using --set argument   Navigate to the $WORKDIR/kubernetes/helm directory:\n$ cd $WORKDIR/kubernetes/helm   Run the following command to create OUDSM instance:\n$ helm install --namespace \u0026lt;namespace\u0026gt; \\ --set oidConfig.realmDN=\u0026#34;\u0026lt;baseDN\u0026gt;\u0026#34;,oidConfig.domainName=\u0026lt;domainName\u0026gt;,oidConfig.orcladminPassword=\u0026lt;password\u0026gt; \\ --set oidConfig.dbUser=sys,oidConfig.dbPassword=\u0026lt;password\u0026gt;,oidConfig.dbschemaPassword=\u0026lt;password\u0026gt; \\ --set oidConfig.rcuSchemaPrefix=\u0026#34;\u0026lt;rcu_prefix\u0026gt;\u0026#34;,oidConfig.rcuDatabaseURL=\u0026#34;\u0026lt;db_hostname\u0026gt;:\u0026lt;db_port\u0026gt;/\u0026lt;db_service\u0026gt;\u0026#34;,oidConfig.sslwalletPassword=\u0026lt;password\u0026gt; \\ --set persistence.filesystem.hostPath.path=\u0026lt;persistent_volume\u0026gt;/oid_user_projects \\ --set image.repository=\u0026lt;image_location\u0026gt;,image.tag=\u0026lt;image_tag\u0026gt; \\ --set odsm.adminUser=weblogic,odsm.adminPassword=\u0026lt;password\u0026gt; \\ --set imagePullSecrets[0].name=\u0026#34;orclcred\u0026#34; \\ \u0026lt;release name\u0026gt; oid For example:\n$ helm install --namespace oidns \\ --set oidConfig.realmDN=\u0026#34;dc=oid,dc=example,dc=com\u0026#34;,oidConfig.domainName=oid_domain,oidConfig.orcladminPassword=\u0026lt;password\u0026gt; \\ --set oidConfig.dbUser=sys,oidConfig.dbPassword=\u0026lt;password\u0026gt;,oidConfig.dbschemaPassword=\u0026lt;password\u0026gt; \\ --set oidConfig.rcuSchemaPrefix=\u0026#34;OIDK8S\u0026#34;,oidConfig.rcuDatabaseURL=\u0026#34;oiddb.example.com:1521/oiddb.example.com\u0026#34;,oidConfig.sslwalletPassword=\u0026lt;password\u0026gt; \\ --set persistence.filesystem.hostPath.path=/scratch/shared/oid_user_projects \\  --set image.repository=container-registry.oracle.com/middleware/oid_cpu,image.tag=12.2.1.4-jdk8-ol7-220223.1744 \\ --set odsm.adminUser=weblogic,odsm.adminPassword=\u0026lt;password\u0026gt; \\ --set imagePullSecrets[0].name=\u0026#34;orclcred\u0026#34; \\ oid oid The following caveats exist:\n \u0026lt;baseDN\u0026gt; should be set to the value for the base DN to be created. \u0026lt;domainName\u0026gt; should be set to the value for the domain name to be created. rcuDatabaseURL, dbUser and dbPassword should be set to the relevant values for the database created as per Prerequisites. rcuSchemaPrefix and dbschemaPassword should be set to a value of your choice. This creates the OID schema in the database. Replace \u0026lt;password\u0026gt; with a the relevant passwords. If you are not using Oracle Container Registry or your own container registry for your OID container image, then you can remove the following: --set imagePullSecrets[0].name=\u0026quot;orclcred\u0026quot; If using using NFS for your persistent volume then use persistence.networkstorage.nfs.path=\u0026lt;persistent_volume\u0026gt;/oid_user_projects,persistence.networkstorage.nfs.server:\u0026lt;NFS IP address\u0026gt;    Check the OID deployment as per Verify the OID deployment.\n  Helm command output In all the examples above, the following output is shown following a successful execution of the helm install command.\nNAME: oid LAST DEPLOYED: Fri Mar 25 09:43:25 2022 NAMESPACE: oidns STATUS: deployed REVISION: 1 TEST SUITE: None Verify the OID deployment Run the following command to verify the OID deployment:\nCommand:\n$ kubectl --namespace oidns get all For example:\n$ kubectl --namespace oidns get all The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE pod/oidhost1 1/1 Running 0 35m pod/oidhost2 1/1 Running 0 35m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/oid-lbr-ldap ClusterIP 10.110.118.113 \u0026lt;none\u0026gt; 3060/TCP,3131/TCP 35m service/oidhost1 ClusterIP 10.97.17.125 \u0026lt;none\u0026gt; 3060/TCP,3131/TCP,7001/TCP,7002/TCP 35m service/oidhost2 ClusterIP 10.106.32.187 \u0026lt;none\u0026gt; 3060/TCP,3131/TCP 35m Note: If the OID deployment fails refer to Troubleshooting for instructions on how to view pod logs or describe the pod. Once the problem is identified follow Undeploy an OID deployment to clean down the deployment before deploying again.\nKubernetes Objects Kubernetes objects created by the Helm chart are detailed in the table below:\n   Type Name Example Name Purpose     Secret \u0026lt;deployment/release name\u0026gt;-creds oid-creds Secret object for Oracle Internet Directory related critical values like passwords   Persistent Volume \u0026lt;deployment/release name\u0026gt;-pv oid-pv Persistent Volume for user_projects mount.   Persistent Volume Claim \u0026lt;deployment/release name\u0026gt;-pvc oid-pvc Persistent Volume Claim for user_projects mount.   Pod \u0026lt;deployment/release name\u0026gt;1 oidhost1 Pod/Container for base Oracle Internet Directory Instance which would be populated first with base configuration (like number of sample entries)   Pod \u0026lt;deployment/release name\u0026gt;N oidhost2, oidhost3, \u0026hellip; Pod(s)/Container(s) for Oracle Internet Directory Instances   Service \u0026lt;deployment/release name\u0026gt;lbr-ldap oid-lbr-ldap Service for LDAP/LDAPS access load balanced across the base Oracle Internet Directory instances   Service \u0026lt;deployment/release name\u0026gt; oidhost1, oidhost2, oidhost3, \u0026hellip; Service for LDAP/LDAPS access for each base Oracle Internet Directory instance   Ingress \u0026lt;deployment/release name\u0026gt;-ingress-nginx oid-ingress-nginx Ingress Rules for LDAP/LDAPS access.     In the table above the \u0026lsquo;Example Name\u0026rsquo; for each Object is based on the value \u0026lsquo;oid\u0026rsquo; as deployment/release name for the Helm chart installation.  Ingress Configuration With OID instance(s) now deployed you are now ready to configure an ingress controller to direct traffic to OID as per Configure an ingress for an OID.\nUndeploy an OID deployment Remove OID schemas from the database Note: These steps must be performed if cleaning down a failed install. Failure to do so will cause any new OID deployment to fail.\nTo remove the OID schemas from the database:\n  Run the following to enter a bash session in an oid pod:\n$ kubectl exec -ti \u0026lt;pod\u0026gt; -n \u0026lt;namespace\u0026gt; -- bash For example:\n$ kubectl exec -ti oidhost2 -n oidns -- bash This will take you into a bash session in the pod:\n[oracle@oidhost2 oracle]$   Inside the container drop the RCU schemas as follows:\n[oracle@oidhost2 oracle]$ export CONNECTION_STRING=\u0026lt;db_host.domain\u0026gt;:\u0026lt;db_port\u0026gt;/\u0026lt;service_name\u0026gt; [oracle@oidhost2 oracle]$ export RCUPREFIX=\u0026lt;rcu_schema_prefix\u0026gt; [oracle@oidhost2 oracle]$ export DB_USER=sys [oracle@oidhost2 oracle]$ echo -e \u0026lt;db_pwd\u0026gt;\u0026#34;\\n\u0026#34;\u0026lt;rcu_schema_pwd\u0026gt; \u0026gt; /tmp/pwd.txt ${ORACLE_HOME}/oracle_common/bin/rcu -silent -dropRepository -databaseType ORACLE -connectString ${CONNECTION_STRING} \\ -dbUser ${DB_USER} -dbRole sysdba -selectDependentsForComponents true -schemaPrefix ${RCUPREFIX} \\ -component MDS -component OPSS -component STB -component OID -component IAU -component WLS -f \u0026lt; /tmp/pwd.txt where:\n \u0026lt;db_host.domain\u0026gt;:\u0026lt;db_port\u0026gt;/\u0026lt;service_name\u0026gt; is your database connect string \u0026lt;rcu_schema_prefix\u0026gt; is the RCU schema prefix \u0026lt;db_pwd\u0026gt; is the SYS password for the database \u0026lt;rcu_schema_pwd\u0026gt; is the password for the \u0026lt;rcu_schema_prefix\u0026gt;  For example:\n[oracle@oidhost2 oracle]$ export CONNECTION_STRING=oiddb.example.com:1521/oiddb.example.com [oracle@oidhost2 oracle]$ export RCUPREFIX=OIDK8S [oracle@oidhost2 oracle]$ export DB_USER=sys [oracle@oidhost2 oracle]$ echo -e \u0026lt;password\u0026gt;\u0026#34;\\n\u0026#34;\u0026lt;password\u0026gt; \u0026gt; /tmp/pwd.txt ${ORACLE_HOME}/oracle_common/bin/rcu -silent -dropRepository -databaseType ORACLE -connectString ${CONNECTION_STRING} \\ -dbUser ${DB_USER} -dbRole sysdba -selectDependentsForComponents true -schemaPrefix ${RCUPREFIX} \\ -component MDS -component OPSS -component STB -component OID -component IAU -component WLS -f \u0026lt; /tmp/pwd.txt The output will look similar to the following:\nRCU Logfile: /tmp/RCU2022-03-28_10-08_535715154/logs/rcu.log Processing command line .... Repository Creation Utility - Checking Prerequisites Checking Global Prerequisites Repository Creation Utility - Checking Prerequisites Checking Component Prerequisites Repository Creation Utility - Drop Repository Drop in progress. Percent Complete: 2 Percent Complete: 13 Percent Complete: 15 Dropping Audit Services(IAU) Percent Complete: 23 Percent Complete: 29 Percent Complete: 44 Percent Complete: 45 Dropping Oracle Internet Directory(OID) Percent Complete: 46 etc.. etc.. Dropping Audit Services Viewer(IAU_VIEWER) Dropping Audit Services Append(IAU_APPEND) Dropping Common Infrastructure Services(STB) Dropping tablespaces in the repository database Repository Creation Utility: Drop - Completion Summary Database details: ----------------------------- Host Name : oiddb.example.com Port : 1521 Service Name : oiddb.example.com Connected As : sys Prefix for (prefixable) Schema Owners : OIDK8S Prefix for (non-prefixable) Schema Owners : DEFAULT_PREFIX RCU Logfile : /tmp/RCU2022-03-28_10-08_535715154/logs/rcu.log Component schemas dropped: ----------------------------- Component Status Logfile Common Infrastructure Services Success /tmp/RCU2022-03-28_10-08_535715154/logs/stb.log Oracle Platform Security Services Success /tmp/RCU2022-03-28_10-08_535715154/logs/opss.log Oracle Internet Directory Success /tmp/RCU2022-03-28_10-08_535715154/logs/oid.log Audit Services Success /tmp/RCU2022-03-28_10-08_535715154/logs/iau.log Audit Services Append Success /tmp/RCU2022-03-28_10-08_535715154/logs/iau_append.log Audit Services Viewer Success /tmp/RCU2022-03-28_10-08_535715154/logs/iau_viewer.log Metadata Services Success /tmp/RCU2022-03-28_10-08_535715154/logs/mds.log WebLogic Services Success /tmp/RCU2022-03-28_10-08_535715154/logs/wls.log Repository Creation Utility - Drop : Operation Completed   Delete the OID deployment   Find the deployment release name:\n$ helm --namespace \u0026lt;namespace\u0026gt; list For example:\n$ helm --namespace oidns list The output will look similar to the following:\nNAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION oid oidns 2 2022-03-21 16:46:34.05531056 +0000 UTC deployed oid-0.1 12.2.1.4.0   Delete the deployment using the following command:\n$ helm uninstall --namespace \u0026lt;namespace\u0026gt; \u0026lt;release\u0026gt; For example:\n$ helm uninstall --namespace oidns oid The output will look similar to the following:\nrelease \u0026quot;oid\u0026quot; uninstalled   Delete the persistent volume contents   Delete the contents of the oid_user_projects directory in the persistent volume:\n$ cd \u0026lt;persistent_volume\u0026gt;/oid_user_projects $ rm -rf * For example:\n$ cd /scratch/shared/oid_user_projects $ rm -rf *   Appendix: Configuration Parameters The following table lists the configurable parameters of the oid chart and their default values.\n   Parameter Description Default Value     replicaCount Number of base Oracle Internet Directory instances/pods/services to be created. 1   restartPolicyName restartPolicy to be configured for each POD containing Oracle Internet Directory instance OnFailure   image.repository Oracle Internet Directory Image Registry/Repository and name. Based on this, the image parameter will be configured for Oracle Internet Directory pods/containers oracle/oid   image.tag Oracle Internet Directory Image Tag. Based on this, the image parameter will be configured for Oracle Internet Directory pods/containers 12.2.1.4.0   image.pullPolicy policy to pull the image IfnotPresent   imagePullSecrets.name name of Secret resource containing private registry credentials regcred   nameOverride override the fullname with this name    fullnameOverride Overrides the fullname with the provided string    serviceAccount.create Specifies whether a service account should be created true   serviceAccount.name If not set and create is true, a name is generated using the fullname template oid-\u0026lt; fullname \u0026gt;-token-\u0026lt; randomalphanum \u0026gt;   podSecurityContext Security context policies to add to the controller pod    securityContext Security context policies to add by default    service.type Type of Service to be created for OID Interfaces (like LDAP, HTTP, Admin) ClusterIP   service.lbrtype Service Type for loadbalancer services exposing LDAP, HTTP interfaces from available/accessible OID pods ClusterIP   ingress.enabled  true   ingress.nginx.http.host Hostname to be used with Ingress Rules. If not set, hostname would be configured according to fullname. Hosts would be configured as \u0026lt; fullname \u0026gt;-http.\u0026lt; domain \u0026gt;, \u0026lt; fullname \u0026gt;-http-0.\u0026lt; domain \u0026gt;, \u0026lt; fullname \u0026gt;-http-1.\u0026lt; domain \u0026gt;, etc.    ingress.nginx.http.domain Domain name to be used with Ingress Rules. In ingress rules, hosts would be configured as \u0026lt; host \u0026gt;.\u0026lt; domain \u0026gt;, \u0026lt; host \u0026gt;-0.\u0026lt; domain \u0026gt;, \u0026lt; host \u0026gt;-1.\u0026lt; domain \u0026gt;, etc.    ingress.nginx.http.backendPort  http   ingress.nginx.http.nginxAnnotations  { kubernetes.io/ingress.class: “nginx\u0026rdquo; }   ingress.nginx.admin.host Hostname to be used with Ingress Rules. If not set, hostname would be configured according to fullname. Hosts would be configured as \u0026lt; fullname \u0026gt;-admin.\u0026lt; domain \u0026gt;, \u0026lt; fullname \u0026gt;-admin-0.\u0026lt; domain \u0026gt;, \u0026lt; fullname \u0026gt;-admin-1.\u0026lt; domain \u0026gt;, etc.    ingress.nginx.admin.domain Domain name to be used with Ingress Rules. In ingress rules, hosts would be configured as \u0026lt; host \u0026gt;.\u0026lt; domain \u0026gt;, \u0026lt; host \u0026gt;-0.\u0026lt; domain \u0026gt;, \u0026lt; host \u0026gt;-1.\u0026lt; domain \u0026gt;, etc.    ingress.nginx.admin.nginxAnnotations  { kubernetes.io/ingress.class: “nginx” nginx.ingress.kubernetes.io/backend-protocol: “https\u0026rdquo;}   ingress.ingress.tlsSecret Secret name to use an already created TLS Secret. If such secret is not provided, one would be created with name \u0026lt; fullname \u0026gt;-tls-cert. If the TLS Secret is in different namespace, name can be mentioned as \u0026lt; namespace \u0026gt;/\u0026lt; tlsSecretName \u0026gt;    ingress.certCN Subject’s common name (cn) for SelfSigned Cert \u0026lt; fullname \u0026gt;   ingress.certValidityDays Validity of Self-Signed Cert in days 365   nodeSelector node labels for pod assignment    tolerations node taints to tolerate    affinity node/pod affinities    persistence.enabled If enabled, it will use the persistent volume. if value is false, PV and PVC would not be used and pods would be using the default emptyDir mount volume true   persistence.pvname pvname to use an already created Persistent Volume , If blank will use the default name oid-\u0026lt; fullname \u0026gt;-pv   persistence.pvcname pvcname to use an already created Persistent Volume Claim , If blank will use default name oid-\u0026lt; fullname \u0026gt;-pvc   persistence.type supported values: either filesystem or networkstorage or custom filesystem   persistence.filesystem.hostPath.path The path location mentioned should be created and accessible from the local host provided with necessary privileges for the user /scratch/shared/oid_user_projects   persistence.networkstorage.nfs.path Path of NFS Share location /scratch/shared/oid_user_projects   persistence.networkstorage.nfs.server IP or hostname of NFS Server 0.0.0.0   persistence.custom.* Based on values/data, YAML content would be included in PersistenceVolume Object    persistence.accessMode Specifies the access mode of the location provided ReadWriteMany   persistence.size Specifies the size of the storage 20Gi   persistence.storageClass Specifies the storageclass of the persistence volume. manual   persistence.annotations specifies any annotations that will be used { }   secret.enabled If enabled it will use the secret created with base64 encoding. if value is false, secret would not be used and input values (through –set, –values, etc.) would be used while creation of pods. true   secret.name secret name to use an already created Secret oid-\u0026lt; fullname \u0026gt;-creds   secret.type Specifies the type of the secret opaque   oidPorts.ldap Port on which Oracle Internet Directory Instance in the container should listen for LDAP Communication. 3060   oidPorts.ldaps Port on which Oracle Internet Directory Instance in the container should listen for LDAPS Communication.    oidConfig.realmDN BaseDN for OID Instances    oidConfig.domainName WebLogic Domain Name oid_domain   oidConfig.domainHome WebLogic Domain Home /u01/oracle/user_projects/domains/oid_domain   oidConfig.orcladminPassword Password for orcladmin user. Value will be added to Secret and Pod(s) will use the Secret    oidConfig.dbUser Value for login into db usually sys. Value would be added to Secret and Pod(s) would be using the Secret    oidConfig.dbPassword dbPassword is the SYS password for the database. Value would be added to Secret and Pod(s) would be using the Secret    oidConfig.dbschemaPassword Password for DB Schema(s) to be created by RCU. Value would be added to Secret and Pod(s) would be using the Secret    oidConfig.rcuSchemaPrefix The schema prefix to use in the database, for example OIDPD.    oidConfig.rcuDatabaseURL The database URL. Sample: \u0026lt;db_host.domain\u0026gt;:\u0026lt;db_port\u0026gt;/\u0026lt;service_name\u0026gt;    oidConfig.sleepBeforeConfig Based on the value for this parameter, initialization/configuration of each OID additional server (oid)n would be delayed and readiness probes would be configured. This is to make sure that OID additional servers (oid)n are initialized in sequence. 600   oidConfig.sslwalletPassword SSL enabled password to be used for ORAPKI    deploymentConfig.startupTime Based on the value for this parameter, initialization/configuration of each OID additional servers (oid)n will be delayed and readiness probes would be configured. initialDelaySeconds would be configured as sleepBeforeConfig + startupTime 480   deploymentConfig.livenessProbeInitialDelay Parameter to decide livenessProbe initialDelaySeconds 900   baseOID Configuration for Base OID instance (oid1)    baseOID.envVarsConfigMap Reference to ConfigMap which can contain additional environment variables to be passed on to POD for Base OID Instance    baseOID.envVars Environment variables in Yaml Map format. This is helpful when its requried to pass environment variables through \u0026ndash;values file. List of env variables which would not be honored from envVars map is same as list of env var names mentioned for envVarsConfigMap    additionalOID Configuration for additional OID instances (oidN)    additionalOID.envVarsConfigMap Reference to ConfigMap which can contain additional environment variables to be passed on to POD for additional OID Instance    additionalOID.envVars List of env variables which would not be honored from envVars map is same as list of env var names mentioned for envVarsConfigMap    odsm Parameters/Configurations for ODSM Deployment    odsm.adminUser Oracle WebLogic Server Administration User    odsm.adminPassword Password for Oracle WebLogic Server Administration User    odsm.startupTime Expected startup time. After specified seconds readinessProbe will start 900   odsmPorts Configuration for ODSM Ports    odsmPorts.http ODSM HTTP Port 7001   odsmPorts.https ODSM HTTPS Port 7002    "
},
{
	"uri": "/fmw-kubernetes/oud/create-oud-instances/",
	"title": "Create Oracle Unified Directory Instances",
	"tags": [],
	"description": "This document provides details of the oud-ds-rs Helm chart.",
	"content": " Introduction Create a Kubernetes namespace Create a Kubernetes secret for the container registry Create a Kubernetes secret for cronjob images The oud-ds-rs Helm chart Create OUD instances Helm command output Verify the OUD deployment Verify the OUD replication Verify the cronjob Undeploy an OUD deployment Appendix: Configuration parameters  Introduction This chapter demonstrates how to deploy Oracle Unified Directory (OUD) 12c instance(s) and replicated instances using the Helm package manager for Kubernetes.\nThe helm chart can be used to deploy an Oracle Unified Directory instance as a base, with configured sample entries, and multiple replicated Oracle Unified Directory instances/pods/services based on the specified replicaCount.\nBased on the configuration, this chart deploys the following objects in the specified namespace of a Kubernetes cluster.\n Service Account Secret Persistent Volume and Persistent Volume Claim Pod(s)/Container(s) for Oracle Unified Directory Instances Services for interfaces exposed through Oracle Unified Directory Instances Ingress configuration  Create a Kubernetes namespace Create a Kubernetes namespace for the OUD deployment by running the following command:\n$ kubectl create namespace \u0026lt;namespace\u0026gt; For example:\n$ kubectl create namespace oudns The output will look similar to the following:\nnamespace/oudns created Create a Kubernetes secret for the container registry Create a Kubernetes secret to stores the credentials for the container registry where the OUD image is stored. This step must be followed if using Oracle Container Registry or your own private container registry. If you are not using a container registry and have loaded the images on each of the master and worker nodes, you can skip this step.\n  Run the following command to create the secret:\nkubectl create secret docker-registry \u0026#34;orclcred\u0026#34; --docker-server=\u0026lt;CONTAINER_REGISTRY\u0026gt; \\ --docker-username=\u0026#34;\u0026lt;USER_NAME\u0026gt;\u0026#34; \\ --docker-password=\u0026lt;PASSWORD\u0026gt; --docker-email=\u0026lt;EMAIL_ID\u0026gt; \\ --namespace=\u0026lt;domain_namespace\u0026gt; For example, if using Oracle Container Registry:\n$ kubectl create secret docker-registry \u0026#34;orclcred\u0026#34; --docker-server=container-registry.oracle.com \\ --docker-username=\u0026#34;user@example.com\u0026#34; \\ --docker-password=password --docker-email=user@example.com \\ --namespace=oudns Replace \u0026lt;USER_NAME\u0026gt; and \u0026lt;PASSWORD\u0026gt; with the credentials for the registry with the following caveats:\n  If using Oracle Container Registry to pull the OUD container image, this is the username and password used to login to Oracle Container Registry. Before you can use this image you must login to Oracle Container Registry, navigate to Middleware \u0026gt; oud_cpu and accept the license agreement.\n  If using your own container registry to store the OUD container image, this is the username and password (or token) for your container registry.\n  The output will look similar to the following:\nsecret/orclcred created   Create a Kubernetes secret for cronjob images Once OUD is deployed, if the Kubernetes node where the OUD pod(s) is/are running goes down after the pod eviction time-out, the pod(s) don\u0026rsquo;t get evicted but move to a Terminating state. The pod(s) will then remain in that state forever. To avoid this problem a cron-job is created during OUD deployment that checks for any pods in Terminating state, deletes them, and then starts the pod again. This cron job requires access to images on hub.docker.com. A Kubernetes secret must therefore be created to enable access to these images.\n  Create a Kubernetes secret to access the required images on hub.docker.com:\nNote: You must first have a user account on hub.docker.com:\n$ kubectl create secret docker-registry \u0026#34;dockercred\u0026#34; --docker-server=\u0026#34;https://index.docker.io/v1/\u0026#34; --docker-username=\u0026#34;\u0026lt;docker_username\u0026gt;\u0026#34; --docker-password=\u0026lt;password\u0026gt; --docker-email=\u0026lt;docker_email_credentials\u0026gt; --namespace=\u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl create secret docker-registry \u0026quot;dockercred\u0026quot; --docker-server=\u0026quot;https://index.docker.io/v1/\u0026quot; --docker-username=\u0026quot;username\u0026quot; --docker-password=\u0026lt;password\u0026gt; --docker-email=user@example.com --namespace=oudns The output will look similar to the following:\nsecret/dockercred created   The oud-ds-rs Helm chart The oud-ds-rs Helm chart allows you to create or deploy a group of replicated Oracle Unified Directory instances along with Kubernetes objects in a specified namespace.\nThe deployment can be initiated by running the following Helm command with reference to the oud-ds-rs Helm chart, along with configuration parameters according to your environment.\n$ cd $WORKDIR/kubernetes/helm $ helm install --namespace \u0026lt;namespace\u0026gt; \\ \u0026lt;Configuration Parameters\u0026gt; \\ \u0026lt;deployment/release name\u0026gt; \\ \u0026lt;Helm Chart Path/Name\u0026gt; Configuration Parameters (override values in chart) can be passed on with --set arguments on the command line and/or with -f / --values arguments when referring to files.\nNote: The examples in Create OUD instances below provide values which allow the user to override the default values provided by the Helm chart. A full list of configuration parameters and their default values is shown in Appendix: Configuration parameters.\nFor more details about the helm command and parameters, please execute helm --help and helm install --help.\nCreate OUD instances You can create OUD instances using one of the following methods:\n Using a YAML file Using --set argument  Note: While it is possible to install sample data during the OID deployment is it not possible to load your own data via an ldif file . In order to load data in OUD, create the OUD deployment and then use ldapmodify post the ingress deployment. See Using LDAP utilities.\nUsing a YAML file   Navigate to the $WORKDIR/kubernetes/helm directory:\n$ cd $WORKDIR/kubernetes/helm   Create an oud-ds-rs-values-override.yaml as follows:\nimage: repository: \u0026lt;image_location\u0026gt; tag: \u0026lt;image_tag\u0026gt; pullPolicy: IfNotPresent imagePullSecrets: - name: orclcred oudConfig: rootUserPassword: \u0026lt;password\u0026gt; sampleData: \u0026#34;200\u0026#34; persistence: type: filesystem filesystem: hostPath: path: \u0026lt;persistent_volume\u0026gt;/oud_user_projects cronJob: kubectlImage: repository: bitnami/kubectl tag: \u0026lt;version\u0026gt; pullPolicy: IfNotPresent helmImage: repository: alpine/helm tag: \u0026lt;version\u0026gt; pullPolicy: IfNotPresent cronPersistence: enabled: true type: filesystem filesystem: hostPath: path: \u0026lt;$WORKDIR\u0026gt;/kubernetes/helm imagePullSecrets: - name: dockercred For example:\nimage: repository: container-registry.oracle.com/middleware/oud_cpu tag: 12.2.1.4-jdk8-ol7-220119.2051 pullPolicy: IfNotPresent imagePullSecrets: - name: orclcred oudConfig: rootUserPassword: \u0026lt;password\u0026gt; sampleData: \u0026#34;200\u0026#34; persistence: type: filesystem filesystem: hostPath: path: /scratch/shared/oud_user_projects cronJob: kubectlImage: repository: bitnami/kubectl tag: 1.21.0 pullPolicy: IfNotPresent helmImage: repository: alpine/helm tag: 3.2.0 pullPolicy: IfNotPresent cronPersistence: enabled: true type: filesystem filesystem: hostPath: path: /scratch/shared/OUDContainer/fmw-kubernetes/OracleUnifiedDirectory/kubernetes/helm imagePullSecrets: - name: dockercred The following caveats exist:\n  Replace \u0026lt;password\u0026gt; with the relevant password.\n  sampleData: \u0026quot;200\u0026quot; will load 200 sample users into the default baseDN dc=example,dc=com. If you do not want sample data, remove this entry.\n  The \u0026lt;version\u0026gt; in kubectlImage tag: should be set to the same version as your Kubernetes version (kubectl version). For example if your Kubernetes version is 1.21.6 set to 1.21.0.\n  The \u0026lt;version\u0026gt; in helmimage tag: should be set to the same version as your Helm version (helm version). For example if your helm version is 3.2.4 set to 3.2.0.\n  The cronPersistence path must point to the helm charts directory on the persistent volume.\n  If you are not using Oracle Container Registry or your own container registry for your OUD container image, then you can remove the following:\nimagePullSecrets: - name: orclcred   If using NFS for your persistent volume then change the persistence and `cronPersistence section as follows:\npersistence: type: networkstorage networkstorage: nfs: path: \u0026lt;persistent_volume\u0026gt;/oud_user_projects server: \u0026lt;NFS IP address\u0026gt; cronPersistence: enabled: true type: networkstorage networkstorage: nfs: path: \u0026lt;$WORKDIR\u0026gt;/kubernetes/helm server: \u0026lt;NFS_IP_Address\u0026gt;     Run the following command to deploy OUD:\n$ helm install --namespace \u0026lt;namespace\u0026gt; \\ --values oud-ds-rs-values-override.yaml \\ \u0026lt;release_name\u0026gt; oud-ds-rs For example:\n$ helm install --namespace oudns \\ --values oud-ds-rs-values-override.yaml \\ oud-ds-rs oud-ds-rs   Check the OUD deployment as per Verify the OUD deployment and Verify the OUD replication.\n  Using --set argument   Navigate to the $WORKDIR/kubernetes/helm directory:\n$ cd $WORKDIR/kubernetes/helm   Run the following command to create OUD instances:\n$ helm install --namespace \u0026lt;namespace\u0026gt; \\ --set oudConfig.rootUserPassword=\u0026lt;password\u0026gt;,persistence.filesystem.hostPath.path=\u0026lt;persistent_volume\u0026gt;/oud_user_projects,image.repository=\u0026lt;image_location\u0026gt;,image.tag=\u0026lt;image_tag\u0026gt; \\  --set imagePullSecrets[0].name=\u0026#34;orclcred\u0026#34; \\ --set sampleData=\u0026#34;200\u0026#34; \\ --set cronJob.kubectlImage.repository=bitnami/kubectl,cronJob.kubectlImage.tag=\u0026lt;version\u0026gt; \\ --set cronJob.helmImage.repository=alpine/helm,cronJob.helmImage.tag=\u0026lt;version\u0026gt; \\ --set cronJob.cronPersistence.filesystem.hostPath.path=\u0026lt;$WORKDIR\u0026gt;/kubernetes/helm \\ --set cronJob.imagePullSecrets[0].name=\u0026#34;dockercred\u0026#34; \\ \u0026lt;release_name\u0026gt; oud-ds-rs For example:\n$ helm install --namespace oudns \\ --set oudConfig.rootUserPassword=\u0026lt;password\u0026gt;,persistence.filesystem.hostPath.path=/scratch/shared/oud_user_projects,image.repository=container-registry.oracle.com/middleware/oud_cpu,image.tag=12.2.1.4-jdk8-ol7-220119.2051 \\ --set sampleData=\u0026#34;200\u0026#34; \\ --set cronJob.kubectlImage.repository=bitnami/kubectl,cronJob.kubectlImage.tag=1.21.0 \\ --set cronJob.helmImage.repository=alpine/helm,cronJob.helmImage.tag=3.2.0 \\ --set cronJob.cronPersistence.filesystem.hostPath.path=/scratch/shared/OUDContainer/fmw-kubernetes/OracleUnifiedDirectory/kubernetes/helm/ \\ --set cronJob.imagePullSecrets[0].name=\u0026#34;dockercred\u0026#34; \\ --set imagePullSecrets[0].name=\u0026#34;orclcred\u0026#34; \\ oud-ds-rs oud-ds-rs The following caveats exist:\n Replace \u0026lt;password\u0026gt; with a the relevant password. sampleData: \u0026quot;200\u0026quot; will load 200 sample users into the default baseDN dc=example,dc=com. If you do not want sample data, remove this entry. The \u0026lt;version\u0026gt; in kubectlImage tag: should be set to the same version as your Kubernetes version (kubectl version). For example if your Kubernetes version is 1.21.6 set to 1.21.0. The \u0026lt;version\u0026gt; in helmimage tag: should be set to the same version as your Helm version (helm version). For example if your helm version is 3.2.4 set to 3.2.0. The cronPersistence path must point to the helm charts directory on the persistent volume. If using using NFS for your persistent volume then use persistence.networkstorage.nfs.path=\u0026lt;persistent_volume\u0026gt;/oud_user_projects,persistence.networkstorage.nfs.server:\u0026lt;NFS IP address\u0026gt;. If you are not using Oracle Container Registry or your own container registry for your OUD container image, then you can remove the following: --set imagePullSecrets[0].name=\u0026quot;orclcred\u0026quot;.    Check the OUD deployment as per Verify the OUD deployment and Verify the OUD replication.\n  Helm command output In all the examples above, the following output is shown following a successful execution of the helm install command.\nNAME: oud-ds-rs LAST DEPLOYED: Wed Mar 16 12:02:40 2022 NAMESPACE: oudns STATUS: deployed REVISION: 4 NOTES: # # Copyright (c) 2020, Oracle and/or its affiliates. # # Licensed under the Universal Permissive License v 1.0 as shown at # https://oss.oracle.com/licenses/upl # # Since \u0026#34;nginx\u0026#34; has been chosen, follow the steps below to configure nginx ingress controller. Add Repo reference to helm for retriving/installing Chart for nginx-ingress implementation. command-# helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx Command helm install to install nginx-ingress related objects like pod, service, deployment, etc. # helm install --namespace \u0026lt;namespace for ingress\u0026gt; --values nginx-ingress-values-override.yaml lbr-nginx ingress-nginx/ingress-nginx For details of content of nginx-ingress-values-override.yaml refer README.md file of this chart. Run these commands to check port mapping and services: # kubectl --namespace \u0026lt;namespace for ingress\u0026gt; get services -o wide -w lbr-nginx-ingress-controller # kubectl describe --namespace \u0026lt;namespace for oud-ds-rs chart\u0026gt; ingress.extensions/oud-ds-rs-http-ingress-nginx # kubectl describe --namespace \u0026lt;namespace for oud-ds-rs chart\u0026gt; ingress.extensions/oud-ds-rs-admin-ingress-nginx Accessible interfaces through ingress: (External IP Address for LoadBalancer NGINX Controller can be determined through details associated with lbr-nginx-ingress-controller) 1. OUD Admin REST: Port: http/https 2. OUD Data REST: Port: http/https 3. OUD Data SCIM: Port: http/https 4. OUD LDAP/LDAPS: Port: ldap/ldaps 5. OUD Admin LDAPS: Port: ldaps Please refer to README.md from Helm Chart to find more details about accessing interfaces and configuration parameters. Verify the OUD deployment Run the following command to verify the OUD deployment:\n$ kubectl --namespace \u0026lt;namespace\u0026gt; get pod,service,secret,pv,pvc,ingress -o wide For example:\n$ kubectl --namespace oudns get pod,service,secret,pv,pvc,ingress -o wide The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/oud-ds-rs-0 1/1 Running 0 17m 10.244.0.195 \u0026lt;Worker Node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/oud-ds-rs-1 1/1 Running 0 17m 10.244.0.194 \u0026lt;Worker Node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/oud-ds-rs-2 1/1 Running 0 17m 10.244.0.193 \u0026lt;Worker Node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service/oud-ds-rs-0 ClusterIP 10.99.232.83 \u0026lt;none\u0026gt; 1444/TCP,1888/TCP,1898/TCP 8m44s kubernetes.io/instance=oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=oud-ds-rs-0 service/oud-ds-rs-1 ClusterIP 10.100.186.42 \u0026lt;none\u0026gt; 1444/TCP,1888/TCP,1898/TCP 8m45s app.kubernetes.io/instance=oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=oud-ds-rs-1 service/oud-ds-rs-2 ClusterIP 10.104.55.53 \u0026lt;none\u0026gt; 1444/TCP,1888/TCP,1898/TCP 8m45s app.kubernetes.io/instance=oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=oud-ds-rs-2 service/oud-ds-rs-http-0 ClusterIP 10.102.116.145 \u0026lt;none\u0026gt; 1080/TCP,1081/TCP 8m45s app.kubernetes.io/instance=oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=oud-ds-rs-0 service/oud-ds-rs-http-1 ClusterIP 10.111.103.84 \u0026lt;none\u0026gt; 1080/TCP,1081/TCP 8m44s app.kubernetes.io/instance=oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=oud-ds-rs-1 service/oud-ds-rs-http-2 ClusterIP 10.105.53.24 \u0026lt;none\u0026gt; 1080/TCP,1081/TCP 8m45s app.kubernetes.io/instance=oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=oud-ds-rs-2 service/oud-ds-rs-lbr-admin ClusterIP 10.98.39.206 \u0026lt;none\u0026gt; 1888/TCP,1444/TCP 8m45s app.kubernetes.io/instance=oud-ds-rs,app.kubernetes.io/name=oud-ds-rs service/oud-ds-rs-lbr-http ClusterIP 10.110.77.132 \u0026lt;none\u0026gt; 1080/TCP,1081/TCP 8m45s app.kubernetes.io/instance=oud-ds-rs,app.kubernetes.io/name=oud-ds-rs service/oud-ds-rs-lbr-ldap ClusterIP 10.111.55.122 \u0026lt;none\u0026gt; 1389/TCP,1636/TCP 8m45s app.kubernetes.io/instance=oud-ds-rs,app.kubernetes.io/name=oud-ds-rs service/oud-ds-rs-ldap-0 ClusterIP 10.108.155.81 \u0026lt;none\u0026gt; 1389/TCP,1636/TCP 8m44s app.kubernetes.io/instance=oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=oud-ds-rs-0 service/oud-ds-rs-ldap-1 ClusterIP 10.104.88.44 \u0026lt;none\u0026gt; 1389/TCP,1636/TCP 8m45s app.kubernetes.io/instance=oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=oud-ds-rs-1 service/oud-ds-rs-ldap-2 ClusterIP 10.105.253.120 \u0026lt;none\u0026gt; 1389/TCP,1636/TCP 8m45s app.kubernetes.io/instance=oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=oud-ds-rs-2 NAME TYPE DATA AGE secret/default-token-tbjr5 kubernetes.io/service-account-token 3 25d secret/orclcred kubernetes.io/dockerconfigjson 1 3d secret/oud-ds-rs-creds opaque 8 8m48s secret/oud-ds-rs-token-cct26 kubernetes.io/service-account-token 3 8m50s secret/sh.helm.release.v1.oud-ds-rs.v1 helm.sh/release.v1 1 8m51s NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE persistentvolume/oud-ds-rs-pv 20Gi RWX Retain Bound oudns/oud-ds-rs-pvc manual 8m47s NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistentvolumeclaim/oud-ds-rs-pvc Bound oud-ds-rs-pv 20Gi RWX manual 8m48s NAME HOSTS ADDRESS PORTS AGE ingress.extensions/oud-ds-rs-admin-ingress-nginx oud-ds-rs-admin-0,oud-ds-rs-admin-1,oud-ds-rs-admin-2 + 2 more... 10.229.141.78 80 8m45s ingress.extensions/oud-ds-rs-http-ingress-nginx oud-ds-rs-http-0,oud-ds-rs-http-1,oud-ds-rs-http-2 + 3 more... 10.229.141.78 80 8m45s Note: It will take several minutes before all the services listed above show. While the oud-ds-rs pods have a STATUS of 0/1 the pod is started but the OUD server associated with it is currently starting. While the pod is starting you can check the startup status in the pod logs, by running the following command:\n$ kubectl logs oud-ds-rs-0 -n oudns $ kubectl logs oud-ds-rs-1 -n oudns $ kubectl logs oud-ds-rs-2 -n oudns Note : If the OUD deployment fails additionally refer to Troubleshooting for instructions on how describe the failing pod(s). Once the problem is identified follow Undeploy an OUD deployment to clean down the deployment before deploying again.\nKubernetes Objects Kubernetes objects created by the Helm chart are detailed in the table below:\n   Type Name Example Name Purpose     Service Account \u0026lt;deployment/release name\u0026gt; oud-ds-rs Kubernetes Service Account for the Helm Chart deployment   Secret \u0026lt;deployment/release name\u0026gt;-creds oud-ds-rs-creds Secret object for Oracle Unified Directory related critical values like passwords   Persistent Volume \u0026lt;deployment/release name\u0026gt;-pv oud-ds-rs-pv Persistent Volume for user_projects mount.   Persistent Volume Claim \u0026lt;deployment/release name\u0026gt;-pvc oud-ds-rs-pvc Persistent Volume Claim for user_projects mount.   Persistent Volume \u0026lt;deployment/release name\u0026gt;-pv-config oud-ds-rs-pv-config Persistent Volume for mounting volume in containers for configuration files like ldif, schema, jks, java.security, etc.   Persistent Volume Claim \u0026lt;deployment/release name\u0026gt;-pvc-config oud-ds-rs-pvc-config Persistent Volume Claim for mounting volume in containers for configuration files like ldif, schema, jks, java.security, etc.   Pod \u0026lt;deployment/release name\u0026gt;-0 oud-ds-rs-0 Pod/Container for base Oracle Unified Directory Instance which would be populated first with base configuration (like number of sample entries)   Pod \u0026lt;deployment/release name\u0026gt;-N oud-ds-rs-1, oud-ds-rs-2, \u0026hellip; Pod(s)/Container(s) for Oracle Unified Directory Instances - each would have replication enabled against base Oracle Unified Directory instance \u0026lt;deployment/release name\u0026gt;-0   Service \u0026lt;deployment/release name\u0026gt;-0 oud-ds-rs-0 Service for LDAPS Admin, REST Admin and Replication interfaces from base Oracle Unified Directory instance \u0026lt;deployment/release name\u0026gt;-0   Service \u0026lt;deployment/release name\u0026gt;-http-0 oud-ds-rs-http-0 Service for HTTP and HTTPS interfaces from base Oracle Unified Directory instance \u0026lt;deployment/release name\u0026gt;-0   Service \u0026lt;deployment/release name\u0026gt;-ldap-0 oud-ds-rs-ldap-0 Service for LDAP and LDAPS interfaces from base Oracle Unified Directory instance \u0026lt;deployment/release name\u0026gt;-0   Service \u0026lt;deployment/release name\u0026gt;-N oud-ds-rs-1, oud-ds-rs-2, \u0026hellip; Service(s) for LDAPS Admin, REST Admin and Replication interfaces from base Oracle Unified Directory instance \u0026lt;deployment/release name\u0026gt;-N   Service \u0026lt;deployment/release name\u0026gt;-http-N oud-ds-rs-http-1, oud-ds-rs-http-2, \u0026hellip; Service(s) for HTTP and HTTPS interfaces from base Oracle Unified Directory instance \u0026lt;deployment/release name\u0026gt;-N   Service \u0026lt;deployment/release name\u0026gt;-ldap-N oud-ds-rs-ldap-1, oud-ds-rs-ldap-2, \u0026hellip; Service(s) for LDAP and LDAPS interfaces from base Oracle Unified Directory instance \u0026lt;deployment/release name\u0026gt;-N   Service \u0026lt;deployment/release name\u0026gt;-lbr-admin oud-ds-rs-lbr-admin Service for LDAPS Admin, REST Admin and Replication interfaces from all Oracle Unified Directory instances   Service \u0026lt;deployment/release name\u0026gt;-lbr-http oud-ds-rs-lbr-http Service for HTTP and HTTPS interfaces from all Oracle Unified Directory instances   Service \u0026lt;deployment/release name\u0026gt;-lbr-ldap oud-ds-rs-lbr-ldap Service for LDAP and LDAPS interfaces from all Oracle Unified Directory instances   Ingress \u0026lt;deployment/release name\u0026gt;-admin-ingress-nginx oud-ds-rs-admin-ingress-nginx Ingress Rules for HTTP Admin interfaces.   Ingress \u0026lt;deployment/release name\u0026gt;-http-ingress-nginx oud-ds-rs-http-ingress-nginx Ingress Rules for HTTP (Data/REST) interfaces.     In the table above the \u0026lsquo;Example Name\u0026rsquo; for each Object is based on the value \u0026lsquo;oud-ds-rs\u0026rsquo; as deployment/release name for the Helm chart installation.  Verify the OUD replication Once all the PODs created are visible as READY (i.e. 1/1), you can verify your replication across multiple Oracle Unified Directory instances.\n  To verify the replication group, connect to the container and issue an OUD administration command to show the details. The name of the container can be found by issuing the following:\n$ kubectl get pods -n \u0026lt;namespace\u0026gt; -o jsonpath=\u0026#39;{.items[*].spec.containers[*].name}\u0026#39; For example:\n$ kubectl get pods -n oudns -o jsonpath=\u0026#39;{.items[*].spec.containers[*].name}\u0026#39; The output will look similar to the following:\noud-ds-rs oud-ds-rs oud-ds-rs Once you have the container name you can verify the replication status in the following ways:\n Run dresplication inside the pod Using kubectl commands    Run dresplication inside the pod   Run the following command to create a bash shell in the pod:\n$ kubectl --namespace \u0026lt;namespace\u0026gt; exec -it -c \u0026lt;containername\u0026gt; \u0026lt;podname\u0026gt; -- bash For example:\n$ kubectl --namespace oudns exec -it -c oud-ds-rs oud-ds-rs-0 -- bash This will take you into the pod:\n[oracle@oud-ds-rs-0 oracle]$   From the prompt, use the dsreplication command to check the status of your replication group:\n$ cd /u01/oracle/user_projects/oud-ds-rs-0/OUD/bin $ ./dsreplication status --trustAll \\ --hostname oud-ds-rs-0 --port 1444 --adminUID admin \\ --dataToDisplay compat-view --dataToDisplay rs-connections The output will look similar to the following. Enter credentials where prompted:\n\u0026gt;\u0026gt;\u0026gt;\u0026gt; Specify Oracle Unified Directory LDAP connection parameters Password for user \u0026#39;admin\u0026#39;: Establishing connections and reading configuration ..... Done. dc=example,dc=com - Replication Enabled ======================================= Server : Entries : M.C. [1] : A.O.M.C. [2] : Port [3] : Encryption [4] : Trust [5] : U.C. [6] : Status [7] : ChangeLog [8] : Group ID [9] : Connected To [10] ---------------------:---------:----------:--------------:----------:----------------:-----------:----------:------------:---------------:--------------:------------------------------- oud-ds-rs-0:1444 : 1 : 0 : 0 : 1898 : Disabled : Trusted : -- : Normal : Enabled : 1 : oud-ds-rs-0:1898 : : : : : : : : : : : (GID=1) oud-ds-rs-1:1444 : 1 : 0 : 0 : 1898 : Disabled : Trusted : -- : Normal : Enabled : 1 : oud-ds-rs-1:1898 : : : : : : : : : : : (GID=1) oud-ds-rs-2:1444 : 1 : 0 : 0 : 1898 : Disabled : Trusted : -- : Normal : Enabled : 1 : oud-ds-rs-2:1898 : : : : : : : : : : : (GID=1) Replication Server [11] : RS #1 : RS #2 : RS #3 -------------------------------:-------:-------:------ oud-ds-rs-0:1898 : -- : Yes : Yes (#1) : : : oud-ds-rs-1:1898 : Yes : -- : Yes (#2) : : : oud-ds-rs-2:1898 : Yes : Yes : -- (#3) : : : [1] The number of changes that are still missing on this element (and that have been applied to at least one other server). [2] Age of oldest missing change: the age (in seconds) of the oldest change that has not yet arrived on this element. [3] The replication port used to communicate between the servers whose contents are being replicated. [4] Whether the replication communication initiated by this element is encrypted or not. [5] Whether the directory server is trusted or not. Updates coming from an untrusted server are discarded and not propagated. [6] The number of untrusted changes. These are changes generated on this server while it is untrusted. Those changes are not propagated to the rest of the topology but are effective on the untrusted server. [7] The status of the replication on this element. [8] Whether the external change log is enabled for the base DN on this server or not. [9] The ID of the replication group to which the server belongs. [10] The replication server this server is connected to with its group ID between brackets. [11] This table represents the connections between the replication servers. The headers of the columns use a number as identifier for each replication server. See the values of the first column to identify the corresponding replication server for each number.   Type exit to exit the pod.\n  Using kubectl commands   The dsreplication status command can be invoked using the following kubectl command:\n$ kubectl --namespace \u0026lt;namespace\u0026gt; exec -it -c \u0026lt;containername\u0026gt; \u0026lt;podname\u0026gt; -- \\ /u01/oracle/user_projects/\u0026lt;OUD Instance/Pod Name\u0026gt;/OUD/bin/dsreplication status \\ --trustAll --hostname \u0026lt;OUD Instance/Pod Name\u0026gt; --port 1444 --adminUID admin \\ --dataToDisplay compat-view --dataToDisplay rs-connections For example:\n$ kubectl --namespace oudns exec -it -c oud-ds-rs oud-ds-rs-0 -- \\ /u01/oracle/user_projects/oud-ds-rs-0/OUD/bin/dsreplication status \\ --trustAll --hostname oud-ds-rs-0 --port 1444 --adminUID admin \\ --dataToDisplay compat-view --dataToDisplay rs-connections The output will be the same as per Run dresplication inside the pod.\n  Verify the cronjob   Run the following command to make sure the cronjob is created:\n$ kubectl get cronjob -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl get cronjob -n oudns The output will look similar to the following:\nNAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE oud-pod-cron-job */30 * * * * False 0 \u0026lt;none\u0026gt; 15s   Run the following command to make sure the job(s) is created:\n$ kubectl get job -n \u0026lt;namespace\u0026gt; -o wide For example:\n$ kubectl get job -n oudns -o wide The output will look similar to the following:\nNAME COMPLETIONS DURATION AGE CONTAINERS IMAGES SELECTOR oud-pod-cron-job-27467340 1/1 17s 6m48s cron-kubectl,cron-helm bitnami/kubectl:1.21.0,alpine/helm:3.2.0 controller-uid=e8e7dfe2-d197-4b84-a5a4-d203d54caaac Note: The jobs(s) will only be displayed after the time schedule originally set has elapsed. The default is 30 minutes).\n  Disabling the cronjob If you need to disable the job, for example if maintenance needs to be performed on the node, you can disable the job as follows:\n  Run the following command to edit the cronjob:\n$ kubectl edit cronjob pod-cron-job -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl edit cronjob oud-pod-cron-job -n oudns Note: This opens an edit session for the cronjob where parameters can be changed using standard vi commands.\n  In the edit session search for suspend and change the vaule from false to true:\n... - name: oud-ds-rs-job-pv persistentVolumeClaim: claimName: oud-ds-rs-job-pvc schedule: '*/30 * * * *' successfulJobsHistoryLimit: 3 suspend: true ...   Save the file and exit (wq!).\n  Run the following to make sure the cronjob is suspended:\n$ kubectl get cronjob -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl get cronjob -n oudns The output will look similar to the following:\nNAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE oud-pod-cron-job */30 * * * * True 0 11m 33m   To enable the cronjob again, repeat the above steps and set suspend to false.\n  Ingress Configuration With an OUD instance now deployed you are now ready to configure an ingress controller to direct traffic to OUD as per Configure an ingress for an OUD.\nUndeploy an OUD deployment Delete the OUD deployment   Find the deployment release name:\n$ helm --namespace \u0026lt;namespace\u0026gt; list For example:\n$ helm --namespace oudns list The output will look similar to the following:\nNAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION oud-ds-rs oudns 1 2021-03-16 12:02:40.616927678 -0700 PDT deployed oud-ds-rs-12.2.1.4.0 12.2.1.4.0   Delete the deployment using the following command:\n$ helm uninstall --namespace \u0026lt;namespace\u0026gt; \u0026lt;release\u0026gt; For example:\n$ helm uninstall --namespace oudns oud-ds-rs release \u0026#34;oud-ds-rs\u0026#34; uninstalled   Delete the persistent volume contents   Delete the contents of the oud_user_projects directory in the persistent volume:\n$ cd \u0026lt;persistent_volume\u0026gt;/oud_user_projects $ rm -rf * For example:\n$ cd /scratch/shared/oud_user_projects $ rm -rf *   Appendix: Configuration Parameters The following table lists the configurable parameters of the oud-ds-rs chart and their default values.\n   Parameter Description Default Value     replicaCount Number of DS+RS instances/pods/services to be created with replication enabled against a base Oracle Unified Directory instance/pod. 3   restartPolicyName restartPolicy to be configured for each POD containing Oracle Unified Directory instance OnFailure   image.repository Oracle Unified Directory Image Registry/Repository and name. Based on this, image parameter would be configured for Oracle Unified Directory pods/containers oracle/oud   image.tag Oracle Unified Directory Image Tag. Based on this, image parameter would be configured for Oracle Unified Directory pods/containers 12.2.1.4.0   image.pullPolicy policy to pull the image IfnotPresent   imagePullSecrets.name name of Secret resource containing private registry credentials regcred   nameOverride override the fullname with this name    fullnameOverride Overrides the fullname with the provided string    serviceAccount.create Specifies whether a service account should be created true   serviceAccount.name If not set and create is true, a name is generated using the fullname template oud-ds-rs-\u0026lt; fullname \u0026gt;-token-\u0026lt; randomalphanum \u0026gt;   podSecurityContext Security context policies to add to the controller pod    securityContext Security context policies to add by default    service.type type of controller service to create ClusterIP   nodeSelector node labels for pod assignment    tolerations node taints to tolerate    affinity node/pod affinities    ingress.enabled  true   ingress.type Supported value: nginx nginx   ingress.nginx.http.host Hostname to be used with Ingress Rules. If not set, hostname would be configured according to fullname. Hosts would be configured as \u0026lt; fullname \u0026gt;-http.\u0026lt; domain \u0026gt;, \u0026lt; fullname \u0026gt;-http-0.\u0026lt; domain \u0026gt;, \u0026lt; fullname \u0026gt;-http-1.\u0026lt; domain \u0026gt;, etc.    ingress.nginx.http.domain Domain name to be used with Ingress Rules. In ingress rules, hosts would be configured as \u0026lt; host \u0026gt;.\u0026lt; domain \u0026gt;, \u0026lt; host \u0026gt;-0.\u0026lt; domain \u0026gt;, \u0026lt; host \u0026gt;-1.\u0026lt; domain \u0026gt;, etc.    ingress.nginx.http.backendPort  http   ingress.nginx.http.nginxAnnotations  { kubernetes.io/ingress.class: \u0026ldquo;nginx\u0026quot;}   ingress.nginx.admin.host Hostname to be used with Ingress Rules. If not set, hostname would be configured according to fullname. Hosts would be configured as \u0026lt; fullname \u0026gt;-admin.\u0026lt; domain \u0026gt;, \u0026lt; fullname \u0026gt;-admin-0.\u0026lt; domain \u0026gt;, \u0026lt; fullname \u0026gt;-admin-1.\u0026lt; domain \u0026gt;, etc.    ingress.nginx.admin.domain Domain name to be used with Ingress Rules. In ingress rules, hosts would be configured as \u0026lt; host \u0026gt;.\u0026lt; domain \u0026gt;, \u0026lt; host \u0026gt;-0.\u0026lt; domain \u0026gt;, \u0026lt; host \u0026gt;-1.\u0026lt; domain \u0026gt;, etc.    ingress.nginx.admin.nginxAnnotations  { kubernetes.io/ingress.class: \u0026ldquo;nginx\u0026rdquo; nginx.ingress.kubernetes.io/backend-protocol: \u0026ldquo;https\u0026quot;}   ingress.ingress.tlsSecret Secret name to use an already created TLS Secret. If such secret is not provided, one would be created with name \u0026lt; fullname \u0026gt;-tls-cert. If the TLS Secret is in different namespace, name can be mentioned as \u0026lt; namespace \u0026gt;/\u0026lt; tlsSecretName \u0026gt;    ingress.certCN Subject\u0026rsquo;s common name (cn) for SelfSigned Cert. \u0026lt; fullname \u0026gt;   ingress.certValidityDays Validity of Self-Signed Cert in days 365   secret.enabled If enabled it will use the secret created with base64 encoding. if value is false, secret would not be used and input values (through \u0026ndash;set, \u0026ndash;values, etc.) would be used while creation of pods. true   secret.name secret name to use an already created Secret oud-ds-rs-\u0026lt; fullname \u0026gt;-creds   secret.type Specifies the type of the secret Opaque   persistence.enabled If enabled, it will use the persistent volume. if value is false, PV and PVC would not be used and pods would be using the default emptyDir mount volume. true   persistence.pvname pvname to use an already created Persistent Volume , If blank will use the default name oud-ds-rs-\u0026lt; fullname \u0026gt;-pv   persistence.pvcname pvcname to use an already created Persistent Volume Claim , If blank will use default name oud-ds-rs-\u0026lt; fullname \u0026gt;-pvc   persistence.type supported values: either filesystem or networkstorage or custom filesystem   persistence.filesystem.hostPath.path The path location mentioned should be created and accessible from the local host provided with necessary privileges for the user. /scratch/shared/oud_user_projects   persistence.networkstorage.nfs.path Path of NFS Share location /scratch/shared/oud_user_projects   persistence.networkstorage.nfs.server IP or hostname of NFS Server 0.0.0.0   persistence.custom.* Based on values/data, YAML content would be included in PersistenceVolume Object    persistence.accessMode Specifies the access mode of the location provided ReadWriteMany   persistence.size Specifies the size of the storage 10Gi   persistence.storageClass Specifies the storageclass of the persistence volume. empty   persistence.annotations specifies any annotations that will be used { }   configVolume.enabled If enabled, it will use the persistent volume. If value is false, PV and PVC would not be used and pods would be using the default emptyDir mount volume. true   configVolume.mountPath If enabled, it will use the persistent volume. If value is false, PV and PVC would not be used and there would not be any mount point available for config false   configVolume.pvname pvname to use an already created Persistent Volume , If blank will use the default name oud-ds-rs-\u0026lt; fullname \u0026gt;-pv-config   configVolume.pvcname pvcname to use an already created Persistent Volume Claim , If blank will use default name oud-ds-rs-\u0026lt; fullname \u0026gt;-pvc-config   configVolume.type supported values: either filesystem or networkstorage or custom filesystem   configVolume.filesystem.hostPath.path The path location mentioned should be created and accessible from the local host provided with necessary privileges for the user. /scratch/shared/oud_user_projects   configVolume.networkstorage.nfs.path Path of NFS Share location /scratch/shared/oud_config   configVolume.networkstorage.nfs.server IP or hostname of NFS Server 0.0.0.0   configVolume.custom.* Based on values/data, YAML content would be included in PersistenceVolume Object    configVolume.accessMode Specifies the access mode of the location provided ReadWriteMany   configVolume.size Specifies the size of the storage 10Gi   configVolume.storageClass Specifies the storageclass of the persistence volume. empty   configVolume.annotations specifies any annotations that will be used { }   oudPorts.adminldaps Port on which Oracle Unified Directory Instance in the container should listen for Administration Communication over LDAPS Protocol 1444   oudPorts.adminhttps Port on which Oracle Unified Directory Instance in the container should listen for Administration Communication over HTTPS Protocol. 1888   oudPorts.ldap Port on which Oracle Unified Directory Instance in the container should listen for LDAP Communication. 1389   oudPorts.ldaps Port on which Oracle Unified Directory Instance in the container should listen for LDAPS Communication. 1636   oudPorts.http Port on which Oracle Unified Directory Instance in the container should listen for HTTP Communication. 1080   oudPorts.https Port on which Oracle Unified Directory Instance in the container should listen for HTTPS Communication. 1081   oudPorts.replication Port value to be used while setting up replication server. 1898   oudConfig.baseDN BaseDN for Oracle Unified Directory Instances dc=example,dc=com   oudConfig.rootUserDN Root User DN for Oracle Unified Directory Instances cn=Directory Manager   oudConfig.rootUserPassword Password for Root User DN RandomAlphanum   oudConfig.sampleData To specify that the database should be populated with the specified number of sample entries. 0   oudConfig.sleepBeforeConfig Based on the value for this parameter, initialization/configuration of each Oracle Unified Directory replica would be delayed. 120   oudConfig.adminUID AdminUID to be configured with each replicated Oracle Unified Directory instance admin   oudConfig.adminPassword Password for AdminUID. If the value is not passed, value of rootUserPassword would be used as password for AdminUID. rootUserPassword   baseOUD.envVarsConfigMap Reference to ConfigMap which can contain additional environment variables to be passed on to POD for Base Oracle Unified Directory Instance. Following are the environment variables which would not be honored from the ConfigMap. instanceType, sleepBeforeConfig, OUD_INSTANCE_NAME, hostname, baseDN, rootUserDN, rootUserPassword, adminConnectorPort, httpAdminConnectorPort, ldapPort, ldapsPort, httpPort, httpsPort, replicationPort, sampleData. -   baseOUD.envVars Environment variables in Yaml Map format. This is helpful when its requried to pass environment variables through \u0026ndash;values file. List of env variables which would not be honored from envVars map is same as list of env var names mentioned for envVarsConfigMap. -   replOUD.envVarsConfigMap Reference to ConfigMap which can contain additional environment variables to be passed on to PODs for Replicated Oracle Unified Directory Instances. Following are the environment variables which would not be honored from the ConfigMap. instanceType, sleepBeforeConfig, OUD_INSTANCE_NAME, hostname, baseDN, rootUserDN, rootUserPassword, adminConnectorPort, httpAdminConnectorPort, ldapPort, ldapsPort, httpPort, httpsPort, replicationPort, sampleData, sourceHost, sourceServerPorts, sourceAdminConnectorPort, sourceReplicationPort, dsreplication_1, dsreplication_2, dsreplication_3, dsreplication_4, post_dsreplication_dsconfig_1, post_dsreplication_dsconfig_2 -   replOUD.envVars Environment variables in Yaml Map format. This is helpful when its required to pass environment variables through \u0026ndash;values file. List of env variables which would not be honored from envVars map is same as list of env var names mentioned for envVarsConfigMap. -   replOUD.groupId Group ID to be used/configured with each Oracle Unified Directory instance in replicated topology. 1   elk.elasticsearch.enabled If enabled it will create the elastic search statefulset deployment false   elk.elasticsearch.image.repository Elastic Search Image name/Registry/Repository . Based on this elastic search instances will be created docker.elastic.co/elasticsearch/elasticsearch   elk.elasticsearch.image.tag Elastic Search Image tag .Based on this, image parameter would be configured for Elastic Search pods/instances 6.4.3   elk.elasticsearch.image.pullPolicy policy to pull the image IfnotPresent   elk.elasticsearch.esreplicas Number of Elastic search Instances will be created 3   elk.elasticsearch.minimumMasterNodes The value for discovery.zen.minimum_master_nodes. Should be set to (esreplicas / 2) + 1. 2   elk.elasticsearch.esJAVAOpts Java options for Elasticsearch. This is where you should configure the jvm heap size -Xms512m -Xmx512m   elk.elasticsearch.sysctlVmMaxMapCount Sets the sysctl vm.max_map_count needed for Elasticsearch 262144   elk.elasticsearch.resources.requests.cpu cpu resources requested for the elastic search 100m   elk.elasticsearch.resources.limits.cpu total cpu limits that are configures for the elastic search 1000m   elk.elasticsearch.esService.type Type of Service to be created for elastic search ClusterIP   elk.elasticsearch.esService.lbrtype Type of load balancer Service to be created for elastic search ClusterIP   elk.kibana.enabled If enabled it will create a kibana deployment false   elk.kibana.image.repository Kibana Image Registry/Repository and name. Based on this Kibana instance will be created docker.elastic.co/kibana/kibana   elk.kibana.image.tag Kibana Image tag. Based on this, Image parameter would be configured. 6.4.3   elk.kibana.image.pullPolicy policy to pull the image IfnotPresent   elk.kibana.kibanaReplicas Number of Kibana instances will be created 1   elk.kibana.service.tye Type of service to be created NodePort   elk.kibana.service.targetPort Port on which the kibana will be accessed 5601   elk.kibana.service.nodePort nodePort is the port on which kibana service will be accessed from outside 31119   elk.logstash.enabled If enabled it will create a logstash deployment false   elk.logstash.image.repository logstash Image Registry/Repository and name. Based on this logstash instance will be created logstash   elk.logstash.image.tag logstash Image tag. Based on this, Image parameter would be configured. 6.6.0   elk.logstash.image.pullPolicy policy to pull the image IfnotPresent   elk.logstash.containerPort Port on which the logstash container will be running 5044   elk.logstash.service.tye Type of service to be created NodePort   elk.logstash.service.targetPort Port on which the logstash will be accessed 9600   elk.logstash.service.nodePort nodePort is the port on which logstash service will be accessed from outside 32222   elk.logstash.logstashConfigMap Provide the configmap name which is already created with the logstash conf. if empty default logstash configmap will be created and used    elk.elkPorts.rest Port for REST 9200   elk.elkPorts.internode port used for communication between the nodes 9300   elk.busybox.image busy box image name. Used for initcontianers busybox   elk.elkVolume.enabled If enabled, it will use the persistent volume. if value is false, PV and pods would be using the default emptyDir mount volume. true   elk.elkVolume.pvname pvname to use an already created Persistent Volume , If blank will use the default name oud-ds-rs-\u0026lt; fullname \u0026gt;-espv   elk.elkVolume.type supported values: either filesystem or networkstorage or custom filesystem   elk.elkVolume.filesystem.hostPath.path The path location mentioned should be created and accessible from the local host provided with necessary privileges for the user. /scratch/shared/oud_elk/data   elk.elkVolume.networkstorage.nfs.path Path of NFS Share location /scratch/shared/oud_elk/data   elk.elkVolume.networkstorage.nfs.server IP or hostname of NFS Server 0.0.0.0   elk.elkVolume.custom.* Based on values/data, YAML content would be included in PersistenceVolume Object    elk.elkVolume.accessMode Specifies the access mode of the location provided ReadWriteMany   elk.elkVolume.size Specifies the size of the storage 20Gi   elk.elkVolume.storageClass Specifies the storageclass of the persistence volume. elk   elk.elkVolume.annotations specifies any annotations that will be used { }    "
},
{
	"uri": "/fmw-kubernetes/oudsm/create-oudsm-instances/",
	"title": "Create Oracle Unified Directory Services Manager Instances",
	"tags": [],
	"description": "This document provides details of the oudsm Helm chart.",
	"content": " Introduction Create a Kubernetes namespace Create a Kubernetes secret for the container registry Create a persistent volume directory The oudsm Helm chart Create OUDSM instances Helm command output Verify the OUDSM deployment Undeploy an OUDSM deployment Appendix: Configuration parameters  Introduction This chapter demonstrates how to deploy Oracle Unified Directory Services Manager (OUDSM) 12c instance(s) using the Helm package manager for Kubernetes.\nBased on the configuration, this chart deploys the following objects in the specified namespace of a Kubernetes cluster.\n Service Account Secret Persistent Volume and Persistent Volume Claim Pod(s)/Container(s) for Oracle Unified Directory Services Manager Instances Services for interfaces exposed through Oracle Unified Directory Services Manager Instances Ingress configuration  Create a Kubernetes namespace Create a Kubernetes namespace for the OUDSM deployment by running the following command:\n$ kubectl create namespace \u0026lt;namespace\u0026gt; For example:\n$ kubectl create namespace oudsmns The output will look similar to the following:\nnamespace/oudsmns created Create a Kubernetes secret for the container registry Create a Kubernetes secret that stores the credentials for the container registry where the OUDSM image is stored. This step must be followed if using Oracle Container Registry or your own private container registry. If you are not using a container registry and have loaded the images on each of the master and worker nodes, you can skip this step.\n  Run the following command to create the secret:\nkubectl create secret docker-registry \u0026#34;orclcred\u0026#34; --docker-server=\u0026lt;CONTAINER_REGISTRY\u0026gt; \\ --docker-username=\u0026#34;\u0026lt;USER_NAME\u0026gt;\u0026#34; \\ --docker-password=\u0026lt;PASSWORD\u0026gt; --docker-email=\u0026lt;EMAIL_ID\u0026gt; \\ --namespace=\u0026lt;domain_namespace\u0026gt; For example, if using Oracle Container Registry:\nkubectl create secret docker-registry \u0026#34;orclcred\u0026#34; --docker-server=container-registry.oracle.com \\ --docker-username=\u0026#34;user@example.com\u0026#34; \\ --docker-password=password --docker-email=user@example.com \\ --namespace=oudsmns Replace \u0026lt;USER_NAME\u0026gt; and \u0026lt;PASSWORD\u0026gt; with the credentials for the registry with the following caveats:\n  If using Oracle Container Registry to pull the OUDSM container image, this is the username and password used to login to Oracle Container Registry. Before you can use this image you must login to Oracle Container Registry, navigate to Middleware \u0026gt; oudsm_cpu and accept the license agreement.\n  If using your own container registry to store the OUDSM container image, this is the username and password (or token) for your container registry.\n  The output will look similar to the following:\nsecret/orclcred created   Create a persistent volume directory As referenced in Prerequisites the nodes in the Kubernetes cluster must have access to a persistent volume such as a Network File System (NFS) mount or a shared file system.\nMake sure the persistent volume path has full access permissions, and that the folder is empty. In this example /scratch/shared/ is a shared directory accessible from all nodes.\n  On the master node run the following command to create a user_projects directory:\n$ cd \u0026lt;persistent_volume\u0026gt; $ mkdir oudsm_user_projects $ chmod 777 oudsm_user_projects For example:\n$ cd /scratch/shared $ mkdir oudsm_user_projects $ chmod 777 oudsm_user_projects   On the master node run the following to ensure it is possible to read and write to the persistent volume:\n$ cd \u0026lt;persistent_volume\u0026gt;/oudsm_user_projects $ touch file.txt $ ls filemaster.txt For example:\n$ cd /scratch/shared/oudsm_user_projects $ touch filemaster.txt $ ls filemaster.txt On the first worker node run the following to ensure it is possible to read and write to the persistent volume:\n$ cd /scratch/shared/oudsm_user_projects $ ls filemaster.txt $ touch fileworker1.txt $ ls fileworker1.txt Repeat the above for any other worker nodes e.g fileworker2.txt etc. Once proven that it\u0026rsquo;s possible to read and write from each node to the persistent volume, delete the files created.\n  The oudsm Helm chart The oudsm Helm chart allows you to create or deploy Oracle Unified Directory Services Manager instances along with Kubernetes objects in a specified namespace.\nThe deployment can be initiated by running the following Helm command with reference to the oudsm Helm chart, along with configuration parameters according to your environment.\ncd $WORKDIR/kubernetes/helm $ helm install --namespace \u0026lt;namespace\u0026gt; \\ \u0026lt;Configuration Parameters\u0026gt; \\ \u0026lt;deployment/release name\u0026gt; \\ \u0026lt;Helm Chart Path/Name\u0026gt; Configuration Parameters (override values in chart) can be passed on with --set arguments on the command line and/or with -f / --values arguments when referring to files.\nNote: The examples in Create OUDSM instances below provide values which allow the user to override the default values provided by the Helm chart. A full list of configuration parameters and their default values is shown in Appendix: Configuration parameters.\nFor more details about the helm command and parameters, please execute helm --help and helm install --help.\nCreate OUDSM instances You can create OUDSM instances using one of the following methods:\n Using a YAML file Using --set argument  Using a YAML file   Navigate to the $WORKDIR/kubernetes/helm directory:\n$ cd $WORKDIR/kubernetes/helm   Create an oudsm-values-override.yaml as follows:\nimage: repository: \u0026lt;image_location\u0026gt; tag: \u0026lt;image_tag\u0026gt; pullPolicy: IfNotPresent imagePullSecrets: - name: orclcred oudsm: adminUser: weblogic adminPass: \u0026lt;password\u0026gt; persistence: type: filesystem filesystem: hostPath: path: \u0026lt;persistent_volume\u0026gt;/oudsm_user_projects For example:\nimage: repository: container-registry.oracle.com/middleware/oudsm_cpu tag: 12.2.1.4-jdk8-ol7-220223.2053 pullPolicy: IfNotPresent imagePullSecrets: - name: orclcred oudsm: adminUser: weblogic adminPass: \u0026lt;password\u0026gt; persistence: type: filesystem filesystem: hostPath: path: /scratch/shared/oudsm_user_projects The following caveats exist:\n  Replace \u0026lt;password\u0026gt; with a the relevant passwords.\n  If you are not using Oracle Container Registry or your own container registry for your OUD container image, then you can remove the following:\nimagePullSecrets: - name: orclcred   If using NFS for your persistent volume the change the persistence section as follows:\n  persistence: type: networkstorage networkstorage: nfs: path: \u0026lt;persistent_volume\u0026gt;/oudsm_user_projects server: \u0026lt;NFS IP address\u0026gt;   Run the following command to deploy OUDSM:\n$ helm install --namespace \u0026lt;namespace\u0026gt; \\ --values oudsm-values-override.yaml \\ \u0026lt;release_name\u0026gt; oudsm $ helm install --namespace oudsmns \\ --values oudsm-values-override.yaml \\ oudsm oudsm   Check the OUDSM deployment as per Verify the OUDSM deployment\n  Using --set argument   Navigate to the $WORKDIR/kubernetes/helm directory:\n$ cd $WORKDIR/kubernetes/helm   Run the following command to create OUDSM instance:\n$ helm install --namespace oudsmns \\ --set oudsm.adminUser=weblogic,oudsm.adminPass=\u0026lt;password\u0026gt;,persistence.filesystem.hostPath.path=\u0026lt;persistent_volume\u0026gt;/oudsm_user_projects,image.repository=\u0026lt;image_location\u0026gt;,image.tag=\u0026lt;image_tag\u0026gt; \\ --set imagePullSecrets[0].name=\u0026#34;orclcred\u0026#34; \\ \u0026lt;release_name\u0026gt; oudsm For example:\n$ helm install --namespace oudsmns \\ --set oudsm.adminUser=weblogic,oudsm.adminPass=\u0026lt;password\u0026gt;,persistence.filesystem.hostPath.path=/scratch/shared/oudsm_user_projects,image.repository=container-registry.oracle.com/middleware/oudsm_cpu,image.tag=12.2.1.4-jdk8-ol7-220223.2053 \\ --set imagePullSecrets[0].name=\u0026#34;orclcred\u0026#34; \\ oudsm oudsm The following caveats exist:\n Replace \u0026lt;password\u0026gt; with a the relevant password. If you are not using Oracle Container Registry or your own container registry for your OUDSM container image, then you can remove the following: --set imagePullSecrets[0].name=\u0026quot;orclcred\u0026quot; If using using NFS for your persistent volume then use persistence.networkstorage.nfs.path=\u0026lt;persistent_volume\u0026gt;/oudsm_user_projects,persistence.networkstorage.nfs.server:\u0026lt;NFS IP address\u0026gt;.    Check the OUDSM deployment as per Verify the OUDSM deployment\n  Helm command output In all the examples above, the following output is shown following a successful execution of the helm install command.\nNAME: oudsm LAST DEPLOYED: Mon Mar 21 12:21:06 2022 NAMESPACE: oudsmns STATUS: deployed REVISION: 1 TEST SUITE: None Verify the OUDSM deployment Run the following command to verify the OUDSM deployment:\n$ kubectl --namespace \u0026lt;namespace\u0026gt; get pod,service,secret,pv,pvc,ingress -o wide For example:\n$ kubectl --namespace oudsmns get pod,service,secret,pv,pvc,ingress -o wide The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/oudsm-1 1/1 Running 0 73m 10.244.0.19 \u0026lt;worker-node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service/oudsm-1 ClusterIP 10.96.108.200 \u0026lt;none\u0026gt; 7001/TCP,7002/TCP 73m app.kubernetes.io/instance=oudsm,app.kubernetes.io/name=oudsm,oudsm/instance=oudsm-1 service/oudsm-lbr ClusterIP 10.96.41.201 \u0026lt;none\u0026gt; 7001/TCP,7002/TCP 73m app.kubernetes.io/instance=oudsm,app.kubernetes.io/name=oudsm NAME TYPE DATA AGE secret/default-token-w4jft kubernetes.io/service-account-token 3 3h15m secret/orclcred kubernetes.io/dockerconfigjson 1 3h13m secret/oudsm-creds opaque 2 73m secret/oudsm-token-ksr4g kubernetes.io/service-account-token 3 73m secret/sh.helm.release.v1.oudsm.v1 helm.sh/release.v1 1 73m NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE VOLUMEMODE persistentvolume/oudsm-pv 30Gi RWX Retain Bound myoudsmns/oudsm-pvc manual 73m Filesystem NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE VOLUMEMODE persistentvolumeclaim/oudsm-pvc Bound oudsm-pv 30Gi RWX manual 73m Filesystem NAME HOSTS ADDRESS PORTS AGE ingress.extensions/oudsm-ingress-nginx oudsm-1,oudsm-2,oudsm + 1 more... 100.102.51.230 80 73m Note: It will take several minutes before all the services listed above show. While the oudsm pods have a STATUS of 0/1 the pod is started but the OUDSM server associated with it is currently starting. While the pod is starting you can check the startup status in the pod logs, by running the following command:\n$ kubectl logs oudsm-1 -n oudsmns Note : If the OUD deployment fails additionally refer to Troubleshooting for instructions on how describe the failing pod(s). Once the problem is identified follow Undeploy an OUDSM deployment to clean down the deployment before deploying again.\nKubernetes Objects Kubernetes objects created by the Helm chart are detailed in the table below:\n   Type Name Example Name Purpose     Service Account \u0026lt;deployment/release name\u0026gt; oudsm Kubernetes Service Account for the Helm Chart deployment   Secret \u0026lt;deployment/release name\u0026gt;-creds oudsm-creds Secret object for Oracle Unified Directory Services Manager related critical values like passwords   Persistent Volume \u0026lt;deployment/release name\u0026gt;-pv oudsm-pv Persistent Volume for user_projects mount.   Persistent Volume Claim \u0026lt;deployment/release name\u0026gt;-pvc oudsm-pvc Persistent Volume Claim for user_projects mount.   Pod \u0026lt;deployment/release name\u0026gt;-N oudsm-1, oudsm-2, \u0026hellip; Pod(s)/Container(s) for Oracle Unified Directory Services Manager Instances   Service \u0026lt;deployment/release name\u0026gt;-N oudsm-1, oudsm-2, \u0026hellip; Service(s) for HTTP and HTTPS interfaces from Oracle Unified Directory Services Manager instance \u0026lt;deployment/release name\u0026gt;-N   Ingress \u0026lt;deployment/release name\u0026gt;-ingress-nginx oudsm-ingress-nginx Ingress Rules for HTTP and HTTPS interfaces.     In the table above, the Example Name for each Object is based on the value \u0026lsquo;oudsm\u0026rsquo; as the deployment/release name for the Helm chart installation.  Ingress Configuration With an OUDSM instance now deployed you are now ready to configure an ingress controller to direct traffic to OUDSM as per Configure an ingress for an OUDSM.\nUndeploy an OUDSM deployment Delete the OUDSM deployment   Find the deployment release name:\n$ helm --namespace \u0026lt;namespace\u0026gt; list For example:\n$ helm --namespace oudsmns list The output will look similar to the following:\nNAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION oudsm oudsmns 2 2022-03-21 16:46:34.05531056 +0000 UTC deployed oudsm-0.1 12.2.1.4.0   Delete the deployment using the following command:\n$ helm uninstall --namespace \u0026lt;namespace\u0026gt; \u0026lt;release\u0026gt; For example:\n$ helm uninstall --namespace oudsmns oudsm release \u0026#34;oudsm\u0026#34; uninstalled   Delete the persistent volume contents   Delete the contents of the oudsm_user_projects directory in the persistent volume:\n$ cd \u0026lt;persistent_volume\u0026gt;/oudsm_user_projects $ rm -rf * For example:\n$ cd /scratch/shared/oudsm_user_projects $ rm -rf *   Appendix: Configuration Parameters The following table lists the configurable parameters of the \u0026lsquo;oudsm\u0026rsquo; chart and their default values.\n   Parameter Description Default Value     replicaCount Number of Oracle Unified Directory Services Manager instances/pods/services to be created 1   restartPolicyName restartPolicy to be configured for each POD containing Oracle Unified Directory Services Manager instance OnFailure   image.repository Oracle Unified Directory Services Manager Image Registry/Repository and name. Based on this, image parameter would be configured for Oracle Unified Directory Services Manager pods/containers oracle/oudsm   image.tag Oracle Unified Directory Services Manager Image Tag. Based on this, image parameter would be configured for Oracle Unified Directory Services Manager pods/containers 12.2.1.4.0   image.pullPolicy policy to pull the image IfnotPresent   imagePullSecrets.name name of Secret resource containing private registry credentials regcred   nameOverride override the fullname with this name    fullnameOverride Overrides the fullname with the provided string    serviceAccount.create Specifies whether a service account should be created true   serviceAccount.name If not set and create is true, a name is generated using the fullname template oudsm-\u0026lt; fullname \u0026gt;-token-\u0026lt; randomalphanum \u0026gt;   podSecurityContext Security context policies to add to the controller pod    securityContext Security context policies to add by default    service.type type of controller service to create ClusterIP   nodeSelector node labels for pod assignment    tolerations node taints to tolerate    affinity node/pod affinities    ingress.enabled  true   ingress.type Supported value: nginx nginx   ingress.host Hostname to be used with Ingress Rules. If not set, hostname would be configured according to fullname. Hosts would be configured as \u0026lt; fullname \u0026gt;-http.\u0026lt; domain \u0026gt;, \u0026lt; fullname \u0026gt;-http-0.\u0026lt; domain \u0026gt;, \u0026lt; fullname \u0026gt;-http-1.\u0026lt; domain \u0026gt;, etc.    ingress.domain Domain name to be used with Ingress Rules. In ingress rules, hosts would be configured as \u0026lt; host \u0026gt;.\u0026lt; domain \u0026gt;, \u0026lt; host \u0026gt;-0.\u0026lt; domain \u0026gt;, \u0026lt; host \u0026gt;-1.\u0026lt; domain \u0026gt;, etc.    ingress.backendPort  http   ingress.nginxAnnotations  { kubernetes.io/ingress.class: \u0026ldquo;nginx\u0026quot;nginx.ingress.kubernetes.io/affinity-mode: \u0026ldquo;persistent\u0026rdquo; nginx.ingress.kubernetes.io/affinity: \u0026ldquo;cookie\u0026rdquo; }   ingress.ingress.tlsSecret Secret name to use an already created TLS Secret. If such secret is not provided, one would be created with name \u0026lt; fullname \u0026gt;-tls-cert. If the TLS Secret is in different namespace, name can be mentioned as \u0026lt; namespace \u0026gt;/\u0026lt; tlsSecretName \u0026gt;    ingress.certCN Subject\u0026rsquo;s common name (cn) for SelfSigned Cert. \u0026lt; fullname \u0026gt;   ingress.certValidityDays Validity of Self-Signed Cert in days 365   secret.enabled If enabled it will use the secret created with base64 encoding. if value is false, secret would not be used and input values (through \u0026ndash;set, \u0026ndash;values, etc.) would be used while creation of pods. true   secret.name secret name to use an already created Secret oudsm-\u0026lt; fullname \u0026gt;-creds   secret.type Specifies the type of the secret Opaque   persistence.enabled If enabled, it will use the persistent volume. if value is false, PV and PVC would not be used and pods would be using the default emptyDir mount volume. true   persistence.pvname pvname to use an already created Persistent Volume , If blank will use the default name oudsm-\u0026lt; fullname \u0026gt;-pv   persistence.pvcname pvcname to use an already created Persistent Volume Claim , If blank will use default name oudsm-\u0026lt; fullname \u0026gt;-pvc   persistence.type supported values: either filesystem or networkstorage or custom filesystem   persistence.filesystem.hostPath.path The path location mentioned should be created and accessible from the local host provided with necessary privileges for the user. /scratch/shared/oudsm_user_projects   persistence.networkstorage.nfs.path Path of NFS Share location /scratch/shared/oudsm_user_projects   persistence.networkstorage.nfs.server IP or hostname of NFS Server 0.0.0.0   persistence.custom.* Based on values/data, YAML content would be included in PersistenceVolume Object    persistence.accessMode Specifies the access mode of the location provided ReadWriteMany   persistence.size Specifies the size of the storage 10Gi   persistence.storageClass Specifies the storageclass of the persistence volume. empty   persistence.annotations specifies any annotations that will be used { }   oudsm.adminUser Weblogic Administration User weblogic   oudsm.adminPass Password for Weblogic Administration User    oudsm.startupTime Expected startup time. After specified seconds readinessProbe would start 900   oudsm.livenessProbeInitialDelay Paramter to decide livenessProbe initialDelaySeconds 1200   elk.elasticsearch.enabled If enabled it will create the elastic search statefulset deployment false   elk.elasticsearch.image.repository Elastic Search Image name/Registry/Repository . Based on this elastic search instances will be created docker.elastic.co/elasticsearch/elasticsearch   elk.elasticsearch.image.tag Elastic Search Image tag .Based on this, image parameter would be configured for Elastic Search pods/instances 6.4.3   elk.elasticsearch.image.pullPolicy policy to pull the image IfnotPresent   elk.elasticsearch.esreplicas Number of Elastic search Instances will be created 3   elk.elasticsearch.minimumMasterNodes The value for discovery.zen.minimum_master_nodes. Should be set to (esreplicas / 2) + 1. 2   elk.elasticsearch.esJAVAOpts Java options for Elasticsearch. This is where you should configure the jvm heap size -Xms512m -Xmx512m   elk.elasticsearch.sysctlVmMaxMapCount Sets the sysctl vm.max_map_count needed for Elasticsearch 262144   elk.elasticsearch.resources.requests.cpu cpu resources requested for the elastic search 100m   elk.elasticsearch.resources.limits.cpu total cpu limits that are configures for the elastic search 1000m   elk.elasticsearch.esService.type Type of Service to be created for elastic search ClusterIP   elk.elasticsearch.esService.lbrtype Type of load balancer Service to be created for elastic search ClusterIP   elk.kibana.enabled If enabled it will create a kibana deployment false   elk.kibana.image.repository Kibana Image Registry/Repository and name. Based on this Kibana instance will be created docker.elastic.co/kibana/kibana   elk.kibana.image.tag Kibana Image tag. Based on this, Image parameter would be configured. 6.4.3   elk.kibana.image.pullPolicy policy to pull the image IfnotPresent   elk.kibana.kibanaReplicas Number of Kibana instances will be created 1   elk.kibana.service.tye Type of service to be created NodePort   elk.kibana.service.targetPort Port on which the kibana will be accessed 5601   elk.kibana.service.nodePort nodePort is the port on which kibana service will be accessed from outside 31119   elk.logstash.enabled If enabled it will create a logstash deployment false   elk.logstash.image.repository logstash Image Registry/Repository and name. Based on this logstash instance will be created logstash   elk.logstash.image.tag logstash Image tag. Based on this, Image parameter would be configured. 6.6.0   elk.logstash.image.pullPolicy policy to pull the image IfnotPresent   elk.logstash.containerPort Port on which the logstash container will be running 5044   elk.logstash.service.tye Type of service to be created NodePort   elk.logstash.service.targetPort Port on which the logstash will be accessed 9600   elk.logstash.service.nodePort nodePort is the port on which logstash service will be accessed from outside 32222   elk.logstash.logstashConfigMap Provide the configmap name which is already created with the logstash conf. if empty default logstash configmap will be created and used    elk.elkPorts.rest Port for REST 9200   elk.elkPorts.internode port used for communication between the nodes 9300   elk.busybox.image busy box image name. Used for initcontianers busybox   elk.elkVolume.enabled If enabled, it will use the persistent volume. if value is false, PV and pods would be using the default emptyDir mount volume. true   elk.elkVolume.pvname pvname to use an already created Persistent Volume , If blank will use the default name oudsm-\u0026lt; fullname \u0026gt;-espv   elk.elkVolume.type supported values: either filesystem or networkstorage or custom filesystem   elk.elkVolume.filesystem.hostPath.path The path location mentioned should be created and accessible from the local host provided with necessary privileges for the user. /scratch/shared/oud_elk/data   elk.elkVolume.networkstorage.nfs.path Path of NFS Share location /scratch/shared/oudsm_elk/data   elk.elkVolume.networkstorage.nfs.server IP or hostname of NFS Server 0.0.0.0   elk.elkVolume.custom.* Based on values/data, YAML content would be included in PersistenceVolume Object    elk.elkVolume.accessMode Specifies the access mode of the location provided ReadWriteMany   elk.elkVolume.size Specifies the size of the storage 20Gi   elk.elkVolume.storageClass Specifies the storageclass of the persistence volume. elk   elk.elkVolume.annotations specifies any annotations that will be used { }    "
},
{
	"uri": "/fmw-kubernetes/wccontent-domains/installguide/additional-steps-to-launch-native-binaries/",
	"title": "Launch Oracle Webcenter Content Native Applications in Containers",
	"tags": [],
	"description": "How to launch Oracle WebCenter Content native binaries from inside containerized environment.",
	"content": "This section provides the steps required to use product native binaries with user interfaces.\nIssue with Launching Headful User Interfaces for Oracle WebCenter Content Native Binaries Oracle WebCenter Content (UCM) provide a set of native binaries with headful UIs, which are located inside the persistent volume, as part of the domain. WebCenter Content container images are, by default, created with Oracle slim linux image, which doesn\u0026rsquo;t come with all the packages pre-installed to support headful applications with UIs to be launched. With current Oracle WebCenter Content container images, running native applications fails, being unable to launch UIs.\nThe following sections document the solution, by providing a set of instructions, enabling users to run UCM native applications with UIs.\nThese instructions are divided in two parts -\n Steps to update the existing container image Steps to launch native apps using VNC sessions  Steps to Update out-of-the-box Oracle WebCenter Content Container Image Using WebLogic Image Tool This section describes the method to update image with a OS package using WebLogic Image Tool. Please refer this for setting up the WebLogic Image Tool.\nAdditional Build Commands The installation of required OS packages in the image, can be done using yum command in additional build command option available in WebLogic Image Tool. Here is the sample additionalBuildCmds.txt file, to be used, to install required Linux packages (libXext.x86_64, libXrender.x86_64 and libXtst.x86_64).\n[final-build-commands] USER root RUN yum -y --downloaddir=/tmp/imagetool install libXext libXrender libXtst \\ \u0026amp;\u0026amp; yum -y --downloaddir=/tmp/imagetool clean all \\ \u0026amp;\u0026amp; rm -rf /var/cache/yum/* \\ \u0026amp;\u0026amp; rm -rf /tmp/imagetool USER oracle  Note: It is important to change the user to oracle, otherwise the user during the container execution will be root.\n Build arguments The arguments required for updating the image can be passed as file to the WebLogic Image Tool.\n'update' is the sub command to Image Tool for updating an existing docker image. '--fromImage' option provides the existing docker image that has to be updated. '--tag' option should be provided with the new tag for the updated image. '--additionalBuildCommands' option should be provided with the above created additional build commands file.  Below is a sample build argument (buildArgs) file, to be used for updating the image,\n update --fromImage \u0026lt;existing_WCContent_image_without_dependent_packages\u0026gt; --tag \u0026lt;name_of_updated_WCContent_image_to_be_built\u0026gt; --additionalBuildCommands ./additionalBuildCmds.txt Update Oracle WebCenter Content Container Image Now we can execute the WebLogic Image Tool to update the out-of-the-box image, using the build-argument file described above -\n$ imagetool @buildArgs WebLogic Image Tool provides multiple options for updating the image. For detailed information on the update options, please refer to this document.\nUpdating the image does not modify the \u0026lsquo;CMD\u0026rsquo; from the source image unless it is modified in the additional build commands.\n$ docker inspect -f '{{.Config.Cmd}}' \u0026lt;name_of_updated_Wccontent_image\u0026gt; [/u01/oracle/container-scripts/createDomainandStartAdmin.sh] Steps to launch Oracle WebCenter Content native applications using VNC sessions. Once updated image is successfully built and available on all required nodes, do the following: a. Update the domain.yaml file with updated image name and apply the domain.yaml file.\n$ kubectl apply -f domain.yaml b. After applying the modified domain.yaml, pods will get restarted and start running with updated image with required packages.\n$ kubectl get pods -n \u0026lt;namespace_being_used_for_wccontent_domain\u0026gt; c. Create VNC sessions on the master node to launch native apps. These are the steps to be followed using the VNC session.\nd. Run this command on each VNC session:\n$ xhost + \u0026lt;HOST-IP or HOST-NAME of the node, on which POD is deployed\u0026gt;  Note: The above command works for multi-node clusters (in which master node and worker nodes are deployed on different hosts and pods are distributed among worker nodes, running on different hosts). In case of single node clusters (where there is only master node and no worker nodes and all pods are deployed on the host, on which master node is running), one needs to use container/pod’s IP instead of the master-node’s HOST-IP itself.\n To obtain the container IP, follow the command mentioned in step g, from within that container\u0026rsquo;s shell.\n$ xhost + \u0026lt;IP of the container, from which binaries are to be run \u0026gt; e. Get into the pod\u0026rsquo;s (for example, wccinfra-ucm-server1) shell:\n$ kubectl exec -n wccns -it wccinfra-ucm-server1 -- /bin/bash f. Traverse to the binaries location:\n$ cd /u01/oracle/user_projects/domains/wccinfra/ucm/cs/bin g. Get the container IP:\n$ hostname -i h. Set DISPLAY variable within the container:\n$ export DISPLAY=\u0026lt;HOST-IP/HOST-NAME of the master node, where VNC session was created\u0026gt;:vnc-session display-id i. Launch any native UCM application, from within the container, like this:\n$ ./SystemProperties If the application has an UI, it will get launched now.\n"
},
{
	"uri": "/fmw-kubernetes/oig/manage-oig-domains/logging-and-visualization/",
	"title": "Logging and visualization",
	"tags": [],
	"description": "Describes the steps for logging and visualization with Elasticsearch and Kibana.",
	"content": "After the OIG domain is set up you can publish operator and WebLogic Server logs into Elasticsearch and interact with them in Kibana.\nInstall Elasticsearch and Kibana   If your domain namespace is anything other than oigns, edit the $WORKDIR/kubernetes/elasticsearch-and-kibana/elasticsearch_and_kibana.yaml and change all instances of oigns to your domain namespace.\n  Create a Kubernetes secret to access the elasticsearch and kibana container images:\nNote: You must first have a user account on hub.docker.com.\n$ kubectl create secret docker-registry \u0026#34;dockercred\u0026#34; --docker-server=\u0026#34;https://index.docker.io/v1/\u0026#34; --docker-username=\u0026#34;\u0026lt;docker_username\u0026gt;\u0026#34; --docker-password=\u0026lt;password\u0026gt; --docker-email=\u0026lt;docker_email_credentials\u0026gt; --namespace=\u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl create secret docker-registry \u0026quot;dockercred\u0026quot; --docker-server=\u0026quot;https://index.docker.io/v1/\u0026quot; --docker-username=\u0026quot;username\u0026quot; --docker-password=\u0026lt;password\u0026gt; --docker-email=user@example.com --namespace=oigns The output will look similar to the following:\nsecret/dockercred created   Create the Kubernetes resource using the following command:\n$ kubectl apply -f $WORKDIR/kubernetes/elasticsearch-and-kibana/elasticsearch_and_kibana.yaml The output will look similar to the following:\ndeployment.apps/elasticsearch created service/elasticsearch created deployment.apps/kibana created service/kibana created   Run the following command to ensure Elasticsearch is used by the operator:\n$ helm get values --all weblogic-kubernetes-operator -n opns The output will look similar to the following:\nCOMPUTED VALUES: clusterSizePaddingValidationEnabled: true domainNamespaceLabelSelector: weblogic-operator=enabled domainNamespaceSelectionStrategy: LabelSelector domainNamespaces: - default elasticSearchHost: elasticsearch.default.svc.cluster.local elasticSearchPort: 9200 elkIntegrationEnabled: true enableClusterRoleBinding: true externalDebugHttpPort: 30999 externalRestEnabled: false externalRestHttpsPort: 31001 externalServiceNameSuffix: -ext image: ghcr.io/oracle/weblogic-kubernetes-operator:3.3.0 imagePullPolicy: IfNotPresent internalDebugHttpPort: 30999 introspectorJobNameSuffix: -introspector javaLoggingFileCount: 10 javaLoggingFileSizeLimit: 20000000 javaLoggingLevel: FINE logStashImage: logstash:6.6.0 remoteDebugNodePortEnabled: false serviceAccount: op-sa suspendOnDebugStartup: false   To check that Elasticsearch and Kibana are deployed in the Kubernetes cluster, run the following command:\n$ kubectl get pods -n \u0026lt;namespace\u0026gt; | grep \u0026#39;elasticsearch\\|kibana\u0026#39; For example:\n$ kubectl get pods -n oigns | grep \u0026#39;elasticsearch\\|kibana\u0026#39; The output will look similar to the following:\nelasticsearch-857bd5ff6b-tvqdn 1/1 Running 0 2m9s kibana-594465687d-zc2rt 1/1 Running 0 2m9s   Create the logstash pod OIG Server logs can be pushed to the Elasticsearch server using the logstash pod. The logstash pod needs access to the persistent volume of the OIG domain created previously, for example governancedomain-domain-pv. The steps to create the logstash pod are as follows:\n  Obtain the OIG domain persistence volume details:\n$ kubectl get pv -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get pv -n oigns The output will look similar to the following:\nNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE governancedomain-domain-pv 10Gi RWX Retain Bound oigns/governancedomain-domain-pvc governancedomain-oim-storage-class 28h Make note of the CLAIM value, for example in this case governancedomain-oim-pvc\n  Run the following command to get the mountPath of your domain:\n$ kubectl describe domains \u0026lt;domain_uid\u0026gt; -n \u0026lt;domain_namespace\u0026gt; | grep \u0026#34;Mount Path\u0026#34; For example:\n$ kubectl describe domains governancedomain -n oigns | grep \u0026#34;Mount Path\u0026#34; The output will look similar to the following:\nMount Path: /u01/oracle/user_projects/domains   Navigate to the $WORKDIR/kubernetes/elasticsearch-and-kibana directory and create a logstash.yaml file as follows. Change the claimName and mountPath values to match the values returned in the previous commands. Change namespace to your domain namespace e.g oigns:\napiVersion: apps/v1 kind: Deployment metadata: name: logstash-wls namespace: oigns spec: selector: matchLabels: k8s-app: logstash-wls template: # create pods using pod definition in this template metadata: labels: k8s-app: logstash-wls spec: volumes: - name: weblogic-domain-storage-volume persistentVolumeClaim: claimName: governancedomain-domain-pvc - name: shared-logs emptyDir: {} imagePullSecrets: - name: dockercred containers: - name: logstash image: logstash:6.6.0 command: [\u0026quot;/bin/sh\u0026quot;] args: [\u0026quot;/usr/share/logstash/bin/logstash\u0026quot;, \u0026quot;-f\u0026quot;, \u0026quot;/u01/oracle/user_projects/domains/logstash/logstash.conf\u0026quot;] imagePullPolicy: IfNotPresent volumeMounts: - mountPath: /u01/oracle/user_projects/domains name: weblogic-domain-storage-volume - name: shared-logs mountPath: /shared-logs ports: - containerPort: 5044 name: logstash   In the persistent volume directory that corresponds to the mountPath /u01/oracle/user_projects/domains, create a logstash directory. For example:\n$ mkdir -p /scratch/shared/governancedomainpv/logstash   Create a logstash.conf in the newly created logstash directory that contains the following. Make sure the paths correspond to your mountPath and domain name. Also, if your namespace is anything other than oigns change \u0026quot;elasticsearch.oigns.svc.cluster.local:9200\u0026quot; to \u0026quot;elasticsearch.\u0026lt;namespace\u0026gt;.svc.cluster.local:9200\u0026quot;::\ninput { file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/logs/governancedomain/AdminServer*.log\u0026quot; tags =\u0026gt; \u0026quot;Adminserver_log\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/logs/governancedomain/soa_server*.log\u0026quot; tags =\u0026gt; \u0026quot;soaserver_log\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/logs/governancedomain/oim_server*.log\u0026quot; tags =\u0026gt; \u0026quot;Oimserver_log\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/governancedomain/servers/AdminServer/logs/AdminServer-diagnostic.log\u0026quot; tags =\u0026gt; \u0026quot;Adminserver_diagnostic\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/governancedomain/servers/**/logs/soa_server*-diagnostic.log\u0026quot; tags =\u0026gt; \u0026quot;Soa_diagnostic\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/governancedomain/servers/**/logs/oim_server*-diagnostic.log\u0026quot; tags =\u0026gt; \u0026quot;Oimserver_diagnostic\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/governancedomain/servers/**/logs/access*.log\u0026quot; tags =\u0026gt; \u0026quot;Access_logs\u0026quot; start_position =\u0026gt; beginning } } filter { grok { match =\u0026gt; [ \u0026quot;message\u0026quot;, \u0026quot;\u0026lt;%{DATA:log_timestamp}\u0026gt; \u0026lt;%{WORD:log_level}\u0026gt; \u0026lt;%{WORD:thread}\u0026gt; \u0026lt;%{HOSTNAME:hostname}\u0026gt; \u0026lt;%{HOSTNAME:servername}\u0026gt; \u0026lt;%{DATA:timer}\u0026gt; \u0026lt;\u0026lt;%{DATA:kernel}\u0026gt;\u0026gt; \u0026lt;\u0026gt; \u0026lt;%{DATA:uuid}\u0026gt; \u0026lt;%{NUMBER:timestamp}\u0026gt; \u0026lt;%{DATA:misc}\u0026gt; \u0026lt;%{DATA:log_number}\u0026gt; \u0026lt;%{DATA:log_message}\u0026gt;\u0026quot; ] } if \u0026quot;_grokparsefailure\u0026quot; in [tags] { mutate { remove_tag =\u0026gt; [ \u0026quot;_grokparsefailure\u0026quot; ] } } } output { elasticsearch { hosts =\u0026gt; [\u0026quot;elasticsearch.oigns.svc.cluster.local:9200\u0026quot;] } }   Deploy the logstash pod by executing the following command:\n$ kubectl create -f $WORKDIR/kubernetes/elasticsearch-and-kibana/logstash.yaml The output will look similar to the following:\ndeployment.apps/logstash-wls created   Run the following command to check the logstash pod is created correctly:\n$ kubectl get pods -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl get pods -n oigns The output should look similar to the following:\nNAME READY STATUS RESTARTS AGE elasticsearch-678ff4fb5-89rpf 1/1 Running 0 13m governancedomain-adminserver 1/1 Running 0 90m governancedomain-create-fmw-infra-sample-domain-job-8cww8 0/1 Completed 0 25h governancedomain-oim-server1 1/1 Running 0 87m governancedomain-soa-server1 1/1 Running 0 87m kibana-589466bb89-k8wdr 1/1 Running 0 13m logstash-wls-f448b44c8-92l27 1/1 Running 0 7s   Verify and access the Kibana console   Check if the indices are created correctly in the elasticsearch pod shown above:\n$ kubectl exec -it \u0026lt;elasticsearch-pod\u0026gt; -n \u0026lt;namespace\u0026gt; -- /bin/bash For example:\n$ kubectl exec -it elasticsearch-678ff4fb5-89rpf -n oigns -- /bin/bash This will take you into a bash shell in the elasticsearch pod:\n[root@elasticsearch-678ff4fb5-89rpf elasticsearch]#   In the elasticsearch bash shell run the following to check the indices:\n[root@elasticsearch-678ff4fb5-89rpf elasticsearch]# curl -i \u0026#34;127.0.0.1:9200/_cat/indices?v\u0026#34; The output will look similar to the following:\nHTTP/1.1 200 OK content-type: text/plain; charset=UTF-8 content-length: 580 health status index uuid pri rep docs.count docs.deleted store.size pri.store.size yellow open logstash-2022.03.10 7oXXCureSWKwNY0626Szeg 5 1 46887 0 11.7mb 11.7mb green open .kibana_task_manager alZtnv2WRy6Y4iSRIbmCrQ 1 0 2 0 12.6kb 12.6kb green open .kibana_1 JeZKrO4fS_GnRL92qRmQDQ 1 0 2 0 7.6kb 7.6kb Exit the bash shell by typing exit.\n  Find the Kibana port by running the following command:\n$ kubectl get svc -n \u0026lt;namespace\u0026gt; | grep kibana For example:\n$ kubectl get svc -n oigns | grep kibana The output will look similar to the following:\nkibana NodePort 10.111.224.230 \u0026lt;none\u0026gt; 5601:31490/TCP 11m In the example above the Kibana port is 31490.\n  Access the Kibana console with http://${MASTERNODE-HOSTNAME}:${KIBANA-PORT}/app/kibana.\n  Click on Dashboard in the left hand Navigation Menu.\n  In the Create index pattern page enter logstash* and click Next Step.\n  From the Time Filter field name drop down menu select @timestamp and click Create index pattern.\n  Once the index pattern is created click on Discover in the navigation menu to view the logs.\n  For more details on how to use the Kibana console see the Kibana Guide\nCleanup To clean up the Elasticsearch and Kibana install:\n  Run the following command to delete logstash:\n$ kubectl delete -f $WORKDIR/kubernetes/elasticsearch-and-kibana/logstash.yaml The output will look similar to the following:\ndeployment.apps \u0026quot;logstash-wls\u0026quot; deleted   Run the following command to delete Elasticsearch and Kibana:\n$ kubectl delete -f $WORKDIR/kubernetes/elasticsearch-and-kibana/elasticsearch_and_kibana.yaml The output will look similar to the following:\ndeployment.apps \u0026quot;elasticsearch\u0026quot; deleted service \u0026quot;elasticsearch\u0026quot; deleted deployment.apps \u0026quot;kibana\u0026quot; deleted service \u0026quot;kibana\u0026quot; deleted   "
},
{
	"uri": "/fmw-kubernetes/wccontent-domains/oracle-cloud/configure-wcc-for-idcs/",
	"title": "Configuring Oracle WebCenter Content for  Oracle Identity Cloud Service (IDCS)",
	"tags": [],
	"description": "Configuring Oracle WebCenter Content for  Oracle Identity Cloud Service (IDCS)",
	"content": "Contents  Introduction Updating SSL.hostnameVerifier Property Configuring IDCS Security Provider Configuring Oracle Identity Cloud Integrator Provider Setting Up Trust between IDCS and WebLogic Creating Admin User in IDCS Admin Console for WebCenter Content Managing Group Memberships, Roles, and Accounts Configuring WebCenter Content for User Logout  Introduction Configuring WebCenter Content for Oracle Identity Cloud Service (IDCS) on OKE. Configuration information is provided in the following sections:\n Updating SSL.hostnameVerifier Property Configuring IDCS Security Provider Configuring WebCenter Content for User Logout  Updating SSL.hostnameVerifier Property To update SSL.hostnameVerifier property, do the following: This is necessary for the IDCS provider to access IDCS.\n  Stop all the servers in the domain including Administration server and all Managed WebLogic servers.\n  Update the SSL.hostnameVerifier property:\nedit the file \u0026lt;DOMAIN_HOME\u0026gt;//bin/setDomainEnv.sh: go to pv location file system and modify the file setDomainEnv.sh sample: /WCCFS/wccinfra/bin/setDomainEnv.sh\nOR\nAlternatively create or modify the file \u0026lt;DOMAIN_HOME\u0026gt;/\u0026lt;domain_name\u0026gt;/bin/setUserOverrides.sh. Add the SSL.hostnameVerifier property for the IDCS Authenticator: sample: /WCCFS/wccinfra/bin/setUserOverrides.sh\nEXTRA_JAVA_PROPERTIES=\u0026#34;${EXTRA_JAVA_PROPERTIES}-Dweblogic.security.SSL.hostnameVerifier=weblogic.security.utils.SSLWLSWildcardHostnameVerifier\u0026#34; export EXTRA_JAVA_PROPERTIES   Start the Administration server and all Managed WebLogic servers.\n  Configuring IDCS Security Provider   Log in to the IDCS administration console.\n  Create a trusted application. In the Add Confidential Application wizard:\n Enter the client name and the description (optional). Select the Configure this application as a client now option. To configure this application, expand the Client Configuration in the Configuration tab. In the Allowed Grant Types , select Client Credentials field the check box. In the Grant the client access to Identity Cloud Service Admin APIs section, click Add to add the APP Roles (application roles). You can add the Identity Domain Administrator role. Keep the default settings for the pages and click Finish. Record/Copy the Client ID and Client Secret.This is needed when you will create the IDCS provider. Activate the application.    Configuring Oracle Identity Cloud Integrator Provider To configure Identity Cloud Integrator Provider:\n  Log in to the WebLogic Server Administration console.\n  Select Security Realm in the Domain Structure pane.\n  On the Summary of Security Realms page, select the name of the realm (for example, myrealm). Click myrealm. The Settings for myrealm page appears.\n  On the Settings for Realm Name page, select Providers and then Authentication. To create a new Authentication Provider, in the Authentication Providers table, click New.\n  In the Create a New Authentication Provider page, enter the name of the authentication provider, for example, IDCSIntegrator and select the OracleIdentityCloudIntegrator type of authentication provider from the drop-down list and click OK.\n  In the Authentication Providers table, click the newly created Oracle Identity Cloud Integrator, IDCSIntegrator link.\n  In the Settings for IDCSIntegrator page, for the Control Flag field, select the Sufficient option from the drop-down list Click Save.\n  Go to the Provider Specific page to configure the additional attributes for the security provider. Enter the values for the following fields \u0026amp; Click Save:\n Host Port 443(default) select SSLEnabled Tenant Client Id Client Secret.   NOTE: If IDCS URL is idcs-abcde.identity.example.com, then IDCS host would be identity.example.com and tenant name would be idcs-abcde. Keep the default settings for other sections of the page.\n   Select Security Realm, then myrealm, and then Providers. In the Authentication Providers table, click Reorder.\n  In the Reorder Authentication Providers page, move IDCSIntegrator on the top and click OK.\n  In the Authentication Providers table, click the DefaultAuthenticator link. In the Settings for DefaultAuthenticator page, for the Control Flag field, select the Sufficient option from the drop-down list. Click Save.\n  All changes will be activated. Restart the Administration server.\n  Setting Up Trust between IDCS and WebLogic To set up trust between IDCS and WebLogic\n Import certificate in KSS store.  Run this from the Administration Server node. Get IDCS certificate: echo -n | openssl s_client -showcerts -servername \u0026lt;IDCS_URL\u0026gt; -connect \u0026lt;IDCS_URL\u0026gt;:443|sed -ne \u0026#39;/-BEGIN CERTIFICATE-/,/-END CERTIFICATE-/p\u0026#39; \u0026gt; /tmp/idcs_cert_chain.crt #sample echo -n | openssl s_client -showcerts -servername xyz.identity.oraclecloud.com -connect idcs-xyz.identity.oraclecloud.com:443|sed -ne \u0026#39;/-BEGIN CERTIFICATE-/,/-END CERTIFICATE-/p\u0026#39; \u0026gt; /tmp/idcs_cert_chain.crt #copy the certificate inside the admin_pod kubectl cp /tmp/idcs_cert_chain.crt wccns/xyz-adminserver:/u01/idcs_cert_chain.crt  Import certificate. Run \u0026lt;ORACLE_HOME\u0026gt;/oracle_common/common/bin/wlst.sh file. connect(\u0026#39;weblogic\u0026#39;,\u0026#39;Welcome_1\u0026#39;,\u0026#39;t3://\u0026lt;WEBLOGIC_HOST\u0026gt;:7001\u0026#39;) svc=getOpssService(name=\u0026#39;KeyStoreService\u0026#39;) svc.importKeyStoreCertificate(appStripe=\u0026#39;system\u0026#39;,name=\u0026#39;trust\u0026#39;,password=\u0026#39;\u0026#39;,alias=\u0026#39;idcs_cert_chain\u0026#39;,type=\u0026#39;TrustedCertificate\u0026#39;,filepath=\u0026#39;/tmp/idcs_cert_chain.crt\u0026#39;,keypassword=\u0026#39;\u0026#39;) syncKeyStores(appStripe=\u0026#39;system\u0026#39;,keystoreFormat=\u0026#39;KSS\u0026#39;) #sample $./wlst.sh wls:/offline\u0026gt; connect(\u0026#39;weblogic\u0026#39;,\u0026#39;welcome\u0026#39;,\u0026#39;t3://xyz-adminserver:7001\u0026#39;) wls:/wccinfra/serverConfig/\u0026gt; svc=getOpssService(name=\u0026#39;KeyStoreService\u0026#39;) wls:/wccinfra/serverConfig/\u0026gt;svc.importKeyStoreCertificate(appStripe=\u0026#39;system\u0026#39;,name=\u0026#39;trust\u0026#39;,password=\u0026#39;\u0026#39;,alias=\u0026#39;idcs_cert_chain\u0026#39;,type=\u0026#39;TrustedCertificate\u0026#39;,filepath=\u0026#39;/u01/idcs_cert_chain.crt\u0026#39;,keypassword=\u0026#39;\u0026#39;) wls:/wccinfra/domainRuntime/\u0026gt;syncKeyStores(appStripe=\u0026#39;system\u0026#39;,keystoreFormat=\u0026#39;KSS\u0026#39;)  exit()   Restart the Administration server and Managed servers  Creating Admin User in IDCS Administration Console for WebCenter Content It is important to create the Admin user in IDCS because once the Managed servers are configured for SAML, the domain admin user (typically weblogic user) will not be able to log into the Managed servers.\nTo create WebLogic Admin user in IDCS for WebCenter Content JaxWS connection:\n Go to the Groups tab and create Administrators and sysmanager roles in IDCS. Go to the Users tab and create a wls admin user, for example, weblogic and assign it to Administrators and sysmanager groups. Restart all the Managed servers.  Managing Group Memberships, Roles, and Accounts This will require modifying OPSS and libOVD to access IDCS. The following steps are required if using IDCS for user authorization. Do not run these steps if you are using IDCS only for user authentication. Ensure that all the servers are stopped (including Administration) before proceeding with the following steps:\n NOTE: Shutdown all the servers using WebLogic Server Administration Console. Please keep in mind - kubectl patch domain command is the recommended way for starting/stopping pods. Please refrain from using WebLogic Server Administration Console for the same, anywhere else.\n   Run the following script:\n#exec the Administration server kubectl exec -n wccns -it wccinfra-adminserver -- /bin/bash #Run the wlst.sh cd /u01/oracle/oracle_common/common/bin/ ./wlst.sh  NOTE: It\u0026rsquo;s not required to connect to WebLogic Administration Server.\n   Read the domain:\nreadDomain(\u0026lt;DOMAIN_HOME\u0026gt;) #sample wls:/offline\u0026gt; readDomain(\u0026#39;/u01/oracle/user_projects/domains/wccinfra\u0026#39;)   Add the template:\naddTemplate(\u0026lt;MIDDLEWARE_HOME\u0026gt;/oracle_common/common/templates/wls/oracle.opss_scim_template.jar\u0026#34;) #sample wls:/offline/wccinfra\u0026gt;addTemplate(\u0026#39;/u01/oracle/oracle_common/common/templates/wls/oracle.opss_scim_template.jar\u0026#39;)  NOTE: This step may throw a warning, which can be ignored. The addTemplate is deprecated. Use selectTemplate followed by loadTemplates in place of addTemplate.\n   Update the domain:\nupdateDomain() #sample wls:/offline/wccinfra\u0026gt; updateDomain()   Close the domain:\ncloseDomain() #sample wls:/offline/wccinfra\u0026gt; closeDomain()   Exit from the Administration server container:\nexit   Start the servers (Administration and Managed).\n  Configuring WebCenter Content for User Logout If the Logout link is selected, you will be re-authenticated by SAML. To be able to select the Logout link:\n Log in to WebCenter Content Server as an administrator. Select Administration, then Admin Server, and then General Configuration. In the Additional Configuration Variables pane, add the following parameter: EXTRA_JAVA_PROPERTIES=\u0026#34;${EXTRA_JAVA_PROPERTIES}-Dweblogic.security.SSL.hostnameVerifier=weblogic.security.utils.SSLWLSWildcardHostnameVerifier\u0026#34;  Click Save. Restart the Administration and Managed servers.  "
},
{
	"uri": "/fmw-kubernetes/wccontent-domains/create-or-update-image/",
	"title": "Create or update an image",
	"tags": [],
	"description": "Create or update an Oracle WebCenter Content Docker image used for deploying Oracle WebCenter Content domains. An Oracle WebCenter Content Docker image can be created using the WebLogic Image Tool or using the Dockerfile approach.",
	"content": "If you have access to the My Oracle Support (MOS), and there is a need to build a new image with a patch (bundle or interim), it is recommended to use the WebLogic Image Tool to build an Oracle WebCenter Content image for production deployments.\n Create or update an Oracle WebCenter Content Docker image using the WebLogic Image Tool  Set up the WebLogic Image Tool Create an image Update an image   Create an Oracle WebCenter Content Docker image using Dockerfile  Create or update an Oracle WebCenter Content Docker image using the WebLogic Image Tool Using the WebLogic Image Tool, you can create a new Oracle WebCenter Content Docker image (can include patches as well) or update an existing image with one or more patches (bundle patch and interim patches).\n Recommendations:\n Use create for creating a new Oracle WebCenter Content Docker image either:  without any patches or, containing the Oracle WebCenter Content binaries, bundle patch and interim patches. This is the recommended approach if you have access to the Oracle WebCenter Content patches because it optimizes the size of the image.   Use update for patching an existing Oracle WebCenter Content Docker image with a single interim patch. Note that the patched image size may increase considerably due to additional image layers introduced by the patch application tool.   Set up the WebLogic Image Tool  Prerequisites Set up the WebLogic Image Tool Validate setup WebLogic Image Tool build directory WebLogic Image Tool cache Set up additional build scripts  Prerequisites Verify that your environment meets the following prerequisites:\n Docker client and daemon on the build machine, with minimum Docker version 18.03.1.ce. Bash version 4.0 or later, to enable the command complete feature. JAVA_HOME environment variable set to the appropriate JDK location.  Set up the WebLogic Image Tool To set up the WebLogic Image Tool:\n  Create a working directory and change to it. In these steps, this directory is imagetool-setup.\n$ mkdir imagetool-setup $ cd imagetool-setup   Download the latest version of the WebLogic Image Tool from the releases page.\n  Unzip the release ZIP file to the imagetool-setup directory.\n  Execute the following commands to set up the WebLogic Image Tool on a Linux environment:\n$ cd imagetool-setup/imagetool/bin $ source setup.sh   Validate setup To validate the setup of the WebLogic Image Tool:\n  Enter the following command to retrieve the version of the WebLogic Image Tool:\n$ imagetool --version   Enter imagetool then press the Tab key to display the available imagetool commands:\n$ imagetool \u0026lt;TAB\u0026gt; cache create help rebase update   WebLogic Image Tool build directory The WebLogic Image Tool creates a temporary Docker context directory, prefixed by wlsimgbuilder_temp, every time the tool runs. Under normal circumstances, this context directory will be deleted. However, if the process is aborted or the tool is unable to remove the directory, it is safe for you to delete it manually. By default, the WebLogic Image Tool creates the Docker context directory under the user\u0026rsquo;s home directory. If you prefer to use a different directory for the temporary context, set the environment variable WLSIMG_BLDDIR:\n$ export WLSIMG_BLDDIR=\u0026#34;/path/to/buid/dir\u0026#34; WebLogic Image Tool cache The WebLogic Image Tool maintains a local file cache store. This store is used to look up where the Java, WebLogic Server installers, and WebLogic Server patches reside in the local file system. By default, the cache store is located in the user\u0026rsquo;s $HOME/cache directory. Under this directory, the lookup information is stored in the .metadata file. All automatically downloaded patches also reside in this directory. You can change the default cache store location by setting the environment variable WLSIMG_CACHEDIR:\n$ export WLSIMG_CACHEDIR=\u0026#34;/path/to/cachedir\u0026#34; Set up additional build scripts Creating an Oracle WebCenter Content Docker image using the WebLogic Image Tool requires additional container scripts for Oracle WebCenter Content domains.\n  Clone the docker-images repository to set up those scripts. In these steps, this directory is DOCKER_REPO:\n$ cd imagetool-setup $ git clone https://github.com/oracle/docker-images.git   Copy the additional WebLogic Image Tool build files from the WebLogic Kubernetes Operator source repository to the imagetool-setup location:\n$ mkdir -p imagetool-setup/docker-images/WebCenterContent/imagetool/12.2.1.4.0 $ cd imagetool-setup/docker-images/WebCenterContent/imagetool/12.2.1.4.0 $ cp -rf ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/imagetool-scripts/* .   Create an image After setting up the WebLogic Image Tool and required build scripts, follow these steps to use the WebLogic Image Tool to create a new Oracle WebCenter Content Docker image.\nDownload the Oracle WebCenter Content installation binaries and patches You must download the required Oracle WebCenter Content installation binaries and patches as listed below from the Oracle Software Delivery Cloud and save them in a directory of your choice. In these steps, this directory is download location.\n  Click here to see the sample list of installation binaries and patches:     JDK:\n jdk-8u251-linux-x64.tar.gz    Fusion MiddleWare Infrastructure installer:\n fmw_12.2.1.4.0_infrastructure_generic.jar    WebCenter Content installers:\n fmw_12.2.1.4.0_wccontent.jar    Fusion MiddleWare Infrastructure patches:\n p28186730_139424_Generic-23574493.zip (Opatch)    WebCenter Content patches:\n p31390302_122140_Generic.zip (wcc)       Note: This is a sample list of patches. You must get the appropriate list of patches for your Oracle WebCenter Content image.\n Update required build files The following files available in the code repository location \u0026lt;imagetool-setup-location\u0026gt;/docker-images/OracleWebCenterContent/imagetool/12.2.1.4.0 are used for creating the image.\n additionalBuildCmds.txt buildArgs    In the buildArgs file, update all the occurrences of %DOCKER_REPO% with the docker-images repository location, which is the complete path of imagetool-setup/docker-images.\nFor example, update:\n%DOCKER_REPO%/OracleWebCenterContent/imagetool/12.2.1.4.0/\nto:\n\u0026lt;imagetool-setup-location\u0026gt;/docker-images/OracleWebCenterContent/imagetool/12.2.1.4.0/\n  Similarly, update the placeholders %JDK_VERSION% and %BUILDTAG% with appropriate values.\n  Create the image   Add a JDK package to the WebLogic Image Tool cache:\n$ imagetool cache addInstaller --type jdk --version 8u251 --path \u0026lt;download location\u0026gt;/jdk-8u251-linux-x64.tar.gz   Add the downloaded installation binaries to the WebLogic Image Tool cache:\n$ imagetool cache addInstaller --type fmw --version 12.2.1.4.0 --path \u0026lt;download location\u0026gt;/fmw_12.2.1.4.0_infrastructure.jar $ imagetool cache addInstaller --type wcc --version 12.2.1.4.0 --path \u0026lt;download location\u0026gt;/fmw_12.2.1.4.0_wccontent.jar   Add the downloaded patches to the WebLogic Image Tool cache:\n  Click here to see the commands to add patches in to the cache:   ``` bash $ imagetool cache addEntry --key p33578966_122140_Generic --path \u0026lt;download location\u0026gt;/p33578966_122140_Generic.zip $ imagetool cache addEntry --key 28186730_13.9.4.2.8 --path \u0026lt;download location\u0026gt;/p28186730_139428_Generic-24497645.zip ```      Update the patches list to buildArgs.\nTo the create command in the buildArgs file, append the Oracle WebCenter Content patches list using the --patches flag and Opatch patch using the --opatchBugNumber flag. Sample options for the list of patches above are:\n--patches 33578966_12.2.1.4.0 --opatchBugNumber=28186730_13.9.4.2.8 Example buildArgs file after appending product\u0026rsquo;s list of patches and Opatch patch:\ncreate --jdkVersion=8u251 --type WCC --version=12.2.1.4.0 --tag=oracle/wccontent_create_1015:12.2.1.4.0 --pull --chown oracle:root --additionalBuildCommands \u0026lt;imagetool-setup-location\u0026gt;/docker-images/OracleWebCenterContent/imagetool/12.2.1.4.0/additionalBuildCmds.txt --additionalBuildFiles \u0026lt;imagetool-setup-location\u0026gt;/docker-images/OracleWebCenterContent/dockerfiles/12.2.1.4.0/container-scripts --patches 33578966_12.2.1.4.0 --opatchBugNumber=28186730_13.9.4.2.8 Refer to this page for the complete list of options available with the WebLogic Image Tool create command.\n  Enter the following command to create the Oracle WebCenter Content image:\n$ imagetool @\u0026lt;absolute path to `buildargs` file\u0026gt;\u0026#34;     Click here to see the sample Dockerfile generated with the imagetool command.   ########## BEGIN DOCKERFILE ########## # # Copyright (c) 2019, 2021, Oracle and/or its affiliates. # # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # # FROM ghcr.io/oracle/oraclelinux:7-slim as os_update LABEL com.oracle.weblogic.imagetool.buildid=\u0026#34;f46ab190-077e-4ed7-b747-7bb170fe592c\u0026#34; USER root RUN yum -y --downloaddir=/tmp/imagetool install gzip tar unzip libaio jq hostname \\  \u0026amp;\u0026amp; yum -y --downloaddir=/tmp/imagetool clean all \\  \u0026amp;\u0026amp; rm -rf /var/cache/yum/* \\  \u0026amp;\u0026amp; rm -rf /tmp/imagetool ## Create user and group RUN if [ -z \u0026#34;$(getent group root)\u0026#34; ]; then hash groupadd \u0026amp;\u0026gt; /dev/null \u0026amp;\u0026amp; groupadd root || exit -1 ; fi \\  \u0026amp;\u0026amp; if [ -z \u0026#34;$(getent passwd oracle)\u0026#34; ]; then hash useradd \u0026amp;\u0026gt; /dev/null \u0026amp;\u0026amp; useradd -g root oracle || exit -1; fi \\  \u0026amp;\u0026amp; mkdir -p /u01 \\  \u0026amp;\u0026amp; chown oracle:root /u01 \\  \u0026amp;\u0026amp; chmod 775 /u01 # Install Java FROM os_update as jdk_build LABEL com.oracle.weblogic.imagetool.buildid=\u0026#34;f46ab190-077e-4ed7-b747-7bb170fe592c\u0026#34; ENV JAVA_HOME=/u01/jdk COPY --chown=oracle:root jdk-8u251-linux-x64.tar.gz /tmp/imagetool/ USER oracle RUN tar xzf /tmp/imagetool/jdk-8u251-linux-x64.tar.gz -C /u01 \\  \u0026amp;\u0026amp; $(test -d /u01/jdk* \u0026amp;\u0026amp; mv /u01/jdk* /u01/jdk || mv /u01/graal* /u01/jdk) \\  \u0026amp;\u0026amp; rm -rf /tmp/imagetool \\  \u0026amp;\u0026amp; rm -f /u01/jdk/javafx-src.zip /u01/jdk/src.zip # Install Middleware FROM os_update as wls_build LABEL com.oracle.weblogic.imagetool.buildid=\u0026#34;f46ab190-077e-4ed7-b747-7bb170fe592c\u0026#34; ENV JAVA_HOME=/u01/jdk \\  ORACLE_HOME=/u01/oracle \\  OPATCH_NO_FUSER=true RUN mkdir -p /u01/oracle \\  \u0026amp;\u0026amp; mkdir -p /u01/oracle/oraInventory \\  \u0026amp;\u0026amp; chown oracle:root /u01/oracle/oraInventory \\  \u0026amp;\u0026amp; chown oracle:root /u01/oracle COPY --from=jdk_build --chown=oracle:root /u01/jdk /u01/jdk/ COPY --chown=oracle:root fmw_12.2.1.4.0_infrastructure_generic.jar fmw.rsp /tmp/imagetool/ COPY --chown=oracle:root fmw_12.2.1.4.0_wccontent.jar wcc.rsp /tmp/imagetool/ COPY --chown=oracle:root oraInst.loc /u01/oracle/ USER oracle RUN echo \u0026#34;INSTALLING MIDDLEWARE\u0026#34; \\  \u0026amp;\u0026amp; echo \u0026#34;INSTALLING fmw\u0026#34; \\  \u0026amp;\u0026amp; \\  /u01/jdk/bin/java -Xmx1024m -jar /tmp/imagetool/fmw_12.2.1.4.0_infrastructure_generic.jar -silent ORACLE_HOME=/u01/oracle \\  -responseFile /tmp/imagetool/fmw.rsp -invPtrLoc /u01/oracle/oraInst.loc -ignoreSysPrereqs -force -novalidation \\  \u0026amp;\u0026amp; echo \u0026#34;INSTALLING wcc\u0026#34; \\  \u0026amp;\u0026amp; \\  /u01/jdk/bin/java -Xmx1024m -jar /tmp/imagetool/fmw_12.2.1.4.0_wccontent.jar -silent ORACLE_HOME=/u01/oracle \\  -responseFile /tmp/imagetool/wcc.rsp -invPtrLoc /u01/oracle/oraInst.loc -ignoreSysPrereqs -force -novalidation \\  \u0026amp;\u0026amp; chmod -R g+r /u01/oracle FROM os_update as final_build ARG ADMIN_NAME ARG ADMIN_HOST ARG ADMIN_PORT ARG MANAGED_SERVER_PORT ENV ORACLE_HOME=/u01/oracle \\  JAVA_HOME=/u01/jdk \\  PATH=${PATH}:/u01/jdk/bin:/u01/oracle/oracle_common/common/bin:/u01/oracle/wlserver/common/bin:/u01/oracle LABEL com.oracle.weblogic.imagetool.buildid=\u0026#34;f46ab190-077e-4ed7-b747-7bb170fe592c\u0026#34; COPY --from=jdk_build --chown=oracle:root /u01/jdk /u01/jdk/ COPY --from=wls_build --chown=oracle:root /u01/oracle /u01/oracle/ USER oracle WORKDIR /u01/oracle #ENTRYPOINT /bin/bash ENV ORACLE_HOME=/u01/oracle \\  VOLUME_DIR=/u01/oracle/user_projects \\  SCRIPT_FILE=/u01/oracle/container-scripts/* \\  USER_MEM_ARGS=\u0026#34;-Djava.security.egd=file:/dev/./urandom\u0026#34; \\  PATH=$PATH:$JAVA_HOME/bin:$ORACLE_HOME/oracle_common/common/bin:/u01/oracle/wlserver/common/bin:/u01/oracle/container-scripts USER root RUN mkdir -p $VOLUME_DIR \u0026amp;\u0026amp; \\  mkdir -p /u01/oracle/container-scripts \u0026amp;\u0026amp; \\  mkdir -p /u01/oracle/silent-install-files-tmp/config \u0026amp;\u0026amp; \\  mkdir -p /u01/oracle/logs \u0026amp;\u0026amp; \\  chown oracle:root -R /u01 $VOLUME_DIR \u0026amp;\u0026amp; \\  chmod a+xr /u01 COPY --chown=oracle:root files/container-scripts/ /u01/oracle/container-scripts/ RUN chmod +xr $SCRIPT_FILE USER oracle EXPOSE $UCM_PORT $UCM_INTRADOC_PORT $IBR_INTRADOC_PORT $IBR_PORT $ADMIN_PORT WORKDIR ${ORACLE_HOME} CMD [\u0026#34;/u01/oracle/container-scripts/createDomainandStartAdmin.sh\u0026#34;] ########## END DOCKERFILE ##########      Check the created image using the docker images command:\n$ docker images | grep wcc   Update an image After setting up the WebLogic Image Tool and required build scripts, use the WebLogic Image Tool to update an existing Oracle WebCenter Content Docker image:\n  Enter the following command for each patch to add the required patch(es) to the WebLogic Image Tool cache:\n$ cd \u0026lt;imagetool-setup\u0026gt; $ imagetool cache addEntry --key=33578966_12.2.1.4.0 --value \u0026lt;downloaded-patches-location\u0026gt;/p33578966_122140_Generic.zip [INFO ] Added entry 33578966_12.2.1.4.0=\u0026lt;downloaded-patches-location\u0026gt;/p33578966_122140_Generic.zip   Provide the following arguments to the WebLogic Image Tool update command:\n –-fromImage - Identify the image that needs to be updated. In the example below, the image to be updated is wccontent:12.2.1.4.0. –-patches - Multiple patches can be specified as a comma-separated list. --tag - Specify the new tag to be applied for the image being built.  Refer here for the complete list of options available with the WebLogic Image Tool update command.\n Note: The WebLogic Image Tool cache should have the latest OPatch zip. The WebLogic Image Tool will update the OPatch if it is not already updated in the image.\n Examples     Click here to see the example `update` command:    # If you are using a pre-built Oracle WebCenter Content image, obtained from My Oracle Support, then please use this command: $ imagetool update --fromImage oracle/wccontent:12.2.1.4.0 --tag=oracle/wccontent_update_1015:12.2.1.4.0 --patches=33578966_12.2.1.4.0 --opatchBugNumber=28186730_13.9.4.2.8 # In case, you chose to build an Oracle WebCenter Content image, please use the command given below: $ imagetool update --chown oracle:root --fromImage oracle/wccontent:12.2.1.4.0 --tag=oracle/wccontent_update_1015:12.2.1.4.0 --patches=33578966_12.2.1.4.0 --opatchBugNumber=28186730_13.9.4.2.8     Check the built image using the docker images command: $ docker images | grep wcc   Create an Oracle WebCenter Content Docker image using Dockerfile For test and development purposes, you can create an Oracle WebCenter Content image using the Dockerfile. Consult the README file for important prerequisite steps, such as building or pulling the Server JRE Docker image, Oracle FMW Infrastructure Docker image, and downloading the Oracle WebCenter Content installer and bundle patch binaries.\nA prebuilt Oracle Fusion Middleware Infrastructure image, container-registry.oracle.com/middleware/fmw-infrastructure:12.2.1.4-210407, is available at container-registry.oracle.com. We recommend that you pull and rename this image to build the Oracle WebCenter Content image.\n$ docker pull container-registry.oracle.com/middleware/fmw-infrastructure:12.2.1.4-210407 $ docker tag container-registry.oracle.com/middleware/fmw-infrastructure:12.2.1.4-210407 oracle/fmw-infrastructure:12.2.1.4.0 Follow these steps to build an Oracle WebCenter Content image :\n  Make a local clone of the sample repository:\n$ git clone https://github.com/oracle/docker-images   Download the Oracle WebCenter Content installer from the Oracle Technology Network or e-delivery.\n Note: Copy the installer binaries to the same location as the Dockerfile.\n   Create the Oracle WebCenter Content image by running the provided script:\n$ cd docker-images/OracleWebCenterContent/dockerfiles $ ./buildDockerImage.sh -v 12.2.1.4 -s The image produced will be named oracle/wccontent:12.2.1.4. The samples and instructions assume the Oracle WebCenter Content image is named wccontent:12.2.1.4.0. You must rename your image to match this name, or update the samples to refer to the image you created.\n$ docker tag oracle/wccontent:12.2.1.4 wccontent:12.2.1.4.0   "
},
{
	"uri": "/fmw-kubernetes/wccontent-domains/adminguide/logging-fluentd-setup/",
	"title": "Publish logs to Elasticsearch Using Fluentd",
	"tags": [],
	"description": "Configure a WebLogic domain to use Fluentd to send log information to Elasticsearch.",
	"content": "Introduction This page describes to how to configure a WebLogic domain to use Fluentd to send log information to Elasticsearch. Here’s the general mechanism for how this works:\n fluentd runs as a separate container in the Administration Server and Managed Server pods The log files reside on a volume that is shared between the weblogic-server and fluentd containers fluentd tails the domain logs files and exports them to Elasticsearch A ConfigMap contains the filter and format rules for exporting log records.  Create fluentd configuration Create a ConfigMap named fluentd-config in the namespace of the domain. The ConfigMap contains the parsing rules and Elasticsearch configuration. Here’s an explanation of some elements defined in the ConfigMap:\n The @type tail indicates that tail will be used to obtain updates to the log file The path of the log file is obtained from the LOG_PATH environment variable that is defined in the fluentd container The tag value of log records is obtained from the DOMAIN_UID environment variable that is defined in the fluentd container The parse section defines how to interpret and tag each element of a log record The match section contains the configuration information for connecting to Elasticsearch and defines the index name of each record to be the domainUID  Here is a sample configmap for fluentd configuration,\n  Click here to see sample configmap for fluentd configuration `fluentd_configmap.yaml`.   apiVersion: v1 kind: ConfigMap metadata: labels: weblogic.domainUID: wccinfra weblogic.resourceVersion: domain-v2 name: fluentd-config namespace: wccns data: fluentd.conf: | \u0026lt;match fluent.**\u0026gt; @type null \u0026lt;/match\u0026gt; \u0026lt;source\u0026gt; @type tail path \u0026quot;#{ENV['LOG_PATH']}\u0026quot; pos_file /tmp/server.log.pos read_from_head true tag \u0026quot;#{ENV['DOMAIN_UID']}\u0026quot; # multiline_flush_interval 20s \u0026lt;parse\u0026gt; @type multiline format_firstline /^####/ format1 /^####\u0026lt;(?\u0026lt;timestamp\u0026gt;(.*?))\u0026gt;/ format2 / \u0026lt;(?\u0026lt;level\u0026gt;(.*?))\u0026gt;/ format3 / \u0026lt;(?\u0026lt;subSystem\u0026gt;(.*?))\u0026gt;/ format4 / \u0026lt;(?\u0026lt;serverName\u0026gt;(.*?))\u0026gt;/ format5 / \u0026lt;(?\u0026lt;serverName2\u0026gt;(.*?))\u0026gt;/ format6 / \u0026lt;(?\u0026lt;threadName\u0026gt;(.*?))\u0026gt;/ format7 / \u0026lt;(?\u0026lt;info1\u0026gt;(.*?))\u0026gt;/ format8 / \u0026lt;(?\u0026lt;info2\u0026gt;(.*?))\u0026gt;/ format9 / \u0026lt;(?\u0026lt;info3\u0026gt;(.*?))\u0026gt;/ format10 / \u0026lt;(?\u0026lt;sequenceNumber\u0026gt;(.*?))\u0026gt;/ format11 / \u0026lt;(?\u0026lt;severity\u0026gt;(.*?))\u0026gt;/ format12 / \u0026lt;(?\u0026lt;messageID\u0026gt;(.*?))\u0026gt;/ format13 / \u0026lt;(?\u0026lt;message\u0026gt;(.*?))\u0026gt;/ \u0026lt;/parse\u0026gt; \u0026lt;/source\u0026gt; \u0026lt;match **\u0026gt; @type elasticsearch host \u0026quot;#{ENV['ELASTICSEARCH_HOST']}\u0026quot; port \u0026quot;#{ENV['ELASTICSEARCH_PORT']}\u0026quot; user \u0026quot;#{ENV['ELASTICSEARCH_USER']}\u0026quot; password \u0026quot;#{ENV['ELASTICSEARCH_PASSWORD']}\u0026quot; index_name \u0026quot;#{ENV['DOMAIN_UID']}\u0026quot; \u0026lt;/match\u0026gt;    Create the ConfigMap using the following command\n$kubectl create -f fluentd_configmap.yaml Mount fluentd configuration - Configmap as volume in the WebLogic container. Edit the domain definition and configure a volume for the ConfigMap containing the fluentd configuration.\n$kubectl edit domain -n wccns Below sample yaml code add Configmap as volume,\n volumes: - name: weblogic-domain-storage-volume persistentVolumeClaim: claimName: wccinfra-domain-pvc - configMap: defaultMode: 420 name: fluentd-config name: fluentd-config-volume Add fluentd container to WebLogic Server pods Add a \u0026ldquo;fluentd container yaml\u0026rdquo; to the domain under serverPod: section that will run fluentd in the Administration Server and Managed Server pods.\nNotice the container definition:\n Defines a LOG_PATH environment variable that points to the log location of WebLogic servers. Defines ELASTICSEARCH_HOST, ELASTICSEARCH_PORT, ELASTICSEARCH_USER, and ELASTICSEARCH_PASSWORD environment variables. Has volume mounts for the fluentd-config ConfigMap and the volume containing the domain logs.  $kubectl edit domain -n wccns    Click here to see sample fluentd container yaml `fluentd container`.   containers: - args: - -c - /etc/fluent.conf env: - name: DOMAIN_UID valueFrom: fieldRef: fieldPath: metadata.labels['weblogic.domainUID'] - name: SERVER_NAME valueFrom: fieldRef: fieldPath: metadata.labels['weblogic.serverName'] - name: LOG_PATH value: /u01/oracle/user_projects/domains/logs/wccinfra/$(SERVER_NAME).log - name: FLUENTD_CONF value: fluentd.conf - name: FLUENT_ELASTICSEARCH_SED_DISABLE value: \u0026quot;true\u0026quot; - name: ELASTICSEARCH_HOST value: elasticsearch.default.svc.cluster.local - name: ELASTICSEARCH_PORT value: \u0026quot;9200\u0026quot; - name: ELASTICSEARCH_USER value: elastic - name: ELASTICSEARCH_PASSWORD value: changeme image: fluent/fluentd-kubernetes-daemonset:v1.3.3-debian-elasticsearch-1.3 imagePullPolicy: IfNotPresent name: fluentd resources: {} volumeMounts: - mountPath: /fluentd/etc/fluentd.conf name: fluentd-config-volume subPath: fluentd.conf - mountPath: /u01/oracle/user_projects/domains name: weblogic-domain-storage-volume    Restart WebLogic Servers To restart the servers, edit the domain and change serverStartPolicy to NEVER for the WebLogic servers to shutdown\n$kubectl edit domain -n wccns After all the servers are shutdown edit domain again and set serverStartPolicy to IF_NEEDED for the servers to start again.\nCreate index pattern in Kibana Create an index pattern \u0026ldquo;wccinfra*\u0026rdquo; in Kibana \u0026gt; Management. After the server starts, you will be able to see the log data in the Kibana dashboard,\n"
},
{
	"uri": "/fmw-kubernetes/soa-domains/create-or-update-image/",
	"title": "Create or update an image",
	"tags": [],
	"description": "Create or update an Oracle SOA Suite Docker image used for deploying Oracle SOA Suite domains. An Oracle SOA Suite Docker image can be created using the WebLogic Image Tool or using the Dockerfile approach.",
	"content": "If you have access to the My Oracle Support (MOS), and there is a need to build a new image with a patch (bundle or interim), it is recommended to use the WebLogic Image Tool to build an Oracle SOA Suite image for production deployments.\n Create or update an Oracle SOA Suite Docker image using the WebLogic Image Tool  Set up the WebLogic Image Tool Create an image Update an image   Create an Oracle SOA Suite Docker image using Dockerfile  Create or update an Oracle SOA Suite Docker image using the WebLogic Image Tool Using the WebLogic Image Tool, you can create a new Oracle SOA Suite Docker image (can include patches as well) or update an existing image with one or more patches (bundle patch and interim patches).\n Recommendations:\n Use create for creating a new Oracle SOA Suite Docker image either:  without any patches or, containing the Oracle SOA Suite binaries, bundle patch and interim patches. This is the recommended approach if you have access to the Oracle SOA Suite patches because it optimizes the size of the image.   Use update for patching an existing Oracle SOA Suite Docker image with a single interim patch. Note that the patched image size may increase considerably due to additional image layers introduced by the patch application tool.   Set up the WebLogic Image Tool  Prerequisites Set up the WebLogic Image Tool Validate setup WebLogic Image Tool build directory WebLogic Image Tool cache Set up additional build scripts  Prerequisites Verify that your environment meets the following prerequisites:\n Docker client and daemon on the build machine, with minimum Docker version 18.03.1.ce. Bash version 4.0 or later, to enable the command complete feature. JAVA_HOME environment variable set to the appropriate JDK location.  Set up the WebLogic Image Tool To set up the WebLogic Image Tool:\n  Create a working directory and change to it. In these steps, this directory is imagetool-setup.\n$ mkdir imagetool-setup $ cd imagetool-setup   Download the latest version of the WebLogic Image Tool from the releases page.\n  Unzip the release ZIP file to the imagetool-setup directory.\n  Execute the following commands to set up the WebLogic Image Tool on a Linux environment:\n$ cd imagetool-setup/imagetool/bin $ source setup.sh   Validate setup To validate the setup of the WebLogic Image Tool:\n  Enter the following command to retrieve the version of the WebLogic Image Tool:\n$ imagetool --version   Enter imagetool then press the Tab key to display the available imagetool commands:\n$ imagetool \u0026lt;TAB\u0026gt; cache create help rebase update   WebLogic Image Tool build directory The WebLogic Image Tool creates a temporary Docker context directory, prefixed by wlsimgbuilder_temp, every time the tool runs. Under normal circumstances, this context directory will be deleted. However, if the process is aborted or the tool is unable to remove the directory, it is safe for you to delete it manually. By default, the WebLogic Image Tool creates the Docker context directory under the user\u0026rsquo;s home directory. If you prefer to use a different directory for the temporary context, set the environment variable WLSIMG_BLDDIR:\n$ export WLSIMG_BLDDIR=\u0026#34;/path/to/buid/dir\u0026#34; WebLogic Image Tool cache The WebLogic Image Tool maintains a local file cache store. This store is used to look up where the Java, WebLogic Server installers, and WebLogic Server patches reside in the local file system. By default, the cache store is located in the user\u0026rsquo;s $HOME/cache directory. Under this directory, the lookup information is stored in the .metadata file. All automatically downloaded patches also reside in this directory. You can change the default cache store location by setting the environment variable WLSIMG_CACHEDIR:\n$ export WLSIMG_CACHEDIR=\u0026#34;/path/to/cachedir\u0026#34; Set up additional build scripts Creating an Oracle SOA Suite Docker image using the WebLogic Image Tool requires additional container scripts for Oracle SOA Suite domains.\n  Clone the docker-images repository to set up those scripts. In these steps, this directory is DOCKER_REPO:\n$ cd imagetool-setup $ git clone https://github.com/oracle/docker-images.git   Copy the additional WebLogic Image Tool build files from the operator source repository to the imagetool-setup location:\n$ mkdir -p imagetool-setup/docker-images/OracleSOASuite/imagetool/12.2.1.4.0 $ cd imagetool-setup/docker-images/OracleSOASuite/imagetool/12.2.1.4.0 $ cp -rf ${WORKDIR}/imagetool-scripts/* .    Note: If you want to create the image continue with the following steps, otherwise to update the image see update an image.\n Create an image After setting up the WebLogic Image Tool and required build scripts, follow these steps to use the WebLogic Image Tool to create a new Oracle SOA Suite Docker image.\nDownload the Oracle SOA Suite installation binaries and patches You must download the required Oracle SOA Suite installation binaries and patches as listed below from the Oracle Software Delivery Cloud and save them in a directory of your choice. In these steps, this directory is download location.\nThe installation binaries and patches required for release 22.2.2 are:\n  JDK:\n jdk-8u331-linux-x64.tar.gz    Fusion Middleware Infrastructure installer:\n fmw_12.2.1.4.0_infrastructure.jar    Oracle SOA Suite installers:\n fmw_12.2.1.4.0_soa.jar fmw_12.2.1.4.0_osb.jar fmw_12.2.1.4.0_b2bhealthcare.jar    In this release, Oracle B2B is not supported to be configured, but the installer is required for completeness.\n   Fusion Middleware Infrastructure patches:\n p28186730_139428_Generic.zip (OPATCH 13.9.4.2.8 FOR EM 13.4, 13.5 AND FMW/WLS 12.2.1.3.0, 12.2.1.4.0 AND 14.1.1.0.0) p34012040_122140_Generic.zip (WLS PATCH SET UPDATE 12.2.1.4.220329) p34044738_122140_Generic.zip (FMW Thirdparty Bundle Patch 12.2.1.4.220406) p33950717_122140_Generic.zip (OPSS BUNDLE PATCH 12.2.1.4.220311) p33618954_122140_Generic.zip (OWSM BUNDLE PATCH 12.2.1.4.211129) p33958532_122140_Generic.zip (ADF BUNDLE PATCH 12.2.1.4.220314) p33902201_122140_Generic.zip (Coherence 12.2.1.4 Cumulative Patch 12 (12.2.1.4.13)) p33093748_122140_Generic.zip (FMW PLATFORM 12.2.1.4.0 SPU FOR APRCPU2021) p31544353_122140_Linux-x86-64.zip (ADR FOR WEBLOGIC SERVER 12.2.1.4.0 JULY CPU 2020) p32720458_122140_Generic.zip (JDBC One Off) p33678607_204070122_Generic.zip (FMW Thirdparty One Off) p33546536_12214211129_Generic.zip (OWSM One Off) p34077658_122140_Generic.zip (RDA release 22.2-20220307 for FMW 12.2.1.4.0) p34065178_122140_Generic.zip (OVD One Off)    Oracle SOA Suite and Oracle Service Bus patches\n p33965482_122140_Generic.zip (SOA BUNDLE PATCH 12.2.1.4.220315) p32121987_122140_Generic.zip (Oracle Service Bus BUNDLE PATCH 12.2.1.4.201105) p33404495_122140_Generic.zip (SOA One-off) p31857456_122140_Generic.zip (Oracle Service Bus One-off) p30741105_122140_Generic.zip (Oracle Service Bus One-off) p31713053_122140_Linux-x86-64.zip (One-off patch)    Update required build files The following files in the code repository location \u0026lt;imagetool-setup-location\u0026gt;/docker-images/OracleSOASuite/imagetool/12.2.1.4.0 are used for creating the image:\n additionalBuildCmds.txt buildArgs    In the buildArgs file, update all occurrences of %DOCKER_REPO% with the docker-images repository location, which is the complete path of \u0026lt;imagetool-setup-location\u0026gt;/docker-images.\nFor example, update:\n%DOCKER_REPO%/OracleSOASuite/imagetool/12.2.1.4.0/\nto:\n\u0026lt;imagetool-setup-location\u0026gt;/docker-images/OracleSOASuite/imagetool/12.2.1.4.0/\n  Similarly, update the placeholders %JDK_VERSION% and %BUILDTAG% with appropriate values.\n  Update the response file \u0026lt;imagetool-setup-location\u0026gt;/docker-images/OracleFMWInfrastructure/dockerfiles/12.2.1.4/install.file to add the parameter INSTALL_TYPE=\u0026quot;Fusion Middleware Infrastructure\u0026quot; in the [GENERIC] section.\n  Create the image   Add a JDK package to the WebLogic Image Tool cache:\n$ imagetool cache addInstaller --type jdk --version 8u331 --path \u0026lt;download location\u0026gt;/jdk-8u331-linux-x64.tar.gz   Add the downloaded installation binaries to the WebLogic Image Tool cache:\n$ imagetool cache addInstaller --type fmw --version 12.2.1.4.0 --path \u0026lt;download location\u0026gt;/fmw_12.2.1.4.0_infrastructure.jar $ imagetool cache addInstaller --type soa --version 12.2.1.4.0 --path \u0026lt;download location\u0026gt;/fmw_12.2.1.4.0_soa.jar $ imagetool cache addInstaller --type osb --version 12.2.1.4.0 --path \u0026lt;download location\u0026gt;/fmw_12.2.1.4.0_osb.jar $ imagetool cache addInstaller --type b2b --version 12.2.1.4.0 --path \u0026lt;download location\u0026gt;/fmw_12.2.1.4.0_b2bhealthcare.jar   Add the downloaded OPatch patch to the WebLogic Image Tool cache:\n$ imagetool cache addEntry --key 28186730_13.9.4.2.8 --value \u0026lt;download location\u0026gt;/p28186730_139428_Generic.zip   Append the --opatchBugNumber flag and the OPatch patch key to the create command in the buildArgs file:\n--opatchBugNumber 28186730_13.9.4.2.8   Add the downloaded product patches to the WebLogic Image Tool cache:\n$ imagetool cache addEntry --key 28186730_13.9.4.2.8 --value \u0026lt;download location\u0026gt;/p28186730_139428_Generic.zip $ imagetool cache addEntry --key 30741105_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p30741105_122140_Generic.zip $ imagetool cache addEntry --key 31544353_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p31544353_122140_Linux-x86-64.zip $ imagetool cache addEntry --key 31713053_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p31713053_122140_Linux-x86-64.zip $ imagetool cache addEntry --key 31857456_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p31857456_122140_Generic.zip $ imagetool cache addEntry --key 32121987_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p32121987_122140_Generic.zip $ imagetool cache addEntry --key 32720458_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p32720458_122140_Generic.zip $ imagetool cache addEntry --key 33093748_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p33093748_122140_Generic.zip $ imagetool cache addEntry --key 33404495_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p33404495_122140_Generic.zip $ imagetool cache addEntry --key 33678607_20.4.0.7.0 --value \u0026lt;download location\u0026gt;/p33678607_204070122_Generic.zip $ imagetool cache addEntry --key 33546536_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p33546536_122140_Generic.zip $ imagetool cache addEntry --key 33618954_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p33618954_122140_Generic.zip $ imagetool cache addEntry --key 33902201_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p33902201_122140_Generic.zip $ imagetool cache addEntry --key 33950717_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p33950717_122140_Generic.zip $ imagetool cache addEntry --key 33958532_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p33958532_122140_Generic.zip $ imagetool cache addEntry --key 33965482_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p33965482_122140_Generic.zip $ imagetool cache addEntry --key 34012040_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p34012040_122140_Generic.zip $ imagetool cache addEntry --key 34044738_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p34044738_122140_Generic.zip $ imagetool cache addEntry --key 34065178_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p34065178_122140_Generic.zip $ imagetool cache addEntry --key 34077658_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p34077658_122140_Generic.zip   Append the --patches flag and the product patch keys to the create command in the buildArgs file. The --patches list must be a comma-separated collection of patch --key values used in the imagetool cache addEntry commands above.\nSample --patches list for the product patches added in to the cache:\n--patches 30741105_12.2.1.4.0,31544353_12.2.1.4.0,31713053_12.2.1.4.0,31857456_12.2.1.4.0,32121987_12.2.1.4.0,32720458_12.2.1.4.0,33093748_12.2.1.4.0,33404495_12.2.1.4.0,33546536_12.2.1.4.0,33618954_12.2.1.4.0,33678607_20.4.0.7.0,33902201_12.2.1.4.0,33950717_12.2.1.4.0,33958532_12.2.1.4.0,33965482_12.2.1.4.0,34012040_12.2.1.4.0,34044738_12.2.1.4.0,34065178_12.2.1.4.0,34077658_12.2.1.4.0 Example buildArgs file after appending the OPatch patch and product patches:\ncreate --jdkVersion 8u331 --type soa_osb_b2b --version 12.2.1.4.0 --tag oracle/soasuite:12.2.1.4.0 --pull --fromImage ghcr.io/oracle/oraclelinux:7-slim --chown oracle:root --additionalBuildCommands \u0026lt;imagetool-setup-location\u0026gt;/docker-images/OracleSOASuite/imagetool/12.2.1.4.0/additionalBuildCmds.txt --additionalBuildFiles \u0026lt;imagetool-setup-location\u0026gt;/docker-images/OracleSOASuite/dockerfiles/12.2.1.4/container-scripts --installerResponseFile \u0026lt;imagetool-setup-location\u0026gt;/docker-images/OracleFMWInfrastructure/dockerfiles/12.2.1.4/install.file,\u0026lt;imagetool-setup-location\u0026gt;/docker-images/OracleSOASuite/dockerfiles/12.2.1.4/install/soasuite.response,\u0026lt;imagetool-setup-location\u0026gt;/docker-images/OracleSOASuite/dockerfiles/12.2.1.4/install/osb.response,\u0026lt;imagetool-setup-location\u0026gt;/docker-images/OracleSOASuite/dockerfiles/12.2.1.4/install/b2b.response --patches 30741105_12.2.1.4.0,31544353_12.2.1.4.0,31713053_12.2.1.4.0,31857456_12.2.1.4.0,32121987_12.2.1.4.0,32720458_12.2.1.4.0,33093748_12.2.1.4.0,33404495_12.2.1.4.0,33546536_12.2.1.4.0,33618954_12.2.1.4.0,33678607_20.4.0.7.0,33902201_12.2.1.4.0,33950717_12.2.1.4.0,33958532_12.2.1.4.0,33965482_12.2.1.4.0,34012040_12.2.1.4.0,34044738_12.2.1.4.0,34065178_12.2.1.4.0,34077658_12.2.1.4.0  Note: In the buildArgs file:\n --jdkVersion value must match the --version value used in the imagetool cache addInstaller command for --type jdk. --version value must match the --version value used in the imagetool cache addInstaller command for --type soa. --pull always pulls the latest base Linux image oraclelinux:7-slim from the Docker registry. This flag can be removed if you want to use the Linux image oraclelinux:7-slim, which is already available on the host where the SOA image is created.   Refer to this page for the complete list of options available with the WebLogic Image Tool create command.\n  Create the Oracle SOA Suite image:\n$ imagetool @\u0026lt;absolute path to buildargs file\u0026gt;  Note: Make sure that the absolute path to the buildargs file is prepended with a @ character, as shown in the example above.\n For example:\n$ imagetool @\u0026lt;imagetool-setup-location\u0026gt;/docker-images/OracleSOASuite/imagetool/12.2.1.4.0/buildArgs    Click here to see the sample Dockerfile generated with the `imagetool` command.   ########## BEGIN DOCKERFILE ########## # Copyright (c) 2019, 2021, Oracle and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # FROM ghcr.io/oracle/oraclelinux:7-slim as os_update LABEL com.oracle.weblogic.imagetool.buildid=\u0026quot;b4554a25-22dd-4793-b121-9989bd4be40a\u0026quot; USER root # Use package manager to make sure that unzip, tar, and other required packages are installed # # Copyright (c) 2021, Oracle and/or its affiliates. # # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # # Ensure necessary OS packages are installed RUN yum -y --downloaddir=/tmp/imagetool install gzip tar unzip libaio jq findutils diffutils hostname \\ \u0026amp;\u0026amp; yum -y --downloaddir=/tmp/imagetool clean all \\ \u0026amp;\u0026amp; rm -rf /var/cache/yum/* \\ \u0026amp;\u0026amp; rm -rf /tmp/imagetool # Create the Oracle user that will be the owner of the installed software # # Copyright (c) 2021, Oracle and/or its affiliates. # # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # # Create user and group RUN if [ -z \u0026quot;$(getent group root)\u0026quot; ]; then hash groupadd \u0026amp;\u0026gt; /dev/null \u0026amp;\u0026amp; groupadd root || exit -1 ; fi \\ \u0026amp;\u0026amp; if [ -z \u0026quot;$(getent passwd oracle)\u0026quot; ]; then hash useradd \u0026amp;\u0026gt; /dev/null \u0026amp;\u0026amp; useradd -g root oracle || exit -1; fi \\ \u0026amp;\u0026amp; mkdir -p /u01 \\ \u0026amp;\u0026amp; chown oracle:root /u01 \\ \u0026amp;\u0026amp; chmod 775 /u01 # If Java is not already in the base image, install it # Copyright (c) 2021, Oracle and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # # Installing Java FROM os_update as jdk_build LABEL com.oracle.weblogic.imagetool.buildid=\u0026quot;b4554a25-22dd-4793-b121-9989bd4be40a\u0026quot; ENV JAVA_HOME=/u01/jdk COPY --chown=oracle:root jdk-8u301-linux-x64.tar.gz /tmp/imagetool/ USER oracle RUN tar xzf /tmp/imagetool/jdk-8u301-linux-x64.tar.gz -C /u01 \\ \u0026amp;\u0026amp; $(test -d /u01/jdk* \u0026amp;\u0026amp; mv /u01/jdk* /u01/jdk || mv /u01/graal* /u01/jdk) \\ \u0026amp;\u0026amp; rm -rf /tmp/imagetool \\ \u0026amp;\u0026amp; rm -f /u01/jdk/javafx-src.zip /u01/jdk/src.zip # If an Oracle Home is not already in the base image, install the middleware components # Copyright (c) 2021, Oracle and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # # Installing Middleware FROM os_update as wls_build LABEL com.oracle.weblogic.imagetool.buildid=\u0026quot;b4554a25-22dd-4793-b121-9989bd4be40a\u0026quot; ENV JAVA_HOME=/u01/jdk \\ ORACLE_HOME=/u01/oracle \\ OPATCH_NO_FUSER=true RUN mkdir -p /u01/oracle \\ \u0026amp;\u0026amp; mkdir -p /u01/oracle/oraInventory \\ \u0026amp;\u0026amp; chown oracle:root /u01/oracle/oraInventory \\ \u0026amp;\u0026amp; chown oracle:root /u01/oracle COPY --from=jdk_build --chown=oracle:root /u01/jdk /u01/jdk/ COPY --chown=oracle:root fmw_12.2.1.4.0_infrastructure.jar install.file /tmp/imagetool/ COPY --chown=oracle:root fmw_12.2.1.4.0_soa.jar soasuite.response /tmp/imagetool/ COPY --chown=oracle:root fmw_12.2.1.4.0_osb.jar osb.response /tmp/imagetool/ COPY --chown=oracle:root fmw_12.2.1.4.0_b2bhealthcare.jar b2b.response /tmp/imagetool/ COPY --chown=oracle:root oraInst.loc /u01/oracle/ USER oracle RUN echo \u0026quot;INSTALLING MIDDLEWARE\u0026quot; \\ \u0026amp;\u0026amp; echo \u0026quot;INSTALLING fmw\u0026quot; \\ \u0026amp;\u0026amp; \\ /u01/jdk/bin/java -Xmx1024m -jar /tmp/imagetool/fmw_12.2.1.4.0_infrastructure.jar -silent ORACLE_HOME=/u01/oracle \\ -responseFile /tmp/imagetool/install.file -invPtrLoc /u01/oracle/oraInst.loc -ignoreSysPrereqs -force -novalidation \\ \u0026amp;\u0026amp; echo \u0026quot;INSTALLING soa\u0026quot; \\ \u0026amp;\u0026amp; \\ /u01/jdk/bin/java -Xmx1024m -jar /tmp/imagetool/fmw_12.2.1.4.0_soa.jar -silent ORACLE_HOME=/u01/oracle \\ -responseFile /tmp/imagetool/soasuite.response -invPtrLoc /u01/oracle/oraInst.loc -ignoreSysPrereqs -force -novalidation \\ \u0026amp;\u0026amp; echo \u0026quot;INSTALLING osb\u0026quot; \\ \u0026amp;\u0026amp; \\ /u01/jdk/bin/java -Xmx1024m -jar /tmp/imagetool/fmw_12.2.1.4.0_osb.jar -silent ORACLE_HOME=/u01/oracle \\ -responseFile /tmp/imagetool/osb.response -invPtrLoc /u01/oracle/oraInst.loc -ignoreSysPrereqs -force -novalidation \\ \u0026amp;\u0026amp; echo \u0026quot;INSTALLING b2b\u0026quot; \\ \u0026amp;\u0026amp; \\ /u01/jdk/bin/java -Xmx1024m -jar /tmp/imagetool/fmw_12.2.1.4.0_b2bhealthcare.jar -silent ORACLE_HOME=/u01/oracle \\ -responseFile /tmp/imagetool/b2b.response -invPtrLoc /u01/oracle/oraInst.loc -ignoreSysPrereqs -force -novalidation \\ \u0026amp;\u0026amp; test $? -eq 0 \\ \u0026amp;\u0026amp; chmod -R g+r /u01/oracle \\ || (grep -vh \u0026quot;NOTIFICATION\u0026quot; /tmp/OraInstall*/install*.log \u0026amp;\u0026amp; exit 1) # # Copyright (c) 2021, Oracle and/or its affiliates. # # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # # Update OPatch and apply WebLogic patches COPY --chown=oracle:root p28186730_139426_Generic-24269359.zip /tmp/imagetool/opatch/ RUN cd /tmp/imagetool/opatch \\ \u0026amp;\u0026amp; /u01/jdk/bin/jar -xf /tmp/imagetool/opatch/p28186730_139426_Generic-24269359.zip \\ \u0026amp;\u0026amp; /u01/jdk/bin/java -jar /tmp/imagetool/opatch/6880880/opatch_generic.jar -silent -ignoreSysPrereqs -force -novalidation oracle_home=/u01/oracle \\ \u0026amp;\u0026amp; rm -rf /tmp/imagetool COPY --chown=oracle:root patches/* /tmp/imagetool/patches/ # Apply all patches provided at the same time RUN /u01/oracle/OPatch/opatch napply -silent -oh /u01/oracle -nonrollbackable -phBaseDir /tmp/imagetool/patches \\ \u0026amp;\u0026amp; test $? -eq 0 \\ \u0026amp;\u0026amp; /u01/oracle/OPatch/opatch util cleanup -silent -oh /u01/oracle FROM os_update as final_build ENV ORACLE_HOME=/u01/oracle \\ JAVA_HOME=/u01/jdk \\ PATH=${PATH}:/u01/jdk/bin:/u01/oracle/oracle_common/common/bin:/u01/oracle/wlserver/common/bin:/u01/oracle LABEL com.oracle.weblogic.imagetool.buildid=\u0026quot;b4554a25-22dd-4793-b121-9989bd4be40a\u0026quot; COPY --from=jdk_build --chown=oracle:root /u01/jdk /u01/jdk/ COPY --from=wls_build --chown=oracle:root /u01/oracle /u01/oracle/ USER oracle WORKDIR /u01/oracle #ENTRYPOINT /bin/bash ENV ORACLE_HOME=/u01/oracle \\ VOLUME_DIR=/u01/oracle/user_projects \\ SCRIPT_FILE=/u01/oracle/container-scripts/* \\ HEALTH_SCRIPT_FILE=/u01/oracle/container-scripts/get_healthcheck_url.sh \\ JAVA_OPTIONS=\u0026quot;-Doracle.jdbc.fanEnabled=false -Dweblogic.StdoutDebugEnabled=false\u0026quot; \\ PATH=$PATH:/u01/oracle/container-scripts:/u01/oracle/oracle_common/modules/thirdparty/org.apache.ant/1.10.5.0.0/apache-ant-1.10.5/bin USER root RUN mkdir -p $VOLUME_DIR \u0026amp;\u0026amp; chown oracle:root /u01 $VOLUME_DIR \u0026amp;\u0026amp; \\ mkdir -p /u01/oracle/container-scripts COPY --chown=oracle:root files/container-scripts/ /u01/oracle/container-scripts/ RUN chmod +xr $SCRIPT_FILE USER oracle RUN if [ -f \u0026quot;${ORACLE_HOME}/soa/soa/thirdparty/edifecs/XEngine_8_4_1_23.tar.gz\u0026quot; ]; then \\ cd $ORACLE_HOME/soa/soa/thirdparty/edifecs \u0026amp;\u0026amp; \\ tar -zxvf XEngine_8_4_1_23.tar.gz; \\ else \\ echo -e \u0026quot;\\nXEngine_8_4_1_23.tar.gz not present in ${ORACLE_HOME}/soa/soa/thirdparty/edifecs directory. Skipping untar.\u0026quot;; \\ fi HEALTHCHECK --start-period=5m --interval=1m CMD curl -k -s --fail `$HEALTH_SCRIPT_FILE` || exit 1 WORKDIR ${ORACLE_HOME} CMD [\u0026quot;/u01/oracle/container-scripts/createDomainAndStart.sh\u0026quot;] ########## END DOCKERFILE ##########      Check the created image using the docker images command:\n$ docker images | grep soasuite   Update an image After setting up the WebLogic Image Tool and required build scripts, use the WebLogic Image Tool to update an existing Oracle SOA Suite Docker image:\n  Enter the following command to add the OPatch patch to the WebLogic Image Tool cache:\n$ imagetool cache addEntry --key 28186730_13.9.4.2.7 --value \u0026lt;download location\u0026gt;/p28186730_139427_Generic.zip   Execute the imagetool cache addEntry command for each patch to add the required patch(es) to the WebLogic Image Tool cache. For example, to add patch p30761841_122140_Generic.zip:\n$ imagetool cache addEntry --key=30761841_12.2.1.4.0 --value \u0026lt;downloaded-patches-location\u0026gt;/p30761841_122140_Generic.zip   Provide the following arguments to the WebLogic Image Tool update command:\n –-fromImage - Identify the image that needs to be updated. In the example below, the image to be updated is soasuite:12.2.1.4. –-patches - Multiple patches can be specified as a comma-separated list. --tag - Specify the new tag to be applied for the image being built.  Refer here for the complete list of options available with the WebLogic Image Tool update command.\n Note: The WebLogic Image Tool cache should have the latest OPatch zip. The WebLogic Image Tool will update the OPatch if it is not already updated in the image.\n Examples   Click here to see the example \u0026#39;update\u0026#39; command:   $ imagetool update --fromImage soasuite:12.2.1.4 --chown oracle:root --tag=soasuite:12.2.1.4-30761841 --patches=30761841_12.2.1.4.0 --opatchBugNumber=28186730_13.9.4.2.5 [INFO ] Image Tool build ID: bd21dc73-b775-4186-ae03-8219bf02113e [INFO ] Temporary directory used for docker build context: \u0026lt;work-directory\u0026gt;/wlstmp/wlsimgbuilder_temp1117031733123594064 [INFO ] Using patch 28186730_13.9.4.2.5 from cache: \u0026lt;downloaded-patches-location\u0026gt;/p28186730_139425_Generic.zip [WARNING] skipping patch conflict check, no support credentials provided [WARNING] No credentials provided, skipping validation of patches [INFO ] Using patch 30761841_12.2.1.4.0 from cache: \u0026lt;downloaded-patches-location\u0026gt;/p30761841_122140_Generic.zip [INFO ] docker cmd = docker build --force-rm=true --no-cache --tag soasuite:12.2.1.4-30761841 --build-arg http_proxy=http://\u0026lt;YOUR-COMPANY-PROXY\u0026gt; --build-arg https_proxy=http://\u0026lt;YOUR-COMPANY-PROXY\u0026gt; --build-arg no_proxy=\u0026lt;IP addresses and Domain address for no_proxy\u0026gt;,/var/run/docker.sock \u0026lt;work-directory\u0026gt;/wlstmp/wlsimgbuilder_temp1117031733123594064 Sending build context to Docker daemon 53.47MB Step 1/7 : FROM soasuite:12.2.1.4 as FINAL_BUILD ---\u0026gt; 445b649a3459 Step 2/7 : USER root ---\u0026gt; Running in 27f45e6958c3 Removing intermediate container 27f45e6958c3 ---\u0026gt; 150ae0161d46 Step 3/7 : ENV OPATCH_NO_FUSER=true ---\u0026gt; Running in daddfbb8fd9e Removing intermediate container daddfbb8fd9e ---\u0026gt; a5fc6b74be39 Step 4/7 : LABEL com.oracle.weblogic.imagetool.buildid=\u0026quot;bd21dc73-b775-4186-ae03-8219bf02113e\u0026quot; ---\u0026gt; Running in cdfec79c3fd4 Removing intermediate container cdfec79c3fd4 ---\u0026gt; 4c773aeb956f Step 5/7 : USER oracle ---\u0026gt; Running in ed3432e43e89 Removing intermediate container ed3432e43e89 ---\u0026gt; 54fe6b07c447 Step 6/7 : COPY --chown=oracle:oracle patches/* /tmp/imagetool/patches/ ---\u0026gt; d6d12f02a9be Step 7/7 : RUN /u01/oracle/OPatch/opatch napply -silent -oh /u01/oracle -phBaseDir /tmp/imagetool/patches \u0026amp;\u0026amp; /u01/oracle/OPatch/opatch util cleanup -silent -oh /u01/oracle \u0026amp;\u0026amp; rm -rf /tmp/imagetool ---\u0026gt; Running in a79addca4d2f Oracle Interim Patch Installer version 13.9.4.2.5 Copyright (c) 2020, Oracle Corporation. All rights reserved. Oracle Home : /u01/oracle Central Inventory : /u01/oracle/oraInventory from : /u01/oracle/oraInst.loc OPatch version : 13.9.4.2.5 OUI version : 13.9.4.0.0 Log file location : /u01/oracle/cfgtoollogs/opatch/opatch2020-06-01_10-56-13AM_1.log OPatch detects the Middleware Home as \u0026quot;/u01/oracle\u0026quot; Verifying environment and performing prerequisite checks... OPatch continues with these patches: 30761841 Do you want to proceed? [y|n] Y (auto-answered by -silent) User Responded with: Y All checks passed. Please shutdown Oracle instances running out of this ORACLE_HOME on the local system. (Oracle Home = '/u01/oracle') Is the local system ready for patching? [y|n] Y (auto-answered by -silent) User Responded with: Y Backing up files... Applying interim patch '30761841' to OH '/u01/oracle' ApplySession: Optional component(s) [ oracle.org.bouncycastle.bcprov.ext.jdk15on, 1.55.0.0.0 ] , [ oracle.org.bouncycastle.bcprov.ext.jdk15on, 1.55.0.0.0 ] , [ oracle.org.bouncycastle.bcprov.ext.jdk15on, 1.5.0.0.0 ] , [ oracle.org.bouncycastle.bcprov.ext.jdk15on, 1.5.0.0.0 ] , [ oracle.org.bouncycastle.bcprov.jdk15on, 1.55.0.0.0 ] , [ oracle.org.bouncycastle.bcprov.jdk15on, 1.55.0.0.0 ] , [ oracle.org.bouncycastle.bcprov.jdk15on, 1.52.0.0.0 ] , [ oracle.org.bouncycastle.bcprov.jdk15on, 1.52.0.0.0 ] , [ oracle.org.bouncycastle.bcprov.ext.jdk15on, 1.48.0.0.0 ] , [ oracle.org.bouncycastle.bcprov.ext.jdk15on, 1.48.0.0.0 ] , [ oracle.org.bouncycastle.bcpkix.jdk15on, 1.49.0.0.0 ] , [ oracle.org.bouncycastle.bcpkix.jdk15on, 1.49.0.0.0 ] , [ oracle.org.bouncycastle.bcprov.jdk15on, 1.51.0.0.0 ] , [ oracle.org.bouncycastle.bcprov.jdk15on, 1.51.0.0.0 ] , [ oracle.org.bouncycastle.bcprov.jdk15on, 1.54.0.0.0 ] , [ oracle.org.bouncycastle.bcprov.jdk15on, 1.54.0.0.0 ] , [ oracle.org.bouncycastle.bcprov.ext.jdk15on, 1.54.0.0.0 ] , [ oracle.org.bouncycastle.bcprov.ext.jdk15on, 1.54.0.0.0 ] , [ oracle.org.bouncycastle.bcpkix.jdk15on, 1.5.0.0.0 ] , [ oracle.org.bouncycastle.bcpkix.jdk15on, 1.5.0.0.0 ] , [ oracle.org.bouncycastle.bcpkix.jdk15on, 1.54.0.0.0 ] , [ oracle.org.bouncycastle.bcpkix.jdk15on, 1.54.0.0.0 ] , [ oracle.org.bouncycastle.bcpkix.jdk15on, 1.55.0.0.0 ] , [ oracle.org.bouncycastle.bcpkix.jdk15on, 1.55.0.0.0 ] , [ oracle.org.bouncycastle.bcprov.jdk15on, 1.49.0.0.0 ] , [ oracle.org.bouncycastle.bcprov.jdk15on, 1.49.0.0.0 ] , [ oracle.org.bouncycastle.bcprov.jdk15on, 1.5.0.0.0 ] , [ oracle.org.bouncycastle.bcprov.jdk15on, 1.5.0.0.0 ] not present in the Oracle Home or a higher version is found. Patching component oracle.org.bouncycastle.bcprov.jdk15on, 1.60.0.0.0... Patching component oracle.org.bouncycastle.bcprov.jdk15on, 1.60.0.0.0... Patching component oracle.org.bouncycastle.bcprov.ext.jdk15on, 1.60.0.0.0... Patching component oracle.org.bouncycastle.bcprov.ext.jdk15on, 1.60.0.0.0... Patching component oracle.org.bouncycastle.bcpkix.jdk15on, 1.60.0.0.0... Patching component oracle.org.bouncycastle.bcpkix.jdk15on, 1.60.0.0.0... Patch 30761841 successfully applied. Log file location: /u01/oracle/cfgtoollogs/opatch/opatch2020-06-01_10-56-13AM_1.log OPatch succeeded. Oracle Interim Patch Installer version 13.9.4.2.5 Copyright (c) 2020, Oracle Corporation. All rights reserved. Oracle Home : /u01/oracle Central Inventory : /u01/oracle/oraInventory from : /u01/oracle/oraInst.loc OPatch version : 13.9.4.2.5 OUI version : 13.9.4.0.0 Log file location : /u01/oracle/cfgtoollogs/opatch/opatch2020-06-01_10-57-19AM_1.log OPatch detects the Middleware Home as \u0026quot;/u01/oracle\u0026quot; Invoking utility \u0026quot;cleanup\u0026quot; OPatch will clean up 'restore.sh,make.txt' files and 'scratch,backup' directories. You will be still able to rollback patches after this cleanup. Do you want to proceed? [y|n] Y (auto-answered by -silent) User Responded with: Y Backup area for restore has been cleaned up. For a complete list of files/directories deleted, Please refer log file. OPatch succeeded. Removing intermediate container a79addca4d2f ---\u0026gt; 2ef2a67a685b Successfully built 2ef2a67a685b Successfully tagged soasuite:12.2.1.4-30761841 [INFO ] Build successful. Build time=112s. Image tag=soasuite:12.2.1.4-30761841      Click here to see the example Dockerfile generated by the WebLogic Image Tool with the \u0026#39;--dryRun\u0026#39; option:   $ imagetool update --fromImage soasuite:12.2.1.4 --chown oracle:root --tag=soasuite:12.2.1.4-30761841 --patches=30761841_12.2.1.4.0 --opatchBugNumber=28186730_13.9.4.2.5 --dryRun [INFO ] Image Tool build ID: f9feea35-c52c-4974-b155-eb7f34d95892 [INFO ] Temporary directory used for docker build context: \u0026lt;work-directory\u0026gt;/wlstmp/wlsimgbuilder_temp1799120592903014749 [INFO ] Using patch 28186730_13.9.4.2.5 from cache: \u0026lt;downloaded-patches-location\u0026gt;/p28186730_139425_Generic.zip [WARNING] skipping patch conflict check, no support credentials provided [WARNING] No credentials provided, skipping validation of patches [INFO ] Using patch 30761841_12.2.1.4.0 from cache: \u0026lt;downloaded-patches-location\u0026gt;/p30761841_122140_Generic.zip [INFO ] docker cmd = docker build --force-rm=true --no-cache --tag soasuite:12.2.1.4-30761841 --build-arg http_proxy=http://www.yourcompany.proxy.com:80 --build-arg https_proxy=http://www.yourcompany.proxy.com:80 --build-arg no_proxy=10.250.109.251,localhost,127.0.0.1,/var/run/docker.sock \u0026lt;work-directory\u0026gt;/wlstmp/wlsimgbuilder_temp1799120592903014749 ########## BEGIN DOCKERFILE ########## # # Copyright (c) 2019, 2020, Oracle and/or its affiliates. # # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # # FROM soasuite:12.2.1.4 as FINAL_BUILD USER root ENV OPATCH_NO_FUSER=true LABEL com.oracle.weblogic.imagetool.buildid=\u0026quot;f9feea35-c52c-4974-b155-eb7f34d95892\u0026quot; USER oracle COPY --chown=oracle:oracle patches/* /tmp/imagetool/patches/ RUN /u01/oracle/OPatch/opatch napply -silent -oh /u01/oracle -phBaseDir /tmp/imagetool/patches \\ \u0026amp;\u0026amp; /u01/oracle/OPatch/opatch util cleanup -silent -oh /u01/oracle \\ \u0026amp;\u0026amp; rm -rf /tmp/imagetool ########## END DOCKERFILE ##########      Check the built image using the docker images command:\n$ docker images | grep soasuite soasuite 12.2.1.4-30761841 2ef2a67a685b About a minute ago 4.84GB $   Create an Oracle SOA Suite Docker image using Dockerfile For test and development purposes, you can create an Oracle SOA Suite image using the Dockerfile. Consult the README file for important prerequisite steps, such as building or pulling the Server JRE Docker image, Oracle FMW Infrastructure Docker image, and downloading the Oracle SOA Suite installer and bundle patch binaries.\nA prebuilt Oracle Fusion Middleware Infrastructure image, container-registry.oracle.com/middleware/fmw-infrastructure:12.2.1.4, is available at container-registry.oracle.com. We recommend that you pull and rename this image to build the Oracle SOA Suite image.\n$ docker pull container-registry.oracle.com/middleware/fmw-infrastructure:12.2.1.4 $ docker tag container-registry.oracle.com/middleware/fmw-infrastructure:12.2.1.4 oracle/fmw-infrastructure:12.2.1.4.0 Follow these steps to build an Oracle Fusion Middleware Infrastructure image, and then the Oracle SOA Suite image as a layer on top of that:\n  Make a local clone of the sample repository:\n$ git clone https://github.com/oracle/docker-images   Build the oracle/fmw-infrastructure:12.2.1.4 image:\n$ cd docker-images/OracleFMWInfrastructure/dockerfiles $ sh buildDockerImage.sh -v 12.2.1.4 -s This will produce an image named oracle/fmw-infrastructure:12.2.1.4.\n  Tag the image as follows:\n$ docker tag oracle/fmw-infrastructure:12.2.1.4 oracle/fmw-infrastructure:12.2.1.4.0   Download the Oracle SOA Suite installer from the Oracle Technology Network or e-delivery.\n Note: Copy the installer binaries to the same location as the Dockerfile.\n   To build the Oracle SOA Suite image with patches, you must download and drop the patch zip files (for example, p29928100_122140_Generic.zip) into the patches/ folder under the version that is required. For example, for 12.2.1.4.0 the folder is 12.2.1.4/patches. Similarly, to build the image by including the OPatch patch, download and drop the OPatch patch zip file (for e.g. p28186730_139424_Generic.zip) into the opatch_patch/ folder.\n  Create the Oracle SOA Suite image by running the provided script:\n$ cd docker-images/OracleSOASuite/dockerfiles $ ./buildDockerImage.sh -v 12.2.1.4 -s The image produced will be named oracle/soasuite:12.2.1.4. The samples and instructions assume the Oracle SOA Suite image is named soasuite:12.2.1.4. You must rename your image to match this name, or update the samples to refer to the image you created.\n$ docker tag oracle/soasuite:12.2.1.4 soasuite:12.2.1.4   "
},
{
	"uri": "/fmw-kubernetes/wcsites-domains/create-or-update-image/",
	"title": "Create or update an image",
	"tags": [],
	"description": "Create or update an Oracle WebCenter Sites Docker image used for deploying Oracle WebCenter Sites domains. An Oracle WebCenter Sites Docker image can be created using the WebLogic Image Tool or using the Dockerfile approach.",
	"content": "If you have access to the My Oracle Support (MOS), and there is a need to build a new image with a patch (bundle or interim), it is recommended to use the WebLogic Image Tool to build an Oracle WebCenter Sites image for production deployments.\n Create or update an Oracle WebCenter Sites Docker image using the WebLogic Image Tool  Set up the WebLogic Image Tool Create an image Update an image   Create an Oracle WebCenter Sites Docker image using Dockerfile  Create or update an Oracle WebCenter Sites Docker image using the WebLogic Image Tool Using the WebLogic Image Tool, you can create a new Oracle WebCenter Sites Docker image (can include patches as well) or update an existing image with one or more patches (bundle patch and interim patches).\n Recommendations:\n Use create for creating a new Oracle WebCenter Sites Docker image either:  without any patches or, containing the Oracle WebCenter Sites binaries, bundle patch and interim patches. This is the recommended approach if you have access to the Oracle WebCenter Sites patches because it optimizes the size of the image.   Use update for patching an existing Oracle WebCenter Sites Docker image with a single interim patch. Note that the patched image size may increase considerably due to additional image layers introduced by the patch application tool.   Set up the WebLogic Image Tool  Prerequisites Set up the WebLogic Image Tool Validate setup WebLogic Image Tool build directory WebLogic Image Tool cache Set up additional build scripts  Prerequisites Verify that your environment meets the following prerequisites:\n Docker client and daemon on the build machine, with minimum Docker version 18.03.1.ce. Bash version 4.0 or later, to enable the command complete feature. JAVA_HOME environment variable set to the appropriate JDK location.  Set up the WebLogic Image Tool To set up the WebLogic Image Tool:\n  Create a working directory and change to it. In these steps, this directory is imagetool-setup.\n$ mkdir imagetool-setup $ cd imagetool-setup   Download the latest version of the WebLogic Image Tool from the releases page.\n  Unzip the release ZIP file to the imagetool-setup directory.\n  Execute the following commands to set up the WebLogic Image Tool on a Linux environment:\n$ cd imagetool-setup/imagetool/bin $ source setup.sh   Validate setup To validate the setup of the WebLogic Image Tool:\n  Enter the following command to retrieve the version of the WebLogic Image Tool:\n$ imagetool --version   Enter imagetool then press the Tab key to display the available imagetool commands:\n$ imagetool \u0026lt;TAB\u0026gt; cache create help rebase update   WebLogic Image Tool build directory The WebLogic Image Tool creates a temporary Docker context directory, prefixed by wlsimgbuilder_temp, every time the tool runs. Under normal circumstances, this context directory will be deleted. However, if the process is aborted or the tool is unable to remove the directory, it is safe for you to delete it manually. By default, the WebLogic Image Tool creates the Docker context directory under the user\u0026rsquo;s home directory. If you prefer to use a different directory for the temporary context, set the environment variable WLSIMG_BLDDIR:\n$ export WLSIMG_BLDDIR=\u0026#34;/path/to/buid/dir\u0026#34; WebLogic Image Tool cache The WebLogic Image Tool maintains a local file cache store. This store is used to look up where the Java, WebLogic Server installers, and WebLogic Server patches reside in the local file system. By default, the cache store is located in the user\u0026rsquo;s $HOME/cache directory. Under this directory, the lookup information is stored in the .metadata file. All automatically downloaded patches also reside in this directory. You can change the default cache store location by setting the environment variable WLSIMG_CACHEDIR:\n$ export WLSIMG_CACHEDIR=\u0026#34;/path/to/cachedir\u0026#34; Set up additional build scripts Creating an Oracle WebCenter Sites Docker image using the WebLogic Image Tool requires additional container scripts for Oracle WebCenter Sites domains.\n  Clone the docker-images repository to set up those scripts. In these steps, this directory is DOCKER_REPO:\n$ cd imagetool-setup $ git clone https://github.com/oracle/docker-images.git   Copy the additional WebLogic Image Tool build files from the operator source repository to the imagetool-setup location:\n$ mkdir -p imagetool-setup/docker-images/OracleWebCenterSites/imagetool/12.2.1.4 $ cd imagetool-setup/docker-images/OracleWebCenterSites/imagetool/12.2.1.4 $ cp -rf ${WORKDIR}/fmw-kubernetes/OracleWebCenterSites/kubernetes/imagetool-scripts/* .   Create an image After setting up the WebLogic Image Tool and required build scripts, follow these steps to use the WebLogic Image Tool to create a new Oracle WebCenter Sites Docker image.\nDownload the Oracle WebCenter Sites installation binaries and patches You must download the required Oracle WebCenter Sites installation binaries and patches as listed below from the Oracle Software Delivery Cloud and save them in a directory of your choice. In these steps, this directory is download location.\n  Click here to see the sample list of installation binaries and patches:     JDK:\n jdk-8u241-linux-x64.tar.gz    Fusion MiddleWare Infrastructure installer:\n fmw_12.2.1.4.0_infrastructure.jar    Fusion MiddleWare Infrastructure patches:\n p28186730_139426_Generic.zip (Opatch) p33059296_122140_Generic.zip (WLS) p32973297_122140_Generic.zip (COH)    WCS installers:\n fmw_12.2.1.4.0_wcsites.jar    WCS patches:\n p33381673_122140_Generic.zip (WCS)       Note: This is a sample list of patches. You must get the appropriate list of patches for your Oracle WebCenter Sites image.\n Update required build files The following files available in the code repository location ${WORKDIR}/fmw-kubernetes/OracleWebCenterSites/kubernetes/imagetool-scripts are used for creating the image.\n additionalBuildCmds.txt buildArgs    In the buildArgs file, update all the occurrences of %DOCKER_REPO% with the docker-images repository location, which is the complete path of imagetool-setup/docker-images.\nFor example, update:\n%DOCKER_REPO%/OracleWebCenterSites/imagetool/12.2.1.4/\nto:\n\u0026lt;imagetool-setup-location\u0026gt;/docker-images/OracleWebCenterSites/imagetool/12.2.1.4/\n  Similarly, update the placeholders %JDK_VERSION% and %BUILDTAG% with appropriate values.\n  Create the image   Add a JDK package to the WebLogic Image Tool cache:\n$ imagetool cache addInstaller --type jdk --version 8u241 --path \u0026lt;download location\u0026gt;/jdk-8u241-linux-x64.tar.gz   Add the downloaded installation binaries to the WebLogic Image Tool cache:\n$ imagetool cache addInstaller --type fmw --version 12.2.1.4.0 --path \u0026lt;download location\u0026gt;/fmw_12.2.1.4.0_infrastructure.jar $ imagetool cache addInstaller --type wcs --version 12.2.1.4.0 --path \u0026lt;download location\u0026gt;/fmw_12.2.1.4.0_wcsites.jar   Add the downloaded patches to the WebLogic Image Tool cache:\n  Click here to see the commands to add patches in to the cache:   ``` bash $ imagetool cache addEntry --key 28186730_13.9.4.2.6 --value \u0026lt;download location\u0026gt;/p28186730_139426_Generic.zip $ imagetool cache addEntry --key 33059296_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p33059296_122140_Generic.zip $ imagetool cache addEntry --key 32973297_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p32973297_122140_Generic.zip $ imagetool cache addEntry --key 33381673_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p33381673_122140_Generic.zip ```      Update the patches list to buildArgs.\nTo the create command in the buildArgs file, append the Oracle WebCenter Sites patches list using the --patches flag and Opatch patch using the --opatchBugNumber flag. Sample options for the list of patches above are:\n--patches 33059296_12.2.1.4.0,32973297_12.2.1.4.0,p33381673_12.2.1.4.0 --opatchBugNumber=28186730_13.9.4.2.6 Example buildArgs file after appending product\u0026rsquo;s list of patches and Opatch patch:\ncreate --jdkVersion=8u241 --type WCS --version=12.2.1.4.0 --tag=oracle/wcsites:12.2.1.4-21.1.1 --installerResponseFile %path-to-downloaded-docker-repo%/OracleWebCenterSites/dockerfiles/12.2.1.4.0/wcs.file,%path-to-downloaded-docker-repo%/OracleWebCenterSites/dockerfiles/12.2.1.4.0/install.file --additionalBuildCommands %path-to-downloaded-docker-repo%/OracleWebCenterSites/imagetool/12.2.1.4.0/addtionalBuildCmds.txt --additionalBuildFiles %path-to-downloaded-docker-repo%/OracleWebCenterSites/dockerfiles/12.2.1.4.0/sites-container-scripts,%path-to-downloaded-docker-repo%/OracleWebCenterSites/dockerfiles/12.2.1.4.0/wcs-wls-docker-install --patches 33059296_12.2.1.4.0,32973297_12.2.1.4.0,33381673_12.2.1.4.0 --opatchBugNumber=28186730_13.9.4.2.6 Refer to this page for the complete list of options available with the WebLogic Image Tool create command.\n  Create a wcs-wls-docker-install installer jar\n  cd %docker_repo%/OracleWebCenterSites/dockerfiles/12.2.1.4/wcs-wls-docker-install docker run --rm -u root -v ${PWD}:/wcs-wls-docker-install groovy:2.4.8-jdk8 /wcs-wls-docker-install/packagejar.sh   Enter the following command to create the Oracle WebCenter Sites image:\n$ imagetool @\u0026lt;absolute path to `buildargs` file\u0026gt;\u0026#34;    Click here to see the sample Dockerfile generated with the imagetool command.    ```bash ########## BEGIN DOCKERFILE ########## # # Copyright (c) 2019, 2020, Oracle and/or its affiliates. # # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # # FROM oraclelinux:7-slim as OS_UPDATE LABEL com.oracle.weblogic.imagetool.buildid=\u0026quot;3b37c045-11c6-4eb8-b69c-f42256c1e082\u0026quot; USER root RUN yum -y --downloaddir= install gzip tar unzip libaio \\ \u0026amp;\u0026amp; yum -y --downloaddir= clean all \\ \u0026amp;\u0026amp; rm -rf /var/cache/yum/* \\ \u0026amp;\u0026amp; rm -rf ## Create user and group RUN if [ -z \u0026quot;$(getent group oracle)\u0026quot; ]; then hash groupadd \u0026amp;\u0026gt; /dev/null \u0026amp;\u0026amp; groupadd oracle || exit -1 ; fi \\ \u0026amp;\u0026amp; if [ -z \u0026quot;$(getent passwd oracle)\u0026quot; ]; then hash useradd \u0026amp;\u0026gt; /dev/null \u0026amp;\u0026amp; useradd -g oracle oracle || exit -1; fi \\ \u0026amp;\u0026amp; mkdir /u01 \\ \u0026amp;\u0026amp; chown oracle:oracle /u01 # Install Java FROM OS_UPDATE as JDK_BUILD LABEL com.oracle.weblogic.imagetool.buildid=\u0026quot;3b37c045-11c6-4eb8-b69c-f42256c1e082\u0026quot; ENV JAVA_HOME=/u01/jdk COPY --chown=oracle:oracle jdk-8u251-linux-x64.tar.gz /tmp/imagetool/ USER oracle RUN tar xzf /tmp/imagetool/jdk-8u251-linux-x64.tar.gz -C /u01 \\ \u0026amp;\u0026amp; mv /u01/jdk* /u01/jdk \\ \u0026amp;\u0026amp; rm -rf /tmp/imagetool # Install Middleware FROM OS_UPDATE as WLS_BUILD LABEL com.oracle.weblogic.imagetool.buildid=\u0026quot;3b37c045-11c6-4eb8-b69c-f42256c1e082\u0026quot; ENV JAVA_HOME=/u01/jdk \\ ORACLE_HOME=/u01/oracle \\ OPATCH_NO_FUSER=true RUN mkdir -p /u01/oracle \\ \u0026amp;\u0026amp; mkdir -p /u01/oracle/oraInventory \\ \u0026amp;\u0026amp; chown oracle:oracle /u01/oracle/oraInventory \\ \u0026amp;\u0026amp; chown oracle:oracle /u01/oracle COPY --from=JDK_BUILD --chown=oracle:oracle /u01/jdk /u01/jdk/ COPY --chown=oracle:oracle fmw_12.2.1.4.0_infrastructure.jar install.file /tmp/imagetool/ COPY --chown=oracle:oracle fmw_12.2.1.4.0_wcsites.jar wcs.file /tmp/imagetool/ COPY --chown=oracle:oracle oraInst.loc /u01/oracle/ COPY --chown=oracle:oracle p28186730_139422_Generic.zip /tmp/imagetool/opatch/ COPY --chown=oracle:oracle patches/* /tmp/imagetool/patches/ USER oracle RUN \\ /u01/jdk/bin/java -Xmx1024m -jar /tmp/imagetool/fmw_12.2.1.4.0_infrastructure.jar -silent ORACLE_HOME=/u01/oracle \\ -responseFile /tmp/imagetool/install.file -invPtrLoc /u01/oracle/oraInst.loc -ignoreSysPrereqs -force -novalidation RUN \\ /u01/jdk/bin/java -Xmx1024m -jar /tmp/imagetool/fmw_12.2.1.4.0_wcsites.jar -silent ORACLE_HOME=/u01/oracle \\ -responseFile /tmp/imagetool/wcs.file -invPtrLoc /u01/oracle/oraInst.loc -ignoreSysPrereqs -force -novalidation RUN cd /tmp/imagetool/opatch \\ \u0026amp;\u0026amp; /u01/jdk/bin/jar -xf /tmp/imagetool/opatch/p28186730_139422_Generic.zip \\ \u0026amp;\u0026amp; /u01/jdk/bin/java -jar /tmp/imagetool/opatch/6880880/opatch_generic.jar -silent -ignoreSysPrereqs -force -novalidation oracle_home=/u01/oracle RUN /u01/oracle/OPatch/opatch napply -silent -oh /u01/oracle -phBaseDir /tmp/imagetool/patches \\ \u0026amp;\u0026amp; /u01/oracle/OPatch/opatch util cleanup -silent -oh /u01/oracle FROM OS_UPDATE as FINAL_BUILD ARG ADMIN_NAME ARG ADMIN_HOST ARG ADMIN_PORT ARG MANAGED_SERVER_PORT ENV ORACLE_HOME=/u01/oracle \\ JAVA_HOME=/u01/jdk \\ LC_ALL=${DEFAULT_LOCALE:-en_US.UTF-8} \\ PATH=${PATH}:/u01/jdk/bin:/u01/oracle/oracle_common/common/bin:/u01/oracle/wlserver/common/bin:/u01/oracle LABEL com.oracle.weblogic.imagetool.buildid=\u0026quot;3b37c045-11c6-4eb8-b69c-f42256c1e082\u0026quot; COPY --from=JDK_BUILD --chown=oracle:oracle /u01/jdk /u01/jdk/ COPY --from=WLS_BUILD --chown=oracle:oracle /u01/oracle /u01/oracle/ USER oracle WORKDIR /u01/oracle #ENTRYPOINT /bin/bash USER root COPY --chown=oracle:oracle files/sites-container-scripts/overrides/oui/ /u01/oracle/wcsites/common/templates/wls/ USER oracle RUN cd /u01/oracle/wcsites/common/templates/wls \u0026amp;\u0026amp; \\ $JAVA_HOME/bin/jar uvf oracle.wcsites.base.template.jar startup-plan.xml file-definition.xml \u0026amp;\u0026amp; \\ rm /u01/oracle/wcsites/common/templates/wls/startup-plan.xml \u0026amp;\u0026amp; \\ rm /u01/oracle/wcsites/common/templates/wls/file-definition.xml # # Install the required packages # ----------------------------- USER root ENV SITES_CONTAINER_SCRIPTS=/u01/oracle/sites-container-scripts \\ SITES_INSTALLER_PKG=wcs-wls-docker-install \\ DOMAIN_ROOT=\u0026quot;${DOMAIN_ROOT:-/u01/oracle/user_projects/domains}\u0026quot; \\ ADMIN_PORT=7001 \\ WCSITES_PORT=7002 \\ ADMIN_SSL_PORT=9001 \\ WCSITES_SSL_PORT=9002 \\ PATH=$PATH:/u01/oracle/sites-container-scripts RUN yum install -y hostname \u0026amp;\u0026amp; \\ rm -rf /var/cache/yum RUN mkdir -p ${SITES_CONTAINER_SCRIPTS} \u0026amp;\u0026amp; \\ mkdir -p /u01/wcs-wls-docker-install COPY --chown=oracle:oracle files/sites-container-scripts/ ${SITES_CONTAINER_SCRIPTS}/ COPY --chown=oracle:oracle files/wcs-wls-docker-install/ /u01/wcs-wls-docker-install/ RUN chown oracle:oracle -R /u01/oracle/sites-container-scripts \u0026amp;\u0026amp; \\ chown oracle:oracle -R /u01/wcs-wls-docker-install \u0026amp;\u0026amp; \\ chmod a+xr /u01/oracle/sites-container-scripts/* \u0026amp;\u0026amp; \\ chmod a+xr /u01/wcs-wls-docker-install/*.sh # Expose all Ports # ------------------------------------------------------------- EXPOSE $ADMIN_PORT $ADMIN_SSL_PORT $WCSITES_PORT $WCSITES_SSL_PORT USER oracle WORKDIR ${ORACLE_HOME} # Define default command to start. # ------------------------------------------------------------- CMD [\u0026quot;/u01/oracle/sites-container-scripts/createOrStartSitesDomain.sh\u0026quot;] ########## END DOCKERFILE ########## ```      Check the created image using the Docker images command:\n$ Docker images | grep wcsites   Update an image After setting up the WebLogic Image Tool and required build scripts, use the WebLogic Image Tool to update an existing Oracle WebCenter Sites Docker image:\n  Enter the following command for each patch to add the required patch(es) to the WebLogic Image Tool cache:\n$ cd \u0026lt;imagetool-setup\u0026gt; $ imagetool cache addEntry --key 33381673_12.2.1.4.0 --value \u0026lt; %path-to-downloaded-pathes%/patches   /p33381673_122140_Generic.zip [INFO ] Added entry 33381673_12.2.1.4.0=\u0026lt; %path-to-downloaded-pathes%/patches /p33381673_122140_Generic.zip\n```    Provide the following arguments to the WebLogic Image Tool update command:\n –-fromImage - Identify the image that needs to be updated. In the example below, the image to be updated is oracle/wcsites:12.2.1.4-21.1.1. –-patches - Multiple patches can be specified as a comma-separated list. --tag - Specify the new tag to be applied for the image being built.  Refer here for the complete list of options available with the WebLogic Image Tool update command.\n Note: The WebLogic Image Tool cache should have the latest OPatch zip. The WebLogic Image Tool will update the OPatch if it is not already updated in the image.\n Examples   Click here to see the example `update` command:   $ imagetool update --fromImage oracle/wcsites:12.2.1.4-21.1.1 --tag=oracle/wcsites:12.2.1.4-21.1.1-29710661 --patches=29710661_12.2.1.4.0 [INFO ] Image Tool build ID: 7c268a9a-723f-424e-a06e-cb615c783e6d [INFO ] Temporary directory used for docker build context: %path-to-temp-directory%/tmpBuild/wlsimgbuilder_temp8555048225669509 [INFO ] Using patch 28186730_13.9.4.2.4 from cache: %path-to-downloaded-pathes%/patches /p28186730_139424_Generic.zip [INFO ] OPatch will not be updated, fromImage has version 13.9.4.2.4, available version is 13.9.4.2.4 [WARNING] skipping patch conflict check, no support credentials provided [WARNING] No credentials provided, skipping validation of patches [INFO ] Using patch 31548912_12.2.1.4.0 from cache: %path-to-downloaded-pathes%/patches /p31548912_122140_Generic.zip [INFO ] docker cmd = docker build --no-cache --force-rm --tag oracle/wcsites:12.2.1.4-21.1.1 --build-arg http_proxy=http://www-proxy-your-company.com:80 --build-arg https_proxy=http://www-proxy-your-company.com:80 --build-arg no_proxy=localhost,127.0.0.0/8,/var/run/docker.sock %path-to-temp-directory%/tmpBuild/wlsimgbuilder_temp8555048225669509 Sending build context to Docker daemon 212.7MB Step 1/7 : FROM oracle/wcsites:12.2.1.4-21.1.1 as FINAL_BUILD ---\u0026gt; 480f1a31c02b Step 2/7 : USER root ---\u0026gt; Running in 9d5a81ad5bde Removing intermediate container 9d5a81ad5bde ---\u0026gt; 71b50b0b34dc Step 3/7 : ENV OPATCH_NO_FUSER=true ---\u0026gt; Running in c361884e8a71 Removing intermediate container c361884e8a71 ---\u0026gt; 2951de256951 Step 4/7 : LABEL com.oracle.weblogic.imagetool.buildid=\u0026quot;7c268a9a-723f-424e-a06e-cb615c783e6d\u0026quot; ---\u0026gt; Running in e2f485ac9039 Removing intermediate container e2f485ac9039 ---\u0026gt; 970f6552ef9a Step 5/7 : USER oracle ---\u0026gt; Running in e3c85228af4b Removing intermediate container e3c85228af4b ---\u0026gt; 4401fdb4ebbe Step 6/7 : COPY --chown=oracle:oracle patches/* /tmp/imagetool/patches/ ---\u0026gt; 978a48e1cc95 Step 7/7 : RUN /u01/oracle/OPatch/opatch napply -silent -oh /u01/oracle -phBaseDir /tmp/imagetool/patches \u0026amp;\u0026amp; /u01/oracle/OPatch/opatch util cleanup -silent -oh /u01/oracle \u0026amp;\u0026amp; rm -rf /tmp/imagetool ---\u0026gt; Running in 5039320b2f10 Oracle Interim Patch Installer version 13.9.4.2.4 Copyright (c) 2020, Oracle Corporation. All rights reserved. Oracle Home : /u01/oracle Central Inventory : /u01/oracle/oraInventory from : /u01/oracle/oraInst.loc OPatch version : 13.9.4.2.4 OUI version : 13.9.4.0.0 Log file location : /u01/oracle/cfgtoollogs/opatch/opatch2020-08-04_05-15-38AM_1.log OPatch detects the Middleware Home as \u0026quot;/u01/oracle\u0026quot; Verifying environment and performing prerequisite checks... OPatch continues with these patches: 31548912 Do you want to proceed? [y|n] Y (auto-answered by -silent) User Responded with: Y All checks passed. Please shutdown Oracle instances running out of this ORACLE_HOME on the local system. (Oracle Home = '/u01/oracle') Is the local system ready for patching? [y|n] Y (auto-answered by -silent) User Responded with: Y Backing up files... Applying interim patch '31548912' to OH '/u01/oracle' ApplySession: Optional component(s) [ oracle.wcsites.wccintegration, 12.2.1.4.0 ] , [ oracle.wcsites.wccintegration, 12.2.1.4.0 ] not present in the Oracle Home or a higher version is found. Patching component oracle.wcsites, 12.2.1.4.0... Patching component oracle.wcsites, 12.2.1.4.0... Patching component oracle.wcsites.visitorservices, 12.2.1.4.0... Patching component oracle.wcsites.visitorservices, 12.2.1.4.0... Patching component oracle.wcsites.examples, 12.2.1.4.0... Patching component oracle.wcsites.examples, 12.2.1.4.0... Patching component oracle.wcsites.developer.tools, 12.2.1.4.0... Patching component oracle.wcsites.developer.tools, 12.2.1.4.0... Patching component oracle.wcsites.satelliteserver, 12.2.1.4.0... Patching component oracle.wcsites.satelliteserver, 12.2.1.4.0... Patching component oracle.wcsites.sitecapture, 12.2.1.4.0... Patching component oracle.wcsites.sitecapture, 12.2.1.4.0... Patch 31548912 successfully applied. Log file location: /u01/oracle/cfgtoollogs/opatch/opatch2020-08-04_05-15-38AM_1.log OPatch succeeded. Oracle Interim Patch Installer version 13.9.4.2.4 Copyright (c) 2020, Oracle Corporation. All rights reserved. Oracle Home : /u01/oracle Central Inventory : /u01/oracle/oraInventory from : /u01/oracle/oraInst.loc OPatch version : 13.9.4.2.4 OUI version : 13.9.4.0.0 Log file location : /u01/oracle/cfgtoollogs/opatch/opatch2020-08-04_05-16-11AM_1.log OPatch detects the Middleware Home as \u0026quot;/u01/oracle\u0026quot; Invoking utility \u0026quot;cleanup\u0026quot; OPatch will clean up 'restore.sh,make.txt' files and 'scratch,backup' directories. You will be still able to rollback patches after this cleanup. Do you want to proceed? [y|n] Y (auto-answered by -silent) User Responded with: Y Backup area for restore has been cleaned up. For a complete list of files/directories deleted, Please refer log file. OPatch succeeded. Removing intermediate container 5039320b2f10 ---\u0026gt; 1be958e1e859 Successfully built 1be958e1e859 Successfully tagged oracle/wcsites:12.2.1.4-21.1.1-29710661 [INFO ] Build successful. Build time=73s. Image tag=oracle/wcsites:12.2.1.4-21.1.1-29710661      Click here to see the example Dockerfile generated by the WebLogic Image Tool with the `–-dryRun` option:   $ imagetool update --fromImage oracle/wcsites:12.2.1.4-21.1.1 --tag=oracle/wcsites:12.2.1.4-21.1.1-29710661 --patches=29710661_12.2.1.4.0 --dryRun [INFO ] Image Tool build ID: a2fca032-7807-4bfb-b5a4-0ed90a710a56 [INFO ] Temporary directory used for docker build context: %path-to-temp-directory%/tmpBuild/wlsimgbuilder_temp4743247141639108603 [INFO ] Using patch 28186730_13.9.4.2.4 from cache: %path-to-downloaded-pathes%/patches /p28186730_139424_Generic.zip [INFO ] OPatch will not be updated, fromImage has version 13.9.4.2.4, available version is 13.9.4.2.4 [WARNING] skipping patch conflict check, no support credentials provided [WARNING] No credentials provided, skipping validation of patches [INFO ] Using patch 29710661_12.2.1.4.0 from cache: %path-to-downloaded-pathes%/patches /p29710661_122140_Generic.zip [INFO ] docker cmd = docker build --no-cache --force-rm --tag oracle/wcsites:12.2.1.4-21.1.1.1 --build-arg http_proxy=http://www-proxy-your-company.com:80 --build-arg https_proxy=http://www-proxy-your-company.com:80 --build-arg no_proxy=localhost,127.0.0.0/8,/var/run/docker.sock %path-to-temp-directory%/tmpBuild/wlsimgbuilder_temp4743247141639108603 ########## BEGIN DOCKERFILE ########## # # Copyright (c) 2019, 2020, Oracle and/or its affiliates. # # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # # FROM oracle/wcsites:12.2.1.4-21.1.1 as FINAL_BUILD USER root ENV OPATCH_NO_FUSER=true LABEL com.oracle.weblogic.imagetool.buildid=\u0026#34;a2fca032-7807-4bfb-b5a4-0ed90a710a56\u0026#34; USER oracle COPY --chown=oracle:oracle patches/* /tmp/imagetool/patches/ RUN /u01/oracle/OPatch/opatch napply -silent -oh /u01/oracle -phBaseDir /tmp/imagetool/patches \\  \u0026amp;\u0026amp; /u01/oracle/OPatch/opatch util cleanup -silent -oh /u01/oracle \\  \u0026amp;\u0026amp; rm -rf /tmp/imagetool ########## END DOCKERFILE ########## [INFO ] Dry run complete. No image created.      Check the built image using the Docker images command:\n$ Docker images | grep wcsites oracle/wcsites 12.2.1.4.0-33381673 2ef2a67a685b About a minute ago 2.84GB oracle/wcsites 12.2.1.4.0 445b649a3459 4 days ago 3.2GB   Create an Oracle WebCenter Sites Docker image using Dockerfile For test and development purposes, you can create an Oracle WebCenter Sites image using the Dockerfile. Consult the README file for important prerequisite steps, such as building or pulling the Server JRE Docker image, Oracle FMW Infrastructure Docker image, and downloading the Oracle WebCenter Sites installer and bundle patch binaries.\nA prebuilt Oracle Fusion Middleware Infrastructure image, container-registry.oracle.com/middleware/fmw-infrastructure:12.2.1.4, is available at container-registry.oracle.com. We recommend that you pull and rename this image to build the Oracle WebCenter Sites image.\n$ docker pull \u0026lt;path-to-container-registry\u0026gt;/fmw-infrastructure:12.2.1.4 $ docker tag \u0026lt;path-to-container-registry\u0026gt;/fmw-infrastructure:12.2.1.4 oracle/fmw-infrastructure:12.2.1.4 Follow these steps to build an Oracle Fusion Middleware Infrastructure image, and then the Oracle WebCenter Sites image as a layer on top of that:\n  Make a local clone of the sample repository:\n$ git clone https://github.com/oracle/docker-images   Build the oracle/fmw-infrastructure:12.2.1.4 image:\n$ cd docker-images/OracleFMWInfrastructure/dockerfiles $ sh buildDockerImage.sh -v 12.2.1.4 -s This will produce an image named oracle/fmw-infrastructure:12.2.1.4.\n  Download the Oracle WebCenter Sites installer from the Oracle Technology Network or e-delivery.\n Note: Copy the installer binaries to the same location as the Dockerfile.\n   To build the Oracle WebCenter Sites image with patches, you must download and drop the patch zip files (for example, p33381673_122140_Generic.zip) into the patches/ folder under the version that is required. For example, for 12.2.1.4 the folder is 12.2.1.4/patches.\n  Create the Oracle WebCenter Sites image by running the provided script:\n$ cd docker-images/OracleWebCenterSites/dockerfiles $ ./buildDockerImage.sh -v 12.2.1.4 -s The image produced will be named oracle/wcsites:12.2.1.4. The samples and instructions assume the Oracle WebCenter Sites image is named wcsites:12.2.1.4. You must rename your image to match this name, or update the samples to refer to the image you created.\n$ docker tag oracle/wcsites:12.2.1.4 oracle/wcsites:12.2.1.4-21.1.1   "
},
{
	"uri": "/fmw-kubernetes/soa-domains/adminguide/enablingt3/",
	"title": "Expose the T3/T3S protocol",
	"tags": [],
	"description": "Create a T3/T3S channel and the corresponding Kubernetes service to expose the T3/T3S protocol for the Administration Server and Managed Servers in an Oracle SOA Suite domain.",
	"content": " Oracle strongly recommends that you do not expose non-HTTPS traffic (T3/T3s/LDAP/IIOP/IIOPs) outside of the external firewall. You can control this access using a combination of network channels and firewalls.\n You can create T3/T3S channels and the corresponding Kubernetes service to expose the T3/T3S protocol for the Administration Server and Managed Servers in an Oracle SOA Suite domain.\nThe WebLogic Kubernetes Operator provides an option to expose a T3 channel for the Administration Server using the exposeAdminT3Channel setting during domain creation, then the matching T3 service can be used to connect. By default, when exposeAdminT3Channel is set, the WebLogic Kubernetes Operator environment exposes the NodePort for the T3 channel of the NetworkAccessPoint at 30012 (use t3ChannelPort to configure the port to a different value).\nIf you miss enabling exposeAdminT3Channel during domain creation, follow these steps to create a T3 channel for Administration Server manually.\nExpose a T3/T3S Channel for the Administration Server To create a custom T3/T3S channel for the Administration Server that has a listen port listen_port and a paired public port public_port:\n  Create t3_admin_config.py with the following content:\nadmin_pod_name = sys.argv[1] admin_port = sys.argv[2] user_name = sys.argv[3] password = sys.argv[4] listen_port = sys.argv[5] public_port = sys.argv[6] public_address = sys.argv[7] AdminServerName = sys.argv[8] channelType = sys.argv[9] print(\u0026#39;custom admin_pod_name : [%s]\u0026#39; % admin_pod_name); print(\u0026#39;custom admin_port : [%s]\u0026#39; % admin_port); print(\u0026#39;custom user_name : [%s]\u0026#39; % user_name); print(\u0026#39;custom password : ********\u0026#39;); print(\u0026#39;public address : [%s]\u0026#39; % public_address); print(\u0026#39;channel listen port : [%s]\u0026#39; % listen_port); print(\u0026#39;channel public listen port : [%s]\u0026#39; % public_port); connect(user_name, password, \u0026#39;t3://\u0026#39; + admin_pod_name + \u0026#39;:\u0026#39; + admin_port) edit() startEdit() cd(\u0026#39;/\u0026#39;) cd(\u0026#39;Servers/%s/\u0026#39; % AdminServerName ) if channelType == \u0026#39;t3\u0026#39;: create(\u0026#39;T3Channel_AS\u0026#39;,\u0026#39;NetworkAccessPoint\u0026#39;) cd(\u0026#39;NetworkAccessPoints/T3Channel_AS\u0026#39;) set(\u0026#39;Protocol\u0026#39;,\u0026#39;t3\u0026#39;) set(\u0026#39;ListenPort\u0026#39;,int(listen_port)) set(\u0026#39;PublicPort\u0026#39;,int(public_port)) set(\u0026#39;PublicAddress\u0026#39;, public_address) print(\u0026#39;Channel T3Channel_AS added\u0026#39;) elif channelType == \u0026#39;t3s\u0026#39;:\tcreate(\u0026#39;T3SChannel_AS\u0026#39;,\u0026#39;NetworkAccessPoint\u0026#39;) cd(\u0026#39;NetworkAccessPoints/T3SChannel_AS\u0026#39;) set(\u0026#39;Protocol\u0026#39;,\u0026#39;t3s\u0026#39;) set(\u0026#39;ListenPort\u0026#39;,int(listen_port)) set(\u0026#39;PublicPort\u0026#39;,int(public_port)) set(\u0026#39;PublicAddress\u0026#39;, public_address) set(\u0026#39;HttpEnabledForThisProtocol\u0026#39;, true) set(\u0026#39;OutboundEnabled\u0026#39;, false) set(\u0026#39;Enabled\u0026#39;, true) set(\u0026#39;TwoWaySSLEnabled\u0026#39;, true) set(\u0026#39;ClientCertificateEnforced\u0026#39;, false) else: print(\u0026#39;channelType [%s] not supported\u0026#39;,channelType) activate() disconnect()   Copy t3_admin_config.py into the domain home (for example, /u01/oracle/user_projects/domains/soainfra) of the Administration Server pod (for example, soainfra-adminserver in soans namespace).\n $ kubectl cp t3_admin_config.py soans/soainfra-adminserver:/u01/oracle/user_projects/domains/soainfra    Run wlst.sh t3_admin_config.py by using exec into the Administration Server pod with the following parameters:\n admin_pod_name: soainfra-adminserver # Administration Server pod admin_port: 7001 user_name: weblogic password: Welcome1 # weblogic password listen_port: 30014 # New port for T3 Administration Server public_port: 30014 # Kubernetes NodePort which will be used to expose T3 port externally public_address:  AdminServerName: AdminServer # Give administration Server name channelType: t3 # t3 or t3s protocol channel  $ kubectl exec -it \u0026lt;Administration Server pod\u0026gt; -n \u0026lt;namespace\u0026gt; -- /u01/oracle/oracle_common/common/bin/wlst.sh \u0026lt;domain_home\u0026gt;/t3_admin_config.py \u0026lt;Administration Server pod\u0026gt; \u0026lt;Administration Server port\u0026gt; weblogic \u0026lt;password for weblogic\u0026gt; \u0026lt;t3 port on Administration Server\u0026gt; \u0026lt;t3 nodeport\u0026gt; \u0026lt;master_ip\u0026gt; \u0026lt;AdminServerName\u0026gt; \u0026lt;channelType t3 or t3s\u0026gt; For example:\n$ kubectl exec -it soainfra-adminserver -n soans -- /u01/oracle/oracle_common/common/bin/wlst.sh /u01/oracle/user_projects/domains/soainfra/t3_admin_config.py soainfra-adminserver 7001 weblogic Welcome1 30014 30014 xxx.xxx.xxx.xxx AdminServer t3   Create t3_admin_svc.yaml with the following contents to expose T3 at NodePort 30014 for domainName and domainUID as soainfra and domain deployed in soans namespace:\n Note: For T3S, replace NodePort 30014 with the appropriate value used with public_port while creating the T3S channel using wlst.sh in the previous step.\n apiVersion: v1 kind: Service metadata: name: soainfra-adminserver-t3-external namespace: soans labels: weblogic.serverName: AdminServer weblogic.domainName: soainfra weblogic.domainUID: soainfra spec: type: NodePort selector: weblogic.domainName: soainfra weblogic.domainUID: soainfra weblogic.serverName: AdminServer ports: - name: t3adminport protocol: TCP port: 30014 targetPort: 30014 nodePort: 30014   Create the NodePort service for port 30014:\n$ kubectl create -f t3_admin_svc.yaml   Verify that you can access T3 for the Administration Server with the following URL:\nt3://\u0026lt;master_ip\u0026gt;:30014   Similarly, you can access T3S as follows:\na. First get the certificates from the Administration Server to be used for secured (T3S) connection from the client. You can export the certificate from the Administration Server with WLST commands. For example, to export the default demoidentity:\n Note: If you are using the custom SSL certificate, replace the steps accordingly.\n $ kubectl exec -it soainfra-adminserver -n soans -- bash $ /u01/oracle/oracle_common/common/bin/wlst.sh $ connect('weblogic','Welcome1','t3://soainfra-adminserver:7001') $ svc = getOpssService(name='KeyStoreService') $ svc.exportKeyStoreCertificate(appStripe='system', name='demoidentity', password='DemoIdentityKeyStorePassPhrase', alias='DemoIdentity', type='Certificate', filepath='/tmp/cert.txt/') These steps download the certificate at /tmp/cert.txt.\nb. Use the same certificates from the client side and connect using t3s. For example:\n$ export JAVA_HOME=/u01/jdk $ keytool -import -v -trustcacerts -alias soadomain -file cert.txt -keystore $JAVA_HOME/jre/lib/security/cacerts -keypass changeit -storepass changeit $ export WLST_PROPERTIES=\u0026quot;-Dweblogic.security.SSL.ignoreHostnameVerification=true\u0026quot; $ cd $ORACLE_HOME/oracle_common/common/bin $ ./wlst.sh Initializing WebLogic Scripting Tool (WLST) ... Welcome to WebLogic Server Administration Scripting Shell Type help() for help on available commands $ wls:/offline\u0026gt; connect('weblogic','Welcome1','t3s://\u0026lt;Master IP address\u0026gt;:30014')   Expose T3/T3S for Managed Servers To create a custom T3/T3S channel for all Managed Servers, with a listen port listen_port and a paired public port public_port:\n  Create t3_ms_config.py with the following content:\nadmin_pod_name = sys.argv[1] admin_port = sys.argv[2] user_name = sys.argv[3] password = sys.argv[4] listen_port = sys.argv[5] public_port = sys.argv[6] public_address = sys.argv[7] managedNameBase = sys.argv[8] ms_count = sys.argv[9] channelType = sys.argv[10] print(\u0026#39;custom host : [%s]\u0026#39; % admin_pod_name); print(\u0026#39;custom port : [%s]\u0026#39; % admin_port); print(\u0026#39;custom user_name : [%s]\u0026#39; % user_name); print(\u0026#39;custom password : ********\u0026#39;); print(\u0026#39;public address : [%s]\u0026#39; % public_address); print(\u0026#39;channel listen port : [%s]\u0026#39; % listen_port); print(\u0026#39;channel public listen port : [%s]\u0026#39; % public_port); connect(user_name, password, \u0026#39;t3://\u0026#39; + admin_pod_name + \u0026#39;:\u0026#39; + admin_port) edit() startEdit() for index in range(0, int(ms_count)): cd(\u0026#39;/\u0026#39;) msIndex = index+1 cd(\u0026#39;/\u0026#39;) name = \u0026#39;%s%s\u0026#39; % (managedNameBase, msIndex) cd(\u0026#39;Servers/%s/\u0026#39; % name ) if channelType == \u0026#39;t3\u0026#39;: create(\u0026#39;T3Channel_MS\u0026#39;,\u0026#39;NetworkAccessPoint\u0026#39;) cd(\u0026#39;NetworkAccessPoints/T3Channel_MS\u0026#39;) set(\u0026#39;Protocol\u0026#39;,\u0026#39;t3\u0026#39;) set(\u0026#39;ListenPort\u0026#39;,int(listen_port)) set(\u0026#39;PublicPort\u0026#39;,int(public_port)) set(\u0026#39;PublicAddress\u0026#39;, public_address) print(\u0026#39;Channel T3Channel_MS added ...for \u0026#39; + name) elif channelType == \u0026#39;t3s\u0026#39;:\tcreate(\u0026#39;T3SChannel_MS\u0026#39;,\u0026#39;NetworkAccessPoint\u0026#39;) cd(\u0026#39;NetworkAccessPoints/T3SChannel_MS\u0026#39;) set(\u0026#39;Protocol\u0026#39;,\u0026#39;t3s\u0026#39;) set(\u0026#39;ListenPort\u0026#39;,int(listen_port)) set(\u0026#39;PublicPort\u0026#39;,int(public_port)) set(\u0026#39;PublicAddress\u0026#39;, public_address) set(\u0026#39;HttpEnabledForThisProtocol\u0026#39;, true) set(\u0026#39;OutboundEnabled\u0026#39;, false) set(\u0026#39;Enabled\u0026#39;, true) set(\u0026#39;TwoWaySSLEnabled\u0026#39;, true) set(\u0026#39;ClientCertificateEnforced\u0026#39;, false) print(\u0026#39;Channel T3SChannel_MS added ...for \u0026#39; + name) else: print(\u0026#39;Protocol [%s] not supported\u0026#39; % channelType) activate() disconnect()   Copy t3_ms_config.py into the domain home (for example, /u01/oracle/user_projects/domains/soainfra) of the Administration Server pod (for example, soainfra-adminserver in soans namespace).\n$ kubectl cp t3_ms_config.py soans/soainfra-adminserver:/u01/oracle/user_projects/domains/soainfra   Run wlst.sh t3_ms_config.py by exec into the Administration Server pod with the following parameters:\n admin_pod_name: soainfra-adminserver # Administration Server pod admin_port: 7001 user_name: weblogic password: Welcome1 # weblogic password listen_port: 30016 # New port for T3 Managed Servers public_port: 30016 # Kubernetes NodePort which will be used to expose T3 port externally public_address:  managedNameBase: soa_server # Give Managed Server base name. For osb_cluster this will be osb_server ms_count: 5 # Number of configured Managed Servers channelType: t3 # channelType is t3 or t3s  $ kubectl exec -it \u0026lt;Administration Server pod\u0026gt; -n \u0026lt;namespace\u0026gt; -- /u01/oracle/oracle_common/common/bin/wlst.sh \u0026lt;domain_home\u0026gt;/t3_ms_config.py \u0026lt;Administration Server pod\u0026gt; \u0026lt;Administration Server port\u0026gt; weblogic \u0026lt;password for weblogic\u0026gt; \u0026lt;t3 port on Managed Server\u0026gt; \u0026lt;t3 nodeport\u0026gt; \u0026lt;master_ip\u0026gt; \u0026lt;managedNameBase\u0026gt; \u0026lt;ms_count\u0026gt; \u0026lt;channelType t3 or t3s\u0026gt; For example:\n$ kubectl exec -it soainfra-adminserver -n soans -- /u01/oracle/oracle_common/common/bin/wlst.sh /u01/oracle/user_projects/domains/soainfra/t3_ms_config.py soainfra-adminserver 7001 weblogic Welcome1 30016 30016 xxx.xxx.xxx.xxx soa_server 5 t3   Create t3_ms_svc.yaml with the following contents to expose T3 at Managed Server port 30016 for domainName, domainUID as soainfra, and clusterName as soa_cluster for the SOA cluster. Similarly, you can create the Kubernetes service with clusterName as osb_cluster for an Oracle Service Bus cluster:\n Note: For T3S, replace NodePort 30016 with the appropriate value used with public_port while creating the T3S channel using wlst.sh in the previous step.\n apiVersion: v1 kind: Service metadata: name: soainfra-soa-cluster-t3-external namespace: soans labels: weblogic.clusterName: soa_cluster weblogic.domainName: soainfra weblogic.domainUID: soainfra spec: type: NodePort selector: weblogic.domainName: soainfra weblogic.domainUID: soainfra weblogic.clusterName: soa_cluster ports: - name: t3soaport protocol: TCP port: 30016 targetPort: 30016 nodePort: 30016   Create the NodePort service for port 30016:\n$ kubectl create -f t3_ms_svc.yaml   Verify that you can access T3 for the Managed Server with the following URL:\nt3://\u0026lt;master_ip\u0026gt;:30016   Similarly, you can access T3S as follows:\na. First get the certificates from the Administration Server to be used for secured (t3s) connection from client. You can export the certificate from the Administration Server with wlst commands. Sample commands to export the default demoidentity:\n Note: In case you are using the custom SSL certificate, replaces the steps accordingly\n $ kubectl exec -it soainfra-adminserver -n soans -- bash $ /u01/oracle/oracle_common/common/bin/wlst.sh $ connect('weblogic','Welcome1','t3://soainfra-adminserver:7001') $ svc = getOpssService(name='KeyStoreService') $ svc.exportKeyStoreCertificate(appStripe='system', name='demoidentity', password='DemoIdentityKeyStorePassPhrase', alias='DemoIdentity', type='Certificate', filepath='/tmp/cert.txt/') The above steps download the certificate at /tmp/cert.txt.\nb. Use the same certificates from the client side and connect using t3s. For example:\n$ export JAVA_HOME=/u01/jdk $ keytool -import -v -trustcacerts -alias soadomain -file cert.txt -keystore $JAVA_HOME/jre/lib/security/cacerts -keypass changeit -storepass changeit $ export WLST_PROPERTIES=\u0026quot;-Dweblogic.security.SSL.ignoreHostnameVerification=true\u0026quot; $ cd $ORACLE_HOME/oracle_common/common/bin $ ./wlst.sh Initializing WebLogic Scripting Tool (WLST) ... Welcome to WebLogic Server Administration Scripting Shell Type help() for help on available commands $ wls:/offline\u0026gt; connect('weblogic','Welcome1','t3s://\u0026lt;Master IP address\u0026gt;:30016')   Remove T3/T3S configuration For Administration Server   Create t3_admin_delete.py with the following content:\nadmin_pod_name = sys.argv[1] admin_port = sys.argv[2] user_name = sys.argv[3] password = sys.argv[4] AdminServerName = sys.argv[5] channelType = sys.argv[6] print(\u0026#39;custom admin_pod_name : [%s]\u0026#39; % admin_pod_name); print(\u0026#39;custom admin_port : [%s]\u0026#39; % admin_port); print(\u0026#39;custom user_name : [%s]\u0026#39; % user_name); print(\u0026#39;custom password : ********\u0026#39;); connect(user_name, password, \u0026#39;t3://\u0026#39; + admin_pod_name + \u0026#39;:\u0026#39; + admin_port) edit() startEdit() cd(\u0026#39;/\u0026#39;) cd(\u0026#39;Servers/%s/\u0026#39; % AdminServerName ) if channelType == \u0026#39;t3\u0026#39;: delete(\u0026#39;T3Channel_AS\u0026#39;,\u0026#39;NetworkAccessPoint\u0026#39;) elif channelType == \u0026#39;t3s\u0026#39;: delete(\u0026#39;T3SChannel_AS\u0026#39;,\u0026#39;NetworkAccessPoint\u0026#39;) else: print(\u0026#39;channelType [%s] not supported\u0026#39;,channelType) activate() disconnect()   Copy t3_admin_delete.py into the domain home (for example, /u01/oracle/user_projects/domains/soainfra) of the Administration Server pod (for example, soainfra-adminserver in soans namespace).\n$ kubectl cp t3_admin_delete.py soans/soainfra-adminserver:/u01/oracle/user_projects/domains/soainfra   Run wlst.sh t3_admin_delete.py by exec into the Administration Server pod with the following parameters:\n admin_pod_name: soainfra-adminserver # Administration Server pod admin_port: 7001 user_name: weblogic password: Welcome1 # weblogic password AdminServerName: AdminServer # Give administration Server name channelType: t3 # T3 channel  $ kubectl exec -it \u0026lt;Administration Server pod\u0026gt; -n \u0026lt;namespace\u0026gt; -- /u01/oracle/oracle_common/common/bin/wlst.sh \u0026lt;domain_home\u0026gt;/t3_admin_delete.py \u0026lt;Administration Server pod\u0026gt; \u0026lt;Administration Server port\u0026gt; weblogic \u0026lt;password for weblogic\u0026gt; \u0026lt;AdminServerName\u0026gt; \u0026lt;protocol t3 or t3s\u0026gt; For example:\n$ kubectl exec -it soainfra-adminserver -n soans -- /u01/oracle/oracle_common/common/bin/wlst.sh /u01/oracle/user_projects/domains/soainfra/t3_admin_delete.py soainfra-adminserver 7001 weblogic Welcome1 AdminServer t3   Delete the NodePort service for port 30014:\n$ kubectl delete -f t3_admin_svc.yaml   For Managed Servers These steps delete the custom T3/T3S channel created by Expose T3/T3S for Managed Servers for all Managed Servers.\n  Create t3_ms_delete.py with the following content:\nadmin_pod_name = sys.argv[1] admin_port = sys.argv[2] user_name = sys.argv[3] password = sys.argv[4] managedNameBase = sys.argv[5] ms_count = sys.argv[6] channelType = sys.argv[7] print(\u0026#39;custom host : [%s]\u0026#39; % admin_pod_name); print(\u0026#39;custom port : [%s]\u0026#39; % admin_port); print(\u0026#39;custom user_name : [%s]\u0026#39; % user_name); print(\u0026#39;custom password : ********\u0026#39;); connect(user_name, password, \u0026#39;t3://\u0026#39; + admin_pod_name + \u0026#39;:\u0026#39; + admin_port) edit() startEdit() for index in range(0, int(ms_count)): cd(\u0026#39;/\u0026#39;) msIndex = index+1 cd(\u0026#39;/\u0026#39;) name = \u0026#39;%s%s\u0026#39; % (managedNameBase, msIndex) cd(\u0026#39;Servers/%s/\u0026#39; % name ) if channelType == \u0026#39;t3\u0026#39;: delete(\u0026#39;T3Channel_MS\u0026#39;,\u0026#39;NetworkAccessPoint\u0026#39;) elif channelType == \u0026#39;t3s\u0026#39;: delete(\u0026#39;T3SChannel_MS\u0026#39;,\u0026#39;NetworkAccessPoint\u0026#39;) else: print(\u0026#39;Protocol [%s] not supported\u0026#39; % channelType) activate() disconnect()   Copy t3_ms_delete.py into the domain home (for example, /u01/oracle/user_projects/domains/soainfra) of the Administration Server pod (for example, soainfra-adminserver in soans namespace).\n$ kubectl cp t3_ms_delete.py soans/soainfra-adminserver:/u01/oracle/user_projects/domains/soainfra   Run wlst.sh t3_ms_delete.py by exec into the Administration Server pod with the following parameters:\n admin_pod_name: soainfra-adminserver # Administration Server pod admin_port: 7001 user_name: weblogic password: Welcome1 # weblogic password managedNameBase: soa_server # Give Managed Server base name. For osb_cluster this will be osb_server ms_count: 5 # Number of configured Managed Servers channelType: t3 # channelType is t3 or t3s  $ kubectl exec -it \u0026lt;Administration Server pod\u0026gt; -n \u0026lt;namespace\u0026gt; -- /u01/oracle/oracle_common/common/bin/wlst.sh \u0026lt;domain_home\u0026gt;/t3_ms_delete.py \u0026lt;Administration Server pod\u0026gt; \u0026lt;Administration Server port\u0026gt; weblogic \u0026lt;password for weblogic\u0026gt; \u0026lt;t3 port on Managed Server\u0026gt; \u0026lt;t3 nodeport\u0026gt; \u0026lt;master_ip\u0026gt; \u0026lt;managedNameBase\u0026gt; \u0026lt;ms_count\u0026gt; \u0026lt;channelType t3 or t3s\u0026gt; For example:\n$ kubectl exec -it soainfra-adminserver -n soans -- /u01/oracle/oracle_common/common/bin/wlst.sh /u01/oracle/user_projects/domains/soainfra/t3_ms_delete.py soainfra-adminserver 7001 weblogic Welcome1 soa_server 5 t3   Delete the NodePort service for port 30016 (or the NodePort used while creating the Kubernetes service):\n$ kubectl delete -f t3_ms_svc.yaml   "
},
{
	"uri": "/fmw-kubernetes/wcsites-domains/cleanup-domain-setup/",
	"title": "Uninstall",
	"tags": [],
	"description": "Clean up the Oracle WebCenter Sites domain setup.",
	"content": "Learn how to clean up the Oracle WebCenter Sites domain setup.\nStop all Administration and Managed server pods First stop the all pods related to a domain. This can be done by patching domain \u0026ldquo;serverStartPolicy\u0026rdquo; to \u0026ldquo;NEVER\u0026rdquo;. Here is the sample command for the same.\n$ kubectl patch domain wcsites-domain-name -n wcsites-namespace --type=\u0026#39;json\u0026#39; -p=\u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/serverStartPolicy\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;NEVER\u0026#34; }]\u0026#39; For example:\n$ kubectl patch domain wcsitesinfra -n wcsites-ns --type=\u0026#39;json\u0026#39; -p=\u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/serverStartPolicy\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;NEVER\u0026#34; }]\u0026#39; Drop the RCU schemas $ cd work-dir-name/weblogic-kubernetes-operator $ kubectl apply -f kubernetes/create-wcsites-domain/output/weblogic-domains/wcsitesinfra/delete-domain-job.yaml Check if the job has finished.\nRemove the domain   Remove the domain\u0026rsquo;s ingress (for example, Traefik ingress) using Helm:\n$ helm uninstall wcsites-domain-ingress -n wcsites-namespace For example:\n$ helm uninstall wcsitesinfra-ingress -n wsites-ns   Remove the domain resources by using the sample delete-weblogic-domain-resources.sh script present at ${WORKDIR}/fmw-kubernetes/OracleWebCenterSites/kubernetes/delete-domain:\n$ cd ${WORKDIR}/fmw-kubernetes/OracleWebCenterSites/kubernetes/delete-domain $ ./delete-weblogic-domain-resources.sh -d sample-domain1 For example:\n$ cd ${WORKDIR}/fmw-kubernetes/OracleWebCenterSites/kubernetes/delete-domain $ ./delete-weblogic-domain-resources.sh -d wcsites-ns   Use kubectl to confirm that the server pods and domain resource are deleted:\n$ kubectl get pods -n sample-domain1-ns $ kubectl get domains -n sample-domain1-ns For example:\n$ kubectl get pods -n wcsites-ns $ kubectl get domains -n wcsites-ns   Remove the domain namespace   Configure the installed ingress load balancer (for example, Traefik) to stop managing the ingresses in the domain namespace:\n$ helm upgrade traefik-operator stable/traefik \\  --namespace traefik \\  --reuse-values \\  --set \u0026#34;kubernetes.namespaces={traefik}\u0026#34; \\  --wait   Configure the operator to stop managing the domain:\n$ helm upgrade sample-weblogic-operator \\  kubernetes/charts/weblogic-operator \\  --namespace sample-weblogic-operator-ns \\  --reuse-values \\  --set \u0026#34;domainNamespaces={}\u0026#34; \\  --wait \\ For example:\n$ cd ${WORKDIR}/fmw-kubernetes/OracleWebCenterSites $ helm upgrade weblogic-kubernetes-operator \\  kubernetes/charts/weblogic-operator \\  --namespace operator-ns \\  --reuse-values \\  --set \u0026#34;domainNamespaces={}\u0026#34; \\  --wait \\   Delete the domain namespace:\n$ kubectl delete namespace sample-domain1-ns For example:\n$ kubectl delete namespace wcsites-ns   Remove the operator   Remove the operator:\n$ helm uninstall sample-weblogic-operator -n sample-weblogic-operator-ns For example:\n$ helm uninstall weblogic-kubernetes-operator -n operator-ns   Remove the operator\u0026rsquo;s namespace:\n$ kubectl delete namespace sample-weblogic-operator-ns For example:\n$ kubectl delete namespace operator-ns   Remove the load balancer   Remove the installed ingress based load balancer (for example, Traefik):\n$ helm uninstall traefik-operator -n traefik   Remove the Traefik namespace:\n$ kubectl delete namespace traefik   Delete the domain home To remove the domain home that is generated using the create-domain.sh script, with appropriate privileges manually delete the contents of the storage attached to the domain home persistent volume (PV).\nFor example, for the domain\u0026rsquo;s persistent volume of type host_path:\n$ rm -rf /scratch/K8SVolume/WCSites\r"
},
{
	"uri": "/fmw-kubernetes/wcportal-domains/cleanup-domain-setup/",
	"title": "Uninstall an Oracle WebCenter Portal domain",
	"tags": [],
	"description": "Clean up the Oracle WebCenter Portal domain setup.",
	"content": "To clean up the Oracle WebCenter Portal domain setup, follow the steps below.\nDelete the Generated Domain Home To remove a domain home that you generated by running the create-domain.sh script in your production or testing environment, use the delete-domain-job.yaml file located at, \u0026lt;$WORKDIR\u0026gt;/create-wcp-domain/domain-home-on-pv/output/weblogic-domains/wcp-domain\u0026gt; directory.\n$ kubectl create -f delete-domain-job.yaml Clean Up the create-domain-job script After Execution Failure To clean up the create-domain-job script:\n  Get the create domain job and configmaps:\n$ kubectl get configmaps,jobs -n wcpns |grep \u0026#34;create-domain-job\u0026#34;   Delete the job and configmap:\n$ kubectl delete job job.batch/wcp-domain-create-fmw-infra-sample-domain-job -n wcpns $ kubectl delete configmap wcp-domain-create-fmw-infra-sample-domain-job-cm -n wcpns   Delete the contents of the PV, if any:\n$ sudo rm -rf /scratch/kubevolume   "
},
{
	"uri": "/fmw-kubernetes/oam/configure-ingress/",
	"title": "Configure an Ingress for an OAM domain",
	"tags": [],
	"description": "This document provides steps to configure an Ingress to direct traffic to the OAM domain.",
	"content": "Setting up an ingress for NGINX for the OAM Domain The instructions below explain how to set up NGINX as an ingress for the OAM domain with SSL termination.\nNote: All the steps below should be performed on the master node.\n Generate a SSL Certificate Install NGINX Create an Ingress for the Domain Verify that you can access the domain URL  Generate a SSL Certificate   Generate a private key and certificate signing request (CSR) using a tool of your choice. Send the CSR to your certificate authority (CA) to generate the certificate.\nIf you want to use a certificate for testing purposes you can generate a self signed certificate using openssl:\n$ mkdir \u0026lt;workdir\u0026gt;/ssl $ cd \u0026lt;workdir\u0026gt;/ssl $ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \u0026#34;/CN=\u0026lt;nginx-hostname\u0026gt;\u0026#34; For example:\n$ mkdir /scratch/OAMK8S/ssl $ cd /scratch/OAMK8S/ssl $ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \u0026#34;/CN=masternode.example.com\u0026#34; Note: The CN should match the host.domain of the master node in order to prevent hostname problems during certificate verification.\nThe output will look similar to the following:\nGenerating a 2048 bit RSA private key ..........................................+++ .......................................................................................................+++ writing new private key to 'tls.key' -----   Create a secret for SSL by running the following command:\n$ kubectl -n oamns create secret tls \u0026lt;domain_uid\u0026gt;-tls-cert --key \u0026lt;workdir\u0026gt;/tls.key --cert \u0026lt;workdir\u0026gt;/tls.crt For example:\n$ kubectl -n oamns create secret tls accessdomain-tls-cert --key /scratch/OAMK8S/ssl/tls.key --cert /scratch/OAMK8S/ssl/tls.crt The output will look similar to the following:\nsecret/accessdomain-tls-cert created   Install NGINX Use helm to install NGINX.\n  Add the helm chart repository for NGINX using the following command:\n$ helm repo add stable https://kubernetes.github.io/ingress-nginx The output will look similar to the following:\n\u0026quot;stable\u0026quot; has been added to your repositories   Update the repository using the following command:\n$ helm repo update The output will look similar to the following:\nHang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \u0026quot;stable\u0026quot; chart repository Update Complete. ⎈ Happy Helming!⎈   Install NGINX using helm If you can connect directly to the master node IP address from a browser, then install NGINX with the --set controller.service.type=NodePort parameter.\nIf you are using a Managed Service for your Kubernetes cluster, for example Oracle Kubernetes Engine (OKE) on Oracle Cloud Infrastructure (OCI), and connect from a browser to the Load Balancer IP address, then use the --set controller.service.type=LoadBalancer parameter. This instructs the Managed Service to setup a Load Balancer to direct traffic to the NGINX ingress.\n  To install NGINX use the following helm command depending on if you are using NodePort or LoadBalancer:\na) Using NodePort\n$ helm install nginx-ingress -n \u0026lt;domain_namespace\u0026gt; --set controller.extraArgs.default-ssl-certificate=\u0026lt;domain_namespace\u0026gt;/\u0026lt;ssl_secret\u0026gt; --set controller.service.type=NodePort --set controller.admissionWebhooks.enabled=false stable/ingress-nginx For example:\n$ helm install nginx-ingress -n oamns --set controller.extraArgs.default-ssl-certificate=oamns/accessdomain-tls-cert --set controller.service.type=NodePort --set controller.admissionWebhooks.enabled=false stable/ingress-nginx The output will look similar to the following:\nNAME: nginx-ingress LAST DEPLOYED: Mon Mar 7 13:57:21 2022 NAMESPACE: oamns STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: The nginx-ingress controller has been installed. Get the application URL by running these commands: export HTTP_NODE_PORT=$(kubectl --namespace oamns get services -o jsonpath=\u0026quot;{.spec.ports[0].nodePort}\u0026quot; nginx-ingress-controller) export HTTPS_NODE_PORT=$(kubectl --namespace oamns get services -o jsonpath=\u0026quot;{.spec.ports[1].nodePort}\u0026quot; nginx-ingress-controller) export NODE_IP=$(kubectl --namespace oamns get nodes -o jsonpath=\u0026quot;{.items[0].status.addresses[1].address}\u0026quot;) echo \u0026quot;Visit http://$NODE_IP:$HTTP_NODE_PORT to access your application via HTTP.\u0026quot; echo \u0026quot;Visit https://$NODE_IP:$HTTPS_NODE_PORT to access your application via HTTPS.\u0026quot; An example Ingress that makes use of the controller: apiVersion: networking.k8s.io/v1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx name: example namespace: foo spec: ingressClassName: example-class rules: - host: www.example.com http: paths: - path: / pathType: Prefix backend: service: name: exampleService port: 80 # This section is only required if TLS is to be enabled for the Ingress tls: - hosts: - www.example.com secretName: example-tls If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided: apiVersion: v1 kind: Secret metadata: name: example-tls namespace: foo data: tls.crt: \u0026lt;base64 encoded cert\u0026gt; tls.key: \u0026lt;base64 encoded key\u0026gt; type: kubernetes.io/tls b) Using LoadBalancer\n$ helm install nginx-ingress -n oamns --set controller.extraArgs.default-ssl-certificate=oamns/accessdomain-tls-cert --set controller.service.type=LoadBalancer --set controller.admissionWebhooks.enabled=false stable/ingress-nginx The output will look similar to the following:\n$ helm install nginx-ingress -n oamns --set controller.extraArgs.default-ssl-certificate=oamns/accessdomain-tls-cert --set controller.service.type=LoadBalancer --set controller.admissionWebhooks.enabled=false stable/ingress-nginx NAME: nginx-ingress LAST DEPLOYED: Mon Mar 7 13:57:21 2022 NAMESPACE: nginxssl STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: The ingress-nginx controller has been installed. It may take a few minutes for the LoadBalancer IP to be available. You can watch the status by running 'kubectl --namespace oamns get services -o wide -w nginx-ingress-ingress-nginx-controller' An example Ingress that makes use of the controller: apiVersion: networking.k8s.io/v1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx name: example namespace: foo spec: ingressClassName: example-class rules: - host: www.example.com http: paths: - path: / pathType: Prefix backend: service: name: exampleService port: 80 # This section is only required if TLS is to be enabled for the Ingress tls: - hosts: - www.example.com secretName: example-tls If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided: apiVersion: v1 kind: Secret metadata: name: example-tls namespace: foo data: tls.crt: \u0026lt;base64 encoded cert\u0026gt; tls.key: \u0026lt;base64 encoded key\u0026gt; type: kubernetes.io/tls   Create an Ingress for the Domain   Navigate to the following directory:\n$ cd $WORKDIR/kubernetes/charts/ingress-per-domain   Edit the values.yaml and change the domainUID: parameter to match your domainUID, for example domainUID: accessdomain. The file should look as follows:\n# Load balancer type. Supported values are: NGINX type: NGINX # Type of Configuration Supported Values are : SSL and NONSSL sslType: SSL # domainType Supported values are soa,osb and soaosb. #WLS domain as backend to the load balancer wlsDomain: domainUID: accessdomain adminServerName: AdminServer adminServerPort: 7001 adminServerSSLPort: oamClusterName: oam_cluster oamManagedServerPort: 14100 oamManagedServerSSLPort: policyClusterName: policy_cluster policyManagedServerPort: 15100 policyManagedServerSSLPort:   Run the following helm command to install the ingress:\n$ cd $WORKDIR $ helm install oam-nginx kubernetes/charts/ingress-per-domain --namespace \u0026lt;domain_namespace\u0026gt; --values kubernetes/charts/ingress-per-domain/values.yaml For example:\n$ cd $WORKDIR $ helm install oam-nginx kubernetes/charts/ingress-per-domain --namespace oamns --values kubernetes/charts/ingress-per-domain/values.yaml The output will look similar to the following:\nNAME: oam-nginx LAST DEPLOYED: Mon Mar 7 14:01:01 2022 NAMESPACE: oamns STATUS: deployed REVISION: 1 TEST SUITE: None   Run the following command to show the ingress is created successfully:\n$ kubectl get ing -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get ing -n oamns The output will look similar to the following:\nNAME CLASS HOSTS ADDRESS PORTS AGE access-ingress \u0026lt;none\u0026gt; * 10.101.132.251 80 2m53s   Find the node port of NGINX using the following command:\n$ kubectl --namespace \u0026lt;domain_namespace\u0026gt; get services -o jsonpath=\u0026#34;{.spec.ports[1].nodePort}\u0026#34; nginx-ingress-ingress-nginx-controller For example:\n$ kubectl --namespace oamns get services -o jsonpath=\u0026#34;{.spec.ports[1].nodePort}\u0026#34; nginx-ingress-ingress-nginx-controller The output will look similar to the following:\n31051   Run the following command to check the ingress:\n$ kubectl describe ing access-ingress -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl describe ing access-ingress -n oamns The output will look similar to the following:\nName: access-ingress Namespace: oamns Address: 10.101.132.251 Default backend: default-http-backend:80 (\u0026lt;error: endpoints \u0026quot;default-http-backend\u0026quot; not found\u0026gt;) Rules: Host Path Backends ---- ---- -------- * /console accessdomain-adminserver:7001 (10.244.6.63:7001) /rreg/rreg accessdomain-adminserver:7001 (10.244.6.63:7001) /em accessdomain-adminserver:7001 (10.244.6.63:7001) /oamconsole accessdomain-adminserver:7001 (10.244.6.63:7001) /dms accessdomain-adminserver:7001 (10.244.6.63:7001) /oam/services/rest accessdomain-adminserver:7001 (10.244.6.63:7001) /iam/admin/config accessdomain-adminserver:7001 (10.244.6.63:7001) /iam/admin/diag accessdomain-adminserver:7001 (10.244.6.63:7001) /iam/access accessdomain-cluster-oam-cluster:14100 (10.244.5.12:14100,10.244.6.64:14100) /oam/admin/api accessdomain-adminserver:7001 (10.244.6.63:7001) /oam/services/rest/access/api accessdomain-cluster-oam-cluster:14100 (10.244.5.12:14100,10.244.6.64:14100) /access accessdomain-cluster-policy-cluster:15100 (10.244.5.13:15100,10.244.6.65:15100) / accessdomain-cluster-oam-cluster:14100 (10.244.5.12:14100,10.244.6.64:14100) Annotations: kubernetes.io/ingress.class: nginx meta.helm.sh/release-name: oam-nginx meta.helm.sh/release-namespace: oamns nginx.ingress.kubernetes.io/configuration-snippet: more_set_input_headers \u0026quot;X-Forwarded-Proto: https\u0026quot;; more_set_input_headers \u0026quot;WL-Proxy-SSL: true\u0026quot;; nginx.ingress.kubernetes.io/enable-access-log: false nginx.ingress.kubernetes.io/ingress.allow-http: false nginx.ingress.kubernetes.io/proxy-buffer-size: 2000k Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Sync 6m22s (x2 over 6m31s) nginx-ingress-controller Scheduled for sync   To confirm that the new ingress is successfully routing to the domain\u0026rsquo;s server pods, run the following command to send a request to the URL for the WebLogic ReadyApp framework:\n$ curl -v -k https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/weblogic/ready For example:\na) For NodePort\n$ curl -v -k https://masternode.example.com:31051/weblogic/ready b) For LoadBalancer:\n$ curl -v -k https://loadbalancer.example.com/weblogic/ready The output will look similar to the following:\n* Trying 12.345.67.89... * Connected to 12.345.67.89 (12.345.67.89) port 31051 (#0) * Initializing NSS with certpath: sql:/etc/pki/nssdb * skipping SSL peer certificate verification * SSL connection using TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 * Server certificate: * subject: CN=masternode.example.com * start date: Nov 01 14:31:07 2021 GMT * expire date: Nov 01 14:31:07 2022 GMT * common name: masternode.example.com * issuer: CN=masternode.example.com \u0026gt; GET /weblogic/ready HTTP/1.1 \u0026gt; User-Agent: curl/7.29.0 \u0026gt; Host: masternode.example.com:31051 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; Date: Mon, 01 Nov 2021 15:06:12 GMT \u0026lt; Content-Length: 0 \u0026lt; Connection: keep-alive \u0026lt; Strict-Transport-Security: max-age=15724800; includeSubDomains \u0026lt; * Connection #0 to host 12.345.67.89 left intact   Verify that you can access the domain URL After setting up the NGINX ingress, verify that the domain applications are accessible through the NGINX ingress port (for example 31051) as per Validate Domain URLs \n"
},
{
	"uri": "/fmw-kubernetes/oig/configure-ingress/",
	"title": "Configure an ingress for an OIG domain",
	"tags": [],
	"description": "This document provides steps to configure an Ingress to direct traffic to the OIG domain.",
	"content": "Choose one of the following supported methods to configure an Ingress to direct traffic for your OIG domain.\n a. Using an Ingress with NGINX (non-SSL)  Steps to set up an Ingress for NGINX to direct traffic to the OIG domain (non-SSL).\n b. Using an Ingress with NGINX (SSL)  Steps to set up an Ingress for NGINX to direct traffic to the OIG domain using SSL.\n "
},
{
	"uri": "/fmw-kubernetes/oid/configure-ingress/",
	"title": "Configure an Ingress for OID",
	"tags": [],
	"description": "This document provides steps to configure an ingress controller to direct traffic to OID.",
	"content": "  Introduction\n  Install NGINX\na. Configure the repository\nb. Create a namespace\nc. Install NGINX using helm\n  Access to interfaces through ingress\na. Using LDAP utilities\nb. Validate access using LDAP utilities\nc. Validate OID using Oracle Directory Services Manager\n  Introduction The instructions below explain how to set up NGINX as an ingress for OID.\nBy default the ingress configuration only supports HTTP and HTTPS ports. To allow LDAP and LDAPS communication over TCP, configuration is required at the ingress controller/implementation level.\nInstall NGINX Use Helm to install NGINX.\nConfigure the repository   Add the Helm chart repository for installing NGINX using the following command:\n$ helm repo add stable https://kubernetes.github.io/ingress-nginx The output will look similar to the following:\n\u0026quot;stable\u0026quot; has been added to your repositories   Update the repository using the following command:\n$ helm repo update The output will look similar to the following:\nHang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \u0026quot;stable\u0026quot; chart repository Update Complete. Happy Helming!   Create a namespace   Create a Kubernetes namespace for NGINX:\n$ kubectl create namespace \u0026lt;namespace\u0026gt; For example:\n$ kubectl create namespace mynginx The output will look similar to the following:\nnamespace/mynginx created   Install NGINX using helm   Create a $WORKDIR/kubernetes/helm/nginx-ingress-values-override.yaml that contains the following:\nNote: The configuration below:\n Assumes you have oid installed with value oid as a deployment/release name in the namespace oidns. If using a different deployment name and/or namespace change appropriately. Deploys an ingress using NodePort. If using an external loadbalancer, change the configuration accordingly. For more details about NGINX configuration see: NGINX Ingress Controller.  # Configuration for additional TCP ports to be exposed through Ingress # Format for each port would be like: # \u0026lt;PortNumber\u0026gt;: \u0026lt;Namespace\u0026gt;/\u0026lt;Service\u0026gt; tcp: # Map 1389 TCP port to LBR LDAP service to get requests handled through any available POD/Endpoint serving LDAP Port 3060: oidns/oid-lbr-ldap:3060 # Map 1636 TCP port to LBR LDAP service to get requests handled through any available POD/Endpoint serving LDAPS Port 3131: oidns/oid-lbr-ldap:3131 3061: oidns/oidhost1:3060 3130: oidns/oidhost1:3131 3062: oidns/oidhost2:3060 3132: oidns/oidhost2:3131 3063: oidns/oidhost3:3060 3133: oidns/oidhost3:3131 3064: oidns/oidhost4:3060 3134: oidns/oidhost4:3131 3065: oidns/oidhost5:3060 3135: oidns/oidhost5:3131 controller: admissionWebhooks: enabled: false extraArgs: # The secret referred to by this flag contains the default certificate to be used when accessing the catch-all server. # If this flag is not provided NGINX will use a self-signed certificate. # If the TLS Secret is in different namespace, name can be mentioned as \u0026lt;namespace\u0026gt;/\u0026lt;tlsSecretName\u0026gt; default-ssl-certificate: oidns/oid-tls-cert service: # controller service external IP addresses # externalIPs: # - \u0026lt; External IP Address \u0026gt; # To configure Ingress Controller Service as LoadBalancer type of Service # Based on the Kubernetes configuration, External LoadBalancer would be linked to the Ingress Controller Service type: NodePort # Configuration for NodePort to be used for Ports exposed through Ingress # If NodePorts are not defied/configured, Node Port would be assigend automatically by Kubernetes # These NodePorts are helpful while accessing services directly through Ingress and without having External Load Balancer. # nodePorts: # For HTTP Interface exposed through LoadBalancer/Ingress # http: 30080 # For HTTPS Interface exposed through LoadBalancer/Ingress # https: 30443 #tcp: # For LDAP Interface # 3060: 31389 # For LDAPS Interface # 3131: 31636   To install and configure NGINX Ingress issue the following command:\n$ helm install --namespace \u0026lt;namespace\u0026gt; \\ --values nginx-ingress-values-override.yaml \\ lbr-nginx stable/ingress-nginx \\ --set controller.admissionWebhooks.enabled=false Where:\n lbr-nginx is your deployment name stable/ingress-nginx is the chart reference  For example:\n$ helm install --namespace mynginx \\ --values nginx-ingress-values-override.yaml \\ lbr-nginx stable/ingress-nginx \\ --set controller.admissionWebhooks.enabled=false The output will look similar to the following:\nNAME: lbr-nginx LAST DEPLOYED: Wed Mar 16 16:49:35 2022 NAMESPACE: mynginx STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: The ingress-nginx controller has been installed. It may take a few minutes for the LoadBalancer IP to be available. You can watch the status by running 'kubectl --namespace mynginx get services -o wide -w lbr-nginx-ingress-nginx-controller' An example Ingress that makes use of the controller: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx name: example namespace: foo spec: rules: - host: www.example.com http: paths: - backend: serviceName: exampleService servicePort: 80 path: / # This section is only required if TLS is to be enabled for the Ingress tls: - hosts: - www.example.com secretName: example-tls If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided: apiVersion: v1 kind: Secret metadata: name: example-tls namespace: foo data: tls.crt: \u0026lt;base64 encoded cert\u0026gt; tls.key: \u0026lt;base64 encoded key\u0026gt; type: kubernetes.io/tls   Access to interfaces through ingress To view the ports for the ingress run the following command:\n$ kubectl get all -n mynginx The output will look similar to the following:\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/lbr-nginx-ingress-nginx-controller NodePort 10.97.43.76 \u0026lt;none\u0026gt; 80:30096/TCP,443:31581/TCP,3060:31862/TCP,3061:30271/TCP,3062:31507/TCP,3063:30673/TCP,3064:31562/TCP,3065:30294/TCP,3130:31220/TCP,3131:30127/TCP,3132:31969/TCP,3133:32649/TCP,3134:32042/TCP,3135:30408/TCP 71s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/lbr-nginx-ingress-nginx-controller 1/1 1 1 71s NAME DESIRED CURRENT READY AGE replicaset.apps/lbr-nginx-ingress-nginx-controller-d5577cfd7 1 1 1 71s Using LDAP utilities To use Oracle LDAP utilities such as ldapbind, ldapsearch, ldapmodify etc. you can either:\n  Run the LDAP commands from an OID installation outside the Kubernetes cluster. This requires access to an On-Premises OID installation oustide the Kubernetes cluster.\n  Run the LDAP commands from inside the OID Kubernetes pod. Execute the following command to enter the pod:\n$ kubectl exec -ti \u0026lt;pod\u0026gt; -n \u0026lt;namespace\u0026gt; -- bash For example:\n$ kubectl exec -ti oidhost1 -n oidns -- bash This will take you into a bash session in the pod:\n[oracle@oidhost1 oracle]$ Inside the container navigate to /u01/oracle/bin to view the LDAP utilties:\n[oracle@oidhost1 oracle]$ cd /u01/oracle/bin [oracle@oidhost1 bin]$ ls ldap* ldapadd ldapaddmt ldapbind ldapcompare ldapdelete ldapmoddn ldapmodify ldapmodifymt ldapsearch Note: For commands that require an ldif file, copy the file into the \u0026lt;persistent_volume\u0026gt;/oud_user_projects directory:\n$ cp file.ldif \u0026lt;peristent_volume\u0026gt;/oid_user_projects For example:\n$ cp file.ldif /scratch/shared/oid_user_projects The file can then be viewed inside the pod:\n[oracle@oidhost1 bin]$ cd /u01/oracle/user_projects [oracle@oidhost1 user_projects]$ ls *.ldif file.ldif   Validate access using LDAP utilities   Use an LDAP client such as ldapbind to connect to the OID service. In the example below ldapbind is used from inside the OID Kubernetes pod:\n[oracle@oidhost1 bin]$ ldapbind -D cn=orcladmin -w \u0026lt;password\u0026gt; -h \u0026lt;hostname_ingress\u0026gt; -p 31862 where:\n -p 31862 : is the port mapping to the LDAP port 3060 (3060:31862) from the earlier kubectl command -h \u0026lt;hostname_ingress\u0026gt; : is the hostname where the ingress is running  The output should look similar to the following:\nbind successful   Validate OID using Oracle Directory Services Manager  Access the Oracle WebLogic Server Administration Console and Oracle Directory Services Manager (ODSM) via a browser using the service port which maps to HTTPS port 443. In this example the port is 31581 (443:31581) from the earlier kubectl command.    Oracle WebLogic Server Administration Console : https://\u0026lt;hostname_ingress\u0026gt;:31581/console.\nWhen prompted, enter the username and password which corresponds to [adminUser] and [adminPassword] passed in Create OID instances.\n  Oracle Directory Services Manager : https://\u0026lt;hostname_ingress\u0026gt;:31851/odsm.\nSelect Create a New Connection and, when prompted, enter the following values.\n Server: \u0026lt;hostname_ingress\u0026gt; Port: Ingress mapped port for LDAP or LDAPS, in the example above 3060:31862/TCP or 3131:30127/TCP, namely LDAP:31862, LDAPS:30127 SSL Enabled: select if accessing LDAPS. User Name: cn=orcladmin Password: value of orcladminPassword passed in Create OID instances    "
},
{
	"uri": "/fmw-kubernetes/oud/configure-ingress/",
	"title": "Configure an Ingress for OUD",
	"tags": [],
	"description": "This document provides steps to configure an ingress controller to direct traffic to OUD.",
	"content": "  Introduction\n  Install NGINX\na. Configure the repository\nb. Create a namespace\nc. Install NGINX using helm\n  Access to interfaces through ingress\na. Changes in /etc/hosts to validate hostname based ingress rules\nb. Using LDAP utilities\nc. Validate access using LDAP\nd. Validate access using HTTPS\n  Introduction The instructions below explain how to set up NGINX as an ingress for OUD.\nBy default the ingress configuration only supports HTTP and HTTPS ports. To allow LDAP and LDAPS communication over TCP, configuration is required at the ingress controller/implementation level.\nInstall NGINX Use Helm to install NGINX.\nConfigure the repository   Add the Helm chart repository for installing NGINX using the following command:\n$ helm repo add stable https://kubernetes.github.io/ingress-nginx The output will look similar to the following:\n\u0026quot;stable\u0026quot; has been added to your repositories   Update the repository using the following command:\n$ helm repo update The output will look similar to the following:\nHang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \u0026quot;stable\u0026quot; chart repository Update Complete. Happy Helming!   Create a namespace   Create a Kubernetes namespace for NGINX:\n$ kubectl create namespace \u0026lt;namespace\u0026gt; For example:\n$ kubectl create namespace mynginx The output will look similar to the following:\nnamespace/mynginx created   Install NGINX using helm   Create a $WORKDIR/kubernetes/helm/nginx-ingress-values-override.yaml that contains the following:\nNote: The configuration below:\n Assumes that you have oud-ds-rs installed with value oud-ds-rs as a deployment/release name in the namespace oudns. If using a different deployment name and/or namespace change appropriately. Deploys an ingress using LoadBalancer. If you prefer to use NodePort, change the configuration accordingly. For more details about NGINX configuration see: NGINX Ingress Controller.  # Configuration for additional TCP ports to be exposed through Ingress # Format for each port would be like: # \u0026lt;PortNumber\u0026gt;: \u0026lt;Namespace\u0026gt;/\u0026lt;Service\u0026gt; tcp: # Map 1389 TCP port to LBR LDAP service to get requests handled through any available POD/Endpoint serving LDAP Port 1389: oudns/oud-ds-rs-lbr-ldap:ldap # Map 1636 TCP port to LBR LDAP service to get requests handled through any available POD/Endpoint serving LDAPS Port 1636: oudns/oud-ds-rs-lbr-ldap:ldaps controller: admissionWebhooks: enabled: false extraArgs: # The secret referred to by this flag contains the default certificate to be used when accessing the catch-all server. # If this flag is not provided NGINX will use a self-signed certificate. # If the TLS Secret is in different namespace, name can be mentioned as \u0026lt;namespace\u0026gt;/\u0026lt;tlsSecretName\u0026gt; default-ssl-certificate: oudns/oud-ds-rs-tls-cert service: # controller service external IP addresses # externalIPs: # - \u0026lt; External IP Address \u0026gt; # To configure Ingress Controller Service as LoadBalancer type of Service # Based on the Kubernetes configuration, External LoadBalancer would be linked to the Ingress Controller Service type: LoadBalancer # Configuration for NodePort to be used for Ports exposed through Ingress # If NodePorts are not defied/configured, Node Port would be assigend automatically by Kubernetes # These NodePorts are helpful while accessing services directly through Ingress and without having External Load Balancer. nodePorts: # For HTTP Interface exposed through LoadBalancer/Ingress http: 30080 # For HTTPS Interface exposed through LoadBalancer/Ingress https: 30443 tcp: # For LDAP Interface 1389: 31389 # For LDAPS Interface 1636: 31636   To install and configure NGINX Ingress issue the following command:\n$ helm install --namespace \u0026lt;namespace\u0026gt; \\ --values nginx-ingress-values-override.yaml \\ lbr-nginx stable/ingress-nginx Where:\n lbr-nginx is your deployment name stable/ingress-nginx is the chart reference  For example:\n$ helm install --namespace mynginx \\ --values nginx-ingress-values-override.yaml \\ lbr-nginx stable/ingress-nginx The output will look similar to the following:\nNAME: lbr-nginx LAST DEPLOYED: Wed Mar 16 16:49:35 2022 NAMESPACE: mynginx STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: The ingress-nginx controller has been installed. It may take a few minutes for the LoadBalancer IP to be available. You can watch the status by running 'kubectl --namespace mynginx get services -o wide -w lbr-nginx-ingress-nginx-controller' An example Ingress that makes use of the controller: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx name: example namespace: foo spec: rules: - host: www.example.com http: paths: - backend: serviceName: exampleService servicePort: 80 path: / # This section is only required if TLS is to be enabled for the Ingress tls: - hosts: - www.example.com secretName: example-tls If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided: apiVersion: v1 kind: Secret metadata: name: example-tls namespace: foo data: tls.crt: \u0026lt;base64 encoded cert\u0026gt; tls.key: \u0026lt;base64 encoded key\u0026gt; type: kubernetes.io/tls   Optional: Command helm upgrade to update nginx-ingress If required, an nginx-ingress deployment can be updated/upgraded with following command. In this example, nginx-ingress configuration is updated with an additional TCP port and Node Port for accessing the LDAP/LDAPS port of a specific POD:\n  Create a nginx-ingress-values-override.yaml that contains the following:\n# Configuration for additional TCP ports to be exposed through Ingress # Format for each port would be like: # \u0026lt;PortNumber\u0026gt;: \u0026lt;Namespace\u0026gt;/\u0026lt;Service\u0026gt; tcp: # Map 1389 TCP port to LBR LDAP service to get requests handled through any available POD/Endpoint serving LDAP Port 1389: oudns/oud-ds-rs-lbr-ldap:ldap # Map 1636 TCP port to LBR LDAP service to get requests handled through any available POD/Endpoint serving LDAPS Port 1636: oudns/oud-ds-rs-lbr-ldap:ldaps # Map specific ports for LDAP and LDAPS communication from individual Services/Pods # To redirect requests on 3890 port to oudns/oud-ds-rs-ldap-0:ldap 3890: oudns/oud-ds-rs-ldap-0:ldap # To redirect requests on 6360 port to oudns/oud-ds-rs-ldaps-0:ldap 6360: oudns/oud-ds-rs-ldap-0:ldaps # To redirect requests on 3891 port to oudns/oud-ds-rs-ldap-1:ldap 3891: oudns/oud-ds-rs-ldap-1:ldap # To redirect requests on 6361 port to oudns/oud-ds-rs-ldaps-1:ldap 6361: oudns/oud-ds-rs-ldap-1:ldaps # To redirect requests on 3892 port to oudns/oud-ds-rs-ldap-2:ldap 3892: oudns/oud-ds-rs-ldap-2:ldap # To redirect requests on 6362 port to oudns/oud-ds-rs-ldaps-2:ldap 6362: oudns/oud-ds-rs-ldap-2:ldaps # Map 1444 TCP port to LBR Admin service to get requests handled through any available POD/Endpoint serving Admin LDAPS Port 1444: oudns/oud-ds-rs-lbr-admin:adminldaps # To redirect requests on 4440 port to oudns/oud-ds-rs-0:adminldaps 4440: oudns/oud-ds-rs-0:adminldaps # To redirect requests on 4441 port to oudns/oud-ds-rs-1:adminldaps 4441: oudns/oud-ds-rs-1:adminldaps # To redirect requests on 4442 port to oudns/oud-ds-rs-2:adminldaps 4442: oudns/oud-ds-rs-2:adminldaps controller: admissionWebhooks: enabled: false extraArgs: # The secret referred to by this flag contains the default certificate to be used when accessing the catch-all server. # If this flag is not provided NGINX will use a self-signed certificate. # If the TLS Secret is in different namespace, name can be mentioned as \u0026lt;namespace\u0026gt;/\u0026lt;tlsSecretName\u0026gt; default-ssl-certificate: oudns/oud-ds-rs-tls-cert service: # controller service external IP addresses # externalIPs: # - \u0026lt; External IP Address \u0026gt; # To configure Ingress Controller Service as LoadBalancer type of Service # Based on the Kubernetes configuration, External LoadBalancer would be linked to the Ingress Controller Service type: LoadBalancer # Configuration for NodePort to be used for Ports exposed through Ingress # If NodePorts are not defied/configured, Node Port would be assigend automatically by Kubernetes # These NodePorts are helpful while accessing services directly through Ingress and without having External Load Balancer. nodePorts: # For HTTP Interface exposed through LoadBalancer/Ingress http: 30080 # For HTTPS Interface exposed through LoadBalancer/Ingress https: 30443 tcp: # For LDAP Interface referring to LBR LDAP services serving LDAP port 1389: 31389 # For LDAPS Interface referring to LBR LDAP services serving LDAPS port 1636: 31636 # For LDAP Interface from specific service oud-ds-rs-ldap-0 3890: 30890 # For LDAPS Interface from specific service oud-ds-rs-ldap-0 6360: 30360 # For LDAP Interface from specific service oud-ds-rs-ldap-1 3891: 30891 # For LDAPS Interface from specific service oud-ds-rs-ldap-1 6361: 30361 # For LDAP Interface from specific service oud-ds-rs-ldap-2 3892: 30892 # For LDAPS Interface from specific service oud-ds-rs-ldap-2 6362: 30362 # For LDAPS Interface referring to LBR Admin services serving adminldaps port 1444: 31444 # For Admin LDAPS Interface from specific service oud-ds-rs-0 4440: 30440 # For Admin LDAPS Interface from specific service oud-ds-rs-1 4441: 30441 # For Admin LDAPS Interface from specific service oud-ds-rs-2 4442: 30442   Run the following command to upgrade the ingress:\n$ helm upgrade --namespace \u0026lt;namespace\u0026gt; \\ --values nginx-ingress-values-override.yaml \\ lbr-nginx stable/ingress-nginx Where:\n lbr-nginx is your deployment name stable/ingress-nginx is the chart reference  For example:\n$ helm upgrade --namespace mynginx \\ --values nginx-ingress-values-override.yaml \\ lbr-nginx stable/ingress-nginx   Access to interfaces through ingress Using the Helm chart, ingress objects are created according to configuration. The following table details the rules configured in ingress object(s) for access to Oracle Unified Directory Interfaces through ingress.\n   Port NodePort Host Example Hostname Path Backend Service:Port Example Service Name:Port     http/https 30080/30443 \u0026lt;deployment/release name\u0026gt;-admin-0 oud-ds-rs-admin-0 * \u0026lt;deployment/release name\u0026gt;-0:adminhttps oud-ds-rs-0:adminhttps   http/https 30080/30443 \u0026lt;deployment/release name\u0026gt;-admin-N oud-ds-rs-admin-N * \u0026lt;deployment/release name\u0026gt;-N:adminhttps oud-ds-rs-1:adminhttps   http/https 30080/30443 \u0026lt;deployment/release name\u0026gt;-admin oud-ds-rs-admin * \u0026lt;deployment/release name\u0026gt;-lbr-admin:adminhttps oud-ds-rs-lbr-admin:adminhttps   http/https 30080/30443 * * /rest/v1/admin \u0026lt;deployment/release name\u0026gt;-lbr-admin:adminhttps oud-ds-rs-lbr-admin:adminhttps   http/https 30080/30443 \u0026lt;deployment/release name\u0026gt;-http-0 oud-ds-rs-http-0 * \u0026lt;deployment/release name\u0026gt;-http-0:http oud-ds-rs-http-0:http   http/https 30080/30443 \u0026lt;deployment/release name\u0026gt;-http-N oud-ds-rs-http-N * \u0026lt;deployment/release name\u0026gt;-http-N:http oud-ds-rs-http-N:http   http/https 30080/30443 \u0026lt;deployment/release name\u0026gt;-http oud-ds-rs-http * \u0026lt;deployment/release name\u0026gt;-lbr-http:http oud-ds-rs-lbr-http:http   http/https 30080/30443 * * /rest/v1/directory \u0026lt;deployment/release name\u0026gt;-lbr-http:http oud-ds-rs-lbr-http:http   http/https 30080/30443 * * /iam/directory \u0026lt;deployment/release name\u0026gt;-lbr-http:http oud-ds-rs-lbr-http:http     In the table above, example values are based on the value \u0026lsquo;oud-ds-rs\u0026rsquo; as the deployment/release name for Helm chart installation.The NodePorts mentioned in the table are according to ingress configuration described in previous section.When External LoadBalancer is not available/configured, interfaces can be accessed through NodePort on a Kubernetes node.\n For LDAP/LDAPS access (based on the updated/upgraded configuration mentioned in previous section)\n   Port NodePort Backend Service:Port Example Service Name:Port     1389 31389 \u0026lt;deployment/release name\u0026gt;-lbr-ldap:ldap oud-ds-rs-lbr-ldap:ldap   1636 31636 \u0026lt;deployment/release name\u0026gt;-lbr-ldap:ldap oud-ds-rs-lbr-ldap:ldaps   1444 31444 \u0026lt;deployment/release name\u0026gt;-lbr-admin:adminldaps oud-ds-rs-lbr-admin:adminldaps   3890 30890 \u0026lt;deployment/release name\u0026gt;-ldap-0:ldap oud-ds-rs-ldap-0:ldap   6360 30360 \u0026lt;deployment/release name\u0026gt;-ldap-0:ldaps oud-ds-rs-ldap-0:ldaps   3891 30891 \u0026lt;deployment/release name\u0026gt;-ldap-1:ldap oud-ds-rs-ldap-1:ldap   6361 30361 \u0026lt;deployment/release name\u0026gt;-ldap-1:ldaps oud-ds-rs-ldap-1:ldaps   3892 30892 \u0026lt;deployment/release name\u0026gt;-ldap-2:ldap oud-ds-rs-ldap-2:ldap   6362 30362 \u0026lt;deployment/release name\u0026gt;-ldap-2:ldaps oud-ds-rs-ldap-2:ldaps   4440 30440 \u0026lt;deployment/release name\u0026gt;-0:adminldaps oud-ds-rs-ldap-0:adminldaps   4441 30441 \u0026lt;deployment/release name\u0026gt;-1:adminldaps oud-ds-rs-ldap-1:adminldaps   4442 30442 \u0026lt;deployment/release name\u0026gt;-2:adminldaps oud-ds-rs-ldap-2:adminldaps     In the table above, example values are based on value \u0026lsquo;oud-ds-rs\u0026rsquo; as the deployment/release name for helm chart installation. The NodePorts mentioned in the table are according to Ingress configuration described in previous section. When external LoadBalancer is not available/configured, Interfaces can be accessed through NodePort on a Kubernetes Node.  Changes in /etc/hosts to validate hostname based ingress rules If it is not possible to have a LoadBalancer configuration updated to have host names added for Oracle Unified Directory Interfaces then the following entries can be added in /etc/hosts files on the host from where Oracle Unified Directory interfaces will be accessed.\n\u0026lt;IP Address of External LBR or Kubernetes Node\u0026gt;\toud-ds-rs-http oud-ds-rs-http-0 oud-ds-rs-http-1 oud-ds-rs-http-2 oud-ds-rs-http-N \u0026lt;IP Address of External LBR or Kubernetes Node\u0026gt;\toud-ds-rs-admin oud-ds-rs-admin-0 oud-ds-rs-admin-1 oud-ds-rs-admin-2 oud-ds-rs-admin-N  In the table above, host names are based on the value \u0026lsquo;oud-ds-rs\u0026rsquo; as the deployment/release name for Helm chart installation. When External LoadBalancer is not available/configured, Interfaces can be accessed through NodePort on Kubernetes Node.  Using LDAP utilities To use Oracle LDAP utilities such as ldapbind, ldapsearch, ldapmodify etc. you can either:\n  Run the LDAP commands from an OUD installation outside the Kubernetes cluster. This requires access to an On-Premises OUD installation oustide the Kubernetes cluster.\n  Run the LDAP commands from inside the OUD Kubernetes pod.\n$ kubectl exec -ti \u0026lt;pod\u0026gt; -n \u0026lt;namespace\u0026gt; -- bash For example:\n$ kubectl exec -ti oud-ds-rs-0 -n oudns -- bash This will take you into a bash session in the pod:\n[oracle@oud-ds-rs-0 oracle]$ Inside the container navigate to /u01/oracle/oud/bin to view the LDAP utilties:\n[oracle@oud-ds-rs-0 oracle]$ cd /u01/oracle/oud/bin [oracle@oud-ds-rs-0 bin]$ ls ldap* ldapcompare ldapdelete ldapmodify ldappasswordmodify ldapsearch Note: For commands that require an ldif file, copy the file into the \u0026lt;persistent_volume\u0026gt;/oud_user_projects directory:\n$ cp file.ldif \u0026lt;peristent_volume\u0026gt;/oud_user_projects For example:\n$ cp file.ldif /scratch/shared/oud_user_projects The file can then be viewed inside the pod:\n[oracle@oud-ds-rs-0 bin]$ cd /u01/oracle/oud_user_projects [oracle@oud-ds-rs-0 user_projects]$ ls *.ldif file.ldif   Validate access using LDAP Note: The examples assume sample data was installed when creating the OUD instance.\nLDAP against External Load Balancer Note If your ingress is configured with type: LoadBalancer then you cannot connect to the external LoadBalancer hostname and ports from inside the pod and must connect from an OUD installation outside the cluster.\n  Command to perform ldapsearch against External LBR and LDAP port\n$OUD_HOME/bin/ldapsearch --hostname \u0026lt;External LBR\u0026gt; --port 1389 \\ -D \u0026#34;\u0026lt;Root User DN\u0026gt;\u0026#34; -w \u0026lt;Password for Root User DN\u0026gt; \\ -b \u0026#34;\u0026#34; -s base \u0026#34;(objectClass=*)\u0026#34; \u0026#34;*\u0026#34; The output will look similar to the following:\ndn: objectClass: top objectClass: ds-root-dse lastChangeNumber: 0 firstChangeNumber: 0 changelog: cn=changelog entryDN: pwdPolicySubentry: cn=Default Password Policy,cn=Password Policies,cn=config subschemaSubentry: cn=schema supportedAuthPasswordSchemes: SHA256 supportedAuthPasswordSchemes: SHA1 supportedAuthPasswordSchemes: SHA384 supportedAuthPasswordSchemes: SHA512 supportedAuthPasswordSchemes: MD5 numSubordinates: 1 supportedFeatures: 1.3.6.1.1.14 supportedFeatures: 1.3.6.1.4.1.4203.1.5.1 supportedFeatures: 1.3.6.1.4.1.4203.1.5.2 supportedFeatures: 1.3.6.1.4.1.4203.1.5.3 lastExternalChangelogCookie: vendorName: Oracle Corporation vendorVersion: Oracle Unified Directory 12.2.1.4.0 componentVersion: 4 releaseVersion: 1 platformVersion: 0 supportedLDAPVersion: 2 supportedLDAPVersion: 3 supportedControl: 1.2.826.0.1.3344810.2.3 supportedControl: 1.2.840.113556.1.4.1413 supportedControl: 1.2.840.113556.1.4.319 supportedControl: 1.2.840.113556.1.4.473 supportedControl: 1.2.840.113556.1.4.805 supportedControl: 1.3.6.1.1.12 supportedControl: 1.3.6.1.1.13.1 supportedControl: 1.3.6.1.1.13.2 supportedControl: 1.3.6.1.4.1.26027.1.5.2 supportedControl: 1.3.6.1.4.1.26027.1.5.4 supportedControl: 1.3.6.1.4.1.26027.1.5.5 supportedControl: 1.3.6.1.4.1.26027.1.5.6 supportedControl: 1.3.6.1.4.1.26027.2.3.1 supportedControl: 1.3.6.1.4.1.26027.2.3.2 supportedControl: 1.3.6.1.4.1.26027.2.3.4 supportedControl: 1.3.6.1.4.1.42.2.27.8.5.1 supportedControl: 1.3.6.1.4.1.42.2.27.9.5.2 supportedControl: 1.3.6.1.4.1.42.2.27.9.5.8 supportedControl: 1.3.6.1.4.1.4203.1.10.1 supportedControl: 1.3.6.1.4.1.4203.1.10.2 supportedControl: 2.16.840.1.113730.3.4.12 supportedControl: 2.16.840.1.113730.3.4.16 supportedControl: 2.16.840.1.113730.3.4.17 supportedControl: 2.16.840.1.113730.3.4.18 supportedControl: 2.16.840.1.113730.3.4.19 supportedControl: 2.16.840.1.113730.3.4.2 supportedControl: 2.16.840.1.113730.3.4.3 supportedControl: 2.16.840.1.113730.3.4.4 supportedControl: 2.16.840.1.113730.3.4.5 supportedControl: 2.16.840.1.113730.3.4.9 supportedControl: 2.16.840.1.113894.1.8.21 supportedControl: 2.16.840.1.113894.1.8.31 supportedControl: 2.16.840.1.113894.1.8.36 maintenanceVersion: 2 supportedSASLMechanisms: PLAIN supportedSASLMechanisms: EXTERNAL supportedSASLMechanisms: CRAM-MD5 supportedSASLMechanisms: DIGEST-MD5 majorVersion: 12 orclGUID: D41D8CD98F003204A9800998ECF8427E entryUUID: d41d8cd9-8f00-3204-a980-0998ecf8427e ds-private-naming-contexts: cn=schema hasSubordinates: true nsUniqueId: d41d8cd9-8f003204-a9800998-ecf8427e structuralObjectClass: ds-root-dse supportedExtension: 1.3.6.1.4.1.4203.1.11.1 supportedExtension: 1.3.6.1.4.1.4203.1.11.3 supportedExtension: 1.3.6.1.1.8 supportedExtension: 1.3.6.1.4.1.26027.1.6.3 supportedExtension: 1.3.6.1.4.1.26027.1.6.2 supportedExtension: 1.3.6.1.4.1.26027.1.6.1 supportedExtension: 1.3.6.1.4.1.1466.20037 namingContexts: cn=changelog namingContexts: dc=example,dc=com   Command to perform ldapsearch against External LBR and LDAP port for specific Oracle Unified Directory Interface\n$OUD_HOME/bin/ldapsearch --hostname \u0026lt;External LBR\u0026gt; --port 3890 \\ -D \u0026#34;\u0026lt;Root User DN\u0026gt;\u0026#34; -w \u0026lt;Password for Root User DN\u0026gt; \\ -b \u0026#34;\u0026#34; -s base \u0026#34;(objectClass=*)\u0026#34; \u0026#34;*\u0026#34;   LDAPS against Kubernetes NodePort for Ingress Controller Service In the example below LDAP utilities are executed from inside the oud-ds-rs-0 pod. If your ingress is configured with type: LoadBalancer you can connect to the Kubernetes hostname where the ingress is deployed using the NodePorts.\n  Command to perform ldapsearch against Kubernetes NodePort and LDAP port\n[oracle@oud-ds-rs-0 bin]$ ldapsearch --hostname \u0026lt;Kubernetes Node\u0026gt; --port 31636 \\ --useSSL --trustAll \\ -D \u0026#34;\u0026lt;Root User DN\u0026gt;\u0026#34; -w \u0026lt;Password for Root User DN\u0026gt; \\ -b \u0026#34;\u0026#34; -s base \u0026#34;(objectClass=*)\u0026#34; \u0026#34;*\u0026#34;   Validate access using HTTPS HTTPS/REST API against External LBR Host:Port Note: In all the examples below:\na) You need to have an external IP assigned at ingress level.\nb) | json_pp is used to format output in readable json format on the client side. It can be ignored if you do not have the json_pp library.\nc) Base64 of userDN:userPassword can be generated using echo -n \u0026quot;userDN:userPassword\u0026quot; | base64.\n  Command to invoke Data REST API:\n$curl --noproxy \u0026#34;*\u0026#34; -k --location \\ --request GET \u0026#39;https://\u0026lt;External LBR Host\u0026gt;/rest/v1/directory/uid=user.1,ou=People,dc=example,dc=com?scope=sub\u0026amp;attributes=*\u0026#39; \\ --header \u0026#39;Authorization: Basic \u0026lt;Base64 of userDN:userPassword\u0026gt;\u0026#39; | json_pp The output will look similar to the following:\n{ \u0026#34;msgType\u0026#34; : \u0026#34;urn:ietf:params:rest:schemas:oracle:oud:1.0:SearchResponse\u0026#34;, \u0026#34;totalResults\u0026#34; : 1, \u0026#34;searchResultEntries\u0026#34; : [ { \u0026#34;dn\u0026#34; : \u0026#34;uid=user.1,ou=People,dc=example,dc=com\u0026#34;, \u0026#34;attributes\u0026#34; : { \u0026#34;st\u0026#34; : \u0026#34;OH\u0026#34;, \u0026#34;employeeNumber\u0026#34; : \u0026#34;1\u0026#34;, \u0026#34;postalCode\u0026#34; : \u0026#34;93694\u0026#34;, \u0026#34;description\u0026#34; : \u0026#34;This is the description for Aaren Atp.\u0026#34;, \u0026#34;telephoneNumber\u0026#34; : \u0026#34;+1 390 103 6917\u0026#34;, \u0026#34;homePhone\u0026#34; : \u0026#34;+1 280 375 4325\u0026#34;, \u0026#34;initials\u0026#34; : \u0026#34;ALA\u0026#34;, \u0026#34;objectClass\u0026#34; : [ \u0026#34;top\u0026#34;, \u0026#34;inetorgperson\u0026#34;, \u0026#34;organizationalperson\u0026#34;, \u0026#34;person\u0026#34; ], \u0026#34;uid\u0026#34; : \u0026#34;user.1\u0026#34;, \u0026#34;sn\u0026#34; : \u0026#34;Atp\u0026#34;, \u0026#34;street\u0026#34; : \u0026#34;70110 Fourth Street\u0026#34;, \u0026#34;mobile\u0026#34; : \u0026#34;+1 680 734 6300\u0026#34;, \u0026#34;givenName\u0026#34; : \u0026#34;Aaren\u0026#34;, \u0026#34;mail\u0026#34; : \u0026#34;user.1@maildomain.net\u0026#34;, \u0026#34;l\u0026#34; : \u0026#34;New Haven\u0026#34;, \u0026#34;postalAddress\u0026#34; : \u0026#34;Aaren Atp$70110 Fourth Street$New Haven, OH 93694\u0026#34;, \u0026#34;pager\u0026#34; : \u0026#34;+1 850 883 8888\u0026#34;, \u0026#34;cn\u0026#34; : \u0026#34;Aaren Atp\u0026#34; } } ] }   Command to invoke Data REST API against specific Oracle Unified Directory Interface:\n$ curl --noproxy \u0026#34;*\u0026#34; -k --location \\ --request GET \u0026#39;https://oud-ds-rs-http-0/rest/v1/directory/uid=user.1,ou=People,dc=example,dc=com?scope=sub\u0026amp;attributes=*\u0026#39; \\ --header \u0026#39;Authorization: Basic \u0026lt;Base64 of userDN:userPassword\u0026gt;\u0026#39; | json_pp  For this example, it is assumed that the value \u0026lsquo;oud-ds-rs\u0026rsquo; is used as the deployment/release name for helm chart installation. It is assumed that \u0026lsquo;oud-ds-rs-http-0\u0026rsquo; points to an External LoadBalancer    HTTPS/REST API against Kubernetes NodePort for Ingress Controller Service Note: In all the examples below:\na) | json_pp is used to format output in readable json format on the client side. It can be ignored if you do not have the json_pp library.\nb) Base64 of userDN:userPassword can be generated using echo -n \u0026quot;userDN:userPassword\u0026quot; | base64.\nc) It is assumed that the value \u0026lsquo;oud-ds-rs\u0026rsquo; is used as the deployment/release name for helm chart installation.\n  Command to invoke Data SCIM API:\n$ curl --noproxy \u0026#34;*\u0026#34; -k --location \\ --request GET \u0026#39;https://\u0026lt;Kubernetes Node\u0026gt;:30443/iam/directory/oud/scim/v1/Users\u0026#39; \\ --header \u0026#39;Authorization: Basic \u0026lt;Base64 of userDN:userPassword\u0026gt;\u0026#39; | json_pp The output will look similar to the following:\n{ \u0026#34;Resources\u0026#34; : [ { \u0026#34;id\u0026#34; : \u0026#34;ad55a34a-763f-358f-93f9-da86f9ecd9e4\u0026#34;, \u0026#34;userName\u0026#34; : [ { \u0026#34;value\u0026#34; : \u0026#34;user.0\u0026#34; } ], \u0026#34;schemas\u0026#34; : [ \u0026#34;urn:ietf:params:scim:schemas:core:2.0:User\u0026#34;, \u0026#34;urn:ietf:params:scim:schemas:extension:oracle:2.0:OUD:User\u0026#34;, \u0026#34;urn:ietf:params:scim:schemas:extension:enterprise:2.0:User\u0026#34; ], \u0026#34;meta\u0026#34; : { \u0026#34;location\u0026#34; : \u0026#34;http://\u0026lt;Kubernetes Node\u0026gt;:30443/iam/directory/oud/scim/v1/Users/ad55a34a-763f-358f-93f9-da86f9ecd9e4\u0026#34;, \u0026#34;resourceType\u0026#34; : \u0026#34;User\u0026#34; }, \u0026#34;addresses\u0026#34; : [ { \u0026#34;postalCode\u0026#34; : \u0026#34;50369\u0026#34;, \u0026#34;formatted\u0026#34; : \u0026#34;Aaccf Amar$01251 Chestnut Street$Panama City, DE 50369\u0026#34;, \u0026#34;streetAddress\u0026#34; : \u0026#34;01251 Chestnut Street\u0026#34;, \u0026#34;locality\u0026#34; : \u0026#34;Panama City\u0026#34;, \u0026#34;region\u0026#34; : \u0026#34;DE\u0026#34; } ], \u0026#34;urn:ietf:params:scim:schemas:extension:oracle:2.0:OUD:User\u0026#34; : { \u0026#34;description\u0026#34; : [ { \u0026#34;value\u0026#34; : \u0026#34;This is the description for Aaccf Amar.\u0026#34; } ], \u0026#34;mobile\u0026#34; : [ { \u0026#34;value\u0026#34; : \u0026#34;+1 010 154 3228\u0026#34; } ], \u0026#34;pager\u0026#34; : [ { \u0026#34;value\u0026#34; : \u0026#34;+1 779 041 6341\u0026#34; } ], \u0026#34;objectClass\u0026#34; : [ { \u0026#34;value\u0026#34; : \u0026#34;top\u0026#34; }, { \u0026#34;value\u0026#34; : \u0026#34;organizationalperson\u0026#34; }, { \u0026#34;value\u0026#34; : \u0026#34;person\u0026#34; }, { \u0026#34;value\u0026#34; : \u0026#34;inetorgperson\u0026#34; } ], \u0026#34;initials\u0026#34; : [ { \u0026#34;value\u0026#34; : \u0026#34;ASA\u0026#34; } ], \u0026#34;homePhone\u0026#34; : [ { \u0026#34;value\u0026#34; : \u0026#34;+1 225 216 5900\u0026#34; } ] }, \u0026#34;name\u0026#34; : [ { \u0026#34;givenName\u0026#34; : \u0026#34;Aaccf\u0026#34;, \u0026#34;familyName\u0026#34; : \u0026#34;Amar\u0026#34;, \u0026#34;formatted\u0026#34; : \u0026#34;Aaccf Amar\u0026#34; } ], \u0026#34;emails\u0026#34; : [ { \u0026#34;value\u0026#34; : \u0026#34;user.0@maildomain.net\u0026#34; } ], \u0026#34;phoneNumbers\u0026#34; : [ { \u0026#34;value\u0026#34; : \u0026#34;+1 685 622 6202\u0026#34; } ], \u0026#34;urn:ietf:params:scim:schemas:extension:enterprise:2.0:User\u0026#34; : { \u0026#34;employeeNumber\u0026#34; : [ { \u0026#34;value\u0026#34; : \u0026#34;0\u0026#34; } ] } } , . . . }   Command to invoke Data SCIM API against specific Oracle Unified Directory Interface:\n$ curl --noproxy \u0026#34;*\u0026#34; -k --location \\ --request GET \u0026#39;https://oud-ds-rs-http-0:30443/iam/directory/oud/scim/v1/Users\u0026#39; \\ --header \u0026#39;Authorization: Basic \u0026lt;Base64 of userDN:userPassword\u0026gt;\u0026#39; | json_pp   HTTPS/REST Admin API Note: In all the examples below:\na) | json_pp is used to format output in readable json format on the client side. It can be ignored if you do not have the json_pp library.\nb) Base64 of userDN:userPassword can be generated using echo -n \u0026quot;userDN:userPassword\u0026quot; | base64.\n  Command to invoke Admin REST API against External LBR:\n$ curl --noproxy \u0026#34;*\u0026#34; -k --insecure --location \\ --request GET \u0026#39;https://\u0026lt;External LBR Host\u0026gt;/rest/v1/admin/?scope=base\u0026amp;attributes=vendorName\u0026amp;attributes=vendorVersion\u0026amp;attributes=ds-private-naming-contexts\u0026amp;attributes=subschemaSubentry\u0026#39; \\ --header \u0026#39;Content-Type: application/json\u0026#39; \\ --header \u0026#39;Authorization: Basic \u0026lt;Base64 of userDN:userPassword\u0026gt;\u0026#39; | json_pp The output will look similar to the following:\n{ \u0026#34;totalResults\u0026#34; : 1, \u0026#34;searchResultEntries\u0026#34; : [ { \u0026#34;dn\u0026#34; : \u0026#34;\u0026#34;, \u0026#34;attributes\u0026#34; : { \u0026#34;vendorVersion\u0026#34; : \u0026#34;Oracle Unified Directory 12.2.1.4.0\u0026#34;, \u0026#34;ds-private-naming-contexts\u0026#34; : [ \u0026#34;cn=admin data\u0026#34;, \u0026#34;cn=ads-truststore\u0026#34;, \u0026#34;cn=backups\u0026#34;, \u0026#34;cn=config\u0026#34;, \u0026#34;cn=monitor\u0026#34;, \u0026#34;cn=schema\u0026#34;, \u0026#34;cn=tasks\u0026#34;, \u0026#34;cn=virtual acis\u0026#34;, \u0026#34;dc=replicationchanges\u0026#34; ], \u0026#34;subschemaSubentry\u0026#34; : \u0026#34;cn=schema\u0026#34;, \u0026#34;vendorName\u0026#34; : \u0026#34;Oracle Corporation\u0026#34; } } ], \u0026#34;msgType\u0026#34; : \u0026#34;urn:ietf:params:rest:schemas:oracle:oud:1.0:SearchResponse\u0026#34; }   Command to invoke Admin REST API against specific Oracle Unified Directory Admin Interface:\n$ curl --noproxy \u0026#34;*\u0026#34; -k --insecure --location \\ --request GET \u0026#39;https://oud-ds-rs-admin-0/rest/v1/admin/?scope=base\u0026amp;attributes=vendorName\u0026amp;attributes=vendorVersion\u0026amp;attributes=ds-private-naming-contexts\u0026amp;attributes=subschemaSubentry\u0026#39; \\ --header \u0026#39;Content-Type: application/json\u0026#39; \\ --header \u0026#39;Authorization: Basic \u0026lt;Base64 of userDN:userPassword\u0026gt;\u0026#39; | json_pp   Command to invoke Admin REST API against Kubernetes NodePort for Ingress Controller Service\n$ curl --noproxy \u0026#34;*\u0026#34; -k --insecure --location \\ --request GET \u0026#39;https://oud-ds-rs-admin-0:30443/rest/v1/admin/?scope=base\u0026amp;attributes=vendorName\u0026amp;attributes=vendorVersion\u0026amp;attributes=ds-private-naming-contexts\u0026amp;attributes=subschemaSubentry\u0026#39; \\ --header \u0026#39;Content-Type: application/json\u0026#39; \\ --header \u0026#39;Authorization: Basic \u0026lt;Base64 of userDN:userPassword\u0026gt;\u0026#39; | json_pp   "
},
{
	"uri": "/fmw-kubernetes/oudsm/configure-ingress/",
	"title": "Configure an Ingress for OUDSM",
	"tags": [],
	"description": "This document provides steps to configure an ingress controller to direct traffic to OUDSM.",
	"content": "  Introduction\n  Install NGINX\na. Configure the repository\nb. Create a namespace\nc. Install NGINX using helm\n  Access to interfaces through ingress\n  Introduction The instructions below explain how to set up NGINX as an ingress for OUDSM.\nInstall NGINX Use Helm to install NGINX.\nConfigure the repository   Add the Helm chart repository for installing NGINX using the following command:\n$ helm repo add stable https://kubernetes.github.io/ingress-nginx The output will look similar to the following:\n\u0026quot;stable\u0026quot; has been added to your repositories   Update the repository using the following command:\n$ helm repo update The output will look similar to the following:\nHang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \u0026quot;stable\u0026quot; chart repository Update Complete. Happy Helming!   Create a namespace   Create a Kubernetes namespace for NGINX:\n$ kubectl create namespace \u0026lt;namespace\u0026gt; For example:\n$ kubectl create namespace mynginx The output will look similar to the following:\nnamespace/mynginx created   Install NGINX using helm   Create a $WORKDIR/kubernetes/helm/nginx-ingress-values-override.yaml that contains the following:\nNote: The configuration below deploys an ingress using LoadBalancer. If you prefer to use NodePort, change the configuration accordingly. For more details about NGINX configuration see: NGINX Ingress Controller.\ncontroller: admissionWebhooks: enabled: false extraArgs: # The secret referred to by this flag contains the default certificate to be used when accessing the catch-all server. # If this flag is not provided NGINX will use a self-signed certificate. # If the TLS Secret is in different namespace, name can be mentioned as \u0026lt;namespace\u0026gt;/\u0026lt;tlsSecretName\u0026gt; default-ssl-certificate: oudsmns/oudsm-tls-cert service: # controller service external IP addresses # externalIPs: # - \u0026lt; External IP Address \u0026gt; # To configure Ingress Controller Service as LoadBalancer type of Service # Based on the Kubernetes configuration, External LoadBalancer would be linked to the Ingress Controller Service type: LoadBalancer # Configuration for NodePort to be used for Ports exposed through Ingress # If NodePorts are not defined/configured, Node Port would be assigned automatically by Kubernetes # These NodePorts are helpful while accessing services directly through Ingress and without having External Load Balancer. nodePorts: # For HTTP Interface exposed through LoadBalancer/Ingress http: 30080 # For HTTPS Interface exposed through LoadBalancer/Ingress https: 30443   To install and configure NGINX ingress issue the following command:\n$ helm install --namespace \u0026lt;namespace\u0026gt; \\ --values nginx-ingress-values-override.yaml \\ lbr-nginx stable/ingress-nginx Where:\n lbr-nginx is your deployment name stable/ingress-nginx is the chart reference  For example:\n$ helm install --namespace mynginx \\ --values nginx-ingress-values-override.yaml \\ lbr-nginx stable/ingress-nginx The output will be similar to the following:\nNAME: lbr-nginx LAST DEPLOYED: Mon Mar 21 17:07:32 2022 NAMESPACE: mynginx STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: The ingress-nginx controller has been installed. It may take a few minutes for the LoadBalancer IP to be available. You can watch the status by running 'kubectl --namespace mynginx get services -o wide -w lbr-nginx-ingress-nginx-controller' An example Ingress that makes use of the controller: apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: example namespace: foo spec: ingressClassName: nginx rules: - host: www.example.com http: paths: - pathType: Prefix backend: service: name: exampleService port: number: 80 path: / # This section is only required if TLS is to be enabled for the Ingress tls: - hosts: - www.example.com secretName: example-tls If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided: apiVersion: v1 kind: Secret metadata: name: example-tls namespace: foo data: tls.crt: \u0026lt;base64 encoded cert\u0026gt; tls.key: \u0026lt;base64 encoded key\u0026gt; type: kubernetes.io/tls   Access to interfaces through ingress Using the Helm chart, ingress objects are created according to configuration. The following table details the rules configured in ingress object(s) for access to Oracle Unified Directory Services Manager Interfaces through ingress.\n   Port NodePort Host Example Hostname Path Backend Service:Port Example Service Name:Port     http/https 30080/30443 \u0026lt;deployment/release name\u0026gt;-N oudsm-N * \u0026lt;deployment/release name\u0026gt;-N:http oudsm-1:http   http/https 30080/30443 * * /oudsm/console \u0026lt;deployment/release name\u0026gt;-lbr:http oudsm-lbr:http     In the table above, the Example Name for each Object is based on the value \u0026lsquo;oudsm\u0026rsquo; as the deployment/release name for the Helm chart installation. The NodePorts mentioned in the table are according to ingress configuration described in previous section. When an External LoadBalancer is not available/configured, interfaces can be accessed through NodePort on the Kubernetes node.  Changes in /etc/hosts to validate hostname based ingress rules If it is not possible to have LoadBalancer configuration updated to have host names added for Oracle Unified Directory Services Manager Interfaces, then the following entries can be added in /etc/hosts files on the host from where Oracle Unified Directory Services Manager interfaces would be accessed.\n\u0026lt;IP Address of External LBR or Kubernetes Node\u0026gt;\toudsm oudsm-1 oudsm-2 oudsm-N  In the table above, host names are based on the value \u0026lsquo;oudsm\u0026rsquo; as the deployment/release name for the Helm chart installation. When an External LoadBalancer is not available/configured, Interfaces can be accessed through NodePort on the Kubernetes Node.  Validate OUDSM URL\u0026rsquo;s  Launch a browser and access the OUDSM console.   If using an External LoadBalancer: https://\u0026lt;External LBR Host\u0026gt;/oudsm. If not using an External LoadBalancer use https://\u0026lt;Kubernetes Node\u0026gt;:30443/oudsm.   Access the WebLogic Administration console by accessing the following URL and login with weblogic/\u0026lt;password\u0026gt; where weblogic/\u0026lt;password\u0026gt; is the adminUser and adminPass set when creating the OUDSM instance.   If using an External LoadBalancer: https://\u0026lt;External LBR Host\u0026gt;/console. If not using an External LoadBalancer use https://\u0026lt;Kubernetes Node\u0026gt;:30443/console.  "
},
{
	"uri": "/fmw-kubernetes/oig/manage-oig-domains/monitoring-oim-domains/",
	"title": "Monitoring an OIG domain",
	"tags": [],
	"description": "Describes the steps for Monitoring the OIG domain and Publising the logs to Elasticsearch.",
	"content": "After the OIG domain is set up you can monitor the OIG instance using Prometheus and Grafana. See Monitoring a domain.\nThe WebLogic Monitoring Exporter uses the WLS RESTful Management API to scrape runtime information and then exports Prometheus-compatible metrics. It is deployed as a web application in a WebLogic Server (WLS) instance, version 12.2.1 or later, typically, in the instance from which you want to get metrics.\nThere are two ways to setup monitoring and you should choose one method or the other:\n Setup automatically using setup-monitoring.sh Setup using manual configuration  Setup automatically using setup-monitoring.sh The $WORKDIR/kubernetes/monitoring-service/setup-monitoring.sh sets up the monitoring for the OIG domain. It installs Prometheus, Grafana, WebLogic Monitoring Exporter and deploys the web applications to the OIG domain. It also deploys the WebLogic Server Grafana dashboard.\nFor usage details execute ./setup-monitoring.sh -h.\n  Edit the $WORKDIR/kubernetes/monitoring-service/monitoring-inputs.yaml and change the domainUID, domainNamespace, and weblogicCredentialsSecretName to correspond to your deployment. For example:\nversion: create-oimcluster-monitoring-inputs-v1 # Unique ID identifying your domain. # This ID must not contain an underscope (\u0026quot;_\u0026quot;), and must be lowercase and unique across all domains in a Kubernetes cluster. domainUID: governancedomain # Name of the domain namespace domainNamespace: oigns # Boolean value indicating whether to install kube-prometheus-stack setupKubePrometheusStack: true # Additional parameters for helm install kube-prometheus-stack # Refer https://github.com/prometheus-community/helm-charts/blob/main/charts/kube-prometheus-stack/values.yaml for additional parameters # Sample : # additionalParamForKubePrometheusStack: --set nodeExporter.enabled=false --set prometheusOperator.tls.enabled=false --set prometheusOperator.admissionWebhooks.enabled=false additionalParamForKubePrometheusStack: # Name of the monitoring namespace monitoringNamespace: monitoring # Name of the Admin Server adminServerName: AdminServer # # Port number for admin server adminServerPort: 7001 # Cluster name soaClusterName: soa_cluster # Port number for managed server soaManagedServerPort: 8001 # WebLogic Monitoring Exporter to Cluster wlsMonitoringExporterTosoaCluster: true # Cluster name oimClusterName: oim_cluster # Port number for managed server oimManagedServerPort: 14000 # WebLogic Monitoring Exporter to Cluster wlsMonitoringExporterTooimCluster: true # Boolean to indicate if the adminNodePort will be exposed exposeMonitoringNodePort: true # NodePort to expose Prometheus prometheusNodePort: 32101 # NodePort to expose Grafana grafanaNodePort: 32100 # NodePort to expose Alertmanager alertmanagerNodePort: 32102 # Name of the Kubernetes secret for the Admin Server's username and password weblogicCredentialsSecretName: oig-domain-credentials   Run the following command to setup monitoring:\n$ cd $WORKDIR/kubernetes/monitoring-service $ ./setup-monitoring.sh -i monitoring-inputs.yaml The output should be similar to the following:\nMonitoring setup in monitoring in progress node/worker-node1 not labeled node/worker-node2 not labeled node/master-node not labeled Setup prometheus-community/kube-prometheus-stack started \u0026quot;prometheus-community\u0026quot; already exists with the same configuration, skipping Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \u0026quot;stable\u0026quot; chart repository ...Successfully got an update from the \u0026quot;prometheus\u0026quot; chart repository ...Successfully got an update from the \u0026quot;prometheus-community\u0026quot; chart repository ...Successfully got an update from the \u0026quot;appscode\u0026quot; chart repository Update Complete. ⎈Happy Helming!⎈ Setup prometheus-community/kube-prometheus-stack in progress NAME: monitoring LAST DEPLOYED: Thu Mar 10 14:58:56 2022 NAMESPACE: monitoring STATUS: deployed REVISION: 1 NOTES: kube-prometheus-stack has been installed. Check its status by running: kubectl --namespace monitoring get pods -l \u0026quot;release=monitoring\u0026quot; Visit https://github.com/prometheus-operator/kube-prometheus for instructions on how to create \u0026amp; configure Alertmanager and Prometheus instances using the Operator. Setup prometheus-community/kube-prometheus-stack completed Deploy WebLogic Monitoring Exporter started Deploying WebLogic Monitoring Exporter with domainNamespace[oigns], domainUID[governancedomain], adminServerPodName[governancedomain-adminserver] % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 655 100 655 0 0 1159 0 --:--:-- --:--:-- --:--:-- 1159 100 2196k 100 2196k 0 0 1763k 0 0:00:01 0:00:01 --:--:-- 20.7M created $WORKDIR/kubernetes/monitoring-service/scripts/wls-exporter-deploy dir created /tmp/ci-GJSQsiXrFE /tmp/ci-GJSQsiXrFE $WORKDIR/kubernetes/monitoring-service in temp dir adding: WEB-INF/weblogic.xml (deflated 61%) adding: config.yml (deflated 60%) $WORKDIR/kubernetes/monitoring-service created /tmp/ci-KeyZrdouMD /tmp/ci-KeyZrdouMD $WORKDIR/kubernetes/monitoring-service in temp dir adding: WEB-INF/weblogic.xml (deflated 61%) adding: config.yml (deflated 60%) $WORKDIR/kubernetes/monitoring-service created /tmp/ci-QE9HawIIgT /tmp/ci-QE9HawIIgT $WORKDIR/kubernetes/monitoring-service in temp dir adding: WEB-INF/weblogic.xml (deflated 61%) adding: config.yml (deflated 60%) $WORKDIR/kubernetes/monitoring-service Initializing WebLogic Scripting Tool (WLST) ... Welcome to WebLogic Server Administration Scripting Shell Type help() for help on available commands Connecting to t3://governancedomain-adminserver:7001 with userid weblogic ... Successfully connected to Admin Server \u0026quot;AdminServer\u0026quot; that belongs to domain \u0026quot;governancedomain\u0026quot;. Warning: An insecure protocol was used to connect to the server. To ensure on-the-wire security, the SSL port or Admin port should be used instead. Deploying ......... Deploying application from /u01/oracle/wls-exporter-deploy/wls-exporter-adminserver.war to targets AdminServer (upload=true) ... \u0026lt;Mar 10, 2022 3:00:08 PM GMT\u0026gt; \u0026lt;Info\u0026gt; \u0026lt;J2EE Deployment SPI\u0026gt; \u0026lt;BEA-260121\u0026gt; \u0026lt;Initiating deploy operation for application, wls-exporter-adminserver [archive: /u01/oracle/wls-exporter-deploy/wls-exporter-adminserver.war], to AdminServer .\u0026gt; .Completed the deployment of Application with status completed Current Status of your Deployment: Deployment command type: deploy Deployment State : completed Deployment Message : no message Starting application wls-exporter-adminserver. \u0026lt;Mar 10, 2022 3:00:20 PM GMT\u0026gt; \u0026lt;Info\u0026gt; \u0026lt;J2EE Deployment SPI\u0026gt; \u0026lt;BEA-260121\u0026gt; \u0026lt;Initiating start operation for application, wls-exporter-adminserver [archive: null], to AdminServer .\u0026gt; .Completed the start of Application with status completed Current Status of your Deployment: Deployment command type: start Deployment State : completed Deployment Message : no message Deploying ......... Deploying application from /u01/oracle/wls-exporter-deploy/wls-exporter-soa.war to targets soa_cluster (upload=true) ... \u0026lt;Mar 10, 2022 3:00:21 PM GMT\u0026gt; \u0026lt;Info\u0026gt; \u0026lt;J2EE Deployment SPI\u0026gt; \u0026lt;BEA-260121\u0026gt; \u0026lt;Initiating deploy operation for application, wls-exporter-soa [archive: /u01/oracle/wls-exporter-deploy/wls-exporter-soa.war], to soa_cluster .\u0026gt; .Completed the deployment of Application with status completed Current Status of your Deployment: Deployment command type: deploy Deployment State : completed Deployment Message : no message Starting application wls-exporter-soa. \u0026lt;Mar 10, 2022 3:00:28 PM GMT\u0026gt; \u0026lt;Info\u0026gt; \u0026lt;J2EE Deployment SPI\u0026gt; \u0026lt;BEA-260121\u0026gt; \u0026lt;Initiating start operation for application, wls-exporter-soa [archive: null], to soa_cluster .\u0026gt; .Completed the start of Application with status completed Current Status of your Deployment: Deployment command type: start Deployment State : completed Deployment Message : no message Deploying ......... Deploying application from /u01/oracle/wls-exporter-deploy/wls-exporter-oim.war to targets oim_cluster (upload=true) ... \u0026lt;Mar 10, 2022 3:00:31 PM GMT\u0026gt; \u0026lt;Info\u0026gt; \u0026lt;J2EE Deployment SPI\u0026gt; \u0026lt;BEA-260121\u0026gt; \u0026lt;Initiating deploy operation for application, wls-exporter-oim [archive: /u01/oracle/wls-exporter-deploy/wls-exporter-oim.war], to oim_cluster .\u0026gt; .Completed the deployment of Application with status completed Current Status of your Deployment: Deployment command type: deploy Deployment State : completed Deployment Message : no message Starting application wls-exporter-oim. \u0026lt;Mar 10, 2022 3:00:38 PM GMT\u0026gt; \u0026lt;Info\u0026gt; \u0026lt;J2EE Deployment SPI\u0026gt; \u0026lt;BEA-260121\u0026gt; \u0026lt;Initiating start operation for application, wls-exporter-oim [archive: null], to oim_cluster .\u0026gt; .Completed the start of Application with status completed Current Status of your Deployment: Deployment command type: start Deployment State : completed Deployment Message : no message Disconnected from weblogic server: AdminServer Exiting WebLogic Scripting Tool. \u0026lt;Mar 10, 2022 3:00:41 PM GMT\u0026gt; \u0026lt;Warning\u0026gt; \u0026lt;JNDI\u0026gt; \u0026lt;BEA-050001\u0026gt; \u0026lt;WLContext.close() was called in a different thread than the one in which it was created.\u0026gt; Deploy WebLogic Monitoring Exporter completed secret/basic-auth created servicemonitor.monitoring.coreos.com/wls-exporter created Deploying WebLogic Server Grafana Dashboard.... {\u0026quot;id\u0026quot;:25,\u0026quot;slug\u0026quot;:\u0026quot;weblogic-server-dashboard\u0026quot;,\u0026quot;status\u0026quot;:\u0026quot;success\u0026quot;,\u0026quot;uid\u0026quot;:\u0026quot;5yUwzbZWz\u0026quot;,\u0026quot;url\u0026quot;:\u0026quot;/d/5yUwzbZWz/weblogic-server-dashboard\u0026quot;,\u0026quot;version\u0026quot;:1} Deployed WebLogic Server Grafana Dashboard successfully Grafana is available at NodePort: 32100 Prometheus is available at NodePort: 32101 Altermanager is available at NodePort: 32102 ==============================================================   Prometheus service discovery After the ServiceMonitor is deployed, the wls-exporter should be discovered by Prometheus and be able to collect metrics.\n  Access the following URL to view Prometheus service discovery: http://${MASTERNODE-HOSTNAME}:32101/service-discovery\n  Click on serviceMonitor/oigns/wls-exporter/0 and then show more. Verify all the targets are mentioned.\n  Note : It may take several minutes for serviceMonitor/oigns/wls-exporter/0 to appear, so refresh the page until it does.\nGrafana dashboard   Access the Grafana dashboard with the following URL: http://${MASTERNODE-HOSTNAME}:32100 and login with admin/admin. Change your password when prompted.\n  In the Dashboards panel, click on WebLogic Server Dashboard. The dashboard for your OIG domain should be displayed. If it is not displayed, click the Search icon in the left hand menu and search for WebLogic Server Dashboard.\n  Cleanup To uninstall the Prometheus, Grafana, WebLogic Monitoring Exporter and the deployments, you can run the $WORKDIR/monitoring-service/kubernetes/delete-monitoring.sh script. For usage details execute ./delete-monitoring.sh -h\n  To uninstall run the following command:\n$ cd $WORKDIR/kubernetes/monitoring-service $ ./delete-monitoring.sh -i monitoring-inputs.yaml   Setup using manual configuration Install Prometheus, Grafana and WebLogic Monitoring Exporter manually. Create the web applications and deploy to the OIG domain.\nDeploy the Prometheus operator   Kube-Prometheus requires all nodes to be labelled with kubernetes.io/os=linux. To check if your nodes are labelled, run the following:\n$ kubectl get nodes --show-labels If the nodes are labelled the output will look similar to the following:\nNAME STATUS ROLES AGE VERSION LABELS worker-node1 Ready \u0026lt;none\u0026gt; 42d v1.20.10 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=worker-node1,kubernetes.io/os=linux worker-node2 Ready \u0026lt;none\u0026gt; 42d v1.20.10 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=worker-node2,kubernetes.io/os=linux master-node Ready master 42d v1.20.10 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=master-node,kubernetes.io/os=linux,node-role.kubernetes.io/master= If the nodes are not labelled, run the following command:\n$ kubectl label nodes --all kubernetes.io/os=linux   Clone Prometheus by running the following commands:\n$ cd $WORKDIR/kubernetes/monitoring-service $ git clone https://github.com/coreos/kube-prometheus.git -b v0.7.0 Note: Please refer the compatibility matrix of Kube Prometheus. Please download the release of the repository according to the Kubernetes version of your cluster.\n  Run the following command to create the namespace and custom resource definitions:\n$ cd kube-prometheus $ kubectl create -f manifests/setup The output will look similar to the following:\nnamespace/monitoring created customresourcedefinition.apiextensions.k8s.io/alertmanagerconfigs.monitoring.coreos.com created customresourcedefinition.apiextensions.k8s.io/alertmanagers.monitoring.coreos.com created customresourcedefinition.apiextensions.k8s.io/podmonitors.monitoring.coreos.com created customresourcedefinition.apiextensions.k8s.io/probes.monitoring.coreos.com created customresourcedefinition.apiextensions.k8s.io/prometheuses.monitoring.coreos.com created customresourcedefinition.apiextensions.k8s.io/prometheusrules.monitoring.coreos.com created customresourcedefinition.apiextensions.k8s.io/servicemonitors.monitoring.coreos.com created customresourcedefinition.apiextensions.k8s.io/thanosrulers.monitoring.coreos.com created clusterrole.rbac.authorization.k8s.io/prometheus-operator created clusterrolebinding.rbac.authorization.k8s.io/prometheus-operator created deployment.apps/prometheus-operator created service/prometheus-operator created serviceaccount/prometheus-operator created   Run the following command to created the rest of the resources:\n$ kubectl create -f manifests/ The output will look similar to the following:\nalertmanager.monitoring.coreos.com/main created prometheusrule.monitoring.coreos.com/alertmanager-main-rules created secret/alertmanager-main created service/alertmanager-main created serviceaccount/alertmanager-main created servicemonitor.monitoring.coreos.com/alertmanager-main created clusterrole.rbac.authorization.k8s.io/blackbox-exporter created clusterrolebinding.rbac.authorization.k8s.io/blackbox-exporter created configmap/blackbox-exporter-configuration created deployment.apps/blackbox-exporter created service/blackbox-exporter created serviceaccount/blackbox-exporter created servicemonitor.monitoring.coreos.com/blackbox-exporter created secret/grafana-config created secret/grafana-datasources created configmap/grafana-dashboard-alertmanager-overview created configmap/grafana-dashboard-apiserver created configmap/grafana-dashboard-cluster-total created configmap/grafana-dashboard-controller-manager created configmap/grafana-dashboard-k8s-resources-cluster created configmap/grafana-dashboard-k8s-resources-namespace created configmap/grafana-dashboard-k8s-resources-node created configmap/grafana-dashboard-k8s-resources-pod created configmap/grafana-dashboard-k8s-resources-workload created configmap/grafana-dashboard-k8s-resources-workloads-namespace created configmap/grafana-dashboard-kubelet created configmap/grafana-dashboard-namespace-by-pod created configmap/grafana-dashboard-namespace-by-workload created configmap/grafana-dashboard-node-cluster-rsrc-use created configmap/grafana-dashboard-node-rsrc-use created configmap/grafana-dashboard-nodes created configmap/grafana-dashboard-persistentvolumesusage created configmap/grafana-dashboard-pod-total created configmap/grafana-dashboard-prometheus-remote-write created configmap/grafana-dashboard-prometheus created configmap/grafana-dashboard-proxy created configmap/grafana-dashboard-scheduler created configmap/grafana-dashboard-workload-total created configmap/grafana-dashboards created deployment.apps/grafana created service/grafana created serviceaccount/grafana created servicemonitor.monitoring.coreos.com/grafana created prometheusrule.monitoring.coreos.com/kube-prometheus-rules created clusterrole.rbac.authorization.k8s.io/kube-state-metrics created clusterrolebinding.rbac.authorization.k8s.io/kube-state-metrics created deployment.apps/kube-state-metrics created prometheusrule.monitoring.coreos.com/kube-state-metrics-rules created service/kube-state-metrics created serviceaccount/kube-state-metrics created servicemonitor.monitoring.coreos.com/kube-state-metrics created prometheusrule.monitoring.coreos.com/kubernetes-monitoring-rules created servicemonitor.monitoring.coreos.com/kube-apiserver created servicemonitor.monitoring.coreos.com/coredns created servicemonitor.monitoring.coreos.com/kube-controller-manager created servicemonitor.monitoring.coreos.com/kube-scheduler created servicemonitor.monitoring.coreos.com/kubelet created clusterrole.rbac.authorization.k8s.io/node-exporter created clusterrolebinding.rbac.authorization.k8s.io/node-exporter created daemonset.apps/node-exporter created prometheusrule.monitoring.coreos.com/node-exporter-rules created service/node-exporter created serviceaccount/node-exporter created servicemonitor.monitoring.coreos.com/node-exporter created apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created clusterrole.rbac.authorization.k8s.io/prometheus-adapter created clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created clusterrolebinding.rbac.authorization.k8s.io/prometheus-adapter created clusterrolebinding.rbac.authorization.k8s.io/resource-metrics:system:auth-delegator created clusterrole.rbac.authorization.k8s.io/resource-metrics-server-resources created configmap/adapter-config created deployment.apps/prometheus-adapter created rolebinding.rbac.authorization.k8s.io/resource-metrics-auth-reader created service/prometheus-adapter created serviceaccount/prometheus-adapter created servicemonitor.monitoring.coreos.com/prometheus-adapter created clusterrole.rbac.authorization.k8s.io/prometheus-k8s created clusterrolebinding.rbac.authorization.k8s.io/prometheus-k8s created prometheusrule.monitoring.coreos.com/prometheus-operator-rules created servicemonitor.monitoring.coreos.com/prometheus-operator created prometheus.monitoring.coreos.com/k8s created prometheusrule.monitoring.coreos.com/prometheus-k8s-prometheus-rules created rolebinding.rbac.authorization.k8s.io/prometheus-k8s-config created rolebinding.rbac.authorization.k8s.io/prometheus-k8s created rolebinding.rbac.authorization.k8s.io/prometheus-k8s created rolebinding.rbac.authorization.k8s.io/prometheus-k8s created role.rbac.authorization.k8s.io/prometheus-k8s-config created role.rbac.authorization.k8s.io/prometheus-k8s created role.rbac.authorization.k8s.io/prometheus-k8s created role.rbac.authorization.k8s.io/prometheus-k8s created service/prometheus-k8s created serviceaccount/prometheus-k8s created servicemonitor.monitoring.coreos.com/prometheus-k8s created unable to recognize \u0026quot;manifests/alertmanager-podDisruptionBudget.yaml\u0026quot;: no matches for kind \u0026quot;PodDisruptionBudget\u0026quot; in version \u0026quot;policy/v1\u0026quot; unable to recognize \u0026quot;manifests/prometheus-adapter-podDisruptionBudget.yaml\u0026quot;: no matches for kind \u0026quot;PodDisruptionBudget\u0026quot; in version \u0026quot;policy/v1\u0026quot; unable to recognize \u0026quot;manifests/prometheus-podDisruptionBudget.yaml\u0026quot;: no matches for kind \u0026quot;PodDisruptionBudget\u0026quot; in version \u0026quot;policy/v1\u0026quot;   Provide external access for Grafana, Prometheus, and Alertmanager, by running the following commands:\n$ kubectl patch svc grafana -n monitoring --type=json -p \u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/type\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;NodePort\u0026#34; },{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/ports/0/nodePort\u0026#34;, \u0026#34;value\u0026#34;: 32100 }]\u0026#39; $ kubectl patch svc prometheus-k8s -n monitoring --type=json -p \u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/type\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;NodePort\u0026#34; },{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/ports/0/nodePort\u0026#34;, \u0026#34;value\u0026#34;: 32101 }]\u0026#39; $ kubectl patch svc alertmanager-main -n monitoring --type=json -p \u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/type\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;NodePort\u0026#34; },{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/ports/0/nodePort\u0026#34;, \u0026#34;value\u0026#34;: 32102 }]\u0026#39; Note: This assigns port 32100 to Grafana, 32101 to Prometheus, and 32102 to Alertmanager.\nThe output will look similar to the following:\nservice/grafana patched service/prometheus-k8s patched service/alertmanager-main patched   Verify that the Prometheus, Grafana, and Alertmanager pods are running in the monitoring namespace and the respective services have the exports configured correctly:\n$ kubectl get pods,services -o wide -n monitoring The output should look similar to the following:\npod/alertmanager-main-0 2/2 Running 0 40s 10.244.1.29 worker-node1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/alertmanager-main-1 2/2 Running 0 40s 10.244.2.68 worker-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/alertmanager-main-2 2/2 Running 0 40s 10.244.1.28 worker-node1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/grafana-f8cd57fcf-zpjh2 1/1 Running 0 40s 10.244.2.69 worker-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/kube-state-metrics-587bfd4f97-zw9zj 3/3 Running 0 38s 10.244.1.30 worker-node1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/node-exporter-2cgrm 2/2 Running 0 38s 10.196.54.36 master-node \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/node-exporter-fpl7f 2/2 Running 0 38s 10.247.95.26 worker-node1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/node-exporter-kvvnr 2/2 Running 0 38s 10.250.40.59 worker-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/prometheus-adapter-69b8496df6-9vfdp 1/1 Running 0 38s 10.244.2.70 worker-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/prometheus-k8s-0 2/2 Running 0 37s 10.244.2.71 worker-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/prometheus-k8s-1 2/2 Running 0 37s 10.244.1.31 worker-node1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/prometheus-operator-7649c7454f-g5b4l 2/2 Running 0 47s 10.244.2.67 worker-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service/alertmanager-main NodePort 10.105.76.223 \u0026lt;none\u0026gt; 9093:32102/TCP 41s alertmanager=main,app=alertmanager service/alertmanager-operated ClusterIP None \u0026lt;none\u0026gt; 9093/TCP,9094/TCP,9094/UDP 40s app=alertmanager service/grafana NodePort 10.107.86.157 \u0026lt;none\u0026gt; 3000:32100/TCP 40s app=grafana service/kube-state-metrics ClusterIP None \u0026lt;none\u0026gt; 8443/TCP,9443/TCP 40s app.kubernetes.io/name=kube-state-metrics service/node-exporter ClusterIP None \u0026lt;none\u0026gt; 9100/TCP 39s app.kubernetes.io/name=node-exporter service/prometheus-adapter ClusterIP 10.102.244.224 \u0026lt;none\u0026gt; 443/TCP 39s name=prometheus-adapter service/prometheus-k8s NodePort 10.100.241.34 \u0026lt;none\u0026gt; 9090:32101/TCP 39s app=prometheus,prometheus=k8s service/prometheus-operated ClusterIP None \u0026lt;none\u0026gt; 9090/TCP 39s app=prometheus service/prometheus-operator ClusterIP None \u0026lt;none\u0026gt; 8443/TCP 47s app.kubernetes.io/component=controller,app.kubernetes.io/name=prometheus-operator   Deploy WebLogic Monitoring Exporter Generate the WebLogic Monitoring Exporter deployment package. The wls-exporter.war package need to be updated and created for each listening port (Administration Server and Managed Servers) in the domain.\n  Set the below environment values and run the script get-wls-exporter.sh to generate the required WAR files at ${WORKDIR}/kubernetes/monitoring-service/scripts/wls-exporter-deploy:\n$ cd $WORKDIR/kubernetes/monitoring-service/scripts $ export adminServerPort=7001 $ export wlsMonitoringExporterTosoaCluster=true $ export soaManagedServerPort=8001 $ export wlsMonitoringExporterTooimCluster=true $ export oimManagedServerPort=14000 $ sh get-wls-exporter.sh The output will look similar to the following:\n % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 655 100 655 0 0 1159 0 --:--:-- --:--:-- --:--:-- 1159 100 2196k 100 2196k 0 0 1430k 0 0:00:01 0:00:01 --:--:-- 8479k created $WORKDIR/kubernetes/monitoring-service/scripts/wls-exporter-deploy dir domainNamespace is empty, setting to default oimcluster domainUID is empty, setting to default oimcluster weblogicCredentialsSecretName is empty, setting to default \u0026quot;oimcluster-domain-credentials\u0026quot; adminServerPort is empty, setting to default \u0026quot;7001\u0026quot; soaClusterName is empty, setting to default \u0026quot;soa_cluster\u0026quot; oimClusterName is empty, setting to default \u0026quot;oim_cluster\u0026quot; created /tmp/ci-NEZy7NOfoz /tmp/ci-NEZy7NOfoz $WORKDIR/kubernetes/monitoring-service/scripts in temp dir adding: WEB-INF/weblogic.xml (deflated 61%) adding: config.yml (deflated 60%) $WORKDIR/kubernetes/monitoring-service/scripts created /tmp/ci-J7QJ4Nc1lo /tmp/ci-J7QJ4Nc1lo $WORKDIR/kubernetes/monitoring-service/scripts in temp dir adding: WEB-INF/weblogic.xml (deflated 61%) adding: config.yml (deflated 60%) $WORKDIR/kubernetes/monitoring-service/scripts created /tmp/ci-f4GbaxM2aJ /tmp/ci-f4GbaxM2aJ $WORKDIR/kubernetes/monitoring-service/scripts in temp dir adding: WEB-INF/weblogic.xml (deflated 61%) adding: config.yml (deflated 60%) $WORKDIR/kubernetes/monitoring-service/scripts   Deploy the WebLogic Monitoring Exporter WAR files into the Oracle Access Management domain:\n$ cd $WORKDIR/kubernetes/monitoring-service/scripts $ kubectl cp wls-exporter-deploy \u0026lt;domain_namespace\u0026gt;/\u0026lt;domain_uid\u0026gt;-adminserver:/u01/oracle $ kubectl cp deploy-weblogic-monitoring-exporter.py \u0026lt;domain_namespace\u0026gt;/\u0026lt;domain_uid\u0026gt;-adminserver:/u01/oracle/wls-exporter-deploy $ kubectl exec -it -n \u0026lt;domain_namespace\u0026gt; \u0026lt;domain_uid\u0026gt;-adminserver -- /u01/oracle/oracle_common/common/bin/wlst.sh /u01/oracle/wls-exporter-deploy/deploy-weblogic-monitoring-exporter.py -domainName \u0026lt;domain_uid\u0026gt; -adminServerName AdminServer -adminURL \u0026lt;domain_uid\u0026gt;-adminserver:7001 -username weblogic -password \u0026lt;password\u0026gt; -oimClusterName oim_cluster -wlsMonitoringExporterTooimCluster true -soaClusterName soa_cluster -wlsMonitoringExporterTosoaCluster true For example:\n$ cd $WORKDIR/kubernetes/monitoring-service/scripts $ kubectl cp wls-exporter-deploy oigns/governancedomain-adminserver:/u01/oracle $ kubectl cp deploy-weblogic-monitoring-exporter.py oigns/governancedomain-adminserver:/u01/oracle/wls-exporter-deploy $ kubectl exec -it -n oigns governancedomain-adminserver -- /u01/oracle/oracle_common/common/bin/wlst.sh /u01/oracle/wls-exporter-deploy/deploy-weblogic-monitoring-exporter.py -domainName governancedomain -adminServerName AdminServer -adminURL governancedomain-adminserver:7001 -username weblogic -password \u0026lt;password\u0026gt; -oimClusterName oim_cluster -wlsMonitoringExporterTooimCluster true -soaClusterName soa_cluster -wlsMonitoringExporterTosoaCluster true The output will look similar to the following:\nInitializing WebLogic Scripting Tool (WLST) ... Welcome to WebLogic Server Administration Scripting Shell Type help() for help on available commands Connecting to t3://accessdomain-adminserver:7001 with userid weblogic ... Successfully connected to Admin Server \u0026quot;AdminServer\u0026quot; that belongs to domain \u0026quot;accessdomain\u0026quot;. Warning: An insecure protocol was used to connect to the server. To ensure on-the-wire security, the SSL port or Admin port should be used instead. Deploying ......... Deploying application from /u01/oracle/wls-exporter-deploy/wls-exporter-adminserver.war to targets AdminServer (upload=true) ... \u0026lt;Nov 18, 2021 10:35:44 AM GMT\u0026gt; \u0026lt;Info\u0026gt; \u0026lt;J2EE Deployment SPI\u0026gt; \u0026lt;BEA-260121\u0026gt; \u0026lt;Initiating deploy operation for application, wls-exporter-adminserver [archive: /u01/oracle/wls-exporter-deploy/wls-exporter-adminserver.war], to AdminServer .\u0026gt; .Completed the deployment of Application with status completed Current Status of your Deployment: Deployment command type: deploy Deployment State : completed Deployment Message : no message Starting application wls-exporter-adminserver. \u0026lt;Nov 18, 2021 10:35:56 AM GMT\u0026gt; \u0026lt;Info\u0026gt; \u0026lt;J2EE Deployment SPI\u0026gt; \u0026lt;BEA-260121\u0026gt; \u0026lt;Initiating start operation for application, wls-exporter-adminserver [archive: null], to AdminServer .\u0026gt; .Completed the start of Application with status completed Current Status of your Deployment: Deployment command type: start Deployment State : completed Deployment Message : no message Deploying ......... Deploying application from /u01/oracle/wls-exporter-deploy/wls-exporter-soa.war to targets soa_cluster (upload=true) ... \u0026lt;Nov 18, 2021 10:35:59 AM GMT\u0026gt; \u0026lt;Info\u0026gt; \u0026lt;J2EE Deployment SPI\u0026gt; \u0026lt;BEA-260121\u0026gt; \u0026lt;Initiating deploy operation for application, wls-exporter-soa [archive: /u01/oracle/wls-exporter-deploy/wls-exporter-soa.war], to soa_cluster .\u0026gt; ..Completed the deployment of Application with status completed Current Status of your Deployment: Deployment command type: deploy Deployment State : completed Deployment Message : no message Starting application wls-exporter-soa. \u0026lt;Nov 18, 2021 10:36:12 AM GMT\u0026gt; \u0026lt;Info\u0026gt; \u0026lt;J2EE Deployment SPI\u0026gt; \u0026lt;BEA-260121\u0026gt; \u0026lt;Initiating start operation for application, wls-exporter-soa [archive: null], to soa_cluster .\u0026gt; .Completed the start of Application with status completed Current Status of your Deployment: Deployment command type: start Deployment State : completed Deployment Message : no message Deploying ......... Deploying application from /u01/oracle/wls-exporter-deploy/wls-exporter-oim.war to targets oim_cluster (upload=true) ... \u0026lt;Nov 18, 2021 10:36:15 AM GMT\u0026gt; \u0026lt;Info\u0026gt; \u0026lt;J2EE Deployment SPI\u0026gt; \u0026lt;BEA-260121\u0026gt; \u0026lt;Initiating deploy operation for application, wls-exporter-oim [archive: /u01/oracle/wls-exporter-deploy/wls-exporter-oim.war], to oim_cluster .\u0026gt; .Completed the deployment of Application with status completed Current Status of your Deployment: Deployment command type: deploy Deployment State : completed Deployment Message : no message Starting application wls-exporter-oim. \u0026lt;Nov 18, 2021 10:36:24 AM GMT\u0026gt; \u0026lt;Info\u0026gt; \u0026lt;J2EE Deployment SPI\u0026gt; \u0026lt;BEA-260121\u0026gt; \u0026lt;Initiating start operation for application, wls-exporter-oim [archive: null], to oim_cluster .\u0026gt; .Completed the start of Application with status completed Current Status of your Deployment: Deployment command type: start Deployment State : completed Deployment Message : no message Disconnected from weblogic server: AdminServer Exiting WebLogic Scripting Tool. \u0026lt;Nov 18, 2021 10:36:27 AM GMT\u0026gt; \u0026lt;Warning\u0026gt; \u0026lt;JNDI\u0026gt; \u0026lt;BEA-050001\u0026gt; \u0026lt;WLContext.close() was called in a different thread than the one in which it was created.\u0026gt;   Configure Prometheus Operator Prometheus enables you to collect metrics from the WebLogic Monitoring Exporter. The Prometheus Operator identifies the targets using service discovery. To get the WebLogic Monitoring Exporter end point discovered as a target, you must create a service monitor pointing to the service.\nThe exporting of metrics from wls-exporter requires basicAuth, so a Kubernetes Secret is created with the user name and password that are base64 encoded. This Secret is used in the ServiceMonitor deployment. The wls-exporter-ServiceMonitor.yaml has basicAuth with credentials as username: weblogic and password: \u0026lt;password\u0026gt; in base64 encoded.\n  Run the following command to get the base64 encoded version of the weblogic password:\n$ echo -n \u0026#34;\u0026lt;password\u0026gt;\u0026#34; | base64 The output will look similar to the following:\nV2VsY29tZTE=   Update the $WORKDIR/kubernetes/monitoring-service/manifests/wls-exporter-ServiceMonitor.yaml and change the password: value to the value returned above. Also change any reference to the namespace and weblogic.domainName: values to match your OIG namespace and domain name. For example:\napiVersion: v1 kind: Secret metadata: name: basic-auth namespace: oigns data: password: V2VsY29tZTE= user: d2VibG9naWM= type: Opaque --- apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: wls-exporter namespace: oigns labels: k8s-app: wls-exporter release: monitoring spec: namespaceSelector: matchNames: - oigns selector: matchLabels: weblogic.domainName: governancedomain endpoints: - basicAuth: password: name: basic-auth key: password username: name: basic-auth key: user port: default relabelings: - action: labelmap regex: __meta_kubernetes_service_label_(.+) interval: 10s honorLabels: true path: /wls-exporter/metrics   Update the $WORKDIR/kubernetes/monitoring-service/manifests/prometheus-roleSpecific-domain-namespace.yaml and change the namespace to match your OIG namespace. For example:\napiVersion: rbac.authorization.k8s.io/v1 items: - apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: prometheus-k8s namespace: oigns rules: - apiGroups: - \u0026quot;\u0026quot; resources: - services - endpoints - pods verbs: - get - list - watch kind: RoleList   Update the $WORKDIR/kubernetes/monitoring-service/manifests/prometheus-roleBinding-domain-namespace.yaml and change the namespace` to match your OIG namespace. For example:\napiVersion: rbac.authorization.k8s.io/v1 items: - apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: prometheus-k8s namespace: oigns roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: prometheus-k8s subjects: - kind: ServiceAccount name: prometheus-k8s namespace: monitoring kind: RoleBindingList   Run the following command to enable Prometheus:\n$ cd $WORKDIR/kubernetes/monitoring-service/manifests $ kubectl apply -f . The output will look similar to the following:\nrolebinding.rbac.authorization.k8s.io/prometheus-k8s created role.rbac.authorization.k8s.io/prometheus-k8s created secret/basic-auth created servicemonitor.monitoring.coreos.com/wls-exporter created   Prometheus service discovery After the ServiceMonitor is deployed, the wls-exporter should be discovered by Prometheus and be able to collect metrics.\n  Access the following URL to view Prometheus service discovery: http://${MASTERNODE-HOSTNAME}:32101/service-discovery\n  Click on oigns/wls-exporter/0 and then show more. Verify all the targets are mentioned.\n  Note: It may take several minutes for oigns/wls-exporter/0 to appear, so refresh the page until it does.\nGrafana dashboard   Access the Grafana dashboard with the following URL: http://${MASTERNODE-HOSTNAME}:32100 and login with admin/admin. Change your password when prompted.\n  Import the Grafana dashboard by navigating on the left hand menu to Create \u0026gt; Import. Copy the content from $WORKDIR/kubernetes/monitoring-service/config/weblogic-server-dashboard-import.json and paste. Then click Load and Import. The dashboard should be displayed.\n  Cleanup To clean up a manual installation:\n  Run the following commands:\n$ cd $WORKDIR/kubernetes/monitoring-service/manifests/ $ kubectl delete -f .   Delete the deployments:\n$ cd $WORKDIR/kubernetes/monitoring-service/scripts/ $ kubectl cp undeploy-weblogic-monitoring-exporter.py \u0026lt;domain_namespace\u0026gt;/\u0026lt;domain_uid\u0026gt;-adminserver:/u01/oracle/wls-exporter-deploy $ kubectl exec -it -n \u0026lt;domain_namespace\u0026gt; \u0026lt;domain_uid\u0026gt;-adminserver -- /u01/oracle/oracle_common/common/bin/wlst.sh /u01/oracle/wls-exporter-deploy/undeploy-weblogic-monitoring-exporter.py -domainName \u0026lt;domain_uid\u0026gt; -adminServerName AdminServer -adminURL \u0026lt;domain_uid\u0026gt;-adminserver:7001 -username weblogic -password \u0026lt;password\u0026gt; -oimClusterName oim_cluster -wlsMonitoringExporterTooimCluster true -soaClusterName soa_cluster -wlsMonitoringExporterTosoaCluster true   Delete Prometheus:\n$ cd $WORKDIR/kubernetes/monitoring-service/kube-prometheus $ kubectl delete -f manifests $ kubectl delete -f manifests/setup   "
},
{
	"uri": "/fmw-kubernetes/wccontent-domains/cleanup-domain-setup/",
	"title": "Uninstall",
	"tags": [],
	"description": "Clean up the Oracle WebCenter Content domain setup.",
	"content": "Learn how to clean up the Oracle WebCenter Content domain setup.\nStop all Administration and Managed server pods First stop the all pods related to a domain. This can be done by patching domain \u0026ldquo;serverStartPolicy\u0026rdquo; to \u0026ldquo;NEVER\u0026rdquo;. Here is the sample command for the same.\n$ kubectl patch domain wcc-domain-name -n wcc-namespace --type=\u0026#39;json\u0026#39; -p=\u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/serverStartPolicy\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;NEVER\u0026#34; }]\u0026#39; For example:\nkubectl patch domain wccinfra -n wccns --type=\u0026#39;json\u0026#39; -p=\u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/serverStartPolicy\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;NEVER\u0026#34; }]\u0026#39; Remove the domain   Remove the domain\u0026rsquo;s ingress (for example, Traefik ingress) using Helm:\n$ helm uninstall wcc-domain-ingress -n sample-domain1-ns For example:\n$ helm uninstall wccinfra-traefik -n wccns   Remove the domain resources by using the sample delete-weblogic-domain-resources.sh script present at ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/delete-domain:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/delete-domain $ ./delete-weblogic-domain-resources.sh -d sample-domain1 For example:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/delete-domain $ ./delete-weblogic-domain-resources.sh -d wccinfra   Use kubectl to confirm that the server pods and domain resource are deleted:\n$ kubectl get pods -n sample-domain1-ns $ kubectl get domains -n sample-domain1-ns For example:\n$ kubectl get pods -n wccns $ kubectl get domains -n wccns   Drop the RCU schemas Follow these steps to drop the RCU schemas created for Oracle WebCenter Content domain.\nRemove the domain namespace   Configure the installed ingress load balancer (for example, Traefik) to stop managing the ingresses in the domain namespace:\n$ helm upgrade traefik-operator traefik/traefik \\  --namespace traefik \\  --reuse-values \\  --set \u0026#34;kubernetes.namespaces={traefik}\u0026#34; \\  --wait   Configure the WebLogic Kubernetes Operator to stop managing the domain:\n$ helm upgrade sample-weblogic-operator \\  kubernetes/charts/weblogic-operator \\  --namespace sample-weblogic-operator-ns \\  --reuse-values \\  --set \u0026#34;domainNamespaces={}\u0026#34; \\  --wait For example:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator $ helm upgrade weblogic-kubernetes-operator \\  kubernetes/charts/weblogic-operator \\  --namespace opns \\  --reuse-values \\  --set \u0026#34;domainNamespaces={}\u0026#34; \\  --wait   Delete the domain namespace:\n$ kubectl delete namespace sample-domain1-ns For example:\n$ kubectl delete namespace wccns   Remove the WebLogic Kubernetes Operator   Remove the WebLogic Kubernetes Operator:\n$ helm uninstall sample-weblogic-operator -n sample-weblogic-operator-ns For example:\n$ helm uninstall weblogic-kubernetes-operator -n opns   Remove WebLogic Kubernetes Operator\u0026rsquo;s namespace:\n$ kubectl delete namespace sample-weblogic-operator-ns For example:\n$ kubectl delete namespace opns   Remove the load balancer   Remove the installed ingress based load balancer (for example, Traefik):\n$ helm uninstall traefik -n traefik   Remove the Traefik namespace:\n$ kubectl delete namespace traefik   Delete the domain home To remove the domain home that is generated using the create-domain.sh script, with appropriate privileges manually delete the contents of the storage attached to the domain home persistent volume (PV).\nFor example, for the domain\u0026rsquo;s persistent volume of type host_path:\n$ rm -rf /scratch/k8s_dir/WCC "
},
{
	"uri": "/fmw-kubernetes/wccontent-domains/adminguide/configure-mount-share/",
	"title": "Configure an additional mount or shared space to a domain for Imaging and Capture",
	"tags": [],
	"description": "Configure an additional mount or shared space to a domain, for WebCenter Imaging and WebCenter Capture",
	"content": "A volume can be mounted to a server pod which can be accessible directly from outside Kubernetes cluster so that an external application could write new files to it.\nThis can be used specifically in WebCenter Imaging and WebCenter Capture applications for File Imports.\nKubernetes supports several types of volumes as given in Volumes | Kubernetes.\nFurther in this section, we will take nfs volume as an example.\nMount \u0026ldquo;nfs\u0026rdquo; as volume To use a volume, specify the volumes to provide for the Pod in .spec.volumes and declare where to mount those volumes into containers in .spec.containers[*].volumeMounts in domain.yaml file.\nUpdate the domain.yaml and apply the changes as shown in sample below for mounting nfs server (for example, 100.XXX.XXX.X with shared export path at /sharedir) to all the server pods at /u01/sharedir.\nThe path /u01/sharedir can be configured as the file import path in WebCenter Imaging and WebCenter Capture applications and the files put to /sharedir will be processed by the applications.\nSample entry of domain.yaml with nfs-volume configuration\n... serverPod: # an (optional) list of environment variable to be set on the servers env: - name: JAVA_OPTIONS value: \u0026#34;-Dweblogic.StdoutDebugEnabled=false\u0026#34; - name: USER_MEM_ARGS value: \u0026#34;-Djava.security.egd=file:/dev/./urandom -Xms256m -Xmx1024m \u0026#34; volumes: - name: weblogic-domain-storage-volume persistentVolumeClaim: claimName: wccinfra-domain-pvc - name: nfs-volume nfs: server: 100.XXX.XXX.XXX path: /sharedir volumeMounts: - mountPath: /u01/oracle/user_projects/domains name: weblogic-domain-storage-volume - mountPath: /u01/sharedir name: nfs-volume ... "
},
{
	"uri": "/fmw-kubernetes/wcportal-domains/appendix/",
	"title": "Appendix",
	"tags": [],
	"description": "",
	"content": "This section provides information on miscellaneous tasks related to the Oracle WebCenter Portal deployment on Kubernetes.\n Domain resource sizing  Describes the resourse sizing information for the Oracle WebCenter Portal domain setup on Kubernetes cluster.\n Quick start deployment on-premise  Describes how to quickly get an Oracle WebCenter Portal domain instance running (using the defaults, nothing special) for development and test purposes.\n Security hardening  Review resources for the Docker and Kubernetes cluster hardening.\n Additional Configuration  Describes how to create connections to Oracle WebCenter Content Server to enable content integration within Oracle WebCenter Portal.\n "
},
{
	"uri": "/fmw-kubernetes/wcsites-domains/oracle-cloud/",
	"title": "Oracle Cloud Infrastructure",
	"tags": [],
	"description": "Setting up WebCenter Sites domains with WebLogic Kubernetes Operator",
	"content": "This is a guide to run WebLogic Kubernetes Operator managed WebcenterSites domains on Oracle Cloud Infrastructure.\n"
},
{
	"uri": "/fmw-kubernetes/soa-domains/adminguide/deploying-composites/",
	"title": "Deploy composite applications",
	"tags": [],
	"description": "Deploy composite applications for Oracle SOA Suite and Oracle Service Bus domains.",
	"content": "Learn how to deploy the composite applications for Oracle SOA Suite and Oracle Service Bus domains.\n Deploy using JDeveloper  Deploy Oracle SOA Suite and Oracle Service Bus composite applications from Oracle JDeveloper to Oracle SOA Suite in the WebLogic Kubernetes Operator environment.\n Deploy using Maven and Ant  Deploy Oracle SOA Suite and Oracle Service Bus composite applications using the Maven and Ant based approach in an Oracle SOA Suite deployment.\n Deploy using composites in a persistent volume or image  Deploy Oracle SOA Suite and Oracle Service Bus composite applications artifacts in a persistent volume or in an image.\n "
},
{
	"uri": "/fmw-kubernetes/soa-domains/cleanup-domain-setup/",
	"title": "Uninstall",
	"tags": [],
	"description": "Clean up the Oracle SOA Suite domain setup.",
	"content": "Learn how to clean up the Oracle SOA Suite domain setup.\nRemove the domain   Remove the domain\u0026rsquo;s ingress (for example, Traefik ingress) using Helm:\n$ helm uninstall soa-domain-ingress -n sample-domain1-ns For example:\n$ helm uninstall soainfra-traefik -n soans   Remove the domain resources by using the sample delete-weblogic-domain-resources.sh script present at ${WORKDIR}/delete-domain:\n$ cd ${WORKDIR}/delete-domain $ ./delete-weblogic-domain-resources.sh -d sample-domain1 For example:\n$ cd ${WORKDIR}/delete-domain $ ./delete-weblogic-domain-resources.sh -d soainfra   Use kubectl to confirm that the server pods and domain resource are deleted:\n$ kubectl get pods -n sample-domain1-ns $ kubectl get domains -n sample-domain1-ns For example:\n$ kubectl get pods -n soans $ kubectl get domains -n soans   Drop the RCU schemas Follow these steps to drop the RCU schemas created for Oracle SOA Suite domains.\nRemove the domain namespace   Configure the installed ingress load balancer (for example, Traefik) to stop managing the ingresses in the domain namespace:\n$ helm upgrade traefik traefik/traefik \\  --namespace traefik \\  --reuse-values \\  --set \u0026#34;kubernetes.namespaces={traefik}\u0026#34; \\  --wait   Configure the operator to stop managing the domain:\n$ helm upgrade sample-weblogic-operator \\  charts/weblogic-operator \\  --namespace sample-weblogic-operator-ns \\  --reuse-values \\  --set \u0026#34;domainNamespaces={}\u0026#34; \\  --wait For example:\n$ cd ${WORKDIR} $ helm upgrade weblogic-kubernetes-operator \\  charts/weblogic-operator \\  --namespace opns \\  --reuse-values \\  --set \u0026#34;domainNamespaces={}\u0026#34; \\  --wait   Delete the domain namespace:\n$ kubectl delete namespace sample-domain1-ns For example:\n$ kubectl delete namespace soans   Remove the operator   Remove the operator:\n$ helm uninstall sample-weblogic-operator -n sample-weblogic-operator-ns For example:\n$ helm uninstall weblogic-kubernetes-operator -n opns   Remove the operator\u0026rsquo;s namespace:\n$ kubectl delete namespace sample-weblogic-operator-ns For example:\n$ kubectl delete namespace opns   Remove the load balancer   Remove the installed ingress based load balancer (for example, Traefik):\n$ helm uninstall traefik -n traefik   Remove the Traefik namespace:\n$ kubectl delete namespace traefik   Delete the domain home To remove the domain home that is generated using the create-domain.sh script, with appropriate privileges manually delete the contents of the storage attached to the domain home persistent volume (PV).\nFor example, for the domain\u0026rsquo;s persistent volume of type host_path:\n$ rm -rf /scratch/k8s_dir/SOA/* "
},
{
	"uri": "/fmw-kubernetes/oig/manage-oig-domains/delete-domain-home/",
	"title": "Delete the OIG domain home",
	"tags": [],
	"description": "Learn about the steps to cleanup the OIG domain home.",
	"content": "Sometimes in production, but most likely in testing environments, you might want to remove the domain home that is generated using the create-domain.sh script.\n  Run the following command to delete the domain:\n$ cd $WORKDIR/kubernetes/delete-domain $ ./delete-weblogic-domain-resources.sh -d \u0026lt;domain_uid\u0026gt; For example:\n$ cd $WORKDIR/kubernetes/delete-domain $ ./delete-weblogic-domain-resources.sh -d governancedomain   Drop the RCU schemas as follows:\n$ kubectl exec -it helper -n \u0026lt;domain_namespace\u0026gt; -- /bin/bash [oracle@helper ~]$ [oracle@helper ~]$ export CONNECTION_STRING=\u0026lt;db_host.domain\u0026gt;:\u0026lt;db_port\u0026gt;/\u0026lt;service_name\u0026gt; [oracle@helper ~]$ export RCUPREFIX=\u0026lt;rcu_schema_prefix\u0026gt; /u01/oracle/oracle_common/bin/rcu -silent -dropRepository -databaseType ORACLE -connectString $CONNECTION_STRING \\ -dbUser sys -dbRole sysdba -selectDependentsForComponents true -schemaPrefix $RCUPREFIX \\ -component MDS -component IAU -component IAU_APPEND -component IAU_VIEWER -component OPSS \\ -component WLS -component STB -component OIM -component SOAINFRA -component UCSUMS -f \u0026lt; /tmp/pwd.txt For example:\n$ kubectl exec -it helper -n oigns -- /bin/bash [oracle@helper ~]$ export CONNECTION_STRING=mydatabasehost.example.com:1521/orcl.example.com [oracle@helper ~]$ export RCUPREFIX=OIGK8S /u01/oracle/oracle_common/bin/rcu -silent -dropRepository -databaseType ORACLE -connectString $CONNECTION_STRING \\ -dbUser sys -dbRole sysdba -selectDependentsForComponents true -schemaPrefix $RCUPREFIX \\ -component MDS -component IAU -component IAU_APPEND -component IAU_VIEWER -component OPSS \\ -component WLS -component STB -component OIM -component SOAINFRA -component UCSUMS -f \u0026lt; /tmp/pwd.txt   Delete the contents of the persistent volume:\n$ rm -rf \u0026lt;persistent_volume\u0026gt;/governancedomainpv/* For example:\n$ rm -rf /scratch/shared/governancedomainpv/*   Delete the WebLogic Kubernetes Operator, by running the following command:\n$ helm delete weblogic-kubernetes-operator -n opns   Delete the label from the OIG namespace:\n$ kubectl label namespaces \u0026lt;domain_namespace\u0026gt; weblogic-operator- For example:\n$ kubectl label namespaces oigns weblogic-operator-   Delete the service account for the operator:\n$ kubectl delete serviceaccount \u0026lt;sample-kubernetes-operator-sa\u0026gt; -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl delete serviceaccount op-sa -n opns   Delete the operator namespace:\n$ kubectl delete namespace \u0026lt;sample-kubernetes-operator-ns\u0026gt; For example:\n$ kubectl delete namespace opns   To delete NGINX:\n$ helm delete governancedomain-nginx-designconsole -n \u0026lt;domain_namespace\u0026gt; For example:\n$ helm delete governancedomain-nginx-designconsole -n oigns Then run:\n$ helm delete governancedomain-nginx -n \u0026lt;domain_namespace\u0026gt; For example:\n$ helm delete governancedomain-nginx -n oigns Then run:\n$ helm delete nginx-ingress -n \u0026lt;domain_namespace\u0026gt; For example:\n$ helm delete nginx-ingress -n nginxssl Then delete the NGINX namespace:\n$ kubectl delete namespace \u0026lt;namespace\u0026gt; For example:\n$ kubectl delete namespace nginxssl   Delete the OIG namespace:\n$ kubectl delete namespace \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl delete namespace oigns   "
},
{
	"uri": "/fmw-kubernetes/oid/manage-oid-containers/",
	"title": "Manage Oracle Internet Directory Containers",
	"tags": [],
	"description": "This document provides steps to manage Oracle Internet Directory containers.",
	"content": "Important considerations for Oracle Internet Directory instances in Kubernetes.\n a) Scaling Up/Down OID Pods   Describes the steps for scaling up/down for OID pods.\n c) Monitoring an Oracle Internet Directory Instance  Describes the steps for Monitoring the Oracle Internet Directory environment.\n "
},
{
	"uri": "/fmw-kubernetes/oud/manage-oud-containers/",
	"title": "Manage Oracle Unified Directory Containers",
	"tags": [],
	"description": "This document provides steps manage Oracle Unified Directory containers.",
	"content": "Important considerations for Oracle Unified Directory instances in Kubernetes.\n a) Scaling Up/Down OUD Pods   Describes the steps for scaling up/down for OUD pods.\n b) Logging and Visualization for Helm Chart oud-ds-rs Deployment  Describes the steps for logging and visualization with Elasticsearch and Kibana.\n c) Monitoring an Oracle Unified Directory Instance  Describes the steps for Monitoring the Oracle Unified Directory environment.\n "
},
{
	"uri": "/fmw-kubernetes/oudsm/manage-oudsm-containers/",
	"title": "Manage Oracle Unified Directory Services Manager Containers",
	"tags": [],
	"description": "This document provides steps to manage Oracle Unified Directory Services Manager containers.",
	"content": "Important considerations for Oracle Unified Directory Services Manager instances in Kubernetes.\n a) Scaling Up/Down OUDSM Pods   Describes the steps for scaling up/down for OUDSM pods.\n b) Logging and Visualization for Helm Chart oudsm Deployment  Describes the steps for logging and visualization with Elasticsearch and Kibana.\n c) Monitoring an Oracle Unified Directory Services Manager Instance  Describes the steps for Monitoring the Oracle Unified Directory Services Manager environment.\n "
},
{
	"uri": "/fmw-kubernetes/oam/validate-domain-urls/",
	"title": "Validate Domain URLs",
	"tags": [],
	"description": "Sample for validating domain urls.",
	"content": "In this section you validate the OAM domain URLs are accessible via the NGINX ingress.\nMake sure you know the master hostname and ingress port for NGINX before proceeding.\nValidate the OAM domain urls via the Ingress Launch a browser and access the following URL\u0026rsquo;s. Login with the weblogic username and password (weblogic/\u0026lt;password\u0026gt;).\nNote: If using a load balancer for your ingress replace ${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT} with ${LOADBALANCER-HOSTNAME}:${LOADBALANCER-PORT}.\n   Console or Page URL     WebLogic Administration Console https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/console   Oracle Enterprise Manager Console https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/em   Oracle Access Management Console https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/oamconsole   Oracle Access Management Console https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/access   Logout URL https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/oam/server/logout    Note: WebLogic Administration Console and Oracle Enterprise Manager Console should only be used to monitor the servers in the OAM domain. To control the Administration Server and OAM Managed Servers (start/stop) you must use Kubernetes. See Domain Life Cycle  for more information.\nThe browser will give certificate errors if you used a self signed certificate and have not imported it into the browsers Certificate Authority store. If this occurs you can proceed with the connection and ignore the errors.\nAfter validating the URL\u0026rsquo;s proceed to Post Install Configuraton.\n"
},
{
	"uri": "/fmw-kubernetes/oig/validate-domain-urls/",
	"title": "Validate domain URLs",
	"tags": [],
	"description": "Sample for validating domain urls.",
	"content": "In this section you validate the OIG domain URLs that are accessible via the NGINX ingress.\nMake sure you know the master hostname and port before proceeding.\nValidate the OIG domain urls via the ingress Launch a browser and access the following URL\u0026rsquo;s. Use http or https depending on whether you configured your ingress for non-ssl or ssl.\nLogin to the WebLogic Administration Console and Oracle Enterprise Manager Console with the WebLogic username and password (weblogic/\u0026lt;password\u0026gt;).\nLogin to Oracle Identity Governance with the xelsysadm username and password (xelsysadm/\u0026lt;password\u0026gt;).\nNote: If using a load balancer for your ingress replace ${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT} with ${LOADBALANCER-HOSTNAME}:${LOADBALANCER-PORT}.\n   Console or Page URL     WebLogic Administration Console https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/console   Oracle Enterprise Manager Console https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/em   Oracle Identity System Administration https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/sysadmin   Oracle Identity Self Service https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/identity    Note: WebLogic Administration Console and Oracle Enterprise Manager Console should only be used to monitor the servers in the OIG domain. To control the Administration Server and OIG Managed Servers (start/stop) you must use Kubernetes. See Domain Life Cycle  for more information.\nThe browser will give certificate errors if you used a self signed certifcate and have not imported it into the browsers Certificate Authority store. If this occurs you can proceed with the connection and ignore the errors.\nAfter the URL\u0026rsquo;s have been verified follow Post install configuration.\n"
},
{
	"uri": "/fmw-kubernetes/wccontent-domains/oracle-cloud/",
	"title": "Oracle Cloud Infrastructure",
	"tags": [],
	"description": "",
	"content": "This is a guide to run WebLogic Kubernetes Operator managed WebCenter Content domain on Oracle Cloud Infrastructure. This section of the documentation is certified for WebLogic Kubernetes Operator version 3.3.0 and Oracle WebCenter Content 12.2.1.4 Jan'22 PSU (container image for this release can be downloaded from My Oracle Support MOS patch 33771196).\n Preparing an OKE environment  Running WebLogic Kubernetes Operator managed WebCenter Content domain on Oracle Kubernetes Engine (OKE).\n Preparing a file system  Running WebLogic Kubernetes Operator managed Oracle WebCenter Content domains on OKE\n Preparing OCIR  Running WebLogic Kubernetes Operator managed Oracle WebCenter Content domains on OKE\n Prepare environment for WCC domain  Prepare environment for WCC domain on Oracle Kubernetes Engine (OKE).\n Set up a load balancer  Configure different load balancers for Oracle WebCenter Content domains.\n Create Oracle WebCenter Content domain  Create Oracle WebCenter Content domain on Oracle Kubernetes Engine (OKE).\n Configuring Oracle WebCenter Content for Oracle Identity Cloud Service (IDCS)  Configuring Oracle WebCenter Content for Oracle Identity Cloud Service (IDCS)\n "
},
{
	"uri": "/fmw-kubernetes/wccontent-domains/",
	"title": "Oracle WebCenter Content",
	"tags": [],
	"description": "WebLogic Kubernetes Operator (the “operator”) supports deployment of Oracle WebCenter Content servers such as Oracle WebCenter Content(Content Server) and Oracle WebCenter Content(Inbound Refinery Server). Follow the instructions in this guide to set up Oracle WebCenter Content domain on Kubernetes.",
	"content": "In this release, Oracle WebCenter Content domain is supported using the “domain on a persistent volume” model only, where the domain home is located in a persistent volume (PV).\nThe WebLogic Kubernetes Operator has several key features to assist you with deploying and managing Oracle WebCenter Content domain in a Kubernetes environment. You can:\n Create Oracle WebCenter Content instances(Oracle WebCenter Content server \u0026amp; Oracle WebCenter Content Inbounnd Refinery server) in a Kubernetes persistent volume (PV). This PV can reside in an NFS file system or other Kubernetes volume types. Start servers based on declarative startup parameters and desired states. Expose the Oracle WebCenter Content services and composites for external access. Scale Oracle WebCenter Content domains by starting and stopping Managed Servers on demand, or by integrating with a REST API. Publish WebLogic Kubernetes Operator and WebLogic Server logs to Elasticsearch and interact with them in Kibana. Monitor the Oracle WebCenter Content instance using Prometheus and Grafana.  Current production release The current supported production release of WebLogic Kubernetes Operator, for Oracle WebCenter Content domain deployment is 3.3.0.\nRecent changes See the Release Notes for recent changes for Oracle WebCenter Content domain deployment on Kubernetes.\nLimitations See here for limitations in this release.\nAbout this documentation This documentation includes sections targeted to different audiences. To help you find what you are looking for easily, please consult this table of contents:\n  Quick Start explains how to quickly get an Oracle WebCenter Content instance running, using the defaults. Note that this is only for development and test purposes.\n  Install Guide and Administration Guide provide detailed information about all aspects of using WebLogic Kubernetes Operator including:\n Installing and configuring WebLogic Kubernetes Operator. Using WebLogic Kubernetes Operator to create and manage Oracle WebCenter Content domain. Configuring Kubernetes load balancers. Configuring Custom SSL certificates. Configuring Elasticsearch and Kibana to access the WebLogic Kubernetes Operator and WebLogic Server log files. Deploying composite applications for Oracle WebCenter Content. Patching an Oracle WebCenter Content Docker image. Removing/deleting domain. And much more!    Additional reading Oracle WebCenter Content domain deployment on Kubernetes leverages WebLogic Kubernetes Operator framework.\n To develop an understanding of WebLogic Kubernetes Operator, including design, architecture, domain life cycle management, and configuration overrides, review the WebLogic Kubernetes Operator documentation. To learn more about the Oracle WebCenter Content architecture and components, see Understanding Oracle WebCenter Content.  "
},
{
	"uri": "/fmw-kubernetes/soa-domains/faq/",
	"title": "Frequently Asked Questions",
	"tags": [],
	"description": "This section describes known issues for Oracle SOA Suite domains deployment on Kubernetes. Also, provides answers to frequently asked questions.",
	"content": "Overriding tuning parameters is not supported using configuration overrides The WebLogic Kubernetes Operator enables you to override some of the domain configuration using configuration overrides (also called situational configuration). See supported overrides. Overriding the tuning parameters such as MaxMessageSize and PAYLOAD, for Oracle SOA Suite domains is not supported using the configuration overrides feature. However, you can override them using the following steps:\n  Specify the new value using the environment variable K8S_REFCONF_OVERRIDES in serverPod.env section in domain.yaml configuration file (example path: \u0026lt;domain-creation-output-directory\u0026gt;/weblogic-domains/soainfra/domain.yaml) based on the servers to which the changes are to be applied.\nFor example, to override the value at the Administration Server pod level:\nspec: adminServer: serverPod: env: - name: K8S_REFCONF_OVERRIDES value: \u0026#34;-Dweblogic.MaxMessageSize=78787878\u0026#34; - name: USER_MEM_ARGS value: \u0026#39;-Djava.security.egd=file:/dev/./urandom -Xms512m -Xmx1024m \u0026#39; serverStartState: RUNNING For example, to override the value at a specific cluster level (soa_cluster or osb_cluster):\n- clusterName: soa_cluster serverService: precreateService: true serverStartState: \u0026#34;RUNNING\u0026#34; serverPod: env: - name: K8S_REFCONF_OVERRIDES value: \u0026#34;-Dsoa.payload.threshold.kb=102410\u0026#34;  Note: When multiple system properties are specified for serverPod.env.value, make sure each system property is separated by a space.\n   Apply the updated domain.yaml file:\n$ kubectl apply -f domain.yaml  Note: The server pod(s) will be automatically restarted (rolling restart).\n   Deployments in the WebLogic Server Administration Console may display unexpected error In an Oracle SOA Suite environment deployed using the operator, accessing Deployments from the WebLogic Server Administration Console home page may display the error message Unexpected error encountered while obtaining monitoring information for applications. This error does not have any functional impact and can be ignored. You can verify that the applications are in Active state from the Control tab in Summary of deployments page.\nEnterprise Manager Console may display ADF_FACES-30200 error In an Oracle SOA Suite environment deployed using the operator, the Enterprise Manager Console may intermittently display the following error when the domain servers are restarted:\nADF_FACES-30200: For more information, please see the server\u0026#39;s error log for an entry beginning with: The UIViewRoot is null. Fatal exception during PhaseId: RESTORE_VIEW 1. You can refresh the Enterprise Manager Console URL to successfully log in to the Console.\nConfigure the external URL access for Oracle SOA Suite composite applications For Oracle SOA Suite composite applications to access the external URLs over the internet (if your cluster is behind a http proxy server), you must configure the following proxy parameters for Administration Server and Managed Server pods.\n-Dhttp.proxyHost=www-your-proxy.com -Dhttp.proxyPort=proxy-port -Dhttps.proxyHost=www-your-proxy.com -Dhttps.proxyPort=proxy-port -Dhttp.nonProxyHosts=\u0026#34;localhost|soainfra-adminserver|soainfra-soa-server1|soainfra-osb-server1|...soainfra-soa-serverN|*.svc.cluster.local|*.your.domain.com|/var/run/docker.sock\u0026#34; To do this, edit the domain.yaml configuration file and append the proxy parameters to the spec.serverPod.env.JAVA_OPTIONS environment variable value.\nFor example:\nserverPod: env: - name: JAVA_OPTIONS value: -Dweblogic.StdoutDebugEnabled=false -Dweblogic.ssl.Enabled=true -Dweblogic.security.SSL.ignoreHostnameVerification=true -Dhttp.proxyHost=www-your-proxy.com -Dhttp.proxyPort=proxy-port -Dhttps.proxyHost=www-your-proxy.com -Dhttps.proxyPort=proxy-port -Dhttp.nonProxyHosts=\u0026#34;localhost|soainfra-adminserver|soainfra-soa-server1|soainfra-osb-server1|...soainfra-soa-serverN|*.svc.cluster.local|*.your.domain.com|/var/run/docker.sock\u0026#34; - name: USER_MEM_ARGS value: \u0026#39;-Djava.security.egd=file:/dev/./urandom -Xms256m -Xmx1024m \u0026#39; volumeMounts:  Note: The -Dhttp.nonProxyHosts parameter must have the pod names of the Administration Server and each Managed Server. For example: soainfra-adminserver, soainfra-soa-server1, soainfra-osb-server1, and so on.\n Apply the updated domain.yaml file:\n$ kubectl apply -f domain.yaml  Note: The server pod(s) will be automatically restarted (rolling restart).\n Configure the external access for the Oracle Enterprise Scheduler WebServices WSDL URLs In an Oracle SOA Suite domain deployed including the Oracle Enterprise Scheduler (ESS) component, the following ESS WebServices WSDL URLs shown in the table format in the ess/essWebServicesWsdl.jsp page are not reachable outside the Kubernetes cluster.\nESSWebService EssAsyncCallbackService EssWsJobAsyncCallbackService Follow these steps to configure the external access for the Oracle Enterprise Scheduler WebServices WSDL URLs:\n Log in to the Administration Console URL of the domain.\nFor example: http://\u0026lt;LOADBALANCER-HOST\u0026gt;:\u0026lt;port\u0026gt;/console In the Home Page, click Clusters. Then click the soa_cluster. Click the HTTP tab and then click Lock \u0026amp; Edit in the Change Center panel. Update the following values:  Frontend Host: host name of the load balancer. For example, domain1.example.com. Frontend HTTP Port: load balancer port. For example, 30305. Frontend HTTPS Port: load balancer https port. For example, 30443.   Click Save. Click Activate Changes in the Change Center panel. Restart the servers in the SOA cluster.   Note: Do not restart servers from the Administration Console.\n Missing gif images in Oracle Service Bus console pipeline configuration page In an Oracle SOA Suite domain environment upgraded to the release 21.1.2, some gif images are not rendered in the Oracle Serice Bus console pipeline configuration page, as their corresponding url paths are not exposed via the Ingress path rules in the earlier releases (for Non-SSL and SSL termination). To resolve this issue, perform the following steps to apply the latest ingress configuration:\n$ cd ${WORKDIR} $ helm upgrade \u0026lt;helm_release_for_ingress\u0026gt; \\ charts/ingress-per-domain \\ --namespace \u0026lt;domain_namespace\u0026gt; \\ --reuse-values  Note: helm_release_for_ingress is the ingress name used in the corresponding helm install command for the ingress installation.\n For example, to upgrade the NGINX based ingress configuration:\n$ cd ${WORKDIR} $ helm upgrade soa-nginx-ingress \\ charts/ingress-per-domain \\ --namespace soans \\ --reuse-values WebLogic Kubernetes Operator FAQs See the general frequently asked questions for using the WebLogic Kubernetes Operator.\n"
},
{
	"uri": "/fmw-kubernetes/wcsites-domains/faq/",
	"title": "Frequently Asked Questions",
	"tags": [],
	"description": "This section describes known issues for Oracle WebCenter Sites domains deployment on Kubernetes. Also, provides answers to frequently asked questions.",
	"content": "Configure the external URL access for Oracle WebCenter Sites composite applications For Oracle WebCenter Sites composite applications to access the external URLs over the internet (if your cluster is behind a http proxy server), you must configure the following proxy parameters for Administration Server and Managed Server pods.\n-Dhttp.proxyHost=www-your-proxy.com -Dhttp.proxyPort=proxy-port -Dhttps.proxyHost=www-your-proxy.com -Dhttps.proxyPort=proxy-port -Dhttp.nonProxyHosts=\u0026#34;localhost|wcsitesinfra-adminserver|wcsitesinfra-wcsites-server1|*.svc.cluster.local|*.your.domain.com|/var/run/docker.sock\u0026#34; To do this, edit the domain.yaml configuration file and append the proxy parameters to the spec.serverPod.env.JAVA_OPTIONS environment variable value.\nFor example:\nserverPod: env: - name: JAVA_OPTIONS value: -Dweblogic.StdoutDebugEnabled=false -Dweblogic.ssl.Enabled=true -Dweblogic.security.SSL.ignoreHostnameVerification=true -Dhttp.proxyHost=www-your-proxy.com -Dhttp.proxyPort=proxy-port -Dhttps.proxyHost=www-your-proxy.com -Dhttps.proxyPort=proxy-port -Dhttp.nonProxyHosts=\u0026#34;localhost|wcsitesinfra-adminserver|wcsitesinfra-wcsites-server1|*.svc.cluster.local|*.your.domain.com|/var/run/docker.sock\u0026#34; - name: USER_MEM_ARGS value: \u0026#39;-Djava.security.egd=file:/dev/./urandom -Xms256m -Xmx1024m \u0026#39; volumeMounts:  Note: The -Dhttp.nonProxyHosts parameter must have the pod names of the Administration Server and each Managed Server. For example: wcsitesinfra-adminserver, wcsitesinfra-wcsites-server1, and so on.\n Apply the updated domain.yaml file:\n$ kubectl apply -f domain.yaml  Note: The server pod(s) will be automatically restarted (rolling restart).\n Oracle WebLogic Server Kubernetes Operator FAQs See the general frequently asked questions for using the Oracle WebLogic Server Kubernetes operator.\n"
},
{
	"uri": "/fmw-kubernetes/soa-domains/adminguide/persisting-soa-adapters-customizations/",
	"title": "Persist adapter customizations",
	"tags": [],
	"description": "Persist the customizations done for Oracle SOA Suite adapters.",
	"content": "The lifetime for any customization done in a file on a server pod is up to the lifetime of that pod. The changes are not persisted once the pod goes down or is restarted.\nFor example, the following configuration updates DbAdapter.rar to create a new connection instance and creates data source CoffeeShop on the Administration Console for the same with jdbc/CoffeeShopDS.\nFile location: /u01/oracle/soa/soa/connectors/DbAdapter.rar\n\u0026lt;connection-instance\u0026gt; \u0026lt;jndi-name\u0026gt;eis/DB/CoffeeShop\u0026lt;/jndi-name\u0026gt; \u0026lt;connection-properties\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;XADataSourceName\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;jdbc/CoffeeShopDS\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;DataSourceName\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;PlatformClassName\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;org.eclipse.persistence.platform.database.Oracle10Platform\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;/connection-properties\u0026gt; \u0026lt;/connection-instance\u0026gt; If you need to persist the customizations for any of the adapter files under the SOA Oracle Home in the server pod, use one of the following methods.\nMethod 1: Customize the Adapter file using the WebLogic Administration Console:   Log in to the WebLogic Administration Console, and go to Deployments \u0026gt; ABC.rar \u0026gt; Configuration \u0026gt; Outbound Connection Pools.\n  Click New to create a new connection, then provide a new connection name, and click Finish.\n  Go back to the new connection, update the properties as required, and save.\n  Under Deployments, select ABC.rar, then Update.\nThis step asks for the Plan.xml location. This location by default will be in ${ORACLE_HOME}/soa/soa which is not under Persistent Volume (PV). Therefore, provide the domain\u0026rsquo;s PV location such as {DOMAIN_HOME}/soainfra/servers.\nNow the Plan.xml will be persisted under this location for each Managed Server.\n  Method 2: Customize the Adapter file on the Worker Node:   Copy ABC.rar from the server pod to a PV path:\n$ kubectl cp \u0026lt;namespace\u0026gt;/\u0026lt;SOA Managed Server pod name\u0026gt;:\u0026lt;full path of .rar file\u0026gt; \u0026lt;destination path inside PV\u0026gt; For example:\n$ kubectl cp soans/soainfra-soa-server1:/u01/oracle/soa/soa/connectors/ABC.rar ${DockerVolume}/domains/soainfra/servers/ABC.rar or do a normal file copy between these locations after entering (using kubectl exec) in to the Managed Server pod.\n  Unrar ABC.rar.\n  Update the new connection details in the weblogic-ra.xml file under META_INF.\n  In the WebLogic Administration Console, under Deployments, select ABC.rar, then Update.\n  Select the ABC.rar path as the new location, which is ${DOMAIN_HOME}/user_projects/domains/soainfra/servers/ABC.rar and click Update.\n  Verify that the plan.xml or updated .rar should be persisted in the PV.\n  "
},
{
	"uri": "/fmw-kubernetes/oid/create-or-update-image/",
	"title": "Create or update an image",
	"tags": [],
	"description": "Create or update an Oracle Internet Directory (OID) container image used for deploying OID domains.",
	"content": "As described in Prepare Your Environment you can create your own OID container image. If you have access to the My Oracle Support (MOS), and there is a need to build a new image with an interim or one off patch, it is recommended to use the WebLogic Image Tool to build an Oracle Internet Directory image for production deployments.\nCreate or update an Oracle Internet Directory image using the WebLogic Image Tool Using the WebLogic Image Tool, you can create a new Oracle Internet Directory image with PSU\u0026rsquo;s and interim patches or update an existing image with one or more interim patches.\n Recommendations:\n Use create for creating a new Oracle Internet Directory image containing the Oracle Internet Directory binaries, bundle patch and interim patches. This is the recommended approach if you have access to the OID patches because it optimizes the size of the image. Use update for patching an existing Oracle Internet Directory image with a single interim patch. Note that the patched image size may increase considerably due to additional image layers introduced by the patch application tool.   Create an image Set up the WebLogic Image Tool  Prerequisites Set up the WebLogic Image Tool Validate setup WebLogic Image Tool build directory WebLogic Image Tool cache  Prerequisites Verify that your environment meets the following prerequisites:\n Docker client and daemon on the build machine, with minimum Docker version 18.03.1.ce. Bash version 4.0 or later, to enable the command complete feature. JAVA_HOME environment variable set to the appropriate JDK location e.g: /scratch/export/oracle/product/jdk  Set up the WebLogic Image Tool To set up the WebLogic Image Tool:\n  Create a working directory and change to it:\n$ mdir \u0026lt;workdir\u0026gt; $ cd \u0026lt;workdir\u0026gt; For example:\n$ mkdir /scratch/imagetool-setup $ cd /scratch/imagetool-setup   Download the latest version of the WebLogic Image Tool from the releases page.\n$ wget https://github.com/oracle/weblogic-image-tool/releases/download/release-X.X.X/imagetool.zip where X.X.X is the latest release referenced on the releases page.\n  Unzip the release ZIP file in the imagetool-setup directory.\n$ unzip imagetool.zip   Execute the following commands to set up the WebLogic Image Tool:\n$ cd \u0026lt;workdir\u0026gt;/imagetool-setup/imagetool/bin $ source setup.sh For example:\n$ cd /scratch/imagetool-setup/imagetool/bin $ source setup.sh   Validate setup To validate the setup of the WebLogic Image Tool:\n  Enter the following command to retrieve the version of the WebLogic Image Tool:\n$ imagetool --version   Enter imagetool then press the Tab key to display the available imagetool commands:\n$ imagetool \u0026lt;TAB\u0026gt; cache create help rebase update   WebLogic Image Tool build directory The WebLogic Image Tool creates a temporary Docker context directory, prefixed by wlsimgbuilder_temp, every time the tool runs. Under normal circumstances, this context directory will be deleted. However, if the process is aborted or the tool is unable to remove the directory, it is safe for you to delete it manually. By default, the WebLogic Image Tool creates the Docker context directory under the user\u0026rsquo;s home directory. If you prefer to use a different directory for the temporary context, set the environment variable WLSIMG_BLDDIR:\n$ export WLSIMG_BLDDIR=\u0026#34;/path/to/buid/dir\u0026#34; WebLogic Image Tool cache The WebLogic Image Tool maintains a local file cache store. This store is used to look up where the Java, WebLogic Server installers, and WebLogic Server patches reside in the local file system. By default, the cache store is located in the user\u0026rsquo;s $HOME/cache directory. Under this directory, the lookup information is stored in the .metadata file. All automatically downloaded patches also reside in this directory. You can change the default cache store location by setting the environment variable WLSIMG_CACHEDIR:\n$ export WLSIMG_CACHEDIR=\u0026#34;/path/to/cachedir\u0026#34; Set up additional build scripts Creating an Oracle Internet Directory container image using the WebLogic Image Tool requires additional container scripts for Oracle Internet Directory domains.\n  Clone the docker-images repository to set up those scripts. In these steps, this directory is DOCKER_REPO:\n$ cd \u0026lt;workdir\u0026gt;/imagetool-setup $ git clone https://github.com/oracle/docker-images.git For example:\n$ cd /scratch/imagetool-setup $ git clone https://github.com/oracle/docker-images.git    Note: If you want to create the image continue with the following steps, otherwise to update the image see update an image.\n Create an image After setting up the WebLogic Image Tool, follow these steps to use the WebLogic Image Tool to create a new Oracle Internet Directory image.\nDownload the Oracle Internet Directory installation binaries and patches You must download the required Oracle Internet Directory installation binaries and patches as listed below from the Oracle Software Delivery Cloud and save them in a directory of your choice.\nThe installation binaries and patches required are:\n  Oracle Internet Directory 12.2.1.4.0\n fmw_12.2.1.4.0_oid_linux64.bin    Oracle Fusion Middleware 12c Infrastructure 12.2.1.4.0\n fmw_12.2.1.4.0_infrastructure.jar    OID and FMW Infrastructure Patches:\n View document ID 2723908.1 on My Oracle Support. In the Container Image Download/Patch Details section, locate the Oracle Internet Directory (OID) table. For the latest PSU click the README link in the Documentation column. In the README, locate the \u0026ldquo;Installed Software\u0026rdquo; section. All the patch numbers to be download are listed here. Download all these individual patches from My Oracle Support.    Oracle JDK v8\n jdk-8uXXX-linux-x64.tar.gz where XXX is the JDK version referenced in the README above.    Update required build files The following files in the code repository location \u0026lt;imagetool-setup-location\u0026gt;/docker-images/OracleInternetDirectory/imagetool/12.2.1.4.0 are used for creating the image:\n additionalBuildCmds.txt buildArgs    Edit the \u0026lt;workdir\u0026gt;/imagetool-setup/docker-images/OracleInternetDirectory/imagetool/12.2.1.4.0/buildArgs file and change %DOCKER_REPO%,%JDK_VERSION% and %BUILDTAG% appropriately.\nFor example:\ncreate --jdkVersion=8u321 --type oid_wls --version=12.2.1.4.0 --tag=oid-latestpsu:12.2.1.4.0 --pull --installerResponseFile /scratch/imagetool-setup/docker-images/OracleFMWInfrastructure/dockerfiles/12.2.1.4/install.file,/scratch/imagetool-setup/docker-images/OracleInternetDirectory/dockerfiles/12.2.1.4.0/install/oid.response --additionalBuildCommands /scratch/imagetool-setup/docker-images/OracleInternetDirectory/imagetool/12.2.1.4.0/additionalBuildCmds.txt --additionalBuildFiles /scratch/imagetool-setup/docker-images/OracleInternetDirectory/dockerfiles/12.2.1.4.0/container-scripts   The \u0026lt;workdir\u0026gt;/imagetool-setup/imagetool/docker-images/OracleInternetDirectory/imagetool/12.2.1.4.0/additionalBuildCmds.txt contains additional build commands. You may edit this file if you want to customize the image further.\n  Edit the \u0026lt;workdir\u0026gt;/imagetool-setup/docker-images/OracleFMWInfrastructure/dockerfiles/12.2.1.4/install.file and under the GENERIC section add the line INSTALL_TYPE=\u0026quot;Fusion Middleware Infrastructure\u0026rdquo;. For example:\n[GENERIC] INSTALL_TYPE=\u0026quot;Fusion Middleware Infrastructure\u0026quot; DECLINE_SECURITY_UPDATES=true SECURITY_UPDATES_VIA_MYORACLESUPPORT=false   Create the image   Add a JDK package to the WebLogic Image Tool cache. For example:\n$ imagetool cache addInstaller --type jdk --version 8uXXX --path \u0026lt;download location\u0026gt;/jdk-8uXXX-linux-x64.tar.gz where XXX is the JDK version downloaded\n  Add the downloaded installation binaries to the WebLogic Image Tool cache. For example:\n$ imagetool cache addInstaller --type OID --version 12.2.1.4.0 --path \u0026lt;download location\u0026gt;/fmw_12.2.1.4.0_oid_linux64.bin $ imagetool cache addInstaller --type fmw --version 12.2.1.4.0 --path \u0026lt;download location\u0026gt;/fmw_12.2.1.4.0_infrastructure.jar   Add the downloaded OPatch patch to the WebLogic Image Tool cache. For example:\n$ imagetool cache addEntry --key 28186730_13.9.4.2.8 --value \u0026lt;download location\u0026gt;/p28186730_139428_Generic.zip   Add the rest of the downloaded product patches to the WebLogic Image Tool cache:\n$ imagetool cache addEntry --key \u0026lt;patch\u0026gt;_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p\u0026lt;patch\u0026gt;_122140_Generic.zip For example:\n$ imagetool cache addEntry --key 33727616_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p33727616_122140_Generic.zip $ imagetool cache addEntry --key 33093748_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p33093748_122140_Generic.zip $ imagetool cache addEntry --key 32720458_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p32720458_122140_Generic.zip $ imagetool cache addEntry --key 33791665_12.2.1.4.220105 --value \u0026lt;download location\u0026gt;/p33791665_12214220105_Generic.zip $ imagetool cache addEntry --key 33723124_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p33723124_122140_Generic.zip $ imagetool cache addEntry --key 32647448_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p32647448_122140_Linux-x86-64.zip $ imagetool cache addEntry --key 33591019_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p33591019_122140_Generic.zip $ imagetool cache addEntry --key 32999272_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p32999272_122140_Generic.zip $ imagetool cache addEntry --key 33115009_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p33115009_122140_Linux-x86-64.zip $ imagetool cache addEntry --key 33697227_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p33697227_122140_Generic.zip $ imagetool cache addEntry --key 33678607_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p33678607_122140_Generic.zip $ imagetool cache addEntry --key 33735326_12.2.1.4.220105 --value \u0026lt;download location\u0026gt;/p33735326_12214220105_Generic.zip   Edit the \u0026lt;workdir\u0026gt;/imagetool-setup/docker-images/OracleInternetDirectory/imagetool/12.2.1.4.0/buildArgs file and append the product patches and opatch patch as follows:\n--patches 33727616_12.2.1.4.0,33093748_12.2.1.4.0,32720458_12.2.1.4.0,33791665_12.2.1.4.220105,33723124_12.2.1.4.0,32647448_12.2.1.4.0,33591019_12.2.1.4.0,32999272_12.2.1.4.0,33115009_12.2.1.4.0,33591019_12.2.1.4.0,32999272_12.2.1.4.0,33697227_12.2.1.4.0,33678607_12.2.1.4.0,33735326_12.2.1.4.220105 --opatchBugNumber=28186730_13.9.4.2.8 An example buildArgs file is now as follows:\ncreate --jdkVersion=8u321 --type OID --version=12.2.1.4.0 --tag=oid-latestpsu:12.2.1.4.0 --pull --installerResponseFile /scratch/imagetool-setup/docker-images/OracleFMWInfrastructure/dockerfiles/12.2.1.4/install.file,/scratch/imagetool-setup/docker-images/OracleInternetDirectory/dockerfiles/12.2.1.4.0/install/oid.response --additionalBuildCommands /scratch/imagetool-setup/docker-images/OracleInternetDirectory/imagetool/12.2.1.4.0/additionalBuildCmds.txt --additionalBuildFiles /scratch/imagetool-setup/docker-images/OracleInternetDirectory/dockerfiles/12.2.1.4.0/container-scripts --patches 33727616_12.2.1.4.0,33093748_12.2.1.4.0,32720458_12.2.1.4.0,33791665_12.2.1.4.220105,33723124_12.2.1.4.0,32647448_12.2.1.4.0,33591019_12.2.1.4.0,32999272_12.2.1.4.0,33115009_12.2.1.4.0,33591019_12.2.1.4.0,33697227_12.2.1.4.0,33678607_12.2.1.4.0,33735326_12.2.1.4.220105 --opatchBugNumber=28186730_13.9.4.2.8  Note: In the buildArgs file:\n --jdkVersion value must match the --version value used in the imagetool cache addInstaller command for --type jdk. --version value must match the --version value used in the imagetool cache addInstaller command for --type OID.   Refer to this page for the complete list of options available with the WebLogic Image Tool create command.\n  Create the Oracle Internet Directory image:\n$ imagetool @\u0026lt;absolute path to buildargs file\u0026gt; --fromImage ghcr.io/oracle/oraclelinux:7-slim  Note: Make sure that the absolute path to the buildargs file is prepended with a @ character, as shown in the example above.\n For example:\n$ imagetool @\u0026lt;imagetool-setup-location\u0026gt;/docker-images/OracleInternetDirectory/imagetool/12.2.1.4.0/buildArgs --fromImage ghcr.io/oracle/oraclelinux:7-slim   Check the created image using the docker images command:\n$ docker images | grep oid The output will look similar to the following:\noid-latestpsu 12.2.1.4.0 f60ca3f0a4dc 4 minutes ago 6.06GB   Run the following command to save the container image to a tar file:\n$ docker save -o \u0026lt;path\u0026gt;/\u0026lt;file\u0026gt;.tar \u0026lt;image\u0026gt; For example:\n$ docker save -o $WORKDIR/oid-latestpsu.tar oid-latestpsu:12.2.1.4.0   Update an image The steps below show how to update an existing Oracle Internet Directory image with an interim patch.\nThe container image to be patched must be loaded in the local docker images repository before attempting these steps.\nIn the examples below the image oracle/oid:12.2.1.4.0 is updated with an interim patch.\n$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE oracle/oid 12.2.1.4.0 cd7ece753e52 2 months ago 5.82GB   Set up the WebLogic Image Tool.\n  Download the required interim patch and latest Opatch (28186730) from My Oracle Support. and save them in a directory of your choice.\n  Add the OPatch patch to the WebLogic Image Tool cache, for example:\n$ imagetool cache addEntry --key 28186730_13.9.4.2.8 --value \u0026lt;downloaded-patches-location\u0026gt;/p28186730_139428_Generic.zip   Execute the imagetool cache addEntry command for each patch to add the required patch(es) to the WebLogic Image Tool cache. For example, to add patch p33735326_12214220105_Generic.zip:\n$ imagetool cache addEntry --key=33735326_12.2.1.4.220105 --value \u0026lt;downloaded-patches-location\u0026gt;/p33735326_12214220105_Generic.zip   Provide the following arguments to the WebLogic Image Tool update command:\n –-fromImage - Identify the image that needs to be updated. In the example below, the image to be updated is oracle/oid:12.2.1.4.0. –-patches - Multiple patches can be specified as a comma-separated list. --tag - Specify the new tag to be applied for the image being built.  Refer here for the complete list of options available with the WebLogic Image Tool update command.\n Note: The WebLogic Image Tool cache should have the latest OPatch zip. The WebLogic Image Tool will update the OPatch if it is not already updated in the image.\n For example:\n$ imagetool update --fromImage oracle/oid:12.2.1.4.0 --tag=oracle/oid-new:12.2.1.4.0 --patches=33735326_12.2.1.4.220105 --opatchBugNumber=28186730_13.9.4.2.8  Note: If the command fails because the files in the image being upgraded are not owned by oracle:oracle, then add the parameter --chown \u0026lt;userid\u0026gt;:\u0026lt;groupid\u0026gt; to correspond with the values returned in the error.\n   Check the built image using the docker images command:\n$ docker images | grep oid The output will look similar to the following:\nREPOSITORY TAG IMAGE ID CREATED SIZE oracle/oid-new 12.2.1.4.0 e09ae10189db 38 seconds ago 5.9GB oracle/oid 12.2.1.4.0 cd7ece753e52 2 months ago 5.82GB   Run the following command to save the patched container image to a tar file:\n$ docker save -o \u0026lt;path\u0026gt;/\u0026lt;file\u0026gt;.tar \u0026lt;image\u0026gt; For example:\n$ docker save -o $WORKDIR/oid-new.tar oracle/oid-new:12.2.1.4.0   "
},
{
	"uri": "/fmw-kubernetes/oud/create-or-update-image/",
	"title": "Create or update an image",
	"tags": [],
	"description": "Create or update an Oracle Unified Directory (OUD) container image used for deploying OUD domains.",
	"content": "As described in Prepare Your Environment you can create your own OUD container image. If you have access to the My Oracle Support (MOS), and there is a need to build a new image with an interim or one off patch, it is recommended to use the WebLogic Image Tool to build an Oracle Unified Directory image for production deployments.\nCreate or update an Oracle Unified Directory image using the WebLogic Image Tool Using the WebLogic Image Tool, you can create a new Oracle Unified Directory image with PSU\u0026rsquo;s and interim patches or update an existing image with one or more interim patches.\n Recommendations:\n Use create for creating a new Oracle Unified Directory image containing the Oracle Unified Directory binaries, bundle patch and interim patches. This is the recommended approach if you have access to the OUD patches because it optimizes the size of the image. Use update for patching an existing Oracle Unified Directory image with a single interim patch. Note that the patched image size may increase considerably due to additional image layers introduced by the patch application tool.   Create an image Set up the WebLogic Image Tool  Prerequisites Set up the WebLogic Image Tool Validate setup WebLogic Image Tool build directory WebLogic Image Tool cache  Prerequisites Verify that your environment meets the following prerequisites:\n Docker client and daemon on the build machine, with minimum Docker version 18.03.1.ce. Bash version 4.0 or later, to enable the command complete feature. JAVA_HOME environment variable set to the appropriate JDK location e.g: /scratch/export/oracle/product/jdk  Set up the WebLogic Image Tool To set up the WebLogic Image Tool:\n  Create a working directory and change to it:\n$ mdir \u0026lt;workdir\u0026gt; $ cd \u0026lt;workdir\u0026gt; For example:\n$ mkdir /scratch/imagetool-setup $ cd /scratch/imagetool-setup   Download the latest version of the WebLogic Image Tool from the releases page.\n$ wget https://github.com/oracle/weblogic-image-tool/releases/download/release-X.X.X/imagetool.zip where X.X.X is the latest release referenced on the releases page.\n  Unzip the release ZIP file in the imagetool-setup directory.\n$ unzip imagetool.zip   Execute the following commands to set up the WebLogic Image Tool:\n$ cd \u0026lt;workdir\u0026gt;/imagetool-setup/imagetool/bin $ source setup.sh For example:\n$ cd /scratch/imagetool-setup/imagetool/bin $ source setup.sh   Validate setup To validate the setup of the WebLogic Image Tool:\n  Enter the following command to retrieve the version of the WebLogic Image Tool:\n$ imagetool --version   Enter imagetool then press the Tab key to display the available imagetool commands:\n$ imagetool \u0026lt;TAB\u0026gt; cache create help rebase update   WebLogic Image Tool build directory The WebLogic Image Tool creates a temporary Docker context directory, prefixed by wlsimgbuilder_temp, every time the tool runs. Under normal circumstances, this context directory will be deleted. However, if the process is aborted or the tool is unable to remove the directory, it is safe for you to delete it manually. By default, the WebLogic Image Tool creates the Docker context directory under the user\u0026rsquo;s home directory. If you prefer to use a different directory for the temporary context, set the environment variable WLSIMG_BLDDIR:\n$ export WLSIMG_BLDDIR=\u0026#34;/path/to/buid/dir\u0026#34; WebLogic Image Tool cache The WebLogic Image Tool maintains a local file cache store. This store is used to look up where the Java, WebLogic Server installers, and WebLogic Server patches reside in the local file system. By default, the cache store is located in the user\u0026rsquo;s $HOME/cache directory. Under this directory, the lookup information is stored in the .metadata file. All automatically downloaded patches also reside in this directory. You can change the default cache store location by setting the environment variable WLSIMG_CACHEDIR:\n$ export WLSIMG_CACHEDIR=\u0026#34;/path/to/cachedir\u0026#34; Set up additional build scripts Creating an Oracle Unified Directory container image using the WebLogic Image Tool requires additional container scripts for Oracle Unified Directory domains.\n  Clone the docker-images repository to set up those scripts. In these steps, this directory is DOCKER_REPO:\n$ cd \u0026lt;workdir\u0026gt;/imagetool-setup $ git clone https://github.com/oracle/docker-images.git For example:\n$ cd /scratch/imagetool-setup $ git clone https://github.com/oracle/docker-images.git    Note: If you want to create the image continue with the following steps, otherwise to update the image see update an image.\n Create an image After setting up the WebLogic Image Tool, follow these steps to use the WebLogic Image Tool to create a new Oracle Unified Directory image.\nDownload the Oracle Unified Directory installation binaries and patches You must download the required Oracle Unified Directory installation binaries and patches as listed below from the Oracle Software Delivery Cloud and save them in a directory of your choice.\nThe installation binaries and patches required are:\n  Oracle Unified Directory 12.2.1.4.0\n fmw_12.2.1.4.0_oud.jar    OUD Patches:\n View document ID 2723908.1 on My Oracle Support. In the Container Image Download/Patch Details section, locate the Oracle Unified Directory (OUD) table. For the latest PSU click the README link in the Documentation column. In the README, locate the \u0026ldquo;Installed Software\u0026rdquo; section. All the patch numbers to be download are listed here. Download all these individual patches from My Oracle Support.    Oracle JDK v8\n jdk-8uXXX-linux-x64.tar.gz where XXX is the JDK version referenced in the README above.    Update required build files The following files in the code repository location \u0026lt;imagetool-setup-location\u0026gt;/docker-images/OracleUnifiedDirectory/imagetool/12.2.1.4.0 are used for creating the image:\n additionalBuildCmds.txt buildArgs    Edit the \u0026lt;workdir\u0026gt;/imagetool-setup/docker-images/OracleUnifiedDirectory/imagetool/12.2.1.4.0/buildArgs file and change %DOCKER_REPO%,%JDK_VERSION% and %BUILDTAG% appropriately.\nFor example:\ncreate --jdkVersion=8u321 --type oud --version=12.2.1.4.0 --tag=oud-latestpsu:12.2.1.4.0 --pull --installerResponseFile /scratch/imagetool-setup/docker-images/OracleUnifiedDirectory/dockerfiles/12.2.1.4.0/install/oud.response --additionalBuildCommands /scratch/imagetool-setup/docker-images/OracleUnifiedDirectory/imagetool/12.2.1.4.0/additionalBuildCmds.txt --additionalBuildFiles /scratch/imagetool-setup/docker-images/OracleUnifiedDirectory/dockerfiles/12.2.1.4.0/container-scripts   The \u0026lt;workdir\u0026gt;/imagetool-setup/imagetool/docker-images/OracleUnifiedDirectory/imagetool/12.2.1.4.0/additionalBuildCmds.txt contains additional build commands. You may edit this file if you want to customize the image further.\n  Create the image   Add a JDK package to the WebLogic Image Tool cache. For example:\n$ imagetool cache addInstaller --type jdk --version 8uXXX --path \u0026lt;download location\u0026gt;/jdk-8uXXX-linux-x64.tar.gz where XXX is the JDK version downloaded\n  Add the downloaded installation binaries to the WebLogic Image Tool cache. For example:\n$ imagetool cache addInstaller --type OUD --version 12.2.1.4.0 --path \u0026lt;download location\u0026gt;/fmw_12.2.1.4.0_oud.jar   Add the downloaded OPatch patch to the WebLogic Image Tool cache. For example:\n$ imagetool cache addEntry --key 28186730_13.9.4.2.8 --value \u0026lt;download location\u0026gt;/p28186730_139428_Generic.zip   Add the rest of the downloaded product patches to the WebLogic Image Tool cache:\n$ imagetool cache addEntry --key \u0026lt;patch\u0026gt;_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p\u0026lt;patch\u0026gt;_122140_Generic.zip For example:\n$ imagetool cache addEntry --key 32971905_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p33448950_122140_Generic.zip   Edit the \u0026lt;workdir\u0026gt;/imagetool-setup/docker-images/OracleUnifiedDirectory/imagetool/12.2.1.4.0/buildArgs file and append the product patches and opatch patch as follows:\n--patches 33448950_12.2.1.4.0 --opatchBugNumber=28186730_13.9.4.2.8 An example buildArgs file is now as follows:\ncreate --jdkVersion=8u321 --type oud --version=12.2.1.4.0 --tag=oud-latestpsu:12.2.1.4.0 --pull --installerResponseFile /scratch/imagetool-setup/docker-images/OracleUnifiedDirectory/dockerfiles/12.2.1.4.0/install/oud.response --additionalBuildCommands /scratch/imagetool-setup/docker-images/OracleUnifiedDirectory/imagetool/12.2.1.4.0/additionalBuildCmds.txt --additionalBuildFiles /scratch/imagetool-setup/docker-images/OracleUnifiedDirectory/dockerfiles/12.2.1.4.0/container-scripts --patches 33448950_12.2.1.4.0 --opatchBugNumber=28186730_13.9.4.2.8  Note: In the buildArgs file:\n --jdkVersion value must match the --version value used in the imagetool cache addInstaller command for --type jdk. --version value must match the --version value used in the imagetool cache addInstaller command for --type OUD.   Refer to this page for the complete list of options available with the WebLogic Image Tool create command.\n  Create the Oracle Unified Directory image:\n$ imagetool @\u0026lt;absolute path to buildargs file\u0026gt; --fromImage ghcr.io/oracle/oraclelinux:7-slim  Note: Make sure that the absolute path to the buildargs file is prepended with a @ character, as shown in the example above.\n For example:\n$ imagetool @\u0026lt;imagetool-setup-location\u0026gt;/docker-images/OracleUnifiedDirectory/imagetool/12.2.1.4.0/buildArgs --fromImage ghcr.io/oracle/oraclelinux:7-slim   Check the created image using the docker images command:\n$ docker images | grep oud The output will look similar to the following:\noud-latestpsu 12.2.1.4.0 30b02a692fa3 About a minute ago 1.04GB   Run the following command to save the container image to a tar file:\n$ docker save -o \u0026lt;path\u0026gt;/\u0026lt;file\u0026gt;.tar \u0026lt;image\u0026gt; For example:\n$ docker save -o $WORKDIR/oud-latestpsu.tar oud-latestpsu:12.2.1.4.0   Update an image The steps below show how to update an existing Oracle Unified Directory image with an interim patch.\nThe container image to be patched must be loaded in the local docker images repository before attempting these steps.\nIn the examples below the image oracle/oud:12.2.1.4.0 is updated with an interim patch.\n$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE oracle/oud 12.2.1.4.0 b051804ba15f 3 months ago 1.04GB   Set up the WebLogic Image Tool.\n  Download the required interim patch and latest Opatch (28186730) from My Oracle Support. and save them in a directory of your choice.\n  Add the OPatch patch to the WebLogic Image Tool cache, for example:\n$ imagetool cache addEntry --key 28186730_13.9.4.2.8 --value \u0026lt;downloaded-patches-location\u0026gt;/p28186730_139428_Generic.zip   Execute the imagetool cache addEntry command for each patch to add the required patch(es) to the WebLogic Image Tool cache. For example, to add patch p32701831_12214210607_Generic.zip:\n$ imagetool cache addEntry --key=33521773_12.2.1.4.211008 --value \u0026lt;downloaded-patches-location\u0026gt;/p33521773_12214211008_Generic.zip   Provide the following arguments to the WebLogic Image Tool update command:\n –-fromImage - Identify the image that needs to be updated. In the example below, the image to be updated is oracle/oud:12.2.1.4.0. –-patches - Multiple patches can be specified as a comma-separated list. --tag - Specify the new tag to be applied for the image being built.  Refer here for the complete list of options available with the WebLogic Image Tool update command.\n Note: The WebLogic Image Tool cache should have the latest OPatch zip. The WebLogic Image Tool will update the OPatch if it is not already updated in the image.\n For example:\n$ imagetool update --fromImage oracle/oud:12.2.1.4.0 --tag=oracle/oud-new:12.2.1.4.0 --patches=33521773_12.2.1.4.211008 --opatchBugNumber=28186730_13.9.4.2.8  Note: If the command fails because the files in the image being upgraded are not owned by oracle:oracle, then add the parameter --chown \u0026lt;userid\u0026gt;:\u0026lt;groupid\u0026gt; to correspond with the values returned in the error.\n   Check the built image using the docker images command:\n$ docker images | grep oud The output will look similar to the following:\nREPOSITORY TAG IMAGE ID CREATED SIZE oracle/oud-new 12.2.1.4.0 78ccd1ad67eb 5 minutes ago 1.11GB oracle/oud 12.2.1.4.0 b051804ba15f 3 months ago 1.04GB   Run the following command to save the patched container image to a tar file:\n$ docker save -o \u0026lt;path\u0026gt;/\u0026lt;file\u0026gt;.tar \u0026lt;image\u0026gt; For example:\n$ docker save -o $WORKDIR/oud-new.tar oracle/oud-new:12.2.1.4.0   "
},
{
	"uri": "/fmw-kubernetes/oudsm/create-or-update-image/",
	"title": "Create or update an image",
	"tags": [],
	"description": "Create or update an Oracle Unified Directory Services Manager (OUDSM) container image used for deploying OUDSM domains.",
	"content": "As described in Prepare Your Environment you can create your own OUDSM container image. If you have access to the My Oracle Support (MOS), and there is a need to build a new image with an interim or one off patch, it is recommended to use the WebLogic Image Tool to build an Oracle Unified Directory image for production deployments.\nCreate or update an Oracle Unified Directory Services Manager image using the WebLogic Image Tool Using the WebLogic Image Tool, you can create a new Oracle Unified Directory Services Manager image with PSU\u0026rsquo;s and interim patches or update an existing image with one or more interim patches.\n Recommendations:\n Use create for creating a new Oracle Unified Directory Services Manager image containing the Oracle Unified Directory Services Manager binaries, bundle patch and interim patches. This is the recommended approach if you have access to the OUDSM patches because it optimizes the size of the image. Use update for patching an existing Oracle Unified Directory Services Manager image with a single interim patch. Note that the patched image size may increase considerably due to additional image layers introduced by the patch application tool.   Create an image Set up the WebLogic Image Tool  Prerequisites Set up the WebLogic Image Tool Validate setup WebLogic Image Tool build directory WebLogic Image Tool cache  Prerequisites Verify that your environment meets the following prerequisites:\n Docker client and daemon on the build machine, with minimum Docker version 18.03.1.ce. Bash version 4.0 or later, to enable the command complete feature. JAVA_HOME environment variable set to the appropriate JDK location e.g: /scratch/export/oracle/product/jdk  Set up the WebLogic Image Tool To set up the WebLogic Image Tool:\n  Create a working directory and change to it:\n$ mdir \u0026lt;workdir\u0026gt; $ cd \u0026lt;workdir\u0026gt; For example:\n$ mkdir /scratch/imagetool-setup $ cd /scratch/imagetool-setup   Download the latest version of the WebLogic Image Tool from the releases page.\n$ wget https://github.com/oracle/weblogic-image-tool/releases/download/release-X.X.X/imagetool.zip where X.X.X is the latest release referenced on the releases page.\n  Unzip the release ZIP file in the imagetool-setup directory.\n$ unzip imagetool.zip   Execute the following commands to set up the WebLogic Image Tool:\n$ cd \u0026lt;workdir\u0026gt;/imagetool-setup/imagetool/bin $ source setup.sh For example:\n$ cd /scratch/imagetool-setup/imagetool/bin $ source setup.sh   Validate setup To validate the setup of the WebLogic Image Tool:\n  Enter the following command to retrieve the version of the WebLogic Image Tool:\n$ imagetool --version   Enter imagetool then press the Tab key to display the available imagetool commands:\n$ imagetool \u0026lt;TAB\u0026gt; cache create help rebase update   WebLogic Image Tool build directory The WebLogic Image Tool creates a temporary Docker context directory, prefixed by wlsimgbuilder_temp, every time the tool runs. Under normal circumstances, this context directory will be deleted. However, if the process is aborted or the tool is unable to remove the directory, it is safe for you to delete it manually. By default, the WebLogic Image Tool creates the Docker context directory under the user\u0026rsquo;s home directory. If you prefer to use a different directory for the temporary context, set the environment variable WLSIMG_BLDDIR:\n$ export WLSIMG_BLDDIR=\u0026#34;/path/to/buid/dir\u0026#34; WebLogic Image Tool cache The WebLogic Image Tool maintains a local file cache store. This store is used to look up where the Java, WebLogic Server installers, and WebLogic Server patches reside in the local file system. By default, the cache store is located in the user\u0026rsquo;s $HOME/cache directory. Under this directory, the lookup information is stored in the .metadata file. All automatically downloaded patches also reside in this directory. You can change the default cache store location by setting the environment variable WLSIMG_CACHEDIR:\n$ export WLSIMG_CACHEDIR=\u0026#34;/path/to/cachedir\u0026#34; Set up additional build scripts Creating an Oracle Unified Directory Services Manager container image using the WebLogic Image Tool requires additional container scripts for Oracle Unified Directory Services Manager domains.\n  Clone the docker-images repository to set up those scripts. In these steps, this directory is DOCKER_REPO:\n$ cd \u0026lt;workdir\u0026gt;/imagetool-setup $ git clone https://github.com/oracle/docker-images.git For example:\n$ cd /scratch/imagetool-setup $ git clone https://github.com/oracle/docker-images.git    Note: If you want to create the image continue with the following steps, otherwise to update the image see update an image.\n Create an image After setting up the WebLogic Image Tool, follow these steps to use the WebLogic Image Tool to create a new Oracle Unified Directory Services Manager image.\nDownload the Oracle Unified Directory Services Manager installation binaries and patches You must download the required Oracle Unified Directory Services Manager installation binaries and patches as listed below from the Oracle Software Delivery Cloud and save them in a directory of your choice.\nThe installation binaries and patches required are:\n  Oracle Unified Directory 12.2.1.4.0\n fmw_12.2.1.4.0_oud.jar    Oracle Fusion Middleware 12c Infrastructure 12.2.1.4.0\n fmw_12.2.1.4.0_infrastructure.jar    OUDSM and FMW Infrastructure Patches:\n View document ID 2723908.1 on My Oracle Support. In the Container Image Download/Patch Details section, locate the Oracle Unified Directory Services Manager (OUDSM) table. For the latest PSU click the README link in the Documentation column. In the README, locate the \u0026ldquo;Installed Software\u0026rdquo; section. All the patch numbers to be download are listed here. Download all these individual patches from My Oracle Support.    Oracle JDK v8\n jdk-8uXXX-linux-x64.tar.gz where XXX is the JDK version referenced in the README above.    Update required build files The following files in the code repository location \u0026lt;imagetool-setup-location\u0026gt;/docker-images/OracleUnifiedDirectorySM/imagetool/12.2.1.4.0 are used for creating the image:\n additionalBuildCmds.txt buildArgs    Edit the \u0026lt;workdir\u0026gt;/imagetool-setup/docker-images/OracleUnifiedDirectorySM/imagetool/12.2.1.4.0/buildArgs file and change %DOCKER_REPO%,%JDK_VERSION% and %BUILDTAG% appropriately.\nFor example:\ncreate --jdkVersion=8u321 --type oud_wls --version=12.2.1.4.0 --tag=oudsm-latestpsu:12.2.1.4.0 --pull --installerResponseFile /scratch/imagetool-setup/docker-images/OracleFMWInfrastructure/dockerfiles/12.2.1.4/install.file,/scratch/imagetool-setup/docker-images/OracleUnifiedDirectorySM/dockerfiles/12.2.1.4.0/install/oud.response --additionalBuildCommands /scratch/imagetool-setup/docker-images/OracleUnifiedDirectorySM/imagetool/12.2.1.4.0/additionalBuildCmds.txt --additionalBuildFiles /scratch/imagetool-setup/docker-images/OracleUnifiedDirectorySM/dockerfiles/12.2.1.4.0/container-scripts   The \u0026lt;workdir\u0026gt;/imagetool-setup/imagetool/docker-images/OracleUnifiedDirectorySM/imagetool/12.2.1.4.0/additionalBuildCmds.txt contains additional build commands. You may edit this file if you want to customize the image further.\n  Edit the \u0026lt;workdir\u0026gt;/imagetool-setup/docker-images/OracleFMWInfrastructure/dockerfiles/12.2.1.4/install.file and under the GENERIC section add the line INSTALL_TYPE=\u0026quot;Fusion Middleware Infrastructure\u0026rdquo;. For example:\n[GENERIC] INSTALL_TYPE=\u0026quot;Fusion Middleware Infrastructure\u0026quot; DECLINE_SECURITY_UPDATES=true SECURITY_UPDATES_VIA_MYORACLESUPPORT=false   Create the image   Add a JDK package to the WebLogic Image Tool cache. For example:\n$ imagetool cache addInstaller --type jdk --version 8uXXX --path \u0026lt;download location\u0026gt;/jdk-8uXXX-linux-x64.tar.gz where XXX is the JDK version downloaded\n  Add the downloaded installation binaries to the WebLogic Image Tool cache. For example:\n$ imagetool cache addInstaller --type OUD --version 12.2.1.4.0 --path \u0026lt;download location\u0026gt;/fmw_12.2.1.4.0_oud.jar $ imagetool cache addInstaller --type fmw --version 12.2.1.4.0 --path \u0026lt;download location\u0026gt;/fmw_12.2.1.4.0_infrastructure.jar   Add the downloaded OPatch patch to the WebLogic Image Tool cache. For example:\n$ imagetool cache addEntry --key 28186730_13.9.4.2.8 --value \u0026lt;download location\u0026gt;/p28186730_139428_Generic.zip   Add the rest of the downloaded product patches to the WebLogic Image Tool cache:\n$ imagetool cache addEntry --key \u0026lt;patch\u0026gt;_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p\u0026lt;patch\u0026gt;_122140_Generic.zip For example:\n$ imagetool cache addEntry --key 33727616_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p33727616_122140_Generic.zip $ imagetool cache addEntry --key 33093748_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p33093748_122140_Generic.zip $ imagetool cache addEntry --key 32720458_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p32720458_122140_Generic.zip $ imagetool cache addEntry --key 33791665_12.2.1.4.220105 --value \u0026lt;download location\u0026gt;/p33791665_12214220105_Generic.zip $ imagetool cache addEntry --key 33723124_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p33723124_122140_Generic.zip $ imagetool cache addEntry --key 32647448_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p32647448_122140_Linux-x86-64.zip $ imagetool cache addEntry --key 33591019_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p33591019_122140_Generic.zip $ imagetool cache addEntry --key 32999272_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p32999272_122140_Generic.zip $ imagetool cache addEntry --key 33448950_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p33448950_122140_Generic.zip $ imagetool cache addEntry --key 33697227_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p33697227_122140_Generic.zip $ imagetool cache addEntry --key 33678607_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p33678607_122140_Generic.zip $ imagetool cache addEntry --key 33735326_12.2.1.4.220105 --value \u0026lt;download location\u0026gt;/p33735326_12214220105_Generic.zip   Edit the \u0026lt;workdir\u0026gt;/imagetool-setup/docker-images/OracleUnifiedDirectorySM/imagetool/12.2.1.4.0/buildArgs file and append the product patches and opatch patch as follows:\n--patches 33727616_12.2.1.4.0,33093748_12.2.1.4.0,32720458_12.2.1.4.0,33791665_12.2.1.4.220105,33723124_12.2.1.4.0,32647448_12.2.1.4.0,33591019_12.2.1.4.0,32999272_12.2.1.4.0,33448950_12.2.1.4.0,32999272_12.2.1.4.0,33448950_12.2.1.4.0,33697227_12.2.1.4.0,33678607_12.2.1.4.0,33735326_12.2.1.4.220105 --opatchBugNumber=28186730_13.9.4.2.8 An example buildArgs file is now as follows:\ncreate --jdkVersion=8u321 --type oud_wls --version=12.2.1.4.0 --tag=oudsm-latestpsu:12.2.1.4.0 --pull --installerResponseFile /scratch/imagetool-setup/docker-images/OracleFMWInfrastructure/dockerfiles/12.2.1.4/install.file,/scratch/imagetool-setup/docker-images/OracleUnifiedDirectorySM/dockerfiles/12.2.1.4.0/install/oud.response --additionalBuildCommands /scratch/imagetool-setup/docker-images/OracleUnifiedDirectorySM/imagetool/12.2.1.4.0/additionalBuildCmds.txt --additionalBuildFiles /scratch/imagetool-setup/docker-images/OracleUnifiedDirectorySM/dockerfiles/12.2.1.4.0/container-scripts --patches 33727616_12.2.1.4.0,33093748_12.2.1.4.0,32720458_12.2.1.4.0,33791665_12.2.1.4.220105,33723124_12.2.1.4.0,32647448_12.2.1.4.0,33591019_12.2.1.4.0,32999272_12.2.1.4.0,33448950_12.2.1.4.0,33448950_12.2.1.4.0,33697227_12.2.1.4.0,33678607_12.2.1.4.0,33735326_12.2.1.4.220105 --opatchBugNumber=28186730_13.9.4.2.8  Note: In the buildArgs file:\n --jdkVersion value must match the --version value used in the imagetool cache addInstaller command for --type jdk. --version value must match the --version value used in the imagetool cache addInstaller command for --type OUDSM.   Refer to this page for the complete list of options available with the WebLogic Image Tool create command.\n  Create the Oracle Unified Directory Services Manager image:\n$ imagetool @\u0026lt;absolute path to buildargs file\u0026gt; --fromImage ghcr.io/oracle/oraclelinux:7-slim  Note: Make sure that the absolute path to the buildargs file is prepended with a @ character, as shown in the example above.\n For example:\n$ imagetool @\u0026lt;imagetool-setup-location\u0026gt;/docker-images/OracleUnifiedDirectorySM/imagetool/12.2.1.4.0/buildArgs --fromImage ghcr.io/oracle/oraclelinux:7-slim   Check the created image using the docker images command:\n$ docker images | grep oudsm The output will look similar to the following:\noudsm-latestpsu 12.2.1.4.0 f6dd9d2ca0e6 4 minutes ago 3.72GB   Run the following command to save the container image to a tar file:\n$ docker save -o \u0026lt;path\u0026gt;/\u0026lt;file\u0026gt;.tar \u0026lt;image\u0026gt; For example:\n$ docker save -o $WORKDIR/oudsm-latestpsu.tar oudsm-latestpsu:12.2.1.4.0   Update an image The steps below show how to update an existing Oracle Unified Directory Services Manager image with an interim patch.\nThe container image to be patched must be loaded in the local docker images repository before attempting these steps.\nIn the examples below the image oracle/oudsm:12.2.1.4.0 is updated with an interim patch.\n$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE oracle/oudsm 12.2.1.4.0 b051804ba15f 3 months ago 3.72GB   Set up the WebLogic Image Tool.\n  Download the required interim patch and latest Opatch (28186730) from My Oracle Support. and save them in a directory of your choice.\n  Add the OPatch patch to the WebLogic Image Tool cache, for example:\n$ imagetool cache addEntry --key 28186730_13.9.4.2.8 --value \u0026lt;downloaded-patches-location\u0026gt;/p28186730_139428_Generic.zip   Execute the imagetool cache addEntry command for each patch to add the required patch(es) to the WebLogic Image Tool cache. For example, to add patch p33521773_12214211008_Generic.zip:\n$ imagetool cache addEntry --key=33521773_12.2.1.4.211008 --value \u0026lt;downloaded-patches-location\u0026gt;/p33521773_12214211008_Generic.zip   Provide the following arguments to the WebLogic Image Tool update command:\n –-fromImage - Identify the image that needs to be updated. In the example below, the image to be updated is oracle/oudsm:12.2.1.4.0. –-patches - Multiple patches can be specified as a comma-separated list. --tag - Specify the new tag to be applied for the image being built.  Refer here for the complete list of options available with the WebLogic Image Tool update command.\n Note: The WebLogic Image Tool cache should have the latest OPatch zip. The WebLogic Image Tool will update the OPatch if it is not already updated in the image.\n For example:\n$ imagetool update --fromImage oracle/oudsm:12.2.1.4.0 --tag=oracle/oudsm-new:12.2.1.4.0 --patches=33521773_12.2.1.4.211008 --opatchBugNumber=28186730_13.9.4.2.8  Note: If the command fails because the files in the image being upgraded are not owned by oracle:oracle, then add the parameter --chown \u0026lt;userid\u0026gt;:\u0026lt;groupid\u0026gt; to correspond with the values returned in the error.\n   Check the built image using the docker images command:\n$ docker images | grep oudsm The output will look similar to the following:\nREPOSITORY TAG IMAGE ID CREATED SIZE oracle/oudsm-new 12.2.1.4.0 78ccd1ad67eb 5 minutes ago 1.11GB oracle/oudsm 12.2.1.4.0 b051804ba15f 3 months ago 1.04GB   Run the following command to save the patched container image to a tar file:\n$ docker save -o \u0026lt;path\u0026gt;/\u0026lt;file\u0026gt;.tar \u0026lt;image\u0026gt; For example:\n$ docker save -o $WORKDIR/oudsm-new.tar oracle/oudsm-new:12.2.1.4.0   "
},
{
	"uri": "/fmw-kubernetes/oam/post-install-config/",
	"title": "Post Install Configuration",
	"tags": [],
	"description": "Post install configuration.",
	"content": "Follow these post install configuration steps.\n Create a Server Overrides File Removing OAM Server from WebLogic Server 12c Default Coherence Cluster WebLogic Server Tuning Enable Virtualization Modify oamconfig.properties  Create a Server Overrides File   Navigate to the following directory:\n$ cd $WORKDIR/kubernetes/create-access-domain/domain-home-on-pv/output/weblogic-domains/accessdomain   Create a setUserOverrides.sh with the following contents:\nDERBY_FLAG=false JAVA_OPTIONS=\u0026quot;${JAVA_OPTIONS} -Djava.net.preferIPv4Stack=true\u0026quot; MEM_ARGS=\u0026quot;-Xms8192m -Xmx8192m\u0026quot;   Copy the setUserOverrides.sh file to the Administration Server pod:\n$ chmod 755 setUserOverrides.sh $ kubectl cp setUserOverrides.sh oamns/accessdomain-adminserver:/u01/oracle/user_projects/domains/accessdomain/bin/setUserOverrides.sh Where oamns is the OAM namespace and accessdomain is the DOMAIN_NAME/UID.\n  Stop the OAM domain using the following command:\n$ kubectl -n \u0026lt;domain_namespace\u0026gt; patch domains \u0026lt;domain_uid\u0026gt; --type=\u0026#39;json\u0026#39; -p=\u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/serverStartPolicy\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;NEVER\u0026#34; }]\u0026#39; For example:\n$ kubectl -n oamns patch domains accessdomain --type=\u0026#39;json\u0026#39; -p=\u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/serverStartPolicy\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;NEVER\u0026#34; }]\u0026#39; The output will look similar to the following:\ndomain.weblogic.oracle/accessdomain patched   Check that all the pods are stopped:\n$ kubectl get pods -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get pods -n oamns The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE accessdomain-adminserver 1/1 Terminating 0 27m accessdomain-create-oam-infra-domain-job-7c9r9 0/1 Completed 0 4h29m accessdomain-oam-policy-mgr1 1/1 Terminating 0 24m accessdomain-oam-policy-mgr2 1/1 Terminating 0 24m accessdomain-oam-server1 1/1 Terminating 0 24m accessdomain-oam-server2 1/1 Terminating 0 24m helper 1/1 Running 0 4h44m nginx-ingress-ingress-nginx-controller-76fb7678f-k8rhq 1/1 Running 0 108m The Administration Server pods and Managed Server pods will move to a STATUS of Terminating. After a few minutes, run the command again and the pods should have disappeared:\nNAME READY STATUS RESTARTS AGE accessdomain-create-oam-infra-domain-job-7c9r9 0/1 Completed 0 4h30m helper 1/1 Running 0 4h45m nginx-ingress-ingress-nginx-controller-76fb7678f-k8rhq 1/1 Running 0 109m   Start the domain using the following command:\n$ kubectl -n \u0026lt;domain_namespace\u0026gt; patch domains \u0026lt;domain_uid\u0026gt; --type=\u0026#39;json\u0026#39; -p=\u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/serverStartPolicy\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;IF_NEEDED\u0026#34; }]\u0026#39; For example:\n$ kubectl -n oamns patch domains accessdomain --type=\u0026#39;json\u0026#39; -p=\u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/serverStartPolicy\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;IF_NEEDED\u0026#34; }]\u0026#39; Run the following kubectl command to view the pods:\n$ kubectl get pods -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get pods -n oamns The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE accessdomain-create-oam-infra-domain-job-7c9r9 0/1 Completed 0 4h30m accessdomain-introspector-mckp2 1/1 Running 0 8s helper 1/1 Running 0 4h46m nginx-ingress-ingress-nginx-controller-76fb7678f-k8rhq 1/1 Running 0 110m The Administration Server pod will start followed by the OAM Managed Servers pods. This process will take several minutes, so keep executing the command until all the pods are running with READY status 1/1:\nNAME READY STATUS RESTARTS AGE accessdomain-adminserver 1/1 Running 0 5m38s accessdomain-create-oam-infra-domain-job-7c9r9 0/1 Completed 0 4h37m accessdomain-oam-policy-mgr1 1/1 Running 0 2m51s accessdomain-oam-policy-mgr2 1/1 Running 0 2m51s accessdomain-oam-server1 1/1 Running 0 2m50s accessdomain-oam-server2 1/1 Running 0 2m50s helper 1/1 Running 0 4h52m nginx-ingress-ingress-nginx-controller-76fb7678f-k8rhq 1/1 Running 0 116m   Removing OAM Server from WebLogic Server 12c Default Coherence Cluster Exclude all Oracle Access Management (OAM) clusters (including Policy Manager and OAM runtime server) from the default WebLogic Server 12c coherence cluster by using the WebLogic Server Administration Console.\nFrom 12.2.1.3.0 onwards, OAM server-side session management uses the database and does not require coherence cluster to be established. In some environments, warnings and errors are observed due to default coherence cluster initialized by WebLogic. To avoid or fix these errors, exclude all of the OAM clusters from default WebLogic Server coherence cluster using the following steps:\n Login to the WebLogic Server Console at https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/console. Click Lock \u0026amp; Edit. In Domain Structure, expand Environment and select Coherence Clusters. Click defaultCoherenceCluster and select the Members tab. From Servers and Clusters, deselect all OAM clusters (oam_cluster and policy_cluster). Click Save. Click Activate changes.  WebLogic Server Tuning For production environments, the following WebLogic Server tuning parameters must be set:\nAdd Minimum Thread constraint to worker manager \u0026ldquo;OAPOverRestWM\u0026rdquo;  Login to the WebLogic Server Console at https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/console. Click Lock \u0026amp; Edit. In Domain Structure, click Deployments. On the Deployments page click Next until you see oam_server. Expand oam_server by clicking on the + icon, then click /iam/access/binding. Click the Configuration tab, followed by the Workload tab. Click wm/OAPOverRestWM Under Application Scoped Work Managed Components, click New. In Create a New Work Manager Component, select Minumum Threads Constraint and click Next. In Minimum Threads Constraint Properties enter the Count as 400 and click Finish. In the Save Deployment Plan change the Path to the value /u01/oracle/user_projects/domains/accessdomain/Plan.xml, where accessdomain is your domain_UID. Click OK and then Activate Changes.  Remove Max Thread Constraint and Capacity Constraint  Repeat steps 1-7 above. Under Application Scoped Work Managed Components select the check box for Capacity and MaxThreadsCount. Click Delete. In the Delete Work Manage Components screen, click OK to delete. Click on Release Configuration and then Log Out.  oamDS DataSource Tuning  Login to the WebLogic Server Console at https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/console. Click Lock \u0026amp; Edit. In Domain Structure, Expand Services and click Data Sources. Click on oamDS. In Settings for oamDS, select the Configuration tab, and then the Connection Pool tab. Change Initial Capacity, Maximum Capacity, and Minimum Capacity to 800 and click Save. Click Activate Changes.  Enable Virtualization  Log in to Oracle Enterprise Manager Fusion Middleware Control at https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/em Click WebLogic Domain \u0026gt; Security \u0026gt; Security Provider Configuration. Expand Security Store Provider. Expand Identity Store Provider. Click Configure. Add a custom property. Select virtualize property with value true and click OK. Click OK again to persist the change.  Modify oamconfig.properties   Navigate to the following directory and change permissions for the oamconfig_modify.sh:\n$ cd $WORKDIR/kubernetes/create-access-domain/domain-home-on-pv/common $ chmod 777 oamconfig_modify.sh For example:\n$ cd $WORKDIR/kubernetes/create-access-domain/domain-home-on-pv/common $ chmod 777 oamconfig_modify.sh   Edit the oamconfig.properties and change the OAM_NAMESPACE and LBR_HOST to match the values for your OAM Kubernetes environment. For example:\n#Below are only the sample values, please modify them as per your setup # The name space where OAM servers are created OAM_NAMESPACE='oamns' # Define the INGRESS CONTROLLER used. INGRESS=\u0026quot;nginx\u0026quot; # Define the INGRESS CONTROLLER name used during installation. INGRESS_NAME=\u0026quot;nginx-ingress\u0026quot; # FQDN of the LBR Host i.e the host from where you access oam console LBR_HOST=\u0026quot;masternode.example.com\u0026quot;   Run the oamconfig_modify.sh script as follows:\n$ ./oamconfig_modify.sh \u0026lt;OAM_ADMIN_USER\u0026gt;:\u0026lt;OAM_ADMIN_PASSWORD\u0026gt; where:\nOAM_ADMIN_USER is the OAM administrator username\nOAM_ADMIN_PASSWORD is the OAM administrator password\nFor example:\n$ ./oamconfig_modify.sh weblogic:\u0026lt;password\u0026gt; Note: Make sure port 30540 is free before running the command.\nThe output will look similar to the following:\nLBR_PROTOCOL: https domainUID: accessdomain OAM_SERVER: accessdomain-oam-server OAM_NAMESPACE: oamns INGRESS: nginx INGRESS_NAME: nginx-ingress ING_TYPE : NodePort LBR_HOST: masternode.example.com LBR_PORT: 31051 Started Executing Command % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 764k 0 764k 0 0 221k 0 --:--:-- 0:00:03 --:--:-- 221k new_cluster_id: a52fc-masternode service/accessdomain-oamoap-service created accessdomain-oamoap-service NodePort 10.100.202.44 \u0026lt;none\u0026gt; 5575:30540/TCP 1s nginx-ingress-ingress-nginx-controller NodePort 10.101.132.251 \u0026lt;none\u0026gt; 80:32371/TCP,443:31051/TCP 144m HTTP/1.1 100 Continue HTTP/1.1 201 Created Date: Mon, 01 Nov 2021 16:59:12 GMT Content-Type: text/plain Content-Length: 76 Connection: keep-alive X-ORACLE-DMS-ECID: 9234b1a0-83b4-4100-9875-aa00e3f5db27-0000035f X-ORACLE-DMS-RID: 0 Set-Cookie: JSESSIONID=pSXccMR6t8B5QoyaAlOuZYSmhtseX4C4jx-0tnkmNyer8L1mOLET!402058795; path=/; HttpOnly Set-Cookie: _WL_AUTHCOOKIE_JSESSIONID=X1iqH-mtDNGyFx5ZCXMK; path=/; secure; HttpOnly Strict-Transport-Security: max-age=15724800; includeSubDomains https://masternode.example.com:31051/iam/admin/config/api/v1/config?path=%2F $WORKDIR/kubernetes/create-access-domain/domain-home-on-pv/common/output/oamconfig_modify.xml executed successfully --------------------------------------------------------------------------- Initializing WebLogic Scripting Tool (WLST) ... Welcome to WebLogic Server Administration Scripting Shell Type help() for help on available commands Connecting to t3://accessdomain-adminserver:7001 with userid weblogic ... Successfully connected to Admin Server \u0026quot;AdminServer\u0026quot; that belongs to domain \u0026quot;accessdomain\u0026quot;. Warning: An insecure protocol was used to connect to the server. To ensure on-the-wire security, the SSL port or Admin port should be used instead. Location changed to domainRuntime tree. This is a read-only tree with DomainMBean as the root MBean. For more help, use help('domainRuntime') Exiting WebLogic Scripting Tool. Please wait for some time for the server to restart pod \u0026quot;accessdomain-oam-server1\u0026quot; deleted pod \u0026quot;accessdomain-oam-server2\u0026quot; deleted Waiting continuously at an interval of 10 secs for servers to start.. Waiting continuously at an interval of 10 secs for servers to start.. Waiting continuously at an interval of 10 secs for servers to start.. Waiting continuously at an interval of 10 secs for servers to start.. Waiting continuously at an interval of 10 secs for servers to start.. ... Waiting continuously at an interval of 10 secs for servers to start.. Waiting continuously at an interval of 10 secs for servers to start.. accessdomain-oam-server1 1/1 Running 0 4m37s accessdomain-oam-server2 1/1 Running 0 4m36s OAM servers started successfully The script will delete the accessdomain-oam-server1 and accessdomain-oam-server2 pods and then create new ones. Check the pods are running again by issuing the following command:\n$ kubectl get pods -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get pods -n oamns The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE accessdomain-adminserver 1/1 Running 0 43m accessdomain-create-oam-infra-domain-job-7c9r9 0/1 Completed 0 5h14m accessdomain-oam-policy-mgr1 1/1 Running 0 40m accessdomain-oam-policy-mgr2 1/1 Running 0 40m accessdomain-oam-server1 0/1 Running 0 8m3s accessdomain-oam-server2 0/1 Running 0 8m2s helper 0/1 Running 0 5h29m nginx-ingress-ingress-nginx-controller-76fb7678f-k8rhq 1/1 Running 0 154m The accessdomain-oam-server1 and accessdomain-oam-server2 are started, but currently have a READY status of 0/1. This means oam_server1 and oam_server2 are not currently running but are in the process of starting. The servers will take several minutes to start so keep executing the command until READY shows 1/1:\nNAME READY STATUS RESTARTS AGE accessdomain-adminserver 1/1 Running 0 49m accessdomain-create-oam-infra-domain-job-7c9r9 0/1 Completed 0 5h21m accessdomain-oam-policy-mgr1 1/1 Running 0 46m accessdomain-oam-policy-mgr2 1/1 Running 0 46m accessdomain-oam-server1 1/1 Running 0 14m accessdomain-oam-server2 1/1 Running 0 14m helper 1/1 Running 0 5h36m nginx-ingress-ingress-nginx-controller-76fb7678f-k8rhq 1/1 Running 0 160m   "
},
{
	"uri": "/fmw-kubernetes/oig/post-install-config/",
	"title": "Post install configuration",
	"tags": [],
	"description": "Post install configuration.",
	"content": "Follow these post install configuration steps.\n a. Post Install Tasks  Perform post install tasks.\n b. Install and configure connectors  Install and Configure Connectors.\n "
},
{
	"uri": "/fmw-kubernetes/wccontent-domains/appendix/",
	"title": "Appendix",
	"tags": [],
	"description": "",
	"content": "This section provides information on miscellaneous tasks related to Oracle WebCenter Content domain deployment on Kubernetes.\n Domain resource sizing  Describes the resourse sizing information for Oracle WebCenter Content domain setup on Kubernetes cluster.\n Quick start deployment guide  Describes how to quickly get an Oracle WebCenter Content domain instance running (using the defaults, nothing special) for development and test purposes.\n Security hardening  Review resources for the Docker and Kubernetes cluster hardening.\n "
},
{
	"uri": "/fmw-kubernetes/soa-domains/appendix/",
	"title": "Appendix",
	"tags": [],
	"description": "",
	"content": "This section provides information on miscellaneous tasks related to Oracle SOA Suite domains deployment on Kubernetes.\n Domain resource sizing  Describes the resourse sizing information for Oracle SOA Suite domains setup on Kubernetes cluster.\n Quick start deployment on-premise  Describes how to quickly get an Oracle SOA Suite domain instance running (using the defaults, nothing special) for development and test purposes.\n Security hardening  Review resources for the Docker and Kubernetes cluster hardening.\n "
},
{
	"uri": "/fmw-kubernetes/wcsites-domains/appendix/",
	"title": "Appendix",
	"tags": [],
	"description": "",
	"content": "This section provides information on miscellaneous tasks related to Oracle WebCenter Sites domains deployment on Kubernetes.\n Domain resource sizing  Describes the resourse sizing information for Oracle WebCenter Sites domains setup on Kubernetes cluster.\n Quick start deployment on-premise  Describes how to quickly get an Oracle WebCenter Sites domain instance running (using the defaults, nothing special) for development and test purposes.\n Security hardening  Review resources for the Docker and Kubernetes cluster hardening.\n "
},
{
	"uri": "/fmw-kubernetes/soa-domains/adminguide/performing-wlst-operations/",
	"title": "Perform WLST operations",
	"tags": [],
	"description": "Perform WLST administration operations using a helper pod running in the same Kubernetes cluster as the Oracle SOA Suite domain.",
	"content": "You can use the WebLogic Scripting Tool (WLST) to manage a domain running in a Kubernetes cluster. Some of the many ways to do this are provided here.\nIf the Administration Server was configured to expose a T3 channel using exposeAdminT3Channel when creating the domain, refer to Use WLST.\nIf you do not want to expose additional ports and perform WLST administration operations using the existing Kubernetes services created by the WebLogic Server Kubernetes operator, then follow this documentation. Here we will be creating and using a helper pod in the same Kubernetes cluster as the Oracle SOA Suite domain to perform WLST operations.\n Note: To avoid any misconfigurations, Oracle recommends that you do not use the Administration Server pod directly for WLST operations.\n  Create a Kubernetes helper pod Perform WLST operations Sample WLST operations  Create a Kubernetes helper pod Before creating a Kubernetes helper pod, make sure that the Oracle SOA Suite Docker image is available on the node, or you can create an image pull secret so that the pod can pull the Docker image on the host where it gets created.\n  Create an image pull secret to pull image soasuite:12.2.1.4 by the helper pod.\nNote: Skip this step if you are not using an image pull secret.\n$ kubectl create secret docker-registry \u0026lt;secret-name\u0026gt; --namespace soans \\ --docker-server=\u0026lt;docker-registry-name\u0026gt; \\ --docker-username=\u0026lt;docker-user\u0026gt; \\ --docker-password=\u0026lt;docker-user\u0026gt; \\ --docker-email=\u0026lt;email-id\u0026gt; For example:\n$ kubectl create secret docker-registry image-secret --namespace soans \\ --docker-server=your-registry.com \\ --docker-username=xxxxxx \\ --docker-password=xxxxxxx \\ --docker-email=my@company.com   Create a helper pod.\nFor Kubernetes 1.18.10+, 1.19.7+, and 1.20.6+:\n$ kubectl run helper \\ --image \u0026lt;image_name\u0026gt; \\ --namespace \u0026lt;domain_namespace\u0026gt; \\ --overrides='{ \u0026quot;apiVersion\u0026quot;: \u0026quot;v1\u0026quot;, \u0026quot;spec\u0026quot;: { \u0026quot;imagePullSecrets\u0026quot;: [{\u0026quot;name\u0026quot;: \u0026quot;\u0026lt;secret-name\u0026gt;\u0026quot;}] } }' \\ -- sleep infinity For Kubernetes 1.16.15+, and 1.17.13+:\n$ kubectl run helper --generator=run-pod/v1 \\ --image \u0026lt;image_name\u0026gt; \\ --namespace \u0026lt;domain_namespace\u0026gt; \\ --overrides='{ \u0026quot;apiVersion\u0026quot;: \u0026quot;v1\u0026quot;, \u0026quot;spec\u0026quot;: { \u0026quot;imagePullSecrets\u0026quot;: [{\u0026quot;name\u0026quot;: \u0026quot;\u0026lt;secret-name\u0026gt;\u0026quot;}] } }' \\ -- sleep infinity For example:\n$ kubectl run helper \\ --image soasuite:12.2.1.4 \\ --namespace soans \\ --overrides='{ \u0026quot;apiVersion\u0026quot;: \u0026quot;v1\u0026quot;, \u0026quot;spec\u0026quot;: { \u0026quot;imagePullSecrets\u0026quot;: [{\u0026quot;name\u0026quot;: \u0026quot;image-secret\u0026quot;}] } }' \\ -- sleep infinity  Note: If you are not using the image pull secret, remove --overrides='{ \u0026quot;apiVersion\u0026quot;: \u0026quot;v1\u0026quot;, \u0026quot;spec\u0026quot;: { \u0026quot;imagePullSecrets\u0026quot;: [{\u0026quot;name\u0026quot;: \u0026quot;\u0026lt;secret-name\u0026gt;\u0026quot;}] } }' .\n   Perform WLST operations Once the Kubernetes helper pod is deployed, you can exec into the pod, connect to servers using t3 or t3s and perform WLST operations. By default, t3s is not enabled for the Administration Server or Managed Servers. If you enabled SSL with sslEnabled when creating the domain, then you can use t3s to perform WLST operations.\nInteractive mode   Start a bash shell in the helper pod:\n$ kubectl exec -it helper -n \u0026lt;domain_namespace\u0026gt; -- /bin/bash For example:\n$ kubectl exec -it helper -n soans -- /bin/bash This opens a bash shell in the running helper pod:\n[oracle@helper oracle]$   Invoke WLST:\n[oracle@helper oracle]$ cd $ORACLE_HOME/oracle_common/common/bin [oracle@helper bin]$ ./wlst.sh The output will look similar to the following:\n[oracle@helper bin]$ ./wlst.sh Initializing WebLogic Scripting Tool (WLST) ... Jython scans all the jar files it can find at first startup. Depending on the system, this process may take a few minutes to complete, and WLST may not return a prompt right away. Welcome to WebLogic Server Administration Scripting Shell Type help() for help on available commands wls:/offline\u0026gt;   Connect using t3:\na. To connect to the Administration Server or Managed Servers using t3, you can use the Kubernetes services created by the WebLogic Server Kubernetes operator:\nwls:/offline\u0026gt; connect('weblogic','\u0026lt;password\u0026gt;','t3://\u0026lt;domainUID\u0026gt;-\u0026lt;WebLogic Server Name\u0026gt;:\u0026lt;Server Port\u0026gt;') For example, if the domainUID is soainfra, Administration Server name is AdminServer, and Administration Server port is 7001, then you can connect to the Administration Server using t3:\nwls:/offline\u0026gt; connect('weblogic','\u0026lt;password\u0026gt;','t3://soainfra-adminserver:7001') The output will look similar to the following:\nwls:/offline\u0026gt; connect('weblogic','\u0026lt;password\u0026gt;','t3://soainfra-adminserver:7001') Connecting to t3://soainfra-adminserver:7001 with userid weblogic ... Successfully connected to Admin Server \u0026quot;AdminServer\u0026quot; that belongs to domain \u0026quot;soainfra\u0026quot;. Warning: An insecure protocol was used to connect to the server. To ensure on-the-wire security, the SSL port or Admin port should be used instead. wls:/soainfra/serverConfig/\u0026gt; b. To connect a WebLogic Server cluster (SOA or Oracle Service Bus) using t3, you can use the Kubernetes services created by the WebLogic Server Kubernetes operator:\nwls:/offline\u0026gt; connect('weblogic','\u0026lt;password\u0026gt;','t3://\u0026lt;domainUID\u0026gt;-cluster-\u0026lt;Cluster name\u0026gt;:\u0026lt;Managed Server Port\u0026gt;') For example, if the domainUID is soainfra, SOA cluster name is soa-cluster, and SOA Managed Server port is 8001, then you can connect to SOA Cluster using t3:\nwls:/offline\u0026gt; connect('weblogic','\u0026lt;password\u0026gt;','t3://soainfra-cluster-soa-cluster:8001') The output will look similar to the following:\nwls:/offline\u0026gt; connect('weblogic','\u0026lt;password\u0026gt;','t3://soainfra-cluster-soa-cluster:8001') Connecting to t3://soainfra-cluster-soa-cluster:8001 with userid weblogic ... Successfully connected to Managed Server \u0026quot;soa_server1\u0026quot; that belongs to domain \u0026quot;soainfra\u0026quot;. Warning: An insecure protocol was used to connect to the server. To ensure on-the-wire security, the SSL port or Admin port should be used instead. wls:/soainfra/serverConfig/\u0026gt;   Connect using t3s.\nIf you enabled SSL with sslEnabled when creating the domain, then you can use t3s to perform WLST operations:\na. Obtain the certificate from the Administration Server to be used for a secured (t3s) connection from the client by exporting the certificate from the Administration Server using WLST commands. Sample commands to export the default demoidentity:\n[oracle@helper oracle]$ cd $ORACLE_HOME/oracle_common/common/bin [oracle@helper bin]$ ./wlst.sh . . wls:/offline\u0026gt; connect('weblogic','\u0026lt;password\u0026gt;','t3://soainfra-adminserver:7001') . . wls:/soainfra/serverConfig/\u0026gt; svc = getOpssService(name='KeyStoreService') wls:/soainfra/serverConfig/\u0026gt; svc.exportKeyStoreCertificate(appStripe='system', name='demoidentity', password='DemoIdentityKeyStorePassPhrase', alias='DemoIdetityKeyStorePassPhrase', type='Certificate', filepath='/tmp/cert.txt/') These commands download the certificate for the default demoidentity certificate at /tmp/cert.txt.\nb. Import the certificate to the Java trust store:\n[oracle@helper oracle]$ export JAVA_HOME=/u01/jdk [oracle@helper oracle]$ keytool -import -v -trustcacerts -alias soadomain -file /tmp/cert.txt -keystore $JAVA_HOME/jre/lib/security/cacerts -keypass changeit -storepass changeit c. Connect to WLST and set the required environment variable before connecting using t3s:\n[oracle@helper oracle]$ export WLST_PROPERTIES=\u0026quot;-Dweblogic.security.SSL.ignoreHostnameVerification=true\u0026quot; [oracle@helper oracle]$ cd $ORACLE_HOME/oracle_common/common/bin [oracle@helper bin]$ ./wlst.sh d. Access t3s for the Administration Server.\nFor example, if the domainUID is soainfra, Administration Server name is AdminServer, and Administration Server SSL port is 7002, connect to the Administration Server as follows:\nwls:/offline\u0026gt; connect('weblogic','\u0026lt;password\u0026gt;','t3s://soainfra-adminserver:7002') e. Access t3s for the SOA cluster.\nFor example, if the domainUID is soainfra, SOA cluster name is soa-cluster, and SOA Managed Server SSL port is 8002, connect to the SOA cluster as follows:\nwls:/offline\u0026gt; connect('weblogic','\u0026lt;password\u0026gt;','t3s://soainfra-cluster-soa-cluster:8002')   Script mode In script mode, scripts contain WLST commands in a text file with a .py file extension (for example, mywlst.py). Before invoking WLST using the script file, you must copy the .py file into the helper pod.\nTo copy the .py file into the helper pod using WLST operations in script mode:\n  Create a .py file containing all the WLST commands.\n  Copy the .py file into the helper pod:\n$ kubectl cp \u0026lt;filename\u0026gt;.py \u0026lt;domain namespace\u0026gt;/helper:\u0026lt;directory\u0026gt; For example:\n$ kubectl cp mywlst.py soans/helper:/u01/oracle   Run wlst.sh on the .py file by exec into the helper pod:\n$ kubectl exec -it helper -n \u0026lt;domain_namespace\u0026gt; -- /bin/bash [oracle@helper oracle]$ cd $ORACLE_HOME/oracle_common/common/bin [oracle@helper oracle]$ ./wlst.sh \u0026lt;directory\u0026gt;/\u0026lt;filename\u0026gt;.py   Note: Refer to Interactive mode for details on how to connect using t3 or t3s.\nSample WLST operations For a full list of WLST operations, refer to WebLogic Server WLST Online and Offline Command Reference.\nDisplay servers $ kubectl exec -it helper -n soans -- /bin/bash [oracle@helper oracle]$ cd $ORACLE_HOME/oracle_common/common/bin [oracle@helper bin]$ ./wlst.sh Initializing WebLogic Scripting Tool (WLST) ... Jython scans all the jar files it can find at first startup. Depending on the system, this process may take a few minutes to complete, and WLST may not return a prompt right away. Welcome to WebLogic Server Administration Scripting Shell Type help() for help on available commands wls:/offline\u0026gt; connect('weblogic','Welcome1','t3://soainfra-adminserver:7001') Connecting to t3://soainfra-adminserver:7001 with userid weblogic ... Successfully connected to Admin Server \u0026quot;AdminServer\u0026quot; that belongs to domain \u0026quot;soainfra\u0026quot;. Warning: An insecure protocol was used to connect to the server. To ensure on-the-wire security, the SSL port or Admin port should be used instead. wls:/soainfra/serverConfig/\u0026gt; cd('/Servers') wls:/soainfra/serverConfig/Servers\u0026gt; ls() dr-- AdminServer dr-- osb_server1 dr-- osb_server2 dr-- osb_server3 dr-- osb_server4 dr-- osb_server5 dr-- soa_server1 dr-- soa_server2 dr-- soa_server3 dr-- soa_server4 dr-- soa_server5 wls:/soainfra/serverConfig/Servers\u0026gt; "
},
{
	"uri": "/fmw-kubernetes/wcportal-domains/",
	"title": "Oracle WebCenter Portal",
	"tags": [],
	"description": "The WebLogic Kubernetes operator (the “operator”) supports deployment of Oracle WebCenter Portal. Follow the instructions in this guide to set up Oracle WebCenter Portal domain on Kubernetes.",
	"content": "With the WebLogic Kubernetes operator (operator), you can deploy your Oracle WebCenter Portal on Kubernetes.\nIn this release, Oracle WebCenter Portal domain is based on the “domain on a persistent volume” model, where the domain home is located in a persistent volume.\nIn this release the support for Portlet Managed Server has been added.\nThe operator has several key features to assist you with deploying and managing the Oracle WebCenter Portal domain in a Kubernetes environment. You can:\n Create Oracle WebCenter Portal instances in a Kubernetes PV. This PV can reside in an Network File System (NFS) or other Kubernetes volume types. Start servers based on declarative startup parameters and desired states. Expose the Oracle WebCenter Portal services for external access. Scale Oracle WebCenter Portal domain by starting and stopping Managed Servers on demand, or by integrating with a REST API. Publish operator and WebLogic Server logs to Elasticsearch and interact with them in Kibana. Monitor the Oracle WebCenter Portal instance using Prometheus and Grafana.  Current release The current production release for the Oracle WebCenter Portal domain deployment on Kubernetes is 22.2.2. This release uses the WebLogic Kubernetes Operator version 3.3.0.\nRecent changes and known issues See the Release Notes for recent changes and known issues with the Oracle WebCenter Portal domain deployment on Kubernetes.\nAbout this documentation This documentation includes sections targeted to different audiences. To help you find what you are looking for more easily, please use this table of contents:\n  Quick Start explains how to quickly get an Oracle WebCenter Portal domain instance running, using the defaults, nothing special. Note that this is only for development and test purposes.\n  Install Guide and Administration Guide provide detailed information about all aspects of using the Kubernetes operator including:\n Installing and configuring the operator Using the operator to create and manage Oracle WebCenter Portal domain Configuring WebCenter Portal for Search Configuring Kubernetes load balancers Configuring Prometheus and Grafana to monitor WebCenter Portal Configuring Logging using ElasticSearch    Documentation for earlier releases To view documentation for an earlier release, see:\n Version 21.2.3  "
},
{
	"uri": "/fmw-kubernetes/oig/configure-design-console/",
	"title": "Configure Design Console",
	"tags": [],
	"description": "Configure Design Console.",
	"content": "Configure an Ingress to allow Design Console to connect to your Kubernetes cluster.\n a. Using Design Console with NGINX(non-SSL)  Configure Design Console with NGINX(non-SSL).\n b. Using Design Console with NGINX(SSL)  Configure Design Console with NGINX(SSL).\n "
},
{
	"uri": "/fmw-kubernetes/oid/patch-and-upgrade/",
	"title": "Patch and Upgrade",
	"tags": [],
	"description": "This document provides steps to patch or upgrade an OID image",
	"content": "Introduction In this section the Oracle Internet Directory (OID) deployment is updated with a new OID container image.\nNote: If you are not using Oracle Container Registry or your own container registry, then you must first load the new container image on all nodes in your Kubernetes cluster.\nYou can update the deployment with a new OID container image using one of the following methods:\n Using a YAML file Using --set argument  Using a YAML file   Navigate to the $WORKDIR/kubernetes/helm directory:\n$ cd $WORKDIR/kubernetes/helm   Create a oid-patch-override.yaml file that contains:\nimage: repository: \u0026lt;image_location\u0026gt; tag: \u0026lt;image_tag\u0026gt; imagePullSecrets: - name: orclcred For example:\nimage: repository: container-registry.oracle.com/middleware/oid_cpu tag: 12.2.1.4-jdk8-ol7-new imagePullSecrets: - name: orclcred The following caveats exist:\n  If you are not using Oracle Container Registry or your own container registry for your OID container image, then you can remove the following:\nimagePullSecrets: - name: orclcred     Run the following command to upgrade the deployment:\n$ helm upgrade --namespace \u0026lt;namespace\u0026gt; \\ --values oid-patch-override.yaml \\ \u0026lt;release_name\u0026gt; oid --reuse-values For example:\n$ helm upgrade --namespace oidns \\ --values oid-patch-override.yaml \\ oid oid --reuse-values   Using --set argument   Navigate to the $WORKDIR/kubernetes/helm directory:\n$ cd $WORKDIR/kubernetes/helm   Run the following command to update the deployment with a new OID container image:\n$ helm upgrade --namespace \u0026lt;namespace\u0026gt; \\ --set image.repository=\u0026lt;image_location\u0026gt;,image.tag=\u0026lt;image_tag\u0026gt; \\ --set imagePullSecrets[0].name=\u0026#34;orclcred\u0026#34; \\ \u0026lt;release_name\u0026gt; oid --reuse-values For example:\n$ helm upgrade --namespace oidns \\ --set image.repository=container-registry.oracle.com/middleware/oid_cpu,image.tag=12.2.1.4-jdk8-ol7-new \\ --set imagePullSecrets[0].name=\u0026#34;orclcred\u0026#34; \\ oid oid --reuse-values The following caveats exist:\n If you are not using Oracle Container Registry or your own container registry for your OID container image, then you can remove the following: --set imagePullSecrets[0].name=\u0026quot;orclcred\u0026quot;.    Verify the pods   After updating with the new image the pods will restart. Verify the pods are running:\n$ kubectl --namespace \u0026lt;namespace\u0026gt; get pods -o wide For example:\n$ kubectl --namespace oidns get pods -o wide The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/oidhost1 1/1 Running 0 45m 10.244.0.195 \u0026lt;Worker Node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/oidhost2 1/1 Running 0 45m 10.244.0.194 \u0026lt;Worker Node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Note: It will take several minutes before the pods start. While the oid pods have a STATUS of 0/1 the pods are started but the OID server associated with it is currently starting. While the pods are starting you can check the startup status in the pod logs, by running the following command:\n  Verify the pods are using the new image by running the following command:\n$ kubectl describe pod \u0026lt;pod\u0026gt; -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl describe pod oid-0 -n oidns The output will look similar to the following:\nName: oid-0 Namespace: oidns Priority: 0 Node: \u0026lt;Worker Node\u0026gt;/100.102.48.28 Start Time: Wed, 16 Mar 2022 12:07:36 +0000 Labels: app.kubernetes.io/instance=oid app.kubernetes.io/managed-by=Helm app.kubernetes.io/name=oid app.kubernetes.io/version=12.2.1.4.0 helm.sh/chart=oid-0.1 oid/instance=oid-0 Annotations: meta.helm.sh/release-name: oid meta.helm.sh/release-namespace: oidns Status: Running IP: 10.244.1.44 etc... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Killing 4m26s kubelet Container oid definition changed, will be restarted Warning Unhealthy 3m56s kubelet Readiness probe failed: Normal Pulling 3m56s kubelet Pulling image \u0026#34;container-registry.oracle.com/middleware/oid_cpu:12.2.1.4-jdk8-ol7-new\u0026#34; Warning Unhealthy 3m27s kubelet Liveness probe failed: dial tcp 10.244.1.44:1389: connect: connection refused Normal Created 3m22s (x2 over 142m) kubelet Created container oid Normal Started 3m22s (x2 over 142m) kubelet Started container oid Normal Pulled 3m22s kubelet Successfully pulled image \u0026#34;container-registry.oracle.com/middleware/oid_cpu:12.2.1.4-jdk8-ol7-new\u0026#34; in 33.477063844s   "
},
{
	"uri": "/fmw-kubernetes/oud/patch-and-upgrade/",
	"title": "Patch and Upgrade",
	"tags": [],
	"description": "This document provides steps to patch or upgrade an OUD image",
	"content": "Introduction In this section the Oracle Unified Directory (OUD) deployment is updated with a new OUD container image.\nNote: If you are not using Oracle Container Registry or your own container registry, then you must first load the new container image on all nodes in your Kubernetes cluster.\nYou can update the deployment with a new OUD container image using one of the following methods:\n Using a YAML file Using --set argument  Using a YAML file   Navigate to the $WORKDIR/kubernetes/helm directory:\n$ cd $WORKDIR/kubernetes/helm   Create a oud-patch-override.yaml file that contains:\nimage: repository: \u0026lt;image_location\u0026gt; tag: \u0026lt;image_tag\u0026gt; imagePullSecrets: - name: orclcred For example:\nimage: repository: container-registry.oracle.com/middleware/oud_cpu tag: 12.2.1.4-jdk8-ol7-new imagePullSecrets: - name: orclcred The following caveats exist:\n  If you are not using Oracle Container Registry or your own container registry for your OUD container image, then you can remove the following:\nimagePullSecrets: - name: orclcred     Run the following command to upgrade the deployment:\n$ helm upgrade --namespace \u0026lt;namespace\u0026gt; \\ --values oud-patch-override.yaml \\ \u0026lt;release_name\u0026gt; oud-ds-rs --reuse-values For example:\n$ helm upgrade --namespace oudns \\ --values oud-patch-override.yaml \\ oud-ds-rs oud-ds-rs --reuse-values   Using --set argument   Navigate to the $WORKDIR/kubernetes/helm directory:\n$ cd $WORKDIR/kubernetes/helm   Run the following command to update the deployment with a new OUD container image:\n$ helm upgrade --namespace \u0026lt;namespace\u0026gt; \\ --set image.repository=\u0026lt;image_location\u0026gt;,image.tag=\u0026lt;image_tag\u0026gt; \\ --set imagePullSecrets[0].name=\u0026#34;orclcred\u0026#34; \\ \u0026lt;release_name\u0026gt; oud-ds-rs --reuse-values For example:\n$ helm upgrade --namespace oudns \\ --set image.repository=container-registry.oracle.com/middleware/oud_cpu,image.tag=12.2.1.4-jdk8-ol7-new \\ --set imagePullSecrets[0].name=\u0026#34;orclcred\u0026#34; \\ oud-ds-rs oud-ds-rs --reuse-values The following caveats exist:\n If you are not using Oracle Container Registry or your own container registry for your OUD container image, then you can remove the following: --set imagePullSecrets[0].name=\u0026quot;orclcred\u0026quot;.    Verify the pods   After updating with the new image the pods will restart. Verify the pods are running:\n$ kubectl --namespace \u0026lt;namespace\u0026gt; get pods For example:\n$ kubectl --namespace oudns get pods The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/oud-ds-rs-0 1/1 Running 0 45m 10.244.0.195 \u0026lt;Worker Node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/oud-ds-rs-1 1/1 Running 0 45m 10.244.0.194 \u0026lt;Worker Node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/oud-ds-rs-2 1/1 Running 0 45m 10.244.0.193 \u0026lt;Worker Node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Note: It will take several minutes before the pods start. While the oudsm pods have a STATUS of 0/1 the pods are started but the OUD server associated with it is currently starting. While the pods are starting you can check the startup status in the pod logs, by running the following command:\n  Verify the pods are using the new image by running the following command:\n$ kubectl describe pod \u0026lt;pod\u0026gt; -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl describe pod oud-ds-rs-0 -n oudns The output will look similar to the following:\nName: oud-ds-rs-0 Namespace: oudns Priority: 0 Node: \u0026lt;Worker Node\u0026gt;/100.102.48.28 Start Time: Wed, 16 Mar 2022 12:07:36 +0000 Labels: app.kubernetes.io/instance=oud-ds-rs app.kubernetes.io/managed-by=Helm app.kubernetes.io/name=oud-ds-rs app.kubernetes.io/version=12.2.1.4.0 helm.sh/chart=oud-ds-rs-0.1 oud/instance=oud-ds-rs-0 Annotations: meta.helm.sh/release-name: oud-ds-rs meta.helm.sh/release-namespace: oudns Status: Running IP: 10.244.1.44 etc... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Killing 4m26s kubelet Container oud-ds-rs definition changed, will be restarted Warning Unhealthy 3m56s kubelet Readiness probe failed: Normal Pulling 3m56s kubelet Pulling image \u0026#34;container-registry.oracle.com/middleware/oud_cpu:12.2.1.4-jdk8-ol7-new\u0026#34; Warning Unhealthy 3m27s kubelet Liveness probe failed: dial tcp 10.244.1.44:1389: connect: connection refused Normal Created 3m22s (x2 over 142m) kubelet Created container oud-ds-rs Normal Started 3m22s (x2 over 142m) kubelet Started container oud-ds-rs Normal Pulled 3m22s kubelet Successfully pulled image \u0026#34;container-registry.oracle.com/middleware/oud_cpu:12.2.1.4-jdk8-ol7-new\u0026#34; in 33.477063844s   "
},
{
	"uri": "/fmw-kubernetes/oudsm/patch-and-upgrade/",
	"title": "Patch and Upgrade",
	"tags": [],
	"description": "This document provides steps to patch or upgrade an OUDSM image",
	"content": "Introduction In this section the Oracle Unified Directory Services Manager (OUDSM) deployment is updated with a new OUDSM container image.\nNote: If you are not using Oracle Container Registry or your own container registry, then you must first load the new container image on all nodes in your Kubernetes cluster.\nYou can update the deployment with a new OUDSM container image using one of the following methods:\n Using a YAML file Using --set argument  Using a YAML file   Navigate to the $WORKDIR/kubernetes/helm directory:\n$ cd $WORKDIR/kubernetes/helm   Create a oudsm-patch-override.yaml file that contains:\nimage: repository: \u0026lt;image_location\u0026gt; tag: \u0026lt;image_tag\u0026gt; imagePullSecrets: - name: orclcred For example:\nimage: repository: container-registry.oracle.com/middleware/oudsm_cpu tag: 12.2.1.4-jdk8-ol7-new imagePullSecrets: - name: orclcred The following caveats exist:\n  If you are not using Oracle Container Registry or your own container registry for your oudsm container image, then you can remove the following:\nimagePullSecrets: - name: orclcred     Run the following command to upgrade the deployment:\n$ helm upgrade --namespace \u0026lt;namespace\u0026gt; \\ --values oudsm-patch-override.yaml \\ \u0026lt;release_name\u0026gt; oudsm --reuse-values For example:\n$ helm upgrade --namespace oudsmns \\ --values oudsm-patch-override.yaml \\ oudsm oudsm --reuse-values   Using --set argument   Navigate to the $WORKDIR/kubernetes/helm directory:\n$ cd $WORKDIR/kubernetes/helm   Run the following command to update the deployment with a new OUDSM container image:\n$ helm upgrade --namespace \u0026lt;namespace\u0026gt; \\ --set image.repository=\u0026lt;image_location\u0026gt;,image.tag=\u0026lt;image_tag\u0026gt; \\ --set imagePullSecrets[0].name=\u0026#34;orclcred\u0026#34; \\ \u0026lt;release_name\u0026gt; oudsm --reuse-values For example:\n$ helm upgrade --namespace oudsmns \\ --set image.repository=container-registry.oracle.com/middleware/oudsm_cpu,image.tag=12.2.1.4-jdk8-ol7-new \\ --set imagePullSecrets[0].name=\u0026#34;orclcred\u0026#34; \\ oudsm oudsm --reuse-values The following caveats exist:\n If you are not using Oracle Container Registry or your own container registry for your OUDSM container image, then you can remove the following: --set imagePullSecrets[0].name=\u0026quot;orclcred\u0026quot;.    Verify the pods   After updating with the new image the pod will restart. Verify the pod is running:\n$ kubectl --namespace \u0026lt;namespace\u0026gt; get pods For example:\n$ kubectl --namespace oudsmns get pods The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/oudsm-1 1/1 Running 0 73m 10.244.0.19 \u0026lt;worker-node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Note: It will take several minutes before the pod starts. While the oudsm pods have a STATUS of 0/1 the pod is started but the OUDSM server associated with it is currently starting. While the pod is starting you can check the startup status in the pod logs, by running the following command:\n  Verify the pod is using the new image by running the following command:\n$ kubectl describe pod \u0026lt;pod\u0026gt; -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl describe pod oudsm-1 -n oudsmns The output will look similar to the following:\nName: oudsm-1 Namespace: oudsmns Priority: 0 Node: prats-crio-worker2/100.102.48.28 Start Time: Wed, 23 Mar 2022 10:38:20 +0000 Labels: app.kubernetes.io/instance=oudsm app.kubernetes.io/managed-by=Helm app.kubernetes.io/name=oudsm app.kubernetes.io/version=12.2.1.4.0 helm.sh/chart=oudsm-0.1 oudsm/instance=oudsm-1 Annotations: meta.helm.sh/release-name: oudsm meta.helm.sh/release-namespace: oudsmns Status: Running IP: 10.244.1.90 etc... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Killing 22m kubelet Container oudsm definition changed, will be restarted Normal Created 21m (x2 over 61m) kubelet Created container oudsm Normal Pulling 21m kubelet Container image \u0026#34;container-registry.oracle.com/middleware/oudsm_cpu:12.2.1.4-jdk8-ol7-new\u0026#34; Normal Started 21m (x2 over 61m) kubelet Started container oudsm   "
},
{
	"uri": "/fmw-kubernetes/oam/validate-sso-using-webgate/",
	"title": "Validate a Basic SSO Flow using WebGate Registration ",
	"tags": [],
	"description": "Sample for validating a basic SSO flow using WebGate registration.",
	"content": "In this section you validate single-sign on works to the OAM Kubernetes cluster via Oracle WebGate. The instructions below assume you have a running Oracle HTTP Server (for example ohs_k8s) and Oracle WebGate installed on an independent server. The instructions also assume basic knowledge of how to register a WebGate agent.\nNote: At present Oracle HTTP Server and Oracle WebGate are not supported on a Kubernetes cluster.\nUpdate the OAM Hostname and Port for the Loadbalancer If using an NGINX ingress with no load balancer, change {LOADBALANCER-HOSTNAME}:${LOADBALANCER-PORT} to {MASTERNODE-HOSTNAME}:${MASTERNODE-PORT} when referenced below.\n  Launch a browser and access the OAM console (https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-PORT}/oamconsole). Login with the weblogic username and password (weblogic/\u0026lt;password\u0026gt;)\n  Navigate to Configuration → Settings ( View ) → Access Manager.\n  Under Load Balancing modify the OAM Server Host and OAM Server Port, to point to the Loadbalancer HTTP endpoint (e.g loadbalancer.example.com and \u0026lt;port\u0026gt; respectively). In the OAM Server Protocol drop down list select https.\n  Under WebGate Traffic Load Balancer modify the OAM Server Host and OAM Server Port, to point to the Loadbalancer HTTP endpoint (e.g loadbalancer.example.com and \u0026lt;port\u0026gt; repectively). In the OAM Server Protocol drop down list select https.\n  Click Apply.\n  Register a WebGate Agent In all the examples below, change the directory path as appropriate for your installation.\n  Run the following command on the server with Oracle HTTP Server and WebGate installed:\n$ cd \u0026lt;OHS_ORACLE_HOME\u0026gt;/webgate/ohs/tools/deployWebGate $ ./deployWebGateInstance.sh -w \u0026lt;OHS_DOMAIN_HOME\u0026gt;/config/fmwconfig/components/OHS/ohs_k8s -oh \u0026lt;OHS_ORACLE_HOME\u0026gt; -ws ohs The output will look similar to the following:\nCopying files from WebGate Oracle Home to WebGate Instancedir   Run the following command to update the OHS configuration files appropriately:\n$ export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:\u0026lt;OHS_ORACLE_HOME\u0026gt;/lib $ cd \u0026lt;OHS_ORACLE_HOME\u0026gt;/webgate/ohs/tools/setup/InstallTools/ $ ./EditHttpConf -w \u0026lt;OHS_DOMAIN_HOME\u0026gt;/config/fmwconfig/components/OHS/ohs_k8s -oh \u0026lt;OHS_ORACLE_HOME\u0026gt; The output will look similar to the following:\nThe web server configuration file was successfully updated \u0026lt;OHS_DOMAIN_HOME\u0026gt;/config/fmwconfig/components/OHS/ohs_k8s/httpd.conf has been backed up as \u0026lt;OHS_DOMAIN_HOME\u0026gt;/config/fmwconfig/components/OHS/ohs_k8s/httpd.conf.ORIG   Launch a browser, and access the OAM console. Navigate to Application Security → Quick Start Wizards → SSO Agent Registration. Register the agent in the usual way, download the configuration zip file and copy to the OHS WebGate server, for example: \u0026lt;OHS_DOMAIN_HOME\u0026gt;/config/fmwconfig/components/OHS/ohs_k8/webgate/config. Extract the zip file.\n  Copy the Certificate Authority (CA) certificate (cacert.pem) for the load balancer/ingress certificate to the same directory e.g: \u0026lt;OHS_DOMAIN_HOME\u0026gt;/config/fmwconfig/components/OHS/ohs_k8/webgate/config.\nIf you used a self signed certificate for the ingress, instead copy the self signed certificate (e.g: /scratch/ssl/tls.crt) to the above directory. Rename the certificate to cacert.pem.\n  Restart Oracle HTTP Server.\n  Access the configured OHS e.g http://ohs.example.com:7778, and check you are redirected to the SSO login page. Login and make sure you are redirected successfully to the home page.\n  Changing WebGate agent to use OAP Note: This section should only be followed if you need to change the OAM/WebGate Agent communication from HTTPS to OAP.\nTo change the WebGate agent to use OAP:\n  In the OAM Console click Application Security and then Agents.\n  Search for the agent you want modify and select it.\n  In the User Defined Parameters change:\na) OAMServerCommunicationMode from HTTPS to OAP. For example OAMServerCommunicationMode=OAP\nb) OAMRestEndPointHostName=\u0026lt;hostname\u0026gt; to the {$MASTERNODE-HOSTNAME}. For example OAMRestEndPointHostName=masternode.example.com\n  In the Server Lists section click Add to add a new server with the following values:\n Access Server: Other Host Name: \u0026lt;{$MASTERNODE-HOSTNAME}\u0026gt; Host Port: \u0026lt;oamoap-service NodePort\u0026gt;  Note: To find the value for Host Port run the following:\n$ kubectl describe svc accessdomain-oamoap-service -n oamns The output will look similar to the following:\nName: accessdomain-oamoap-service Namespace: oamns Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Selector: weblogic.clusterName=oam_cluster Type: NodePort IP Families: \u0026lt;none\u0026gt; IP: 10.100.202.44 IPs: 10.100.202.44 Port: \u0026lt;unset\u0026gt; 5575/TCP TargetPort: 5575/TCP NodePort: \u0026lt;unset\u0026gt; 30540/TCP Endpoints: 10.244.5.21:5575,10.244.6.76:5575 Session Affinity: None External Traffic Policy: Cluster Events: \u0026lt;none\u0026gt; In the example above the NodePort is 30540.\n  Delete all servers in Server Lists except for the one just created, and click Apply.\n  Click Download to download the webgate zip file. Copy the zip file to the desired WebGate.\n  Delete the cache from \u0026lt;OHS_DOMAIN_HOME\u0026gt;/servers/ohs1/cache and restart Oracle HTTP Server.\n  "
},
{
	"uri": "/fmw-kubernetes/wcsites-domains/",
	"title": "Oracle WebCenter Sites",
	"tags": [],
	"description": "The WebLogic Kubernetes Operator supports deployment of Oracle WebCenter Sites. Follow the instructions in this guide to set up Oracle WebCenter Sites domains on Kubernetes.",
	"content": "In this release, Oracle WebCenter Sites domains are supported using the \u0026ldquo;domain on a persistent volume\u0026rdquo; model only, where the domain home is located in a persistent volume (PV).\nThe operator has several key features to assist you with deploying and managing Oracle WebCenter Sites domains in a Kubernetes environment. You can:\n Create Oracle WebCenter Sites instances in a Kubernetes persistent volume (PV). This PV can reside in an NFS file system or other Kubernetes volume types. Start servers based on declarative startup parameters and desired states. Expose the Oracle WebCenter Sites services and composites for external access. Scale Oracle WebCenter Sites domains by starting and stopping Managed Servers on demand, or by integrating with a REST API to initiate scaling based on WLDF, Prometheus, Grafana, or other rules. Publish operator and WebLogic Server logs to Elasticsearch and interact with them in Kibana. Monitor the Oracle WebCenter Sites instance using Prometheus and Grafana.  Current production release The current supported production release of the Oracle WebLogic Server Kubernetes Operator, for Oracle WebCenter Sites domains deployment is 3.3.0\nRecent changes and known issues See the Release Notes for recent changes and known issues for Oracle WebCenter Sites domains deployment on Kubernetes.\nLimitations See here for limitations in this release.\nAbout this documentation This documentation includes sections targeted to different audiences. To help you find what you are looking for more easily, please consult this table of contents:\n  Quick Start explains how to quickly get an Oracle WebCenter Sites domain instance running, using the defaults, nothing special. Note that this is only for development and test purposes.\n  Install Guide and Administration Guide provide detailed information about all aspects of using the Kubernetes operator including:\n Installing and configuring the operator. Using the operator to create and manage Oracle WebCenter Sites domains. Configuring Kubernetes load balancers. Configuring Elasticsearch and Kibana to access the operator and WebLogic Server log files. Patching an Oracle WebCenter Sites Docker image. Removing/deleting domains. And much more!    Additional reading Oracle WebCenter Sites domains deployment on Kubernetes leverages the Oracle WebLogic Server Kubernetes operator framework.\n To develop an understanding of the operator, including design, architecture, domain life cycle management, and configuration overrides, review the operator documentation. To learn more about the Oracle WebCenter Sites architecture and components, see Understanding Oracle WebCenter Sites. To understand the known issues and common questions for Oracle WebCenter Sites domains deployment on Kubernetes, see the frequently asked questions.  "
},
{
	"uri": "/fmw-kubernetes/oam/manage-oam-domains/",
	"title": "Manage OAM Domains",
	"tags": [],
	"description": "This document provides steps to manage the OAM domain.",
	"content": "Important considerations for Oracle Access Management domains in Kubernetes.\n a. Domain Life Cycle  Learn about the domain life cycle of an OAM domain.\n b. WLST Administration Operations  Describes the steps for WLST administration using helper pod running in the same Kubernetes Cluster as OAM Domain.\n c. Logging and Visualization  Describes the steps for logging and visualization with Elasticsearch and Kibana.\n d. Monitoring an OAM domain  Describes the steps for Monitoring the OAM domain.\n e. Delete the OAM domain home  Learn about the steps to cleanup the OAM domain home.\n "
},
{
	"uri": "/fmw-kubernetes/oig/manage-oig-domains/",
	"title": "Manage OIG domains",
	"tags": [],
	"description": "This document provides steps to manage the OIG domain.",
	"content": "Important considerations for Oracle Identity Governance domains in Kubernetes.\n Domain life cycle  Learn about the domain life cyle of an OIG domain.\n WLST administration operations  Describes the steps for WLST administration using helper pod running in the same Kubernetes Cluster as OIG Domain.\n Runnning OIG utilities  Describes the steps for running OIG utilities in Kubernetes.\n Logging and visualization  Describes the steps for logging and visualization with Elasticsearch and Kibana.\n Monitoring an OIG domain  Describes the steps for Monitoring the OIG domain and Publising the logs to Elasticsearch.\n Delete the OIG domain home  Learn about the steps to cleanup the OIG domain home.\n "
},
{
	"uri": "/fmw-kubernetes/oid/troubleshooting/",
	"title": "Troubleshooting",
	"tags": [],
	"description": "How to Troubleshoot issues.",
	"content": " Check the status of a namespace View pod logs View pod description Cleaning down a failed OID deployment  Check the status of a namespace To check the status of objects in a namespace use the following command:\n$ kubectl --namespace \u0026lt;namespace\u0026gt; get pod,service,secret,pv,pvc,ingress -o wide For example:\n$ kubectl --namespace oidns get pod,service,secret,pv,pvc,ingress -o wide Output will be similar to the following:\n NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/oidhost1 1/1 Running 0 26m 10.244.1.150 \u0026lt;worker\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/oidhost2 1/1 Running 0 26m 10.244.2.157 \u0026lt;worker\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service/oid-lbr-ldap ClusterIP 10.96.82.57 \u0026lt;none\u0026gt; 3060/TCP,3131/TCP 26m app.kubernetes.io/instance=oid,app.kubernetes.io/name=oid service/oidhost1 ClusterIP 10.111.67.10 \u0026lt;none\u0026gt; 3060/TCP,3131/TCP,7001/TCP,7002/TCP 26m app.kubernetes.io/instance=oid,app.kubernetes.io/name=oid,oid/instance=oidhost1 service/oidhost2 ClusterIP 10.96.29.184 \u0026lt;none\u0026gt; 3060/TCP,3131/TCP 26m app.kubernetes.io/instance=oid,app.kubernetes.io/name=oid,oid/instance=oidhost2 NAME TYPE DATA AGE secret/default-token-5nrlh kubernetes.io/service-account-token 3 3d7h secret/oid-creds opaque 7 26m secret/oid-tls-cert kubernetes.io/tls 2 26m secret/oid-token-s95zt kubernetes.io/service-account-token 3 26m secret/orclcred kubernetes.io/dockerconfigjson 1 3d7h secret/sh.helm.release.v1.oid.v1 helm.sh/release.v1 1 26m NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE VOLUMEMODE persistentvolume/oid-pv 20Gi RWX Delete Bound oidns/oid-pvc manual 26m Filesystem NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE VOLUMEMODE persistentvolumeclaim/oid-pvc Bound oid-pv 20Gi RWX manual 26m Filesystem NAME CLASS HOSTS ADDRESS PORTS AGE ingress.networking.k8s.io/oid-ingress-nginx \u0026lt;none\u0026gt; * 80, 443 26m Include/exclude elements (pod,service,secret,pv,pvc,ingress) as required.\nView POD Logs To view logs for a pod use the following command:\n$ kubectl logs \u0026lt;pod\u0026gt; -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl logs oidhost1 -n oidns Output will depend on the application running in the POD.\nView Pod Description Details about a pod can be viewed using the kubectl describe command:\n$ kubectl describe pod \u0026lt;pod\u0026gt; -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl describe pod oidhost1 -n oidns Output will be similar to the following:\nName: oidhost1 Namespace: oidns Priority: 0 Node: \u0026lt;worker\u0026gt;/100.102.48.28 Start Time: Mon, 28 Mar 2022 16:19:54 +0000 Labels: app.kubernetes.io/instance=oid app.kubernetes.io/managed-by=Helm app.kubernetes.io/name=oid app.kubernetes.io/version=12.2.1.4.0 helm.sh/chart=oid-0.1 oid/instance=oidhost1 Annotations: meta.helm.sh/release-name: oid meta.helm.sh/release-namespace: oidns Status: Running IP: 10.244.1.150 IPs: IP: 10.244.1.150 Containers: oid: Container ID: cri-o://4172c7694d84c64c7c16e02fe96a8fc233b530ff5ae3b24d6440ff958c224e85 Image: container-registry.oracle.com/middleware/oid_cpu:12.2.1.4-jdk8-ol7-220223.1744 Image ID: container-registry.oracle.com/middleware/oid_cpu@sha256:ec1483590503837a3aa355dab1e33ab5237017b3924e12c2b1554c373d43a16b Ports: 3060/TCP, 3131/TCP, 7001/TCP, 7002/TCP Host Ports: 0/TCP, 0/TCP, 0/TCP, 0/TCP State: Running Started: Mon, 28 Mar 2022 16:19:55 +0000 Ready: True Restart Count: 0 Readiness: exec [/u01/oracle/dockertools/healthcheck_status.sh] delay=600s timeout=30s period=60s #success=1 #failure=15 Environment: INSTANCE_TYPE: PRIMARY sleepBeforeConfig: 180 INSTANCE_NAME: oid1 ADMIN_LISTEN_HOST: oidhost1 REALM_DN: dc=oid CONNECTION_STRING: oiddb.example.com:1521/oiddb.example.com LDAP_PORT: 3060 LDAPS_PORT: 3131 ADMIN_LISTEN_PORT: 7001 ADMIN_LISTEN_SSL_PORT: 7002 DOMAIN_NAME: oid_domain DOMAIN_HOME: /u01/oracle/user_projects/domains/oid_domain RCUPREFIX: OIDK8S7 ADMIN_USER: \u0026lt;set to the key 'adminUser' in secret 'oid-creds'\u0026gt; Optional: false ADMIN_PASSWORD: \u0026lt;set to the key 'adminPassword' in secret 'oid-creds'\u0026gt; Optional: false DB_USER: \u0026lt;set to the key 'dbUser' in secret 'oid-creds'\u0026gt; Optional: false DB_PASSWORD: \u0026lt;set to the key 'dbPassword' in secret 'oid-creds'\u0026gt; Optional: false DB_SCHEMA_PASSWORD: \u0026lt;set to the key 'dbschemaPassword' in secret 'oid-creds'\u0026gt; Optional: false ORCL_ADMIN_PASSWORD: \u0026lt;set to the key 'orcladminPassword' in secret 'oid-creds'\u0026gt; Optional: false SSL_WALLET_PASSWORD: \u0026lt;set to the key 'sslwalletPassword' in secret 'oid-creds'\u0026gt; Optional: false ldapPort: 3060 ldapsPort: 3131 httpPort: 7001 httpsPort: 7002 Mounts: /u01/oracle/user_projects from oid-pv (rw) /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-r26f6 (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: oid-pv: Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace) ClaimName: oid-pvc ReadOnly: false kube-api-access-r26f6: Type: Projected (a volume that contains injected data from multiple sources) TokenExpirationSeconds: 3607 ConfigMapName: kube-root-ca.crt ConfigMapOptional: \u0026lt;nil\u0026gt; DownwardAPI: true QoS Class: BestEffort Node-Selectors: \u0026lt;none\u0026gt; Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 29m default-scheduler Successfully assigned oidns/oidhost1 to \u0026lt;worker\u0026gt; Normal Pulled 29m kubelet Container image \u0026quot;container-registry.oracle.com/middleware/oid_cpu:12.2.1.4-jdk8-ol7-220223.1744\u0026quot; already present on machine Normal Created 29m kubelet Created container oid Normal Started 29m kubelet Started container oid "
},
{
	"uri": "/fmw-kubernetes/oud/troubleshooting/",
	"title": "Troubleshooting",
	"tags": [],
	"description": "How to Troubleshoot issues.",
	"content": " Check the status of a namespace View pod logs View pod description  Check the status of a namespace To check the status of objects in a namespace use the following command:\n$ kubectl --namespace \u0026lt;namespace\u0026gt; get nodes,pod,service,secret,pv,pvc,ingress -o wide For example:\n$ kubectl --namespace oudns get pod,service,secret,pv,pvc,ingress -o wide The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/oud-ds-rs-0 1/1 Running 1 2d2h 10.244.2.129 \u0026lt;Worker Node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/oud-ds-rs-1 1/1 Running 1 2d2h 10.244.2.128 \u0026lt;Worker Node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/oud-ds-rs-2 1/1 Running 1 2d2h 10.244.1.53 \u0026lt;Worker Node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service/oud-ds-rs-0 ClusterIP 10.111.120.232 \u0026lt;none\u0026gt; 1444/TCP,1888/TCP,1898/TCP 2d2h app.kubernetes.io/instance=oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=oud-ds-rs-0 service/oud-ds-rs-1 ClusterIP 10.98.199.92 \u0026lt;none\u0026gt; 1444/TCP,1888/TCP,1898/TCP 2d2h app.kubernetes.io/instance=oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=oud-ds-rs-1 service/oud-ds-rs-2 ClusterIP 10.103.22.27 \u0026lt;none\u0026gt; 1444/TCP,1888/TCP,1898/TCP 2d2h app.kubernetes.io/instance=oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=oud-ds-rs-2 service/oud-ds-rs-http-0 ClusterIP 10.100.75.60 \u0026lt;none\u0026gt; 1080/TCP,1081/TCP 2d2h app.kubernetes.io/instance=oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=oud-ds-rs-0 service/oud-ds-rs-http-1 ClusterIP 10.96.125.29 \u0026lt;none\u0026gt; 1080/TCP,1081/TCP 2d2h app.kubernetes.io/instance=oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=oud-ds-rs-1 service/oud-ds-rs-http-2 ClusterIP 10.98.147.195 \u0026lt;none\u0026gt; 1080/TCP,1081/TCP 2d2h app.kubernetes.io/instance=oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=oud-ds-rs-2 service/oud-ds-rs-lbr-admin ClusterIP 10.105.146.21 \u0026lt;none\u0026gt; 1888/TCP,1444/TCP 2d2h app.kubernetes.io/instance=oud-ds-rs,app.kubernetes.io/name=oud-ds-rs service/oud-ds-rs-lbr-http ClusterIP 10.101.185.178 \u0026lt;none\u0026gt; 1080/TCP,1081/TCP 2d2h app.kubernetes.io/instance=oud-ds-rs,app.kubernetes.io/name=oud-ds-rs service/oud-ds-rs-lbr-ldap ClusterIP 10.111.134.94 \u0026lt;none\u0026gt; 1389/TCP,1636/TCP 2d2h app.kubernetes.io/instance=oud-ds-rs,app.kubernetes.io/name=oud-ds-rs service/oud-ds-rs-ldap-0 ClusterIP 10.102.210.144 \u0026lt;none\u0026gt; 1389/TCP,1636/TCP 2d2h app.kubernetes.io/instance=oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=oud-ds-rs-0 service/oud-ds-rs-ldap-1 ClusterIP 10.98.75.22 \u0026lt;none\u0026gt; 1389/TCP,1636/TCP 2d2h app.kubernetes.io/instance=oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=oud-ds-rs-1 service/oud-ds-rs-ldap-2 ClusterIP 10.110.130.119 \u0026lt;none\u0026gt; 1389/TCP,1636/TCP 2d2h app.kubernetes.io/instance=oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=oud-ds-rs-2 NAME TYPE DATA AGE secret/default-token-n2pmp kubernetes.io/service-account-token 3 3d1h secret/orclcred kubernetes.io/dockerconfigjson 1 3d secret/oud-ds-rs-creds opaque 8 2d2h secret/oud-ds-rs-job-token-p4pz7 kubernetes.io/service-account-token 3 2d2h secret/oud-ds-rs-tls-cert kubernetes.io/tls 2 2d2h secret/oud-ds-rs-token-qzqt2 kubernetes.io/service-account-token 3 2d2h secret/sh.helm.release.v1.oud-ds-rs.v1 helm.sh/release.v1 1 2d2h secret/sh.helm.release.v1.oud-ds-rs.v2 helm.sh/release.v1 1 2d1h secret/sh.helm.release.v1.oud-ds-rs.v3 helm.sh/release.v1 1 2d1h secret/sh.helm.release.v1.oud-ds-rs.v4 helm.sh/release.v1 1 28h secret/sh.helm.release.v1.oud-ds-rs.v5 helm.sh/release.v1 1 25h secret/sh.helm.release.v1.oud-ds-rs.v6 helm.sh/release.v1 1 23h secret/sh.helm.release.v1.oud-ds-rs.v7 helm.sh/release.v1 1 23h NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE VOLUMEMODE persistentvolume/fmwk8s-jenkins-pv 1Gi RWO,RWX Delete Bound fmwk8s/fmwk8s-jenkins-pvc fmwk8s-jenkins-pv 35d Filesystem persistentvolume/fmwk8s-pv 1Gi RWO,RWX Delete Bound fmwk8s/fmwk8s-pvc fmwk8s-pv 35d Filesystem persistentvolume/fmwk8s-root-pv 1Gi RWO,RWX Delete Bound fmwk8s/fmwk8s-root-pvc fmwk8s-root-pv 35d Filesystem persistentvolume/oud-ds-rs-espv1 20Gi RWX Retain Bound oudns/data-oud-ds-rs-es-cluster-0 elk-oud 23h Filesystem persistentvolume/oud-ds-rs-job-pv 2Gi RWX Delete Bound oudns/oud-ds-rs-job-pvc manual 2d2h Filesystem persistentvolume/oud-ds-rs-pv 20Gi RWX Delete Bound oudns/oud-ds-rs-pvc manual 2d2h Filesystem NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE VOLUMEMODE persistentvolumeclaim/data-oud-ds-rs-es-cluster-0 Bound oud-ds-rs-espv1 20Gi RWX elk-oud 23h Filesystem persistentvolumeclaim/oud-ds-rs-job-pvc Bound oud-ds-rs-job-pv 2Gi RWX manual 2d2h Filesystem persistentvolumeclaim/oud-ds-rs-pvc Bound oud-ds-rs-pv 20Gi RWX manual 2d2h Filesystem NAME CLASS HOSTS ADDRESS PORTS AGE ingress.networking.k8s.io/oud-ds-rs-admin-ingress-nginx \u0026lt;none\u0026gt; oud-ds-rs-admin-0,oud-ds-rs-admin-1,oud-ds-rs-admin-2 + 3 more... 80, 443 2d2h ingress.networking.k8s.io/oud-ds-rs-http-ingress-nginx \u0026lt;none\u0026gt; oud-ds-rs-http-0,oud-ds-rs-http-1,oud-ds-rs-http-2 + 4 more... 80, 443 2d2h Include/exclude elements (nodes,pod,service,secret,pv,pvc,ingress) as required.\nView pod logs To view logs for a pod use the following command:\n$ kubectl logs \u0026lt;pod\u0026gt; -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl logs oud-ds-rs-0 -n oudns View pod description Details about a pod can be viewed using the kubectl describe command:\n$ kubectl describe pod \u0026lt;pod\u0026gt; -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl describe pod oud-ds-rs-0 -n oudns The output will look similar to the following:\nName: oud-ds-rs-0 Namespace: oudns Priority: 0 Node: \u0026lt;Worker Node\u0026gt;/100.102.48.84 Start Time: Wed, 16 Mar 2022 14:39:09 +0000 Labels: app.kubernetes.io/instance=oud-ds-rs app.kubernetes.io/managed-by=Helm app.kubernetes.io/name=oud-ds-rs app.kubernetes.io/version=12.2.1.4.0 helm.sh/chart=oud-ds-rs-0.1 oud/instance=oud-ds-rs-0 Annotations: meta.helm.sh/release-name: oud-ds-rs meta.helm.sh/release-namespace: oudns Status: Running IP: 10.244.2.129 IPs: IP: 10.244.2.129 Containers: oud-ds-rs: Container ID: cri-o://2795176b6af2c17a9426df54214c7e53318db9676bbcf3676d67843174845d68 Image: container-registry.oracle.com/middleware/oud_cpu:12.2.1.4-jdk8-ol7-220119.2051 Image ID: container-registry.oracle.com/middleware/oud_cpu@sha256:6ba20e54d17bb41312618011481e9b35a40f36f419834d751277f2ce2f172dca Ports: 1444/TCP, 1888/TCP, 1389/TCP, 1636/TCP, 1080/TCP, 1081/TCP, 1898/TCP Host Ports: 0/TCP, 0/TCP, 0/TCP, 0/TCP, 0/TCP, 0/TCP, 0/TCP State: Running Started: Wed, 16 Mar 2022 15:38:10 +0000 Last State: Terminated Reason: Error Exit Code: 137 Started: Wed, 16 Mar 2022 14:39:10 +0000 Finished: Wed, 16 Mar 2022 15:37:16 +0000 Ready: True Restart Count: 1 Liveness: tcp-socket :ldap delay=900s timeout=15s period=30s #success=1 #failure=1 Readiness: exec [/u01/oracle/container-scripts/checkOUDInstance.sh] delay=180s timeout=30s period=60s #success=1 #failure=10 Environment: instanceType: Directory sleepBeforeConfig: 3 OUD_INSTANCE_NAME: oud-ds-rs-0 hostname: oud-ds-rs-0 baseDN: dc=example,dc=com rootUserDN: \u0026lt;set to the key 'rootUserDN' in secret 'oud-ds-rs-creds'\u0026gt; Optional: false rootUserPassword: \u0026lt;set to the key 'rootUserPassword' in secret 'oud-ds-rs-creds'\u0026gt; Optional: false adminConnectorPort: 1444 httpAdminConnectorPort: 1888 ldapPort: 1389 ldapsPort: 1636 httpPort: 1080 httpsPort: 1081 replicationPort: 1898 sampleData: 0 Mounts: /u01/oracle/user_projects from oud-ds-rs-pv (rw) /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vr6v8 (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: oud-ds-rs-pv: Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace) ClaimName: oud-ds-rs-pvc ReadOnly: false kube-api-access-vr6v8: Type: Projected (a volume that contains injected data from multiple sources) TokenExpirationSeconds: 3607 ConfigMapName: kube-root-ca.crt ConfigMapOptional: \u0026lt;nil\u0026gt; DownwardAPI: true QoS Class: BestEffort Node-Selectors: \u0026lt;none\u0026gt; Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: \u0026lt;none\u0026gt; "
},
{
	"uri": "/fmw-kubernetes/oudsm/troubleshooting/",
	"title": "Troubleshooting",
	"tags": [],
	"description": "How to Troubleshoot issues.",
	"content": " Check the status of a namespace View pod logs View pod description  Check the status of a namespace To check the status of objects in a namespace use the following command:\n$ kubectl --namespace \u0026lt;namespace\u0026gt; get nodes,pod,service,secret,pv,pvc,ingress -o wide For example:\n$ kubectl --namespace oudsmns get nodes,pod,service,secret,pv,pvc,ingress -o wide The output will look similar to the following:\n$ kubectl --namespace oudsmns get pod,service,secret,pv,pvc,ingress -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/oudsm-1 1/1 Running 0 18m 10.244.1.89 \u0026lt;worker-node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service/oudsm-1 ClusterIP 10.101.79.110 \u0026lt;none\u0026gt; 7001/TCP,7002/TCP 18m app.kubernetes.io/instance=oudsm,app.kubernetes.io/name=oudsm,oudsm/instance=oudsm-1 service/oudsm-lbr ClusterIP 10.106.241.204 \u0026lt;none\u0026gt; 7001/TCP,7002/TCP 18m app.kubernetes.io/instance=oudsm,app.kubernetes.io/name=oudsm NAME TYPE DATA AGE secret/default-token-jtwn2 kubernetes.io/service-account-token 3 22h secret/orclcred kubernetes.io/dockerconfigjson 1 22h secret/oudsm-creds opaque 2 18m secret/oudsm-tls-cert kubernetes.io/tls 2 18m secret/oudsm-token-7kjff kubernetes.io/service-account-token 3 18m secret/sh.helm.release.v1.oudsm.v1 helm.sh/release.v1 1 18m NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE VOLUMEMODE persistentvolume/oudsm-pv 20Gi RWX Delete Bound oudsmns/oudsm-pvc manual 18m Filesystem NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE VOLUMEMODE persistentvolumeclaim/oudsm-pvc Bound oudsm-pv 20Gi RWX manual 18m Filesystem NAME CLASS HOSTS ADDRESS PORTS AGE ingress.networking.k8s.io/oudsm-ingress-nginx \u0026lt;none\u0026gt; oudsm-1,oudsm 80, 443 18m Include/exclude elements (nodes,pod,service,secret,pv,pvc,ingress) as required.\nView pod logs To view logs for a pod use the following command:\n$ kubectl logs \u0026lt;pod\u0026gt; -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl logs oudsm-1 -n oudsmns View pod description Details about a pod can be viewed using the kubectl describe command:\n$ kubectl describe pod \u0026lt;pod\u0026gt; -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl describe pod oudsm-1 -n oudsmns The output will look similar to the following:\nName: oudsm-1 Namespace: oudsmns Priority: 0 Node: \u0026lt;worker-node\u0026gt;/100.102.48.28 Start Time: Tue, 22 Mar 2022 09:56:11 +0000 Labels: app.kubernetes.io/instance=oudsm app.kubernetes.io/managed-by=Helm app.kubernetes.io/name=oudsm app.kubernetes.io/version=12.2.1.4.0 helm.sh/chart=oudsm-0.1 oudsm/instance=oudsm-1 Annotations: meta.helm.sh/release-name: oudsm meta.helm.sh/release-namespace: oudsmns Status: Running IP: 10.244.1.89 IPs: IP: 10.244.1.89 Containers: oudsm: Container ID: cri-o://37dbe00257095adc0a424b8841db40b70bbb65645451e0bc53718a0fd7ce22e4 Image: container-registry.oracle.com/middleware/oudsm_cpu:12.2.1.4-jdk8-ol7-220223.2053 Image ID: container-registry.oracle.com/middleware/oudsm_cpu@sha256:47960d36d502d699bfd8f9b1be4c9216e302db95317c288f335f9c8a32974f2c Ports: 7001/TCP, 7002/TCP Host Ports: 0/TCP, 0/TCP State: Running Started: Tue, 22 Mar 2022 09:56:12 +0000 Ready: True Restart Count: 0 Liveness: http-get http://:7001/oudsm delay=1200s timeout=15s period=60s #success=1 #failure=3 Readiness: http-get http://:7001/oudsm delay=900s timeout=15s period=30s #success=1 #failure=3 Environment: DOMAIN_NAME: oudsmdomain-1 ADMIN_USER: \u0026lt;set to the key 'adminUser' in secret 'oudsm-creds'\u0026gt; Optional: false ADMIN_PASS: \u0026lt;set to the key 'adminPass' in secret 'oudsm-creds'\u0026gt; Optional: false ADMIN_PORT: 7001 ADMIN_SSL_PORT: 7002 WLS_PLUGIN_ENABLED: true Mounts: /u01/oracle/user_projects from oudsm-pv (rw) /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-9ht84 (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: oudsm-pv: Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace) ClaimName: oudsm-pvc ReadOnly: false kube-api-access-9ht84: Type: Projected (a volume that contains injected data from multiple sources) TokenExpirationSeconds: 3607 ConfigMapName: kube-root-ca.crt ConfigMapOptional: \u0026lt;nil\u0026gt; DownwardAPI: true QoS Class: BestEffort Node-Selectors: \u0026lt;none\u0026gt; Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 39m default-scheduler 0/3 nodes are available: 3 pod has unbound immediate PersistentVolumeClaims. Normal Scheduled 39m default-scheduler Successfully assigned oudsmns/oudsm-1 to \u0026lt;worker-node\u0026gt; Normal Pulled 39m kubelet Container image \u0026quot;container-registry.oracle.com/middleware/oudsm_cpu:12.2.1.4-jdk8-ol7-220223.2053\u0026quot; already present on machine Normal Created 39m kubelet Created container oudsm Normal Started 39m kubelet Started container oudsm "
},
{
	"uri": "/fmw-kubernetes/soa-domains/troubleshooting/",
	"title": "Troubleshooting",
	"tags": [],
	"description": "Describes common issues that may occur during Oracle SOA Suite deployment on Kubernetes and the steps to troubleshoot them.",
	"content": "This document describes common issues that may occur during the deployment of Oracle SOA Suite on Kubernetes and the steps to troubleshoot them. Also refer to the FAQs page for frequent issues and steps to resolve them.\n WebLogic Kubernetes Operator installation failure RCU schema creation failure Domain creation failure Common domain creation issues Server pods not started after applying domain configuration file Ingress controller not serving the domain urls  WebLogic Kubernetes Operator installation failure If the WebLogic Kubernetes Operator installation failed with timing out:\n Check the status of the operator Helm release using the command helm ls -n \u0026lt;operator-namespace\u0026gt;. Check if the operator pod is successfully created in the operator namespace. Describe the operator pod using kubectl describe pod \u0026lt;operator-pod-name\u0026gt; -n \u0026lt;operator-namespace\u0026gt; to identify any obvious errors.  RCU schema creation failure When creating the RCU schema using create-rcu-schema.sh, the possible causes for RCU schema creation failure are:\n Database is not up and running Incorrect database connection URL used Invalid database credentials used Schema prefix already exists  Make sure that all the above causes are reviewed and corrected as needed.\nAlso drop the existing schema with the same prefix before rerunning the create-rcu-schema.sh with correct values.\nDomain creation failure If the Oracle SOA Suite domain creation fails when running create-domain.sh, perform the following steps to diagnose the issue:\n  Run the following command to diagnose the create domain job:\n$ kubectl logs jobs/\u0026lt;domain_job\u0026gt; -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl logs jobs/soainfra-create-soa-infra-domain-job -n soans Also run:\n$ kubectl describe pod \u0026lt;domain_job\u0026gt; -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl describe pod soainfra-create-soa-infra-domain-job-mcc6v -n soans Use the output to diagnose the problem and resolve the issue.\n  Clean up the failed domain creation:\n Delete the failed domain creation job in the domain namespace using the command kubectl delete job \u0026lt;domain-creation-job-name\u0026gt; -n \u0026lt;domain-namespace\u0026gt;. Delete the contents of the domain home directory Drop the existing RCU schema    Recreate the domain:\n Recreate the RCU schema Make sure the Persistent Volume and Persistent Volume Claim used for the domain are created with correct permissions and bound together. Rerun the create domain script    Common domain creation issues A common domain creation issue is error Failed to build JDBC Connection object in the create domain job logs.\n  Click here to see the error stack trace:   Configuring the Service Table DataSource... fmwDatabase jdbc:oracle:thin:@orclcdb.soainfra-domain-ns-293-10202010:1521/orclpdb1 Getting Database Defaults... Error: getDatabaseDefaults() failed. Do dumpStack() to see details. Error: runCmd() failed. Do dumpStack() to see details. Problem invoking WLST - Traceback (innermost last): File \u0026quot;/u01/weblogic/..2021_10_20_20_29_37.256759996/createSOADomain.py\u0026quot;, line 943, in ? File \u0026quot;/u01/weblogic/..2021_10_20_20_29_37.256759996/createSOADomain.py\u0026quot;, line 75, in createSOADomain File \u0026quot;/u01/weblogic/..2021_10_20_20_29_37.256759996/createSOADomain.py\u0026quot;, line 695, in extendSoaB2BDomain File \u0026quot;/u01/weblogic/..2021_10_20_20_29_37.256759996/createSOADomain.py\u0026quot;, line 588, in configureJDBCTemplates File \u0026quot;/tmp/WLSTOfflineIni956349269221112379.py\u0026quot;, line 267, in getDatabaseDefaults File \u0026quot;/tmp/WLSTOfflineIni956349269221112379.py\u0026quot;, line 19, in command Failed to build JDBC Connection object: at com.oracle.cie.domain.script.jython.CommandExceptionHandler.handleException(CommandExceptionHandler.java:69) at com.oracle.cie.domain.script.jython.WLScriptContext.handleException(WLScriptContext.java:3085) at com.oracle.cie.domain.script.jython.WLScriptContext.runCmd(WLScriptContext.java:738) at sun.reflect.GeneratedMethodAccessor152.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) com.oracle.cie.domain.script.jython.WLSTException: com.oracle.cie.domain.script.jython.WLSTException: Got exception when auto configuring the schema component(s) with data obtained from shadow table: Failed to build JDBC Connection object: ERROR: /u01/weblogic/create-domain-script.sh failed.    This error is reported when there is an issue with database schema access during domain creation. The possible causes are:\n Incorrect schema name specified in create-domain-inputs.yaml. RCU schema credentials specified in the secret soainfra-rcu-credentials are different from the credentials specified while creating the RCU schema using create-rcu-schema.sh.  To resolve these possible causes, check that the schema name and credentials used during the domain creation are the same as when the RCU schema was created.\nServer pods not started after applying domain configuration file This issue usually happens when the WebLogic Kubernetes Operator is not configured to manage the domain namespace. You can verify the configuration by running the command helm get values \u0026lt;operator-release\u0026gt; -n \u0026lt;operator-namespace\u0026gt; and checking the values under the domainNamespaces section.\nFor example:\n$ helm get values weblogic-kubernetes-operator -n opns USER-SUPPLIED VALUES: domainNamespaces: - soans image: ghcr.io/oracle/weblogic-kubernetes-operator:3.4.0 javaLoggingLevel: FINE serviceAccount: op-sa $ If you don\u0026rsquo;t see the domain namespace value under the domainNamespaces section, run the helm upgrade command in the operator namespace with appropriate values to configure the operator to manage the domain namespace.\n$ helm upgrade --reuse-values --namespace opns --set \u0026quot;domainNamespaces={soans}\u0026quot; --wait weblogic-kubernetes-operator charts/weblogic-operator Ingress controller not serving the domain URLs To diagnose this issue:\n Verify that the Ingress controller is installed successfully.\nFor example, to verify the Traefik Ingress controller status, run the following command: $ helm list -n traefik NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION traefik traefik 2 2022-05-06 09:40:53.205565811 +0000 UTC deployed traefik-10.19.5 2.6.6 $  Verify that the Ingress controller is setup to monitor the domain namespace.\nFor example, to verify the Traefik Ingress controller manages the soans domain namespace, run the following command and check the values under namespaces section. $ helm get values traefik-operator -n traefik USER-SUPPLIED VALUES: kubernetes: namespaces: - traefik - soans $  Verify that the Ingress chart is installed correctly in domain namespace. For example, run the following command: $ helm list -n soans NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION soainfra-traefik soans 1 2021-10-27 11:24:31.7572791 +0000 UTC deployed ingress-per-domain-0.1.0 1.0 $  Verify that the Ingress URL paths and hostnames are configured correctly by running the following commands:   Click here to see the sample commands and output   $ kubectl get ingress soainfra-traefik -n soans NAME CLASS HOSTS ADDRESS PORTS AGE soainfra-traefik \u0026lt;none\u0026gt; \u0026lt;Hostname\u0026gt; 80 20h $ $ kubectl describe ingress soainfra-traefik -n soans Name: soainfra-traefik Namespace: soans Address: Default backend: default-http-backend:80 (\u0026lt;error: endpoints \u0026quot;default-http-backend\u0026quot; not found\u0026gt;) Rules: Host Path Backends ---- ---- -------- \u0026lt;Hostname\u0026gt; /console soainfra-adminserver:7001 (10.244.0.123:7001) /em soainfra-adminserver:7001 (10.244.0.123:7001) /weblogic/ready soainfra-adminserver:7001 (10.244.0.123:7001) /soa-infra soainfra-cluster-soa-cluster:8001 (10.244.0.126:8001,10.244.0.127:8001) /soa/composer soainfra-cluster-soa-cluster:8001 (10.244.0.126:8001,10.244.0.127:8001) /integration/worklistapp soainfra-cluster-soa-cluster:8001 (10.244.0.126:8001,10.244.0.127:8001) /EssHealthCheck soainfra-cluster-soa-cluster:8001 (10.244.0.126:8001,10.244.0.127:8001) Annotations: kubernetes.io/ingress.class: traefik meta.helm.sh/release-name: soainfra-traefik meta.helm.sh/release-namespace: soans Events: \u0026lt;none\u0026gt; $     "
},
{
	"uri": "/fmw-kubernetes/oam/create-or-update-image/",
	"title": "Create or update an image",
	"tags": [],
	"description": "Create or update an Oracle Access Management (OAM) container image used for deploying OAM domains.",
	"content": "As described in Prepare Your Environment you can create your own OAM container image. If you have access to the My Oracle Support (MOS), and there is a need to build a new image with an interim or one off patch, it is recommended to use the WebLogic Image Tool to build an Oracle Access Management image for production deployments.\nCreate or update an Oracle Access Management image using the WebLogic Image Tool Using the WebLogic Image Tool, you can create a new Oracle Access Management image with PSU\u0026rsquo;s and interim patches or update an existing image with one or more interim patches.\n Recommendations:\n Use create for creating a new Oracle Access Management image containing the Oracle Access Management binaries, bundle patch and interim patches. This is the recommended approach if you have access to the OAM patches because it optimizes the size of the image. Use update for patching an existing Oracle Access Management image with a single interim patch. Note that the patched image size may increase considerably due to additional image layers introduced by the patch application tool.   Create an image Set up the WebLogic Image Tool  Prerequisites Set up the WebLogic Image Tool Validate setup WebLogic Image Tool build directory WebLogic Image Tool cache  Prerequisites Verify that your environment meets the following prerequisites:\n Docker client and daemon on the build machine, with minimum Docker version 18.03.1.ce. Bash version 4.0 or later, to enable the command complete feature. JAVA_HOME environment variable set to the appropriate JDK location e.g: /scratch/export/oracle/product/jdk  Set up the WebLogic Image Tool To set up the WebLogic Image Tool:\n  Create a working directory and change to it:\n$ mdir \u0026lt;workdir\u0026gt; $ cd \u0026lt;workdir\u0026gt; For example:\n$ mkdir /scratch/imagetool-setup $ cd /scratch/imagetool-setup   Download the latest version of the WebLogic Image Tool from the releases page.\n$ wget https://github.com/oracle/weblogic-image-tool/releases/download/release-X.X.X/imagetool.zip where X.X.X is the latest release referenced on the releases page.\n  Unzip the release ZIP file in the imagetool-setup directory.\n$ unzip imagetool.zip   Execute the following commands to set up the WebLogic Image Tool:\n$ cd \u0026lt;workdir\u0026gt;/imagetool-setup/imagetool/bin $ source setup.sh For example:\n$ cd /scratch/imagetool-setup/imagetool/bin $ source setup.sh   Validate setup To validate the setup of the WebLogic Image Tool:\n  Enter the following command to retrieve the version of the WebLogic Image Tool:\n$ imagetool --version   Enter imagetool then press the Tab key to display the available imagetool commands:\n$ imagetool \u0026lt;TAB\u0026gt; cache create help rebase update   WebLogic Image Tool build directory The WebLogic Image Tool creates a temporary Docker context directory, prefixed by wlsimgbuilder_temp, every time the tool runs. Under normal circumstances, this context directory will be deleted. However, if the process is aborted or the tool is unable to remove the directory, it is safe for you to delete it manually. By default, the WebLogic Image Tool creates the Docker context directory under the user\u0026rsquo;s home directory. If you prefer to use a different directory for the temporary context, set the environment variable WLSIMG_BLDDIR:\n$ export WLSIMG_BLDDIR=\u0026#34;/path/to/buid/dir\u0026#34; WebLogic Image Tool cache The WebLogic Image Tool maintains a local file cache store. This store is used to look up where the Java, WebLogic Server installers, and WebLogic Server patches reside in the local file system. By default, the cache store is located in the user\u0026rsquo;s $HOME/cache directory. Under this directory, the lookup information is stored in the .metadata file. All automatically downloaded patches also reside in this directory. You can change the default cache store location by setting the environment variable WLSIMG_CACHEDIR:\n$ export WLSIMG_CACHEDIR=\u0026#34;/path/to/cachedir\u0026#34; Set up additional build scripts Creating an Oracle Access Management container image using the WebLogic Image Tool requires additional container scripts for Oracle Access Management domains.\n  Clone the docker-images repository to set up those scripts. In these steps, this directory is DOCKER_REPO:\n$ cd \u0026lt;workdir\u0026gt;/imagetool-setup $ git clone https://github.com/oracle/docker-images.git For example:\n$ cd /scratch/imagetool-setup $ git clone https://github.com/oracle/docker-images.git    Note: If you want to create the image continue with the following steps, otherwise to update the image see update an image.\n Create an image After setting up the WebLogic Image Tool, follow these steps to use the WebLogic Image Tool to create a new Oracle Access Management image.\nDownload the Oracle Access Management installation binaries and patches You must download the required Oracle Access Management installation binaries and patches as listed below from the Oracle Software Delivery Cloud and save them in a directory of your choice.\nThe installation binaries and patches required are:\n  Oracle Identity and Access Management 12.2.1.4.0\n fmw_12.2.1.4.0_idm.jar    Oracle Fusion Middleware 12c Infrastructure 12.2.1.4.0\n fmw_12.2.1.4.0_infrastructure.jar    OAM and FMW Infrastructure Patches:\n View document ID 2723908.1 on My Oracle Support. In the Container Image Download/Patch Details section, locate the Oracle Access Management (OAM) table. For the latest PSU click the README link in the Documentation column. In the README, locate the \u0026ldquo;Installed Software\u0026rdquo; section. All the patch numbers to be download are listed here. Download all these individual patches from My Oracle Support.    Oracle JDK v8\n jdk-8uXXX-linux-x64.tar.gz where XXX is the JDK version referenced in the README above.    Update required build files The following files in the code repository location \u0026lt;imagetool-setup-location\u0026gt;/docker-images/OracleAccessManagement/imagetool/12.2.1.4.0 are used for creating the image:\n additionalBuildCmds.txt buildArgs    Edit the \u0026lt;workdir\u0026gt;/imagetool-setup/docker-images/OracleAccessManagement/imagetool/12.2.1.4.0/buildArgs file and change %DOCKER_REPO%, %JDK_VERSION% and %BUILDTAG% appropriately.\nFor example:\ncreate --jdkVersion=8u301 --type oam --version=12.2.1.4.0 --tag=oam-latestpsu:12.2.1.4.0 --pull --installerResponseFile /scratch/imagetool-setup/docker-images/OracleFMWInfrastructure/dockerfiles/12.2.1.4/install.file,/scratch/imagetool-setup/docker-images/OracleAccessManagement/dockerfiles/12.2.1.4.0/install/iam.response --additionalBuildCommands /scratch/imagetool-setup/docker-images/OracleAccessManagement/imagetool/12.2.1.4.0/addtionalBuildCmds.txt --additionalBuildFiles /scratch/imagetool-setup/docker-images/OracleAccessManagement/dockerfiles/12.2.1.4.0/container-scripts   Edit the \u0026lt;workdir\u0026gt;/imagetool-setup/docker-images/OracleFMWInfrastructure/dockerfiles/12.2.1.4/install.file and under the GENERIC section add the line INSTALL_TYPE=\u0026quot;Fusion Middleware Infrastructure\u0026rdquo;. For example:\n[GENERIC] INSTALL_TYPE=\u0026quot;Fusion Middleware Infrastructure\u0026quot; DECLINE_SECURITY_UPDATES=true SECURITY_UPDATES_VIA_MYORACLESUPPORT=false   Create the image   Add a JDK package to the WebLogic Image Tool cache. For example:\n$ imagetool cache addInstaller --type jdk --version 8uXXX --path \u0026lt;download location\u0026gt;/jdk-8uXXX-linux-x64.tar.gz where XXX is the JDK version downloaded\n  Add the downloaded installation binaries to the WebLogic Image Tool cache. For example:\n$ imagetool cache addInstaller --type fmw --version 12.2.1.4.0 --path \u0026lt;download location\u0026gt;/fmw_12.2.1.4.0_infrastructure.jar $ imagetool cache addInstaller --type OAM --version 12.2.1.4.0 --path \u0026lt;download location\u0026gt;/fmw_12.2.1.4.0_idm.jar   Add the downloaded OPatch patch to the WebLogic Image Tool cache. For example:\n$ imagetool cache addEntry --key 28186730_13.9.4.2.8 --value \u0026lt;download location\u0026gt;/p28186730_139428_Generic.zip   Add the rest of the downloaded product patches to the WebLogic Image Tool cache:\n$ imagetool cache addEntry --key \u0026lt;patch\u0026gt;_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p\u0026lt;patch\u0026gt;_122140_Generic.zip For example:\n$ imagetool cache addEntry --key 32971905_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p32971905_122140_Generic.zip $ imagetool cache addEntry --key 20812896_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p20812896_122140_Generic.zip $ imagetool cache addEntry --key 32880070_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p32880070_122140_Generic.zip $ imagetool cache addEntry --key 33059296_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p33059296_122140_Generic.zip $ imagetool cache addEntry --key 32905339_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p32905339_122140_Generic.zip $ imagetool cache addEntry --key 33084721_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p33084721_122140_Generic.zip $ imagetool cache addEntry --key 31544353_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p31544353_122140_Linux-x86-64.zip $ imagetool cache addEntry --key 32957281_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p32957281_122140_Generic.zip $ imagetool cache addEntry --key 33093748_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p33093748_122140_Generic.zip   Edit the \u0026lt;workdir\u0026gt;/imagetool-setup/docker-images/OracleAccessManagement/imagetool/12.2.1.4.0/buildArgs file and append the product patches and opatch patch as follows:\n--patches 32971905_12.2.1.4.0,20812896_12.2.1.4.0,32880070_12.2.1.4.0,33059296_12.2.1.4.0,32905339_12.2.1.4.0,33084721_12.2.1.4.0,31544353_12.2.1.4.0,32957281_12.2.1.4.0,33093748_12.2.1.4.0 --opatchBugNumber=28186730_13.9.4.2.8 An example buildArgs file is now as follows:\ncreate --jdkVersion=8u301 --type oam --version=12.2.1.4.0 --tag=oam-latestpsu:12.2.1.4.0 --pull --installerResponseFile /scratch/imagetool-setup/docker-images/OracleFMWInfrastructure/dockerfiles/12.2.1.4/install.file,/scratch/imagetool-setup/docker-images/OracleAccessManagement/dockerfiles/12.2.1.4.0/install/iam.response --additionalBuildCommands /scratch/imagetool-setup/docker-images/OracleAccessManagement/imagetool/12.2.1.4.0/additionalBuildCmds.txt --additionalBuildFiles /scratch/imagetool-setup/docker-images/OracleAccessManagement/dockerfiles/12.2.1.4.0/container-scripts --patches 32971905_12.2.1.4.0,20812896_12.2.1.4.0,32880070_12.2.1.4.0,33059296_12.2.1.4.0,32905339_12.2.1.4.0,33084721_12.2.1.4.0,31544353_12.2.1.4.0,32957281_12.2.1.4.0,33093748_12.2.1.4.0 --opatchBugNumber=28186730_13.9.4.2.8  Note: In the buildArgs file:\n --jdkVersion value must match the --version value used in the imagetool cache addInstaller command for --type jdk. --version value must match the --version value used in the imagetool cache addInstaller command for --type OAM.   Refer to this page for the complete list of options available with the WebLogic Image Tool create command.\n  Create the Oracle Access Management image:\n$ imagetool @\u0026lt;absolute path to buildargs file\u0026gt; --fromImage ghcr.io/oracle/oraclelinux:7-slim  Note: Make sure that the absolute path to the buildargs file is prepended with a @ character, as shown in the example above.\n For example:\n$ imagetool @\u0026lt;imagetool-setup-location\u0026gt;/docker-images/OracleAccessManagement/imagetool/12.2.1.4.0/buildArgs --fromImage ghcr.io/oracle/oraclelinux:7-slim   Check the created image using the docker images command:\n$ docker images | grep oam The output will look similar to the following:\noam-latestpsu 12.2.1.4.0 ad732fc7c16b About a minute ago 3.35GB   Run the following command to save the container image to a tar file:\n$ docker save -o \u0026lt;path\u0026gt;/\u0026lt;file\u0026gt;.tar \u0026lt;image\u0026gt; For example:\n$ docker save -o $WORKDIR/oam-latestpsu.tar oam-latestpsu:12.2.1.4.0   Update an image The steps below show how to update an existing Oracle Access Management image with an interim patch.\nThe container image to be patched must be loaded in the local docker images repository before attempting these steps.\nIn the examples below the image oracle/oam:12.2.1.4.0 is updated with an interim patch.\n$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE oracle/oam 12.2.1.4.0 b051804ba15f 3 months ago 3.34GB   Set up the WebLogic Image Tool.\n  Download the required interim patch and latest Opatch (28186730) from My Oracle Support. and save them in a directory of your choice.\n  Add the OPatch patch to the WebLogic Image Tool cache, for example:\n$ imagetool cache addEntry --key 28186730_13.9.4.2.8 --value \u0026lt;downloaded-patches-location\u0026gt;/p28186730_139428_Generic.zip   Execute the imagetool cache addEntry command for each patch to add the required patch(es) to the WebLogic Image Tool cache. For example, to add patch p32701831_12214210607_Generic.zip:\n$ imagetool cache addEntry --key=32701831_12.2.1.4.210607 --value \u0026lt;downloaded-patches-location\u0026gt;/p32701831_12214210607_Generic.zip   Provide the following arguments to the WebLogic Image Tool update command:\n –-fromImage - Identify the image that needs to be updated. In the example below, the image to be updated is oracle/oam:12.2.1.4.0. –-patches - Multiple patches can be specified as a comma-separated list. --tag - Specify the new tag to be applied for the image being built.  Refer here for the complete list of options available with the WebLogic Image Tool update command.\n Note: The WebLogic Image Tool cache should have the latest OPatch zip. The WebLogic Image Tool will update the OPatch if it is not already updated in the image.\n For example:\n$ imagetool update --fromImage oracle/oam:12.2.1.4.0 --tag=oracle/oam-new:12.2.1.4.0 --patches=32701831_12.2.1.4.210607 --opatchBugNumber=28186730_13.9.4.2.8  Note: If the command fails because the files in the image being upgraded are not owned by oracle:oracle, then add the parameter --chown \u0026lt;userid\u0026gt;:\u0026lt;groupid\u0026gt; to correspond with the values returned in the error.\n   Check the built image using the docker images command:\n$ docker images | grep oam The output will look similar to the following:\nREPOSITORY TAG IMAGE ID CREATED SIZE oracle/oam-new 12.2.1.4.0 78ccd1ad67eb 5 minutes ago 3.8GB oracle/oam 12.2.1.4.0 b051804ba15f 3 months ago 3.34GB   Run the following command to save the patched container image to a tar file:\n$ docker save -o \u0026lt;path\u0026gt;/\u0026lt;file\u0026gt;.tar \u0026lt;image\u0026gt; For example:\n$ docker save -o $WORKDIR/oam-new.tar oracle/oam-new:12.2.1.4.0   "
},
{
	"uri": "/fmw-kubernetes/oig/create-or-update-image/",
	"title": "Create or update an image",
	"tags": [],
	"description": "Create or update an Oracle Identity Governance (OIG) container image used for deploying OIG domains",
	"content": "As described in Prepare Your Environment you can create your own OIG container image. If you have access to the My Oracle Support (MOS), and there is a need to build a new image with an interim or one off patch, it is recommended to use the WebLogic Image Tool to build an Oracle Identity Governance image for production deployments.\nCreate or update an Oracle Identity Governance image using the WebLogic Image Tool Using the WebLogic Image Tool, you can create a new Oracle Identity Governance image with PSU\u0026rsquo;s and interim patches or update an existing image with one or more interim patches.\n Recommendations:\n Use create for creating a new Oracle Identity Governance image containing the Oracle Identity Governance binaries, bundle patch and interim patches. This is the recommended approach if you have access to the OIG patches because it optimizes the size of the image. Use update for patching an existing Oracle Identity Governance image with a single interim patch. Note that the patched image size may increase considerably due to additional image layers introduced by the patch application tool.   Create an image Set up the WebLogic Image Tool  Prerequisites Set up the WebLogic Image Tool Validate setup WebLogic Image Tool build directory WebLogic Image Tool cache  Prerequisites Verify that your environment meets the following prerequisites:\n Docker client and daemon on the build machine, with minimum Docker version 18.03.1.ce. Bash version 4.0 or later, to enable the command complete feature. JAVA_HOME environment variable set to the appropriate JDK location e.g: /scratch/export/oracle/product/jdk  Set up the WebLogic Image Tool To set up the WebLogic Image Tool:\n  Create a working directory and change to it:\n$ mkdir \u0026lt;workdir\u0026gt; $ cd \u0026lt;workdir\u0026gt; For example:\n$ mkdir /scratch/imagetool-setup $ cd /scratch/imagetool-setup   Download the latest version of the WebLogic Image Tool from the releases page.\n$ wget https://github.com/oracle/weblogic-image-tool/releases/download/release-X.X.X/imagetool.zip where X.X.X is the latest release referenced on the releases page.\n  Unzip the release ZIP file in the imagetool-setup directory.\n$ unzip imagetool.zip   Execute the following commands to set up the WebLogic Image Tool:\n$ cd \u0026lt;workdir\u0026gt;/imagetool-setup/imagetool/bin $ source setup.sh For example:\n$ cd /scratch/imagetool-setup/imagetool/bin $ source setup.sh   Validate setup To validate the setup of the WebLogic Image Tool:\n  Enter the following command to retrieve the version of the WebLogic Image Tool:\n$ imagetool --version   Enter imagetool then press the Tab key to display the available imagetool commands:\n$ imagetool \u0026lt;TAB\u0026gt; cache create help rebase update   WebLogic Image Tool build directory The WebLogic Image Tool creates a temporary Docker context directory, prefixed by wlsimgbuilder_temp, every time the tool runs. Under normal circumstances, this context directory will be deleted. However, if the process is aborted or the tool is unable to remove the directory, it is safe for you to delete it manually. By default, the WebLogic Image Tool creates the Docker context directory under the user\u0026rsquo;s home directory. If you prefer to use a different directory for the temporary context, set the environment variable WLSIMG_BLDDIR:\n$ export WLSIMG_BLDDIR=\u0026#34;/path/to/buid/dir\u0026#34; WebLogic Image Tool cache The WebLogic Image Tool maintains a local file cache store. This store is used to look up where the Java, WebLogic Server installers, and WebLogic Server patches reside in the local file system. By default, the cache store is located in the user\u0026rsquo;s $HOME/cache directory. Under this directory, the lookup information is stored in the .metadata file. All automatically downloaded patches also reside in this directory. You can change the default cache store location by setting the environment variable WLSIMG_CACHEDIR:\n$ export WLSIMG_CACHEDIR=\u0026#34;/path/to/cachedir\u0026#34; Set up additional build scripts Creating an Oracle Identity Governance container image using the WebLogic Image Tool requires additional container scripts for Oracle Identity Governance domains.\n  Clone the docker-images repository to set up those scripts. In these steps, this directory is DOCKER_REPO:\n$ cd \u0026lt;workdir\u0026gt;/imagetool-setup $ git clone https://github.com/oracle/docker-images.git For example:\n$ cd /scratch/imagetool-setup $ git clone https://github.com/oracle/docker-images.git    Note: If you want to create the image continue with the following steps, otherwise to update the image see update an image.\n Create an image After setting up the WebLogic Image Tool, follow these steps to use the WebLogic Image Tool to create a new Oracle Identity Governance image.\nDownload the Oracle Identity Governance installation binaries and patches You must download the required Oracle Identity Governance installation binaries and patches as listed below from the Oracle Software Delivery Cloud and save them in a directory of your choice.\nThe installation binaries and patches required are:\n  Oracle Identity and Access Management 12.2.1.4.0\n fmw_12.2.1.4.0_idm.jar    Oracle Fusion Middleware 12c Infrastructure 12.2.1.4.0\n fmw_12.2.1.4.0_infrastructure.jar    Oracle SOA Suite for Oracle Middleware 12.2.1.4.0\n fmw_12.2.1.4.0_soa.jar    Oracle Service Bus 12.2.1.4.0\n fmw_12.2.1.4.0_osb.jar    OIG and FMW Infrastructure Patches:\n View document ID 2723908.1 on My Oracle Support. In the Container Image Download/Patch Details section, locate the Oracle Identity Governance (OIG) table. For the latest PSU click the README link in the Documentation column. In the README, locate the \u0026ldquo;Installed Software\u0026rdquo; section. All the patch numbers to be download are listed here. Download all these individual patches from My Oracle Support.    Oracle JDK v8\n jdk-8uXXX-linux-x64.tar.gz where XXX is the JDK version referenced in the README above.    Update required build files The following files in the code repository location \u0026lt;imagetool-setup-location\u0026gt;/docker-images/OracleIdentityGovernance/imagetool/12.2.1.4.0 are used for creating the image:\n additionalBuildCmds.txt buildArgs  . Edit the \u0026lt;workdir\u0026gt;/imagetool-setup/docker-images/OracleIdentityGovernance/imagetool/12.2.1.4.0/buildArgs file and change %DOCKER_REPO%, %JDK_VERSION% and %BUILDTAG% appropriately.\nFor example:\ncreate --jdkVersion=8u311 --type oig --chown oracle:root --version=12.2.1.4.0 --tag=oig-latestpsu:12.2.1.4.0 --pull --installerResponseFile /scratch/imagetool-setup/docker-images/OracleFMWInfrastructure/dockerfiles/12.2.1.4/install.file,/scratch/imagetool-setup/docker-images/OracleSOASuite/dockerfiles/12.2.1.4.0/install/soasuite.response,/scratch/imagetool-setup/docker-images/OracleSOASuite/dockerfiles/12.2.1.4.0/install/osb.response,/scratch/imagetool-setup/docker-images/OracleIdentityGovernance/dockerfiles/12.2.1.4.0/idmqs.response --additionalBuildCommands /scratch/imagetool-setup/docker-images/OracleIdentityGovernance/imagetool/12.2.1.4.0/additionalBuildCmds.txt --additionalBuildFiles /scratch/imagetool-setup/docker-images/OracleIdentityGovernance/dockerfiles/12.2.1.4.0/container-scripts   Edit the \u0026lt;workdir\u0026gt;/imagetool-setup/docker-images/OracleFMWInfrastructure/dockerfiles/12.2.1.4.0/install.file and under the GENERIC section add the line INSTALL_TYPE=\u0026quot;Fusion Middleware Infrastructure\u0026rdquo;. For example:\n[GENERIC] INSTALL_TYPE=\u0026quot;Fusion Middleware Infrastructure\u0026quot; DECLINE_SECURITY_UPDATES=true SECURITY_UPDATES_VIA_MYORACLESUPPORT=false   Create the image   Add a JDK package to the WebLogic Image Tool cache. For example:\n$ imagetool cache addInstaller --type jdk --version 8uXXX --path \u0026lt;download location\u0026gt;/jdk-8uXXX-linux-x64.tar.gz where XXX is the JDK version downloaded\n  Add the downloaded installation binaries to the WebLogic Image Tool cache. For example:\n$ imagetool cache addInstaller --type fmw --version 12.2.1.4.0 --path \u0026lt;download location\u0026gt;/fmw_12.2.1.4.0_infrastructure.jar $ imagetool cache addInstaller --type soa --version 12.2.1.4.0 --path \u0026lt;download location\u0026gt;/fmw_12.2.1.4.0_soa.jar $ imagetool cache addInstaller --type osb --version 12.2.1.4.0 --path \u0026lt;download location\u0026gt;/fmw_12.2.1.4.0_osb.jar $ imagetool cache addInstaller --type idm --version 12.2.1.4.0 --path \u0026lt;download location\u0026gt;/fmw_12.2.1.4.0_idm.jar   Add the downloaded OPatch patch to the WebLogic Image Tool cache. For example:\n$ imagetool cache addEntry --key 28186730_13.9.4.2.8 --value \u0026lt;download location\u0026gt;/p28186730_139428_Generic.zip   Add the rest of the downloaded product patches to the WebLogic Image Tool cache:\n$ imagetool cache addEntry --key \u0026lt;patch\u0026gt;_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p\u0026lt;patch\u0026gt;_122140_Generic.zip For example:\n$ imagetool cache addEntry --key 33416868_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p33416868_122140_Generic.zip $ imagetool cache addEntry --key 33453703_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p33453703_122140_Generic.zip $ imagetool cache addEntry --key 32999272_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p32999272_122140_Generic.zip $ imagetool cache addEntry --key 33093748_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p33093748_122140_Generic.zip $ imagetool cache addEntry --key 33281560_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p33281560_122140_Generic.zip $ imagetool cache addEntry --key 31544353_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p31544353_122140_Linux-x86-64.zip $ imagetool cache addEntry --key 33313802_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p33313802_122140_Generic.zip $ imagetool cache addEntry --key 33408307_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p33408307_122140_Generic.zip $ imagetool cache addEntry --key 33286160_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p33286160_122140_Generic.zip $ imagetool cache addEntry --key 32880070_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p32880070_122140_Generic.zip $ imagetool cache addEntry --key 32905339_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p32905339_122140_Generic.zip $ imagetool cache addEntry --key 32784652_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p32784652_122140_Generic.zip   Edit the \u0026lt;workdir\u0026gt;/imagetool-setup/docker-images/OracleIdentityGovernance/imagetool/12.2.1.4.0/buildArgs file and append the product patches and opatch patch as follows:\n--patches 33416868_12.2.1.4.0,33453703_12.2.1.4.0,32999272_12.2.1.4.0,33093748_12.2.1.4.0,33281560_12.2.1.4.0,31544353_12.2.1.4.0,33313802_12.2.1.4.0,33408307_12.2.1.4.0,33286160_12.2.1.4.0,32880070_12.2.1.4.0,32905339_12.2.1.4.0,32784652_12.2.1.4.0 --opatchBugNumber=28186730_13.9.4.2.8 An example buildArgs file is now as follows:\ncreate --jdkVersion=8u301 --type oig --version=12.2.1.4.0 --tag=oig-latestpsu:12.2.1.4.0 --pull --installerResponseFile /scratch/imagetool-setup/docker-images/OracleFMWInfrastructure/dockerfiles/12.2.1.4/install.file,/scratch/docker-images/OracleSOASuite/dockerfiles/12.2.1.4.0/install/soasuite.response,/scratch/docker-images/OracleSOASuite/dockerfiles/12.2.1.4.0/install/osb.response,/scratch/docker-images/OracleIdentityGovernance/dockerfiles/12.2.1.4.0/idmqs.response --additionalBuildCommands /scratch/imagetool-setup/docker-images/OracleIdentityGovernance/imagetool/12.2.1.4.0/additionalBuildCmds.txt --additionalBuildFiles /scratch/imagetool-setup/docker-images/OracleIdentityGovernance/dockerfiles/12.2.1.4.0/container-scripts --patches 33416868_12.2.1.4.0,33453703_12.2.1.4.0,32999272_12.2.1.4.0,33093748_12.2.1.4.0,33281560_12.2.1.4.0,31544353_12.2.1.4.0,33313802_12.2.1.4.0,33408307_12.2.1.4.0,33286160_12.2.1.4.0,32880070_12.2.1.4.0,32905339_12.2.1.4.0,32784652_12.2.1.4.0 --opatchBugNumber=28186730_13.9.4.2.8  Note: In the buildArgs file:\n --jdkVersion value must match the --version value used in the imagetool cache addInstaller command for --type jdk. --version value must match the --version value used in the imagetool cache addInstaller command for --type idm.   Refer to this page for the complete list of options available with the WebLogic Image Tool create command.\n  Create the Oracle Identity Governance image:\n$ imagetool @\u0026lt;absolute path to buildargs file\u0026gt; --fromImage ghcr.io/oracle/oraclelinux:7-slim  Note: Make sure that the absolute path to the buildargs file is prepended with a @ character, as shown in the example above.\n For example:\n$ imagetool @\u0026lt;imagetool-setup-location\u0026gt;/docker-images/OracleIdentityGovernance/imagetool/12.2.1.4.0/buildArgs --fromImage ghcr.io/oracle/oraclelinux:7-slim   Check the created image using the docker images command:\n$ docker images | grep oig The output will look similar to the following:\noig-latestpsu 12.2.1.4.0 e391ed154bcb 50 seconds ago 4.43GB   Run the following command to save the container image to a tar file:\n$ docker save -o \u0026lt;path\u0026gt;/\u0026lt;file\u0026gt;.tar \u0026lt;image\u0026gt; For example:\n$ docker save -o $WORKDIR/oig-latestpsu.tar oig-latestpsu:12.2.1.4.0   Update an image The steps below show how to update an existing Oracle Identity Governance image with an interim patch.\nThe container image to be patched must be loaded in the local docker images repository before attempting these steps.\nIn the examples below the image oracle/oig:12.2.1.4.0 is updated with an interim patch.\n$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE oracle/oig 12.2.1.4.0 298fdb98e79c 3 months ago 4.42GB   Set up the WebLogic Image Tool.\n  Download the required interim patch and latest Opatch (28186730) from My Oracle Support. and save them in a directory of your choice.\n  Add the OPatch patch to the WebLogic Image Tool cache, for example:\n$ imagetool cache addEntry --key 28186730_13.9.4.2.8 --value \u0026lt;downloaded-patches-location\u0026gt;/p28186730_139428_Generic.zip   Execute the imagetool cache addEntry command for each patch to add the required patch(es) to the WebLogic Image Tool cache. For example, to add patch p32701831_12214210607_Generic.zip:\n$ imagetool cache addEntry --key=33165837_12.2.1.4.210708 --value \u0026lt;downloaded-patches-location\u0026gt;/p33165837_12214210708_Generic.zip   Provide the following arguments to the WebLogic Image Tool update command:\n –-fromImage - Identify the image that needs to be updated. In the example below, the image to be updated is oracle/oig:12.2.1.4.0. –-patches - Multiple patches can be specified as a comma-separated list. --tag - Specify the new tag to be applied for the image being built.  Refer here for the complete list of options available with the WebLogic Image Tool update command.\n Note: The WebLogic Image Tool cache should have the latest OPatch zip. The WebLogic Image Tool will update the OPatch if it is not already updated in the image.\n For example:\n$ imagetool update --fromImage oracle/oig:12.2.1.4.0 --tag=oracle/oig-new:12.2.1.4.0 --patches=33165837_12.2.1.4.210708 --opatchBugNumber=28186730_13.9.4.2.8  Note: If the command fails because the files in the image being upgraded are not owned by oracle:oracle, then add the parameter --chown \u0026lt;userid\u0026gt;:\u0026lt;groupid\u0026gt; to correspond with the values returned in the error.\n   Check the built image using the docker images command:\n$ docker images | grep oig The output will look similar to the following:\nREPOSITORY TAG IMAGE ID CREATED SIZE oracle/oig-new 12.2.1.4.0 0c8381922e95 16 seconds ago 4.91GB oracle/oig 12.2.1.4.0 298fdb98e79c 3 months ago 4.42GB   Run the following command to save the patched container image to a tar file:\n$ docker save -o \u0026lt;path\u0026gt;/\u0026lt;file\u0026gt;.tar \u0026lt;image\u0026gt; For example:\n$ docker save -o $WORKDIR/oig-new.tar oracle/oig-new:12.2.1.4.0   "
},
{
	"uri": "/fmw-kubernetes/oam/patch-and-upgrade/",
	"title": "Patch and Upgrade",
	"tags": [],
	"description": "This document provides steps to patch or upgrade an OAM image, WebLogic Kubernetes Operator or Kubernetes Cluster.",
	"content": "Patch an existing OAM image, or upgrade the WebLogic Kubernetes Operator release.\n a. Patch an image  Instructions on how to update your OAM Kubernetes cluster with a new OAM container image.\n b. Upgrade an operator release  Instructions on how to update the WebLogic Kubernetes Operator version.\n "
},
{
	"uri": "/fmw-kubernetes/oig/patch-and-upgrade/",
	"title": "Patch and upgrade",
	"tags": [],
	"description": "This document provides steps to patch or upgrade an OIG image, or WebLogic Kubernetes Operator.",
	"content": "Patch an existing Oracle OIG image, or upgrade the WebLogic Kubernetes Operator release.\n a. Patch an image  Instructions on how to update your OIG Kubernetes cluster with a new OIG container image.\n b. Upgrade an operator release  Instructions on how to update the WebLogic Kubernetes Operator version.\n "
},
{
	"uri": "/fmw-kubernetes/oam/troubleshooting/",
	"title": "Troubleshooting",
	"tags": [],
	"description": "How to Troubleshoot domain creation failure.",
	"content": "Domain creation failure If the OAM domain creation fails when running create-domain.sh, run the following to diagnose the issue:\n  Run the following command to diagnose the create domain job:\n$ kubectl logs \u0026lt;domain_job\u0026gt; -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl logs accessdomain-create-fmw-infra-sample-domain-job-c6vfb -n oamns Also run:\n$ kubectl describe pod \u0026lt;domain_job\u0026gt; -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl describe pod accessdomain-create-fmw-infra-sample-domain-job-c6vfb -n oamns Using the output you should be able to diagnose the problem and resolve the issue.\nClean down the failed domain creation by following steps 1-3 in Delete the OAM domain home. Then follow RCU schema creation onwards to recreate the RCU schema, kubernetes secrets for domain and RCU, the persistent volume and the persistent volume claim. Then execute the OAM domain creation steps again.\n  If any of the above commands return the following error:\nFailed to start container \u0026quot;create-fmw-infra-sample-domain-job\u0026quot;: Error response from daemon: error while creating mount source path '/scratch/shared/accessdomainpv ': mkdir /scratch/shared/accessdomainpv : permission denied then there is a permissions error on the directory for the PV and PVC and the following should be checked:\na) The directory has 777 permissions: chmod -R 777 \u0026lt;persistent_volume\u0026gt;/accessdomainpv.\nb) If it does have the permissions, check if an oracle user exists and the uid is 1000 and gid is 0.\nCreate the oracle user if it doesn\u0026rsquo;t exist and set the uid to 1000 and gid to 0.\nc) Edit the $WORKDIR/kubernetes/create-weblogic-domain-pv-pvc/create-pv-pvc-inputs.yaml and add a slash to the end of the directory for the weblogicDomainStoragePath parameter:\nweblogicDomainStoragePath: /scratch/shared/accessdomainpv/ Clean down the failed domain creation by following steps 1-3 in Delete the OAM domain home. Then follow RCU schema creation onwards to recreate the RCU schema, kubernetes secrets for domain and RCU, the persistent volume and the persistent volume claim. Then execute the OAM domain creation steps again.\n  "
},
{
	"uri": "/fmw-kubernetes/oig/troubleshooting/",
	"title": "Troubleshooting",
	"tags": [],
	"description": "Sample for creating an OIG domain home on an existing PV or PVC, and the domain resource YAML file for deploying the generated OIG domain.",
	"content": "Domain creation failure If the OIG domain creation fails when running create-domain.sh, run the following to diagnose the issue:\n  Run the following command to diagnose the create domain job:\n$ kubectl logs \u0026lt;job_name\u0026gt; -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl logs governancedomain-create-fmw-infra-sample-domain-job-9wqzb -n oigns Also run:\n$ kubectl describe pod \u0026lt;job_domain\u0026gt; -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl describe pod governancedomain-create-fmw-infra-sample-domain-job-9wqzb -n oigns Using the output you should be able to diagnose the problem and resolve the issue.\nClean down the failed domain creation by following steps 1-3 in Delete the OIG domain home. Then follow RCU schema creation onwards to recreate the RCU schema, kubernetes secrets for domain and RCU, the persistent volume and the persistent volume claim. Then execute the OIG domain creation steps again.\n  If any of the above commands return the following error:\nFailed to start container \u0026quot;create-fmw-infra-sample-domain-job\u0026quot;: Error response from daemon: error while creating mount source path '/scratch/shared/governancedomainpv ': mkdir /scratch/shared/governancedomainpv : permission denied then there is a permissions error on the directory for the PV and PVC and the following should be checked:\na) The directory has 777 permissions: chmod -R 777 \u0026lt;persistent_volume\u0026gt;/governancedomainpv.\nb) If it does have the permissions, check if an oracle user exists and the uid and gid equal 1000, for example:\n$ uid=1000(oracle) gid=1000(spg) groups=1000(spg),59968(oinstall),8500(dba),100(users),1007(cgbudba) Create the oracle user if it doesn\u0026rsquo;t exist and set the uid and gid to 1000.\nc) Edit the $WORKDIR/kubernetes/create-weblogic-domain-pv-pvc/create-pv-pvc-inputs.yaml and add a slash to the end of the directory for the weblogicDomainStoragePath parameter:\nweblogicDomainStoragePath: /scratch/shared/governancedomainpv/ Clean down the failed domain creation by following steps 1-3 in Delete the OIG domain home. Then follow RCU schema creation onwards to recreate the RCU schema, kubernetes secrets for domain and RCU, the persistent volume and the persistent volume claim. Then execute the OIG domain creation steps again.\n  "
},
{
	"uri": "/fmw-kubernetes/oid/manage-oid-containers/scaling-up-down/",
	"title": "a) Scaling Up/Down OID Pods ",
	"tags": [],
	"description": "Describes the steps for scaling up/down for OID pods.",
	"content": "Introduction This section describes how to increase or decrease the number of OID pods in the Kubernetes deployment.\nView existing OID pods By default the oid helm chart deployment starts two pods: oidhost1 and oidhost2.\nThe number of pods started is determined by the replicaCount, which is set to 1 by default. A value of 1 starts the two pods above.\nTo scale up or down the number of OID pods, set replicaCount accordingly.\nRun the following command to view the number of pods in the OID deployment:\n$ kubectl --namespace \u0026lt;namespace\u0026gt; get pods -o wide For example:\n$ kubectl --namespace oidns get pods -o wide The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/oidhost1 1/1 Running 0 34m 10.244.0.195 \u0026lt;Worker Node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/oidhost2 1/1 Running 0 34m 10.244.0.194 \u0026lt;Worker Node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Scaling up OID pods In this example, replicaCount is increased to 2 which creates a new OID pod oidhost3 with associated services created.\nYou can scale up the number of OID pods using one of the following methods:\n Using a YAML file Using --set argument  Using a YAML file   Navigate to the $WORKDIR/kubernetes/helm directory:\n$ cd $WORKDIR/kubernetes/helm   Create a oid-scaleup-override.yaml file that contains:\nreplicaCount: 2   Run the following command to scale up the OID pods:\n$ helm upgrade --namespace \u0026lt;namespace\u0026gt; \\ --values oid-scaleup-override.yaml \\ \u0026lt;release_name\u0026gt; oid --reuse-values For example:\n$ helm upgrade --namespace oidns \\ --values oid-scaleup-override.yaml \\ oid oid --reuse-values   Using --set argument   Run the following command to scale up the OID pods:\n$ helm upgrade --namespace \u0026lt;namespace\u0026gt; \\ --set replicaCount=2 \\ \u0026lt;release_name\u0026gt; oid --reuse-values For example:\n$ helm upgrade --namespace oidns \\ --set replicaCount=2 \\ oid oid --reuse-values   Verify the pods   Verify the new OID pod oidhost3 and has started:\n$ kubectl get pod,service -o wide -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl get pods -n oidns The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/oidhost1 1/1 Running 0 45m 10.244.0.194 \u0026lt;Worker Node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/oidhost2 1/1 Running 0 45m 10.244.0.193 \u0026lt;Worker Node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/oidhost3 1/1 Running 0 17m 10.244.0.195 \u0026lt;Worker Node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/oid-lbr-ldap ClusterIP 10.110.118.113 \u0026lt;none\u0026gt; 3060/TCP,3131/TCP 45m service/oidhost1 ClusterIP 10.97.17.125 \u0026lt;none\u0026gt; 3060/TCP,3131/TCP,7001/TCP,7002/TCP 45m service/oidhost2 ClusterIP 10.106.32.187 \u0026lt;none\u0026gt; 3060/TCP,3131/TCP 45m service/oidhost3 ClusterIP 10.105.33.184 \u0026lt;none\u0026gt; 3060/TCP,3131/TCP 17m Note: It will take several minutes before all the services listed above show. While the oidhost3 pod has a STATUS of 0/1 the pod is started but the OID server associated with it is currently starting. While the pod is starting you can check the startup status in the pod log, by running the following command:\n$ kubectl logs oidhost3 -n oidns   Scaling down OID pods Scaling down OID pods is performed in exactly the same as in Scaling up OID pods except the replicaCount is reduced to the required number of pods.\nOnce the kubectl command is executed the pod(s) will move to a Terminating state. In the example below replicaCount was reduced from 2 to 1 and hence oidhost3 has moved to Terminating:\n$ kubectl get pods -n oidns NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/oidhost1 1/1 Running 0 49m 10.244.0.194 \u0026lt;Worker Node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/oidhost2 1/1 Running 0 49m 10.244.0.193 \u0026lt;Worker Node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/oidhost3 1/1 Terminating 0 21m 10.244.0.195 \u0026lt;Worker Node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; The pod will take a minute or two to stop and then will disappear:\n$ kubectl get pods -n oidns NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/oidhost1 1/1 Running 0 51m 10.244.0.194 \u0026lt;Worker Node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/oidhost2 1/1 Running 0 51m 10.244.0.193 \u0026lt;Worker Node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; "
},
{
	"uri": "/fmw-kubernetes/oud/manage-oud-containers/scaling-up-down/",
	"title": "a) Scaling Up/Down OUD Pods ",
	"tags": [],
	"description": "Describes the steps for scaling up/down for OUD pods.",
	"content": "Introduction This section describes how to increase or decrease the number of OUD pods in the Kubernetes deployment.\nView existing OUD pods By default the oud-ds-rs helm chart deployment starts three pods: oud-ds-rs-0 and two replica pods oud-ds-rs-1 and oud-ds-rs-2.\nThe number of pods started is determined by the replicaCount, which is set to 2 by default. A value of 2 starts the three pods above.\nTo scale up or down the number of OUD pods, set replicaCount accordingly.\nRun the following command to view the number of pods in the OUD deployment:\n$ kubectl --namespace \u0026lt;namespace\u0026gt; get pods -o wide For example:\n$ kubectl --namespace oudns get pods -o wide The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/oud-ds-rs-0 1/1 Running 0 34m 10.244.0.195 \u0026lt;Worker Node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/oud-ds-rs-1 1/1 Running 0 34m 10.244.0.194 \u0026lt;Worker Node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/oud-ds-rs-2 1/1 Running 0 34m 10.244.0.193 \u0026lt;Worker Node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Scaling up OUD pods In this example, replicaCount is increased to 3 which creates a new OUD pod oud-ds-rs-3 with associated services created.\nYou can scale up the number of OUD pods using one of the following methods:\n Using a YAML file Using --set argument  Using a YAML file   Navigate to the $WORKDIR/kubernetes/helm directory:\n$ cd $WORKDIR/kubernetes/helm   Create a oud-scaleup-override.yaml file that contains:\nreplicaCount: 3   Run the following command to scale up the OUD pods:\n$ helm upgrade --namespace \u0026lt;namespace\u0026gt; \\ --values oud-scaleup-override.yaml \\ \u0026lt;release_name\u0026gt; oud-ds-rs --reuse-values For example:\n$ helm upgrade --namespace oudns \\ --values oud-scaleup-override.yaml \\ oud-ds-rs oud-ds-rs --reuse-values   Using --set argument   Run the following command to scale up the OUD pods:\n$ helm upgrade --namespace \u0026lt;namespace\u0026gt; \\ --set replicaCount=3 \\ \u0026lt;release_name\u0026gt; oud-ds-rs --reuse-values For example:\n$ helm upgrade --namespace oudns \\ --set replicaCount=3 \\ oud-ds-rs oud-ds-rs --reuse-values   Verify the pods   Verify the new OUD pod oud-ds-rs-3 and has started:\n$ kubectl get pod,service -o wide -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl get pods,service -n oudns The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/oud-ds-rs-0 1/1 Running 0 45m 10.244.0.195 \u0026lt;Worker Node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/oud-ds-rs-1 1/1 Running 0 45m 10.244.0.194 \u0026lt;Worker Node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/oud-ds-rs-2 1/1 Running 0 45m 10.244.0.193 \u0026lt;Worker Node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/oud-ds-rs-3 1/1 Running 0 17m 10.244.0.193 \u0026lt;Worker Node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service/oud-ds-rs-0 ClusterIP 10.99.232.83 \u0026lt;none\u0026gt; 1444/TCP,1888/TCP,1898/TCP 37m44s kubernetes.io/instance=oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=oud-ds-rs-0 service/oud-ds-rs-1 ClusterIP 10.100.186.42 \u0026lt;none\u0026gt; 1444/TCP,1888/TCP,1898/TCP 37m44s app.kubernetes.io/instance=oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=oud-ds-rs-1 service/oud-ds-rs-2 ClusterIP 10.104.55.53 \u0026lt;none\u0026gt; 1444/TCP,1888/TCP,1898/TCP 37m44s app.kubernetes.io/instance=oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=oud-ds-rs-2 service/oud-ds-rs-3 ClusterIP 10.104.45.52 \u0026lt;none\u0026gt; 1444/TCP,1888/TCP,1898/TCP 8m45s app.kubernetes.io/instance=oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=oud-ds-rs-3 service/oud-ds-rs-http-0 ClusterIP 10.102.116.145 \u0026lt;none\u0026gt; 1080/TCP,1081/TCP 37m44s app.kubernetes.io/instance=oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=oud-ds-rs-0 service/oud-ds-rs-http-1 ClusterIP 10.111.103.84 \u0026lt;none\u0026gt; 1080/TCP,1081/TCP 37m44s app.kubernetes.io/instance=oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=oud-ds-rs-1 service/oud-ds-rs-http-2 ClusterIP 10.105.53.24 \u0026lt;none\u0026gt; 1080/TCP,1081/TCP 37m44s app.kubernetes.io/instance=oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=oud-ds-rs-2 service/oud-ds-rs-http-3 ClusterIP 10.106.51.25 \u0026lt;none\u0026gt; 1080/TCP,1081/TCP 8m45s app.kubernetes.io/instance=oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=oud-ds-rs-3 service/oud-ds-rs-lbr-admin ClusterIP 10.98.39.206 \u0026lt;none\u0026gt; 1888/TCP,1444/TCP 37m44s app.kubernetes.io/instance=oud-ds-rs,app.kubernetes.io/name=oud-ds-rs service/oud-ds-rs-lbr-http ClusterIP 10.110.77.132 \u0026lt;none\u0026gt; 1080/TCP,1081/TCP 37m44s app.kubernetes.io/instance=oud-ds-rs,app.kubernetes.io/name=oud-ds-rs service/oud-ds-rs-lbr-ldap ClusterIP 10.111.55.122 \u0026lt;none\u0026gt; 1389/TCP,1636/TCP 37m44s app.kubernetes.io/instance=oud-ds-rs,app.kubernetes.io/name=oud-ds-rs service/oud-ds-rs-ldap-0 ClusterIP 10.108.155.81 \u0026lt;none\u0026gt; 1389/TCP,1636/TCP 37m44s app.kubernetes.io/instance=oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=oud-ds-rs-0 service/oud-ds-rs-ldap-1 ClusterIP 10.104.88.44 \u0026lt;none\u0026gt; 1389/TCP,1636/TCP 37m44s app.kubernetes.io/instance=oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=oud-ds-rs-1 service/oud-ds-rs-ldap-2 ClusterIP 10.105.253.120 \u0026lt;none\u0026gt; 1389/TCP,1636/TCP 37m44s app.kubernetes.io/instance=oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=oud-ds-rs-2 service/oud-ds-rs-ldap-3 ClusterIP 10.105.253.55 \u0026lt;none\u0026gt; 1389/TCP,1636/TCP 8m45s app.kubernetes.io/instance=oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=oud-ds-rs-3 Note: It will take several minutes before all the services listed above show. While the oud-ds-rs-3 pod has a STATUS of 0/1 the pod is started but the OUD server associated with it is currently starting. While the pod is starting you can check the startup status in the pod log, by running the following command:\n$ kubectl logs oud-ds-rs-3 -n oudns   Scaling down OUD pods Scaling down OUD pods is performed in exactly the same as in Scaling up OUD pods except the replicaCount is reduced to the required number of pods.\nOnce the kubectl command is executed the pod(s) will move to a Terminating state. In the example below replicaCount was reduced from 3 to 2 and hence oud-ds-rs-3 has moved to Terminating:\n$ kubectl get pods -n oudns NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/oud-ds-rs-0 1/1 Running 0 49m 10.244.0.195 \u0026lt;Worker Node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/oud-ds-rs-1 1/1 Running 0 49m 10.244.0.194 \u0026lt;Worker Node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/oud-ds-rs-2 1/1 Running 0 49m 10.244.0.193 \u0026lt;Worker Node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/oud-ds-rs-3 1/1 Terminating 0 21m 10.244.0.193 \u0026lt;Worker Node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; The pod will take a minute or two to stop and then will disappear:\n$ kubectl get pods -n oudns NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/oud-ds-rs-0 1/1 Running 0 51m 10.244.0.195 \u0026lt;Worker Node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/oud-ds-rs-1 1/1 Running 0 51m 10.244.0.194 \u0026lt;Worker Node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/oud-ds-rs-2 1/1 Running 0 51m 10.244.0.193 \u0026lt;Worker Node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; "
},
{
	"uri": "/fmw-kubernetes/oudsm/manage-oudsm-containers/scaling-up-down/",
	"title": "a) Scaling Up/Down OUDSM Pods ",
	"tags": [],
	"description": "Describes the steps for scaling up/down for OUDSM pods.",
	"content": "Introduction This section describes how to increase or decrease the number of OUDSM pods in the Kubernetes deployment.\nView existing OUDSM pods By default the oudsm helm chart deployment starts one pod: oudsm-1.\nThe number of pods started is determined by the replicaCount, which is set to 1 by default. A value of 1 starts the pod above.\nTo scale up or down the number of OUDSM pods, set replicaCount accordingly.\nRun the following command to view the number of pods in the OUDSM deployment:\n$ kubectl --namespace \u0026lt;namespace\u0026gt; get pods -o wide For example:\n$ kubectl --namespace oudsmns get pods -o wide The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/oudsm-1 1/1 Running 0 73m 10.244.0.19 \u0026lt;worker-node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Scaling up OUDSM pods In this example, replicaCount is increased to 2 which creates a new OUDSM pod oudsm-2 with associated services created.\nYou can scale up the number of OUDSM pods using one of the following methods:\n Using a YAML file Using --set argument  Using a YAML file   Navigate to the $WORKDIR/kubernetes/helm directory:\n$ cd $WORKDIR/kubernetes/helm   Create a oudsm-scaleup-override.yaml file that contains:\nreplicaCount: 2   Run the following command to scale up the OUDSM pods:\n$ helm upgrade --namespace \u0026lt;namespace\u0026gt; \\ --values oudsm-scaleup-override.yaml \\ \u0026lt;release_name\u0026gt; oudsm --reuse-values For example:\n$ helm upgrade --namespace oudsmns \\ --values oudsm-scaleup-override.yaml \\ oudsm oudsm --reuse-values   Using --set argument   Run the following command to scale up the OUDSM pods:\n$ helm upgrade --namespace \u0026lt;namespace\u0026gt; \\ --set replicaCount=2 \\ \u0026lt;release_name\u0026gt; oudsm --reuse-values For example:\n$ helm upgrade --namespace oudsmns \\ --set replicaCount=2 \\ oudsm oudsm --reuse-values   Verify the pods   Verify the new OUDSM pod oudsm-2 has started:\n$ kubectl get pod,service -o wide -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl get pods,service -n oudsmns The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/oudsm-1 1/1 Running 0 88m 10.244.0.19 \u0026lt;worker-node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/oudsm-2 1/1 Running 0 15m 10.245.3.45 \u0026lt;worker-node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service/oudsm-1 ClusterIP 10.96.108.200 \u0026lt;none\u0026gt; 7001/TCP,7002/TCP 88m app.kubernetes.io/instance=oudsm,app.kubernetes.io/name=oudsm,oudsm/instance=oudsm-1 service/oudsm-2 ClusterIP 10.96.31.201 \u0026lt;none\u0026gt; 7001/TCP,7002/TCP 15m app.kubernetes.io/instance=oudsm,app.kubernetes.io/name=oudsm,oudsm/instance=oudsm-2 service/oudsm-lbr ClusterIP 10.96.41.201 \u0026lt;none\u0026gt; 7001/TCP,7002/TCP 73m app.kubernetes.io/instance=oudsm,app.kubernetes.io/name=oudsm Note: It will take several minutes before all the services listed above show. While the oudsm-2 pod has a STATUS of 0/1 the pod is started but the OUDSM server associated with it is currently starting. While the pod is starting you can check the startup status in the pod log, by running the following command:\n$ kubectl logs oudsm-2 -n oudsmns   Scaling down OUDSM pods Scaling down OUDSM pods is performed in exactly the same as in Scaling up OUDSM pods except the replicaCount is reduced to the required number of pods.\nOnce the kubectl command is executed the pod(s) will move to a Terminating state. In the example below replicaCount was reduced from 2 to 1 and hence oudsm-2 has moved to Terminating:\n$ kubectl get pods -n oudsmns NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/oudsm-1 1/1 Running 0 92m 10.244.0.19 \u0026lt;worker-node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/oudsm-2 1/1 Terminating 0 19m 10.245.3.45 \u0026lt;worker-node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; The pod will take a minute or two to stop and then will disappear:\n$ kubectl get pods -n oudsmns NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/oudsm-1 1/1 Running 0 94m 10.244.0.19 \u0026lt;worker-node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; "
},
{
	"uri": "/fmw-kubernetes/oam/manage-oam-domains/domain-lifecycle/",
	"title": "a. Domain Life Cycle",
	"tags": [],
	"description": "Learn about the domain life cycle of an OAM domain.",
	"content": " View existing OAM servers Starting/Scaling up OAM Managed servers Stopping/Scaling down OAM Managed servers Stopping and starting the Administration Server and Managed Servers Domain lifecycle sample scripts  As OAM domains use the WebLogic Kubernetes Operator, domain lifecyle operations are managed using the WebLogic Kubernetes Operator itself.\nThis document shows the basic operations for starting, stopping and scaling servers in the OAM domain.\nFor more detailed information refer to Domain Life Cycle in the WebLogic Kubernetes Operator documentation.\nDo not use the WebLogic Server Administration Console or Oracle Enterprise Manager Console to start or stop servers.\n View existing OAM servers The default OAM deployment starts the Administration Server (AdminServer), two OAM Managed Servers (oam_server1 and oam_server2) and two OAM Policy Manager server (oam_policy_mgr1 and oam_policy_mgr2 ).\nThe deployment also creates, but doesn\u0026rsquo;t start, three extra OAM Managed Servers (oam-server3 to oam-server5) and three more OAM Policy Manager servers (oam_policy_mgr3 to oam_policy_mgr5).\nAll these servers are visible in the WebLogic Server Console https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/console by navigating to Domain Structure \u0026gt; oamcluster \u0026gt; Environment \u0026gt; Servers.\nTo view the running servers using kubectl, run the following command:\n$ kubectl get pods -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get pods -n oamns The output should look similar to the following:\nNAME READY STATUS RESTARTS AGE accessdomain-adminserver 1/1 Running 0 3h29m accessdomain-create-oam-infra-domain-job-7c9r9 0/1 Completed 0 3h36m accessdomain-oam-policy-mgr1 1/1 Running 0 3h21m accessdomain-oam-policy-mgr2 1/1 Running 0 3h21m accessdomain-oam-server1 1/1 Running 0 3h21m accessdomain-oam-server2 1/1 Running 0 3h21m helper 1/1 Running 0 3h51m nginx-ingress-ingress-nginx-controller-76fb7678f-k8rhq 1/1 Running 0 55m Starting/Scaling up OAM Managed Servers The number of OAM Managed Servers running is dependent on the replicas parameter configured for the cluster. To start more OAM Managed Servers perform the following steps:\n  Run the following kubectl command to edit the domain:\n$ kubectl edit domain \u0026lt;domain_uid\u0026gt; -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl edit domain accessdomain -n oamns Note: This opens an edit session for the domain where parameters can be changed using standard vi commands.\n  In the edit session search for clusterName: oam_cluster and look for the replicas parameter. By default the replicas parameter is set to \u0026ldquo;2\u0026rdquo; hence two OAM Managed Servers are started (oam_server1 and oam_server2):\n clusters: - clusterName: oam_cluster replicas: 2 serverPod: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - podAffinityTerm: labelSelector: matchExpressions: - key: weblogic.clusterName operator: In values: - $(CLUSTER_NAME)   To start more OAM Managed Servers, increase the replicas value as desired. In the example below, two more managed servers will be started by setting replicas to \u0026ldquo;4\u0026rdquo;:\n clusters: - clusterName: oam_cluster replicas: 4 serverPod: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - podAffinityTerm: labelSelector: matchExpressions: - key: weblogic.clusterName operator: In values: - $(CLUSTER_NAME)   Save the file and exit (:wq!)\nThe output will look similar to the following:\ndomain.weblogic.oracle/accessdomain edited   Run the following kubectl command to view the pods:\n$ kubectl get pods -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get pods -n oamns The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE accessdomain-adminserver 1/1 Running 0 3h33m accessdomain-create-oam-infra-domain-job-7c9r9 0/1 Completed 0 3h40m accessdomain-oam-policy-mgr1 1/1 Running 0 3h25m accessdomain-oam-policy-mgr2 1/1 Running 0 3h25m accessdomain-oam-server1 1/1 Running 0 3h25m accessdomain-oam-server2 1/1 Running 0 3h25m accessdomain-oam-server3 0/1 Running 0 9s accessdomain-oam-server4 0/1 Running 0 9s helper 1/1 Running 0 3h55m nginx-ingress-ingress-nginx-controller-76fb7678f-k8rhq 1/1 Running 0 59m Two new pods (accessdomain-oam-server3 and accessdomain-oam-server4) are started, but currently have a READY status of 0/1. This means oam_server3 and oam_server4 are not currently running but are in the process of starting. The servers will take several minutes to start so keep executing the command until READY shows 1/1:\nNAME READY STATUS RESTARTS AGE accessdomain-adminserver 1/1 Running 0 3h37m accessdomain-create-oam-infra-domain-job-7c9r9 0/1 Completed 0 3h43m accessdomain-oam-policy-mgr1 1/1 Running 0 3h29m accessdomain-oam-policy-mgr2 1/1 Running 0 3h29m accessdomain-oam-server1 1/1 Running 0 3h29m accessdomain-oam-server2 1/1 Running 0 3h29m accessdomain-oam-server3 1/1 Running 0 3m45s accessdomain-oam-server4 1/1 Running 0 3m45s helper 1/1 Running 0 3h59m nginx-ingress-ingress-nginx-controller-76fb7678f-k8rhq 1/1 Running 0 63m Note: To check what is happening during server startup when READY is 0/1, run the following command to view the log of the pod that is starting:\n$ kubectl logs \u0026lt;pod\u0026gt; -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl logs accessdomain-oam-server3 -n oamns   To start more OAM Policy Manager servers, repeat the previous commands but change the replicas parameter for the policy_cluster. In the example below replicas has been increased to \u0026ldquo;4\u0026rdquo;:\n - clusterName: policy_cluster replicas: 2 serverPod: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - podAffinityTerm: labelSelector: matchExpressions: - key: weblogic.clusterName operator: In values: - $(CLUSTER_NAME) After saving the changes two new pods will be started (accessdomain-oam-policy-mgr3 and accessdomain-oam-policy-mgr4). After a few minutes they will have a READY status of 1/1. In the example below accessdomain-oam-policy-mgr3 and accessdomain-oam-policy-mgr4 are started:\nNAME READY STATUS RESTARTS AGE accessdomain-adminserver 1/1 Running 0 3h43m accessdomain-create-oam-infra-domain-job-7c9r9 0/1 Completed 0 3h49m accessdomain-oam-policy-mgr1 1/1 Running 0 3h35m accessdomain-oam-policy-mgr2 1/1 Running 0 3h35m accessdomain-oam-policy-mgr3 1/1 Running 0 4m18s accessdomain-oam-policy-mgr4 1/1 Running 0 4m18s accessdomain-oam-server1 1/1 Running 0 3h35m accessdomain-oam-server2 1/1 Running 0 3h35m accessdomain-oam-server3 1/1 Running 0 9m27s accessdomain-oam-server4 1/1 Running 0 9m27s helper 1/1 Running 0 4h4m nginx-ingress-ingress-nginx-controller-76fb7678f-k8rhq 1/1 Running 0 69m   Stopping/Scaling down OAM Managed Servers As mentioned in the previous section, the number of OAM Managed Servers running is dependent on the replicas parameter configured for the cluster. To stop one or more OAM Managed Servers, perform the following:\n  Run the following kubectl command to edit the domain:\n$ kubectl edit domain \u0026lt;domain_uid\u0026gt; -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl edit domain accessdomain -n oamns   In the edit session search for clusterName: oam_cluster and look for the replicas parameter. In the example below replicas is set to \u0026ldquo;4\u0026rdquo;, hence four OAM Managed Servers are started (access-domain-oam_server1 - access-domain-oam_server4):\n clusters: - clusterName: oam_cluster replicas: 4 serverPod: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - podAffinityTerm: labelSelector: matchExpressions: - key: weblogic.clusterName operator: In values: - $(CLUSTER_NAME)   To stop OAM Managed Servers, decrease the replicas value as desired. In the example below, we will stop two managed servers by setting replicas to \u0026ldquo;2\u0026rdquo;:\n clusters: - clusterName: oam_cluster replicas: 2 serverPod: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - podAffinityTerm: labelSelector: matchExpressions: - key: weblogic.clusterName operator: In values: - $(CLUSTER_NAME)   Save the file and exit (:wq!)\n  Run the following kubectl command to view the pods:\n$ kubectl get pods -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get pods -n oamns The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE accessdomain-adminserver 1/1 Running 0 3h45m accessdomain-create-oam-infra-domain-job-7c9r9 0/1 Completed 0 3h51m accessdomain-oam-policy-mgr1 1/1 Running 0 3h37m accessdomain-oam-policy-mgr2 1/1 Running 0 3h37m accessdomain-oam-policy-mgr3 1/1 Running 0 6m18s accessdomain-oam-policy-mgr4 1/1 Running 0 6m18s accessdomain-oam-server1 1/1 Running 0 3h37m accessdomain-oam-server2 1/1 Running 0 3h37m accessdomain-oam-server3 1/1 Running 0 11m accessdomain-oam-server4 1/1 Terminating 0 11m helper 1/1 Running 0 4h6m nginx-ingress-ingress-nginx-controller-76fb7678f-k8rhq 1/1 Running 0 71m One pod now has a STATUS of Terminating (accessdomain-oam-server4). The server will take a minute or two to stop. Once terminated the other pod (accessdomain-oam-server3) will move to Terminating and then stop. Keep executing the command until the pods have disappeared:\nNAME READY STATUS RESTARTS AGE accessdomain-adminserver 1/1 Running 0 3h48m accessdomain-create-oam-infra-domain-job-7c9r9 0/1 Completed 0 3h54m accessdomain-oam-policy-mgr1 1/1 Running 0 3h40m accessdomain-oam-policy-mgr2 1/1 Running 0 3h40m accessdomain-oam-policy-mgr3 1/1 Running 0 9m18s accessdomain-oam-policy-mgr4 1/1 Running 0 9m18s accessdomain-oam-server1 1/1 Running 0 3h40m accessdomain-oam-server2 1/1 Running 0 3h40m helper 1/1 Running 0 4h9m nginx-ingress-ingress-nginx-controller-76fb7678f-k8rhq 1/1 Running 0 74m   To stop OAM Policy Manager servers, repeat the previous commands but change the replicas parameter for the policy_cluster. In the example below replicas has been decreased from \u0026ldquo;4\u0026rdquo; to \u0026ldquo;2\u0026rdquo;:\n - clusterName: policy_cluster replicas: 1 serverPod: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - podAffinityTerm: labelSelector: matchExpressions: - key: weblogic.clusterName operator: In values: - $(CLUSTER_NAME) After saving the changes one pod will move to a STATUS of Terminating (accessdomain-oam-policy-mgr4).\nNAME READY STATUS RESTARTS AGE accessdomain-adminserver 1/1 Running 0 3h49m accessdomain-create-oam-infra-domain-job-7c9r9 0/1 Completed 0 3h55m accessdomain-oam-policy-mgr1 1/1 Running 0 3h41m accessdomain-oam-policy-mgr2 1/1 Running 0 3h41m accessdomain-oam-policy-mgr3 1/1 Running 0 10m accessdomain-oam-policy-mgr4 1/1 Terminating 0 10m accessdomain-oam-server1 1/1 Running 0 3h41m accessdomain-oam-server2 1/1 Running 0 3h41m helper 1/1 Running 0 4h11m nginx-ingress-ingress-nginx-controller-76fb7678f-k8rhq 1/1 Running 0 75m The pods will take a minute or two to stop, so keep executing the command until the pods has disappeared:\nNAME READY STATUS RESTARTS AGE accessdomain-adminserver 1/1 Running 0 3h50m accessdomain-create-oam-infra-domain-job-7c9r9 0/1 Completed 0 3h57m accessdomain-oam-policy-mgr1 1/1 Running 0 3h42m accessdomain-oam-policy-mgr2 1/1 Running 0 3h42m accessdomain-oam-server1 1/1 Running 0 3h42m accessdomain-oam-server2 1/1 Running 0 3h42m helper 1/1 Running 0 4h12m nginx-ingress-ingress-nginx-controller-76fb7678f-k8rhq 1/1 Running 0 76m   Stopping and Starting the Administration Server and Managed Servers To stop all the OAM Managed Servers and the Administration Server in one operation:\n  Run the following kubectl command to edit the domain:\n$ kubectl edit domain \u0026lt;domain_uid\u0026gt; -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl edit domain accessdomain -n oamns   In the edit session search for serverStartPolicy: IF_NEEDED:\n volumes: - name: weblogic-domain-storage-volume persistentVolumeClaim: claimName: accessdomain-domain-pvc serverStartPolicy: IF_NEEDED webLogicCredentialsSecret: name: accessdomain-credentials   Change serverStartPolicy: IF_NEEDED to NEVER as follows:\n volumes: - name: weblogic-domain-storage-volume persistentVolumeClaim: claimName: accessdomain-domain-pvc serverStartPolicy: NEVER webLogicCredentialsSecret: name: accessdomain-credentials   Save the file and exit (:wq!).\n  Run the following kubectl command to view the pods:\n$ kubectl get pods -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get pods -n oamns The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE accessdomain-adminserver 1/1 Terminating 0 3h52m accessdomain-create-oam-infra-domain-job-7c9r9 0/1 Completed 0 3h59m accessdomain-oam-policy-mgr1 1/1 Terminating 0 3h44m accessdomain-oam-policy-mgr2 1/1 Terminating 0 3h44m accessdomain-oam-server1 1/1 Terminating 0 3h44m accessdomain-oam-server2 1/1 Terminating 0 3h44m helper 1/1 Running 0 4h14m nginx-ingress-ingress-nginx-controller-76fb7678f-k8rhq 1/1 Running 0 78m The Administration Server pods and Managed Server pods will move to a STATUS of Terminating. After a few minutes, run the command again and the pods should have disappeared:\nNAME READY STATUS RESTARTS AGE accessdomain-create-oam-infra-domain-job-7c9r9 0/1 Completed 0 4h helper 1/1 Running 0 4h15m nginx-ingress-ingress-nginx-controller-76fb7678f-k8rhq 1/1 Running 0 80m   To start the Administration Server and Managed Servers up again, repeat the previous steps but change serverStartPolicy: NEVER to IF_NEEDED as follows:\n volumes: - name: weblogic-domain-storage-volume persistentVolumeClaim: claimName: accessdomain-domain-pvc serverStartPolicy: IF_NEEDED webLogicCredentialsSecret: name: accessdomain-credentials   Run the following kubectl command to view the pods:\n$ kubectl get pods -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get pods -n oamns The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE accessdomain-create-oam-infra-domain-job-7c9r9 0/1 Completed 0 4h1m accessdomain-introspector-jwqxw 1/1 Running 0 10s helper 1/1 Running 0 4h17m nginx-ingress-ingress-nginx-controller-76fb7678f-k8rhq 1/1 Running 0 81m The Administration Server pod will start followed by the OAM Managed Servers pods. This process will take several minutes, so keep executing the command until all the pods are running with READY status 1/1 :\nNAME READY STATUS RESTARTS AGE accessdomain-adminserver 1/1 Running 0 10m accessdomain-create-oam-infra-domain-job-7c9r9 0/1 Completed 0 4h12m accessdomain-oam-policy-mgr1 1/1 Running 0 7m35s accessdomain-oam-policy-mgr2 1/1 Running 0 7m35s accessdomain-oam-server1 1/1 Running 0 7m35s accessdomain-oam-server2 1/1 Running 0 7m35s helper 1/1 Running 0 4h28m nginx-ingress-ingress-nginx-controller-76fb7678f-k8rhq 1/1 Running 0 92m   Domain lifecycle sample scripts The WebLogic Kubernetes Operator provides sample scripts to start up or shut down a specific Managed Server or cluster in a deployed domain, or the entire deployed domain.\nNote: Prior to running these scripts, you must have previously created and deployed the domain.\nThe scripts are located in the $WORKDIR/kubernetes/domain-lifecycle directory. For more information, see the README.\n"
},
{
	"uri": "/fmw-kubernetes/oam/patch-and-upgrade/patch_an_image/",
	"title": "a. Patch an image",
	"tags": [],
	"description": "Instructions on how to update your OAM Kubernetes cluster with a new OAM container image.",
	"content": "Choose one of the following options to update your OAM kubernetes cluster to use the new image:\n Run the kubectl edit domain command Run the kubectl patch domain command  In all of the above cases, the WebLogic Kubernetes Operator will restart the Administration Server pod first and then perform a rolling restart on the OAM Managed Servers.\nNote: If you are not using Oracle Container Registry or your own container registry, then you must first load the new container image on all nodes in your Kubernetes cluster.\nRun the kubectl edit domain command   To update the domain with the kubectl edit domain command, run the following:\n$ kubectl edit domain \u0026lt;domainname\u0026gt; -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl edit domain accessdomain -n oamns If using Oracle Container Registry or your own container registry for your OAM container image, update the image to point at the new image, for example:\ndomainHomeInImage: false image: container-registry.oracle.com/middleware/oam_cpu:\u0026lt;tag\u0026gt; imagePullPolicy: IfNotPresent If you are not using a container registry and have loaded the image on each of the master and worker nodes, update the image to point at the new image:\ndomainHomeInImage: false image: oracle/oam:\u0026lt;tag\u0026gt; imagePullPolicy: IfNotPresent   Save the file and exit (:wq!)\n  Run the kubectl patch command   To update the domain with the kubectl patch domain command, run the following:\n$ kubectl patch domain \u0026lt;domain\u0026gt; -n \u0026lt;namespace\u0026gt; --type merge -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;image\u0026#34;:\u0026#34;newimage:tag\u0026#34;}}\u0026#39; For example, if using Oracle Container Registry or your own container registry for your OAM container image:\n$ kubectl patch domain accessdomain -n oamns --type merge -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;image\u0026#34;:\u0026#34;container-registry.oracle.com/middleware/oam_cpu:\u0026lt;tag\u0026gt;\u0026#34;}}\u0026#39; For example, if you are not using a container registry and have loaded the image on each of the master and worker nodes:\n$ kubectl patch domain accessdomain -n oamns --type merge -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;image\u0026#34;:\u0026#34;oracle/oam:\u0026lt;tag\u0026gt;\u0026#34;}}\u0026#39; The output will look similar to the following:\ndomain.weblogic.oracle/accessdomain patched   "
},
{
	"uri": "/fmw-kubernetes/oig/patch-and-upgrade/patch_an_image/",
	"title": "a. Patch an image",
	"tags": [],
	"description": "Instructions on how to update your OIG Kubernetes cluster with a new OIG container image.",
	"content": "Choose one of the following options to update your OIG kubernetes cluster to use the new image:\n Run the kubectl edit domain command Run the kubectl patch domain command  In all of the above cases, the WebLogic Kubernetes Operator will restart the Administration Server pod first and then perform a rolling restart on the OIG Managed Servers.\nNote: If you are not using Oracle Container Registry or your own container registry, then you must first load the new container image on all nodes in your Kubernetes cluster.\nRun the kubectl edit domain command   To update the domain with the kubectl edit domain command, run the following:\n$ kubectl edit domain \u0026lt;domainname\u0026gt; -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl edit domain governancedomain -n oigns If using Oracle Container Registry or your own container registry for your OIG container image, update the image to point at the new image, for example:\ndomainHomeInImage: false image: container-registry.oracle.com/middleware/oig_cpu:\u0026lt;tag\u0026gt; imagePullPolicy: IfNotPresent If you are not using a container registry and have loaded the image on each of the master and worker nodes, update the image to point at the new image:\ndomainHomeInImage: false image: oracle/oig:\u0026lt;tag\u0026gt; imagePullPolicy: IfNotPresent   Save the file and exit (:wq!)\n  Run the kubectl patch command   To update the domain with the kubectl patch domain command, run the following:\n$ kubectl patch domain \u0026lt;domain\u0026gt; -n \u0026lt;namespace\u0026gt; --type merge -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;image\u0026#34;:\u0026#34;newimage:tag\u0026#34;}}\u0026#39; For example, if using Oracle Container Registry or your own container registry for your OIG container image:\n$ kubectl patch domain governancedomain -n oigns --type merge -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;image\u0026#34;:\u0026#34;container-registry.oracle.com/middleware/oig_cpu:\u0026lt;tag\u0026gt;\u0026#34;}}\u0026#39; For example, if you are not using a container registry and have loaded the image on each of the master and worker nodes:\n$ kubectl patch domain governancedomain -n oigns --type merge -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;image\u0026#34;:\u0026#34;oracle/oig:\u0026lt;tag\u0026gt;\u0026#34;}}\u0026#39; The output will look similar to the following:\ndomain.weblogic.oracle/governancedomain patched   Patch the database schemas Once the image has been updated you must patch the schemas in the database.\n  Check to see if the helper pod exists by running:\n$ kubectl get pods -n \u0026lt;domain_namespace\u0026gt; | grep helper For example:\n$ kubectl get pods -n oigns | grep helper The output should look similar to the following:\nhelper 1/1 Running 0 26h If the helper pod exists delete the pod with following command:\n$ kubectl delete pod helper -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl delete pod helper -n oigns   Create a new helper pod by following the instructions in Prepare you environment . Note: The new helper pod should be started using the new image.\n  Run the following command to start a bash shell in the helper pod:\n$ kubectl exec -it helper -n \u0026lt;domain_namespace\u0026gt; -- /bin/bash For example:\n$ kubectl exec -it helper -n oigns -- /bin/bash This will take you into a bash shell in the running helper pod:\n[oracle@helper ~]$   In the helper bash shell run the following commands to set the environment:\n[oracle@helper oracle]$ export DB_HOST=\u0026lt;db_host.domain\u0026gt; [oracle@helper oracle]$ export DB_PORT=\u0026lt;db_port\u0026gt; [oracle@helper oracle]$ export DB_SERVICE=\u0026lt;service_name\u0026gt; [oracle@helper oracle]$ export RCUPREFIX=\u0026lt;rcu_schema_prefix\u0026gt; [oracle@helper oracle]$ export RCU_SCHEMA_PWD=\u0026lt;rcu_schema_pwd\u0026gt; [oracle@helper oracle]$ echo -e \u0026lt;db_pwd\u0026gt;\u0026#34;\\n\u0026#34;\u0026lt;rcu_schema_pwd\u0026gt; \u0026gt; /tmp/pwd.txt [oracle@helper oracle]$ cat /tmp/pwd.txt where:\n\u0026lt;db_host.domain\u0026gt; is the database server hostname\n\u0026lt;db_port\u0026gt; is the database listener port\n\u0026lt;service_name\u0026gt; is the database service name\n\u0026lt;rcu_schema_prefix\u0026gt; is the RCU schema prefix you want to set\n\u0026lt;rcu_schema_pwd\u0026gt; is the password you want to set for the \u0026lt;rcu_schema_prefix\u0026gt;\n\u0026lt;db_pwd\u0026gt; is the SYS password for the database\nFor example:\n[oracle@helper oracle]$ export DB_HOST=mydatabasehost.example.com [oracle@helper oracle]$ export DB_PORT=1521 [oracle@helper oracle]$ export DB_SERVICE=orcl.example.com [oracle@helper oracle]$ export RCUPREFIX=OIGK8S [oracle@helper oracle]$ export RCU_SCHEMA_PWD=\u0026lt;password\u0026gt;   Run the following command to patch the schemas:\nThis command should be run if you are using an OIG image that contains OIG bundle patches. If using an OIG image without OIG bundle patches, then you can skip this step.\n [oracle@helper oracle]$ /u01/oracle/oracle_common/modules/thirdparty/org.apache.ant/1.10.5.0.0/apache-ant-1.10.5/bin/ant \\ -f /u01/oracle/idm/server/setup/deploy-files/automation.xml \\ run-patched-sql-files \\ -logger org.apache.tools.ant.NoBannerLogger \\ -logfile /u01/oracle/idm/server/bin/patch_oim_wls.log \\ -DoperationsDB.host=$DB_HOST \\ -DoperationsDB.port=$DB_PORT \\ -DoperationsDB.serviceName=$DB_SERVICE \\ -DoperationsDB.user=${RCUPREFIX}_OIM \\ -DOIM.DBPassword=$RCU_SCHEMA_PWD \\ -Dojdbc=/u01/oracle/oracle_common/modules/oracle.jdbc/ojdbc8.jar The output will look similar to the following:\nBuildfile: /u01/oracle/idm/server/setup/deploy-files/automation.xml   Verify the database was patched successfully by viewing the patch_oim_wls.log:\n[oracle@helper oracle]$ cat /u01/oracle/idm/server/bin/patch_oim_wls.log The output should look similar to below:\n ... [sql] Executing resource: /u01/oracle/idm/server/db/oim/oracle/StoredProcedures/OfflineDataPurge/oim_pkg_offline_datapurge_pkg_body.sql [sql] Executing resource: /u01/oracle/idm/server/db/oim/oracle/Upgrade/oim12cps4/list/oim12cps4_dml_pty_insert_sysprop_RequestJustificationLocale.sql [sql] Executing resource: /u01/oracle/idm/server/db/oim/oracle/Upgrade/oim12cps4/list/oim12cps4_dml_pty_insert_sysprop_reportee_chain_for_mgr.sql [sql] 36 of 36 SQL statements executed successfully BUILD SUCCESSFUL Total time: 5 second   "
},
{
	"uri": "/fmw-kubernetes/oig/post-install-config/set_oimfronendurl_using_mbeans/",
	"title": "a. Post Install Tasks",
	"tags": [],
	"description": "Perform post install tasks.",
	"content": "Follow these post install configuration steps.\n Create a Server Overrides File Set OIMFrontendURL using MBeans  Create a Server Overrides File   Navigate to the following directory:\ncd $WORKDIR/kubernetes/create-oim-domain/domain-home-on-pv/output/weblogic-domains/governancedomain   Create a setUserOverrides.sh with the following contents:\nDERBY_FLAG=false JAVA_OPTIONS=\u0026quot;${JAVA_OPTIONS} -Djava.net.preferIPv4Stack=true\u0026quot; MEM_ARGS=\u0026quot;-Xms8192m -Xmx8192m\u0026quot;   Copy the setUserOverrides.sh file to the Administration Server pod:\n$ chmod 755 setUserOverrides.sh $ kubectl cp setUserOverrides.sh oigns/governancedomain-adminserver:/u01/oracle/user_projects/domains/governancedomain/bin/setUserOverrides.sh Where oigns is the OIG namespace and governancedomain is the domain_UID.\n  Stop the OIG domain using the following command:\n$ kubectl -n \u0026lt;domain_namespace\u0026gt; patch domains \u0026lt;domain_uid\u0026gt; --type=\u0026#39;json\u0026#39; -p=\u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/serverStartPolicy\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;NEVER\u0026#34; }]\u0026#39; For example:\n$ kubectl -n oigns patch domains governancedomain --type=\u0026#39;json\u0026#39; -p=\u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/serverStartPolicy\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;NEVER\u0026#34; }]\u0026#39; The output will look similar to the following:\ndomain.weblogic.oracle/governancedomain patched   Check that all the pods are stopped:\n$ kubectl get pods -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get pods -n oigns The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE governancedomain-adminserver 1/1 Terminating 0 18h governancedomain-create-fmw-infra-domain-job-8cww8 0/1 Completed 0 24h governancedomain-oim-server1 1/1 Terminating 0 18h governancedomain-soa-server1 1/1 Terminating 0 18h helper 1/1 Running 0 41h The Administration Server pods and Managed Server pods will move to a STATUS of Terminating. After a few minutes, run the command again and the pods should have disappeared:\nNAME READY STATUS RESTARTS AGE governancedomain-create-fmw-infra-domain-job-8cww8 0/1 Completed 0 24h helper 1/1 Running 0 41h   Start the domain using the following command:\n$ kubectl -n \u0026lt;domain_namespace\u0026gt; patch domains \u0026lt;domain_uid\u0026gt; --type=\u0026#39;json\u0026#39; -p=\u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/serverStartPolicy\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;IF_NEEDED\u0026#34; }]\u0026#39; For example:\n$ kubectl -n oigns patch domains governancedomain --type=\u0026#39;json\u0026#39; -p=\u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/serverStartPolicy\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;IF_NEEDED\u0026#34; }]\u0026#39; Run the following kubectl command to view the pods:\n$ kubectl get pods -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get pods -n oigns The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE governancedomain-create-fmw -infra-domain-job-vj69h 0/1 Completed 0 24h governancedomain-introspect-domain-job-7qx29 1/1 Running 0 8s helper 1/1 Running 0 41h The Administration Server pod will start followed by the OIG Managed Servers pods. This process will take several minutes, so keep executing the command until all the pods are running with READY status 1/1:\nNAME READY STATUS RESTARTS AGE governancedomain-adminserver 1/1 Running 0 6m4s governancedomain-create-fmw-infra-domain-job-vj69h 0/1 Completed 0 24h governancedomain-oim-server1 1/1 Running 0 3m5s governancedomain-soa-server1 1/1 Running 0 3m5s helper 1/1 Running 0 41h   Set OIMFrontendURL using MBeans   Login to Oracle Enterprise Manager using the following URL:\nhttps://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/em\n  Click the Target Navigation icon in the top left of the screen and navigate to the following:\n Expand Identity and Access \u0026gt; Access \u0026gt; OIM \u0026gt; oim Right click the instance oim and select System MBean Browser Under Application Defined MBeans, navigate to oracle.iam, Server:oim_server1 \u0026gt; Application:oim \u0026gt; XMLConfig \u0026gt; Config \u0026gt; XMLConfig.DiscoveryConfig \u0026gt; Discovery.    Enter a new value for the OimFrontEndURL attribute, in the format:\n If using an External LoadBalancer for your ingress: https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-PORT} If using NodePort for your ingress: http://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}  Then click Apply.\n  "
},
{
	"uri": "/fmw-kubernetes/oig/configure-ingress/ingress-nginx-setup-for-oig-domain-setup-on-k8s/",
	"title": "a. Using an Ingress with NGINX (non-SSL)",
	"tags": [],
	"description": "Steps to set up an Ingress for NGINX to direct traffic to the OIG domain (non-SSL).",
	"content": "Setting up an ingress for NGINX for the OIG domain on Kubernetes (non-SSL) The instructions below explain how to set up NGINX as an ingress for the OIG domain with non-SSL termination.\nNote: All the steps below should be performed on the master node.\n  Install NGINX\na. Configure the repository\nb. Create a namespace\nc. Install NGINX using helm\nd. Setup routing rules for the domain\n  Create an ingress for the domain\n  Verify that you can access the domain URL\n  Install NGINX Use helm to install NGINX.\nConfigure the repository   Add the Helm chart repository for NGINX using the following command:\n$ helm repo add stable https://kubernetes.github.io/ingress-nginx The output will look similar to the following:\n\u0026quot;stable\u0026quot; has been added to your repositories   Update the repository using the following command:\n$ helm repo update The output will look similar to the following:\nHang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \u0026quot;stable\u0026quot; chart repository Update Complete. Happy Helming!   Create a namespace   Create a Kubernetes namespace for NGINX by running the following command:\n$ kubectl create namespace nginx The output will look similar to the following:\nnamespace/nginx created   Install NGINX using helm If you can connect directly to the master node IP address from a browser, then install NGINX with the --set controller.service.type=NodePort parameter.\nIf you are using a Managed Service for your Kubernetes cluster,for example Oracle Kubernetes Engine (OKE) on Oracle Cloud Infrastructure (OCI), and connect from a browser to the Load Balancer IP address, then use the --set controller.service.type=LoadBalancer parameter. This instructs the Managed Service to setup a Load Balancer to direct traffic to the NGINX ingress.\n  To install NGINX use the following helm command depending on if you are using NodePort or LoadBalancer:\na) Using NodePort\n$ helm install nginx-ingress -n nginx --set controller.service.type=NodePort --set controller.admissionWebhooks.enabled=false stable/ingress-nginx The output will look similar to the following:\nNAME: nginx-ingress LAST DEPLOYED: Thu, 10 Mar 2022 14:13:33 GMT NAMESPACE: nginx STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: The ingress-nginx controller has been installed. Get the application URL by running these commands: export HTTP_NODE_PORT=$(kubectl --namespace nginx get services -o jsonpath=\u0026quot;{.spec.ports[0].nodePort}\u0026quot; nginx-ingress-ingress-nginx-controller) export HTTPS_NODE_PORT=$(kubectl --namespace nginx get services -o jsonpath=\u0026quot;{.spec.ports[1].nodePort}\u0026quot; nginx-ingress-ingress-nginx-controller) export NODE_IP=$(kubectl --namespace nginx get nodes -o jsonpath=\u0026quot;{.items[0].status.addresses[1].address}\u0026quot;) echo \u0026quot;Visit http://$NODE_IP:$HTTP_NODE_PORT to access your application via HTTP.\u0026quot; echo \u0026quot;Visit https://$NODE_IP:$HTTPS_NODE_PORT to access your application via HTTPS.\u0026quot; An example Ingress that makes use of the controller: apiVersion: networking.k8s.io/v1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx name: example namespace: foo spec: ingressClassName: example-class rules: - host: www.example.com http: paths: - path: / pathType: Prefix backend: service: name: exampleService port: 80 # This section is only required if TLS is to be enabled for the Ingress tls: - hosts: - www.example.com secretName: example-tls If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided: apiVersion: v1 kind: Secret metadata: name: example-tls namespace: foo data: tls.crt: \u0026lt;base64 encoded cert\u0026gt; tls.key: \u0026lt;base64 encoded key\u0026gt; type: kubernetes.io/tls b) Using LoadBalancer\n$ helm install nginx-ingress -n nginx --set controller.service.type=LoadBalancer --set controller.admissionWebhooks.enabled=false stable/ingress-nginx The output will look similar to the following:\nNAME: nginx-ingress LAST DEPLOYED: Thu Mar 10 14:15:33 2022 NAMESPACE: nginx STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: The nginx-ingress controller has been installed. It may take a few minutes for the LoadBalancer IP to be available. You can watch the status by running 'kubectl --namespace nginx get services -o wide -w nginx-ingress-controller' An example Ingress that makes use of the controller: apiVersion: networking.k8s.io/v1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx name: example namespace: foo spec: ingressClassName: example-class rules: - host: www.example.com http: paths: - path: / pathType: Prefix backend: service: name: exampleService port: 80 # This section is only required if TLS is to be enabled for the Ingress tls: - hosts: - www.example.com secretName: example-tls If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided: apiVersion: v1 kind: Secret metadata: name: example-tls namespace: foo data: tls.crt: \u0026lt;base64 encoded cert\u0026gt; tls.key: \u0026lt;base64 encoded key\u0026gt; type: kubernetes.io/tls   Setup routing rules for the domain   Setup routing rules by running the following commands:\n$ cd $WORKDIR/kubernetes/charts/ingress-per-domain Edit values.yaml and change the domainUID parameter to match your domainUID, for example domainUID: governancedomain. Also change sslType to NONSSL. The file should look as follows:\n# Load balancer type. Supported values are: TRAEFIK, NGINX type: NGINX # Type of Configuration Supported Values are : NONSSL, SSL sslType: NONSSL # TimeOut value to be set for nginx parameters proxy-read-timeout and proxy-send-timeout nginxTimeOut: 180 # TLS secret name if the mode is SSL secretName: domain1-tls-cert #WLS domain as backend to the load balancer wlsDomain: domainUID: governancedomain adminServerName: AdminServer adminServerPort: 7001 soaClusterName: soa_cluster soaManagedServerPort: 8001 oimClusterName: oim_cluster oimManagedServerPort: 14000   Create an ingress for the domain   Create an Ingress for the domain (governancedomain-nginx), in the domain namespace by using the sample Helm chart:\n$ cd $WORKDIR $ helm install governancedomain-nginx kubernetes/charts/ingress-per-domain --namespace \u0026lt;namespace\u0026gt; --values kubernetes/charts/ingress-per-domain/values.yaml Note: The \u0026lt;workdir\u0026gt;/samples/kubernetes/charts/ingress-per-domain/templates//nginx-ingress-k8s1.19.yaml and nginx-ingress.yaml has nginx.ingress.kubernetes.io/enable-access-log set to false. If you want to enable access logs then set this value to true before executing the command. Enabling access-logs can cause issues with disk space if not regularly maintained.\nFor example:\n$ cd $WORKDIR $ helm install governancedomain-nginx kubernetes/charts/ingress-per-domain --namespace oigns --values kubernetes/charts/ingress-per-domain/values.yaml The output will look similar to the following:\n$ helm install governancedomain-nginx kubernetes/charts/ingress-per-domain --namespace oigns --values kubernetes/charts/ingress-per-domain/values.yaml NAME: governancedomain-nginx LAST DEPLOYED: Thu Mar 10 14:18:23 2022 NAMESPACE: oigns STATUS: deployed REVISION: 1 TEST SUITE: None   Run the following command to show the ingress is created successfully:\n$ kubectl get ing -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get ing -n oigns The output will look similar to the following:\nNAME CLASS HOSTS ADDRESS PORTS AGE governancedomain-nginx \u0026lt;none\u0026gt; * x.x.x.x 80 47s   Find the NodePort of NGINX using the following command (only if you installed NGINX using NodePort):\n$ kubectl get services -n nginx -o jsonpath=”{.spec.ports[0].nodePort}” nginx-ingress-ingress-nginx-controller The output will look similar to the following:\n31530   Run the following command to check the ingress:\n$ kubectl describe ing governancedomain-ingress -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl describe ing governancedomain-nginx -n oigns The output will look similar to the following:\nName: governancedomain-nginx Namespace: oigns Address: Default backend: default-http-backend:80 (\u0026lt;error: endpoints \u0026quot;default-http-backend\u0026quot; not found\u0026gt;) Rules: Host Path Backends ---- ---- -------- * /console governancedomain-adminserver:7001 (10.244.2.59:7001) /em governancedomain-adminserver:7001 (10.244.2.59:7001) /soa governancedomain-cluster-soa-cluster:8001 (10.244.2.60:8001) /integration governancedomain-cluster-soa-cluster:8001 (10.244.2.60:8001) /soa-infra governancedomain-cluster-soa-cluster:8001 (10.244.2.60:8001) /identity governancedomain-cluster-oim-cluster:14000 (10.244.1.25:14000) /admin governancedomain-cluster-oim-cluster:14000 (10.244.1.25:14000) /oim governancedomain-cluster-oim-cluster:14000 (10.244.1.25:14000) /sysadmin governancedomain-cluster-oim-cluster:14000 (10.244.1.25:14000) /workflowservice governancedomain-cluster-oim-cluster:14000 (10.244.1.25:14000) /xlWebApp governancedomain-cluster-oim-cluster:14000 (10.244.1.25:14000) /Nexaweb governancedomain-cluster-oim-cluster:14000 (10.244.1.25:14000) /callbackResponseService governancedomain-cluster-oim-cluster:14000 (10.244.1.25:14000) /spml-xsd governancedomain-cluster-oim-cluster:14000 (10.244.1.25:14000) /HTTPClnt governancedomain-cluster-oim-cluster:14000 (10.244.1.25:14000) /reqsvc governancedomain-cluster-oim-cluster:14000 (10.244.1.25:14000) /iam governancedomain-cluster-oim-cluster:14000 (10.244.1.25:14000) /provisioning-callback governancedomain-cluster-oim-cluster:14000 (10.244.1.25:14000) /CertificationCallbackService governancedomain-cluster-oim-cluster:14000 (10.244.1.25:14000) /ucs governancedomain-cluster-oim-cluster:14000 (10.244.1.25:14000) /FacadeWebApp governancedomain-cluster-oim-cluster:14000 (10.244.1.25:14000) /OIGUI governancedomain-cluster-oim-cluster:14000 (10.244.1.25:14000) /weblogic governancedomain-cluster-oim-cluster:14000 (10.244.1.25:14000) Annotations: kubernetes.io/ingress.class: nginx meta.helm.sh/release-name: governancedomain-nginx meta.helm.sh/release-namespace: oigns nginx.ingress.kubernetes.io/affinity: cookie nginx.ingress.kubernetes.io/enable-access-log: false Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Sync 35s nginx-ingress-controller Scheduled for sync   To confirm that the new ingress is successfully routing to the domain\u0026rsquo;s server pods, run the following command to send a request to the URL for the WebLogic ReadyApp framework:\nNote: If using a load balancer for your ingress replace ${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT} with ${LOADBALANCER-HOSTNAME}:${LOADBALANCER-PORT}.\n$ curl -v http://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/weblogic/ready For example:\na) For NodePort\n$ curl -v http://masternode.example.com:31530/weblogic/ready b) For LoadBalancer\n$ curl -v http://masternode.example.com:80/weblogic/ready The output will look similar to the following:\n$ curl -v http://masternode.example.com:31530/weblogic/ready * About to connect() to masternode.example.com port 31530 (#0) * Trying X.X.X.X... * Connected to masternode.example.com (X.X.X.X) port 31530 (#0) \u0026gt; GET /weblogic/ready HTTP/1.1 \u0026gt; User-Agent: curl/7.29.0 \u0026gt; Host: masternode.example.com:31530 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; Server: nginx/1.19.2 \u0026lt; Date: Thu Mar 10 14:21:14 2022 \u0026lt; Content-Length: 0 \u0026lt; Connection: keep-alive \u0026lt; * Connection #0 to host masternode.example.com left intact   Verify that you can access the domain URL After setting up the NGINX ingress, verify that the domain applications are accessible through the NGINX ingress port (for example 31530) as per Validate Domain URLs \n"
},
{
	"uri": "/fmw-kubernetes/oud/manage-oud-containers/logging-and-visualization/",
	"title": "b) Logging and Visualization for Helm Chart oud-ds-rs Deployment",
	"tags": [],
	"description": "Describes the steps for logging and visualization with Elasticsearch and Kibana.",
	"content": " Introduction Installation  Create a Kubernetes secret Enable Elasticsearch, Logstash, and Kibana Upgrade OUD deployment with ELK configuration Verify the pods   Verify using the Kibana application  Introduction This section describes how to install and configure logging and visualization for the oud-ds-rs Helm chart deployment.\nThe ELK stack consists of Elasticsearch, Logstash, and Kibana. Using ELK you can gain insights in real-time from the log data from your applications.\n Elasticsearch is a distributed, RESTful search and analytics engine capable of solving a growing number of use cases. As the heart of the Elastic Stack, it centrally stores your data so you can discover the expected and uncover the unexpected. Logstash is an open source, server-side data processing pipeline that ingests data from a multitude of sources simultaneously, transforms it, and then sends it to your favorite “stash.” Kibana lets you visualize your Elasticsearch data and navigate the Elastic Stack. It gives you the freedom to select the way you give shape to your data. And you don’t always have to know what you\u0026rsquo;re looking for.  Installation ELK can be enabled for environments created using the Helm charts provided. The example below will demonstrate installation and configuration of ELK for the oud-ds-rs chart.\nCreate a Kubernetes secret A Kubernetes secret to access the required images on hub.docker.com should have been previously created in Create OUD instances.\nIf you have not yet created a Kubernetes secret refer back to Create OUD instances.\nEnable Elasticsearch, Logstash, and Kibana   Create a directory on the persistent volume to store the ELK log files:\n$ mkdir -p \u0026lt;persistent_volume\u0026gt;/oud_elk_data $ chmod 777 \u0026lt;persistent_volume\u0026gt;/oud_elk_data For example:\n$ mkdir -p /scratch/shared/oud_elk_data $ chmod 777 /scratch/shared/oud_elk_data   Navigate to the $WORKDIR/kubernetes/helm directory and create a logging-override-values.yaml with the following:\nelk: enabled: true imagePullSecrets: - name: dockercred elkVolume: # If enabled, it will use the persistent volume. # if value is false, PV and PVC would not be used and there would not be any mount point available for config enabled: true type: filesystem filesystem: hostPath: path: \u0026lt;persistent_volume\u0026gt;/oud_elk_data For example:\nelk: enabled: true imagePullSecrets: - name: dockercred elkVolume: # If enabled, it will use the persistent volume. # if value is false, PV and PVC would not be used and there would not be any mount point available for config enabled: true type: filesystem filesystem: hostPath: path: /scratch/shared/oud_elk_data If using NFS for the persistent volume change the elkVolume section as follows:\nelkVolume: # If enabled, it will use the persistent volume. # if value is false, PV and PVC would not be used and there would not be any mount point available for config enabled: true type: networkstorage networkstorage: nfs: server: myserver path: \u0026lt;persistent_volume\u0026gt;/oud_elk_data   Upgrade OUD deployment with ELK configuration   Run the following command to upgrade the OUD deployment with the ELK configuration:\n$ helm upgrade --namespace \u0026lt;namespace\u0026gt; --values \u0026lt;valuesfile.yaml\u0026gt; \u0026lt;releasename\u0026gt; oud-ds-rs --reuse-values For example:\n$ helm upgrade --namespace oudns --values logging-override-values.yaml oud-ds-rs oud-ds-rs --reuse-values   Verify the pods   Run the following command to verify the elasticsearch, logstash and kibana pods are running:\n$ kubectl get pods -o wide -n \u0026lt;namespace\u0026gt; | grep \u0026#39;es\\|kibana\\|logstash\u0026#39; For example:\n$ kubectl get pods -o wide -n oudns | grep \u0026#39;es\\|kibana\\|logstash\u0026#39; The output will look similar to the following:\noud-ds-rs-es-cluster-0 1/1 Running 0 6m28s oud-ds-rs-kibana-7b7769485f-b9mr4 1/1 Running 0 6m28s oud-ds-rs-logstash-5995948d7f-nqlh6 1/1 Running 0 6m28s From the above identify the elasticsearch pod, for example: oud-ds-rs-es-cluster-0.\n  Run the port-forward command to allow elasticsearch to listen on port 9200:\n$ kubectl port-forward oud-ds-rs-es-cluster-0 9200:9200 --namespace=\u0026lt;namespace\u0026gt; \u0026amp; For example:\n$ kubectl port-forward oud-ds-rs-es-cluster-0 9200:9200 --namespace=oudns \u0026amp; The output will look similar to the following:\n[1] 98458 bash-4.2$ Forwarding from 127.0.0.1:9200 -\u0026gt; 9200 Forwarding from [::1]:9200 -\u0026gt; 9200   Verify that elasticsearch is running by interrogating port 9200:\n$ curl http://localhost:9200 The output will look similar to the following:\n{ \u0026#34;name\u0026#34; : \u0026#34;oud-ds-rs-es-cluster-0\u0026#34;, \u0026#34;cluster_name\u0026#34; : \u0026#34;OUD-elk\u0026#34;, \u0026#34;cluster_uuid\u0026#34; : \u0026#34;J42fuv_XSHGy-uolRyNEtA\u0026#34;, \u0026#34;version\u0026#34; : { \u0026#34;number\u0026#34; : \u0026#34;6.8.0\u0026#34;, \u0026#34;build_flavor\u0026#34; : \u0026#34;default\u0026#34;, \u0026#34;build_type\u0026#34; : \u0026#34;docker\u0026#34;, \u0026#34;build_hash\u0026#34; : \u0026#34;65b6179\u0026#34;, \u0026#34;build_date\u0026#34; : \u0026#34;2019-05-15T20:06:13.172855Z\u0026#34;, \u0026#34;build_snapshot\u0026#34; : false, \u0026#34;lucene_version\u0026#34; : \u0026#34;7.7.0\u0026#34;, \u0026#34;minimum_wire_compatibility_version\u0026#34; : \u0026#34;5.6.0\u0026#34;, \u0026#34;minimum_index_compatibility_version\u0026#34; : \u0026#34;5.0.0\u0026#34; }, \u0026#34;tagline\u0026#34; : \u0026#34;You Know, for Search\u0026#34; }   Verify using the Kibana application   List the Kibana application service using the following command:\n$ kubectl get svc -o wide -n \u0026lt;namespace\u0026gt; | grep kibana For example:\n$ kubectl get svc -o wide -n oudns | grep kibana The output will look similar to the following:\noud-ds-rs-kibana NodePort 10.103.169.218 \u0026lt;none\u0026gt; 5601:31199/TCP 13m app=kibana In this example, the port to access the Kibana application is 31199.\n  Access the Kibana console in a browser with: http://${MASTERNODE-HOSTNAME}:${KIBANA-PORT}/app/kibana.\n  From the Kibana portal navigate to Management\u0026gt; Kibana \u0026gt; Index Patterns.\n  In the Create Index Pattern page enter * for the Index pattern and click Next Step.\n  In the Configure settings page, from the Time Filter field name drop down menu select @timestamp and click Create index pattern.\n  Once the index pattern is created click on Discover in the navigation menu to view the OUD logs.\n  "
},
{
	"uri": "/fmw-kubernetes/oudsm/manage-oudsm-containers/logging-and-visualization/",
	"title": "b) Logging and Visualization for Helm Chart oudsm Deployment",
	"tags": [],
	"description": "Describes the steps for logging and visualization with Elasticsearch and Kibana.",
	"content": " Introduction Installation  Create a Kubernetes secret Enable Elasticsearch, Logstash, and Kibana Upgrade OUDSM deployment with ELK configuration Verify the pods   Verify using the Kibana application  Introduction This section describes how to install and configure logging and visualization for the oudsm Helm Chart deployment.\nThe ELK stack consists of Elasticsearch, Logstash, and Kibana. Using ELK we can gain insights in real-time from the log data from your applications.\n Elasticsearch is a distributed, RESTful search and analytics engine capable of solving a growing number of use cases. As the heart of the Elastic Stack, it centrally stores your data so you can discover the expected and uncover the unexpected. Logstash is an open source, server-side data processing pipeline that ingests data from a multitude of sources simultaneously, transforms it, and then sends it to your favorite “stash.” Kibana lets you visualize your Elasticsearch data and navigate the Elastic Stack. It gives you the freedom to select the way you give shape to your data. And you don’t always have to know what you\u0026rsquo;re looking for.  Installation ELK can be enabled for environments created using the Helm charts provided. The example below will demonstrate installation and configuration of ELK for the oudsm chart.\nCreate a Kubernetes secret   Create a Kubernetes secret to access the required images on hub.docker.com:\nNote: You must first have a user account on hub.docker.com.\n$ kubectl create secret docker-registry \u0026#34;dockercred\u0026#34; --docker-server=\u0026#34;https://index.docker.io/v1/\u0026#34; --docker-username=\u0026#34;\u0026lt;docker_username\u0026gt;\u0026#34; --docker-password=\u0026lt;password\u0026gt; --docker-email=\u0026lt;docker_email_credentials\u0026gt; --namespace=\u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl create secret docker-registry \u0026quot;dockercred\u0026quot; --docker-server=\u0026quot;https://index.docker.io/v1/\u0026quot; --docker-username=\u0026quot;username\u0026quot; --docker-password=\u0026lt;password\u0026gt; --docker-email=user@example.com --namespace=oudsmns The output will look similar to the following:\nsecret/dockercred created   Enable Elasticsearch, Logstash, and Kibana   Create a directory on the persistent volume to store the ELK log files:\n$ mkdir -p \u0026lt;persistent_volume\u0026gt;/oudsm_elk_data $ chmod 777 \u0026lt;persistent_volume\u0026gt;/oudsm_elk_data For example:\n$ mkdir -p /scratch/shared/oudsm_elk_data $ chmod 777 /scratch/shared/oudsm_elk_data   Navigate to the $WORKDIR/kubernetes/helm directory and create a logging-override-values.yaml with the following:\nelk: enabled: true imagePullSecrets: - name: dockercred elkVolume: # If enabled, it will use the persistent volume. # if value is false, PV and PVC would not be used and there would not be any mount point available for config enabled: true type: filesystem filesystem: hostPath: path: \u0026lt;persistent_volume\u0026gt;/oudsm_elk_data For example:\nelk: enabled: true imagePullSecrets: - name: dockercred elkVolume: # If enabled, it will use the persistent volume. # if value is false, PV and PVC would not be used and there would not be any mount point available for config enabled: true type: filesystem filesystem: hostPath: path: /scratch/shared/oudsm_elk_data If using NFS for the persistent volume change the elkVolume section as follows:\nelkVolume: # If enabled, it will use the persistent volume. # if value is false, PV and PVC would not be used and there would not be any mount point available for config enabled: true type: networkstorage networkstorage: nfs: server: myserver path: \u0026lt;persistent_volume\u0026gt;/oudsm_elk_data   Upgrade OUDSM deployment with ELK configuration   Run the following command to upgrade the OUDSM deployment with the ELK configuration:\n$ helm upgrade --namespace \u0026lt;namespace\u0026gt; --values logging-override-values.yaml \u0026lt;release_name\u0026gt; oudsm --reuse-values For example:\n$ helm upgrade --namespace oudsmns --values logging-override-values.yaml oudsm oudsm --reuse-values   Verify the pods   Run the following command to verify the elasticsearch, logstash and kibana pods are running:\n$ kubectl get pods -o wide -n \u0026lt;namespace\u0026gt; | grep \u0026#39;es\\|kibana\\|logstash\u0026#39; For example:\n$ kubectl get pods -o wide -n oudsmns | grep \u0026#39;es\\|kibana\\|logstash\u0026#39; The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES oudsm-es-cluster-0 1/1 Running 0 4m5s 10.244.1.124 \u0026lt;worker-node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; oudsm-kibana-7bf95b4c45-sfst6 1/1 Running 1 4m5s 10.244.2.137 \u0026lt;worker-node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; oudsm-logstash-5bb6bc67bf-l4mdv 1/1 Running 0 4m5s 10.244.2.138 \u0026lt;worker-node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; From the above identify the elasticsearch pod, for example: oudsm-es-cluster-0.\n  Run the port-forward command to allow ElasticSearch to be listening on port 9200:\n$ kubectl port-forward oudsm-es-cluster-0 9200:9200 --namespace=\u0026lt;namespace\u0026gt; \u0026amp; For example:\n$ kubectl port-forward oudsm-es-cluster-0 9200:9200 --namespace=oudsmns \u0026amp; The output will look similar to the following:\n[1] 98458 bash-4.2$ Forwarding from 127.0.0.1:9200 -\u0026gt; 9200 Forwarding from [::1]:9200 -\u0026gt; 9200   Verify that ElasticSearch is running by interrogating port 9200:\n$ curl http://localhost:9200 The output will look similar to the following:\n{ \u0026#34;name\u0026#34; : \u0026#34;oudsm-es-cluster-0\u0026#34;, \u0026#34;cluster_name\u0026#34; : \u0026#34;OUD-elk\u0026#34;, \u0026#34;cluster_uuid\u0026#34; : \u0026#34;TIKKJuK4QdWcOZrEOA1zeQ\u0026#34;, \u0026#34;version\u0026#34; : { \u0026#34;number\u0026#34; : \u0026#34;6.8.0\u0026#34;, \u0026#34;build_flavor\u0026#34; : \u0026#34;default\u0026#34;, \u0026#34;build_type\u0026#34; : \u0026#34;docker\u0026#34;, \u0026#34;build_hash\u0026#34; : \u0026#34;65b6179\u0026#34;, \u0026#34;build_date\u0026#34; : \u0026#34;2019-05-15T20:06:13.172855Z\u0026#34;, \u0026#34;build_snapshot\u0026#34; : false, \u0026#34;lucene_version\u0026#34; : \u0026#34;7.7.0\u0026#34;, \u0026#34;minimum_wire_compatibility_version\u0026#34; : \u0026#34;5.6.0\u0026#34;, \u0026#34;minimum_index_compatibility_version\u0026#34; : \u0026#34;5.0.0\u0026#34; }, \u0026#34;tagline\u0026#34; : \u0026#34;You Know, for Search\u0026#34; }   Verify using the Kibana application   List the Kibana application service using the following command:\n$ kubectl get svc -o wide -n \u0026lt;namespace\u0026gt; | grep kibana For example:\n$ kubectl get svc -o wide -n oudsmns | grep kibana The output will look similar to the following:\noudsm-kibana NodePort 10.101.248.248 \u0026lt;none\u0026gt; 5601:31195/TCP 7m56s app=kibana In this example, the port to access Kibana application via a Web browser will be 31195.\n  Access the Kibana console in a browser with: http://${MASTERNODE-HOSTNAME}:${KIBANA-PORT}/app/kibana.\n  From the Kibana portal navigate to Management\u0026gt; Kibana \u0026gt; Index Patterns.\n  In the Create Index Pattern page enter * for the Index pattern and click Next Step.\n  In the Configure settings page, from the Time Filter field name drop down menu select @timestamp and click Create index pattern.\n  Once the index pattern is created click on Discover in the navigation menu to view the OUDSM logs.\n  "
},
{
	"uri": "/fmw-kubernetes/oig/post-install-config/install_and_configure_connectors/",
	"title": "b. Install and configure connectors",
	"tags": [],
	"description": "Install and Configure Connectors.",
	"content": "Download the connector   Download the Connector you are interested in from Oracle Identity Manager Connector Downloads.\n  Copy the connector zip file to a staging directory on the master node e.g. \u0026lt;workdir\u0026gt;/stage and unzip it:\n$ cp $HOME/Downloads/\u0026lt;connector\u0026gt;.zip \u0026lt;workdir\u0026gt;/\u0026lt;stage\u0026gt;/ $ cd \u0026lt;workdir\u0026gt;/\u0026lt;stage\u0026gt; $ unzip \u0026lt;connector\u0026gt;.zip $ chmod -R 755 * For example:\n$ cp $HOME/Downloads/Exchange-12.2.1.3.0.zip /scratch/OIGK8S/stage/ $ cd /scratch/OIGK8S/stage/ $ unzip exchange-12.2.1.3.0.zip $ chmod -R 755 *   Copy OIG connectors There are two options to copy OIG Connectors to your Kubernetes cluster:\n a) Copy the connector directly to the Persistent Volume b) Use the kubectl cp command to copy the connector to the Persistent Volume  It is recommended to use option a), however there may be cases, for example when using a Managed Service such as Oracle Kubernetes Engine on Oracle Cloud Infrastructure, where it may not be feasible to directly mount the domain directory. In such cases option b) should be used.\na) Copy the connector directly to the persistent volume   Copy the connector zip file to the persistent volume. For example:\n$ cp -R \u0026lt;path_to\u0026gt;/\u0026lt;connector\u0026gt; \u0026lt;persistent_volume\u0026gt;/governancedomainpv/ConnectorDefaultDirectory/ For example:\n$ cp -R /scratch/OIGK8S/stage/Exchange-12.2.1.3.0 /scratch/shared/governancedomainpv/ConnectorDefaultDirectory/   b) Use the kubectl cp command to copy the connector to the persistent volume   Run the following command to copy over the connector:\n$ kubectl -n \u0026lt;domain_namespace\u0026gt; cp \u0026lt;path_to\u0026gt;/\u0026lt;connector\u0026gt; \u0026lt;cluster_name\u0026gt;:/u01/oracle/idm/server/ConnectorDefaultDirectory/ For example:\n$ kubectl -n oigns cp /scratch/OIGK8S/stage/Exchange-12.2.1.3.0 governancedomain-oim-server1:/u01/oracle/idm/server/ConnectorDefaultDirectory/   Install the connector The connectors are installed as they are on a standard on-premises setup, via Application On Boarding or via Connector Installer.\nRefer to your Connector specific documentation for instructions.\n"
},
{
	"uri": "/fmw-kubernetes/oam/patch-and-upgrade/upgrade_an_operator_release/",
	"title": "b. Upgrade an operator release",
	"tags": [],
	"description": "Instructions on how to update the WebLogic Kubernetes Operator version.",
	"content": "These instructions apply to upgrading the operator within the 3.x release family as additional versions are released.\n  On the master node, download the new WebLogic Kubernetes Operator source code from the operator github project:\n$ mkdir \u0026lt;workdir\u0026gt;/weblogic-kubernetes-operator-3.X.X $ cd \u0026lt;workdir\u0026gt;/weblogic-kubernetes-operator-3.X.X $ git clone https://github.com/oracle/weblogic-kubernetes-operator.git --branch v3.X.X For example:\n$ mkdir /scratch/OAMK8S/weblogic-kubernetes-operator-3.X.X $ cd /scratch/OAMK8S/weblogic-kubernetes-operator-3.X.X $ git clone https://github.com/oracle/weblogic-kubernetes-operator.git --branch v3.X.X This will create the directory \u0026lt;workdir\u0026gt;/weblogic-kubernetes-operator-3.X.X/weblogic-kubernetes-operator\n  Run the following helm command to upgrade the operator:\n$ cd \u0026lt;workdir\u0026gt;/weblogic-kubernetes-operator-3.X.X/weblogic-kubernetes-operator $ helm upgrade --reuse-values --set image=ghcr.io/oracle/weblogic-kubernetes-operator:3.X.X --namespace \u0026lt;sample-kubernetes-operator-ns\u0026gt; --wait weblogic-kubernetes-operator kubernetes/charts/weblogic-operator For example:\n$ cd /scratch/OAMK8S/weblogic-kubernetes-operator-3.X.X/weblogic-kubernetes-operator $ helm upgrade --reuse-values --set image=ghcr.io/oracle/weblogic-kubernetes-operator:3.X.X --namespace opns --wait weblogic-kubernetes-operator kubernetes/charts/weblogic-operator The output will look similar to the following:\nRelease \u0026quot;weblogic-kubernetes-operator\u0026quot; has been upgraded. Happy Helming! NAME: weblogic-kubernetes-operator LAST DEPLOYED: Mon Mar 7 18:36:10 2021 NAMESPACE: opns STATUS: deployed REVISION: 3 TEST SUITE: None   Verify that the operator\u0026rsquo;s pod and services are running by executing the following command:\n$ kubectl get all -n \u0026lt;sample-kubernetes-operator-ns\u0026gt; For example:\n$ kubectl get all -n opns The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE pod/weblogic-operator-69546866bd-h58sk 2/2 Running 0 112s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/internal-weblogic-operator-svc ClusterIP 10.106.72.42 \u0026lt;none\u0026gt; 8082/TCP 2d NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/weblogic-operator 1/1 1 1 2d NAME DESIRED CURRENT READY AGE replicaset.apps/weblogic-operator-676d5cc6f4 0 0 0 2d replicaset.apps/weblogic-operator-69546866bd 1 1 1 112s   "
},
{
	"uri": "/fmw-kubernetes/oig/patch-and-upgrade/upgrade_an_operator_release/",
	"title": "b. Upgrade an operator release",
	"tags": [],
	"description": "Instructions on how to update the WebLogic Kubernetes Operator version.",
	"content": "These instructions apply to upgrading operators within the 3.x release family as additional versions are released.\n  On the master node, download the new WebLogic Kubernetes Operator source code from the operator github project:\n$ mkdir \u0026lt;workdir\u0026gt;/weblogic-kubernetes-operator-3.X.X $ cd \u0026lt;workdir\u0026gt;/weblogic-kubernetes-operator-3.X.X $ git clone https://github.com/oracle/weblogic-kubernetes-operator.git --branch v3.X.X For example:\n$ mkdir /scratch/OIGK8S/weblogic-kubernetes-operator-3.X.X $ cd /scratch/OIGK8S/weblogic-kubernetes-operator-3.X.X $ git clone https://github.com/oracle/weblogic-kubernetes-operator.git --branch v3.X.X This will create the directory \u0026lt;workdir\u0026gt;/weblogic-kubernetes-operator-3.X.X/weblogic-kubernetes-operator\n  Run the following helm command to upgrade the operator:\n$ cd \u0026lt;workdir\u0026gt;/weblogic-kubernetes-operator-3.X.X/weblogic-kubernetes-operator $ helm upgrade --reuse-values --set image=ghcr.io/oracle/weblogic-kubernetes-operator:3.X.X --namespace \u0026lt;sample-kubernetes-operator-ns\u0026gt; --wait weblogic-kubernetes-operator kubernetes/charts/weblogic-operator For example:\n$ cd /scratch/OIGK8S/weblogic-kubernetes-operator-3.X.X/weblogic-kubernetes-operator $ helm upgrade --reuse-values --set image=ghcr.io/oracle/weblogic-kubernetes-operator:3.X.X --namespace operator --wait weblogic-kubernetes-operator kubernetes/charts/weblogic-operator The output will look similar to the following:\nRelease \u0026quot;weblogic-kubernetes-operator\u0026quot; has been upgraded. Happy Helming! NAME: weblogic-kubernetes-operator LAST DEPLOYED: Tue Mar 15 09:24:40 2022 NAMESPACE: operator STATUS: deployed REVISION: 3 TEST SUITE: None   Verify that the operator\u0026rsquo;s pod and services are running by executing the following command:\n$ kubectl get all -n \u0026lt;sample-kubernetes-operator-ns\u0026gt; For example:\n$ kubectl get all -n opns The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE pod/weblogic-operator-69546866bd-h58sk 2/2 Running 0 112s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/internal-weblogic-operator-svc ClusterIP 10.106.72.42 \u0026lt;none\u0026gt; 8082/TCP 2d NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/weblogic-operator 1/1 1 1 2d NAME DESIRED CURRENT READY AGE replicaset.apps/weblogic-operator-676d5cc6f4 0 0 0 2d replicaset.apps/weblogic-operator-69546866bd 1 1 1 112s   "
},
{
	"uri": "/fmw-kubernetes/oig/configure-ingress/ingress-nginx-setup-for-oig-domain-setup-on-k8s-ssl/",
	"title": "b. Using an Ingress with NGINX (SSL)",
	"tags": [],
	"description": "Steps to set up an Ingress for NGINX to direct traffic to the OIG domain using SSL.",
	"content": "Setting up an ingress for NGINX for the OIG domain on Kubernetes The instructions below explain how to set up NGINX as an ingress for the OIG domain with SSL termination.\nNote: All the steps below should be performed on the master node.\n  Create a SSL certificate\na. Generate SSL certificate\nb. Create a Kubernetes secret for SSL\n  Install NGINX\na. Configure the repository\nb. Create a namespace\nc. Install NGINX using helm\n  Create an ingress for the domain\n  Verify that you can access the domain URL\n  Create a SSL certificate Generate SSL certificate   Generate a private key and certificate signing request (CSR) using a tool of your choice. Send the CSR to your certificate authority (CA) to generate the certificate.\nIf you want to use a certificate for testing purposes you can generate a self signed certificate using openssl:\n$ mkdir \u0026lt;workdir\u0026gt;/ssl $ cd \u0026lt;workdir\u0026gt;/ssl $ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \u0026#34;/CN=\u0026lt;nginx-hostname\u0026gt;\u0026#34; For example:\n$ mkdir /scratch/OIGK8S/ssl $ cd /scratch/OIGK8S/ssl $ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \u0026#34;/CN=masternode.example.com\u0026#34; Note: The CN should match the host.domain of the master node in order to prevent hostname problems during certificate verification.\nThe output will look similar to the following:\nGenerating a 2048 bit RSA private key ..........................................+++ .......................................................................................................+++ writing new private key to 'tls.key' -----   Create a Kubernetes secret for SSL   Create a secret for SSL containing the SSL certificate by running the following command:\n$ kubectl -n oigns create secret tls \u0026lt;domain_uid\u0026gt;-tls-cert --key \u0026lt;workdir\u0026gt;/tls.key --cert \u0026lt;workdir\u0026gt;/tls.crt For example:\n$ kubectl -n oigns create secret tls governancedomain-tls-cert --key /scratch/OIGK8S/ssl/tls.key --cert /scratch/OIGK8S/ssl/tls.crt The output will look similar to the following:\nsecret/governancedomain-tls-cert created   Confirm that the secret is created by running the following command:\n$ kubectl get secret \u0026lt;domain_uid\u0026gt;-tls-cert -o yaml -n oigns For example:\n$ kubectl get secret governancedomain-tls-cert -o yaml -n oigns The output will look similar to the following:\napiVersion: v1 data: tls.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURGVENDQWYyZ0F3SUJBZ0lKQUl3ZjVRMWVxZnljTUEwR0NTcUdTSWIzRFFFQkN3VUFNQ0V4SHpBZEJnTlYKQkFNTUZtUmxiakF4WlhadkxuVnpMbTl5WVdOc1pTNWpiMjB3SGhjTk1qQXdPREV3TVRReE9UUXpXaGNOTWpFdwpPREV3TVRReE9UUXpXakFoTVI4d0hRWURWUVFEREJaa1pXNHdNV1YyYnk1MWN5NXZjbUZqYkdVdVkyOXRNSUlCCklqQU5CZ2txaGtpRzl3MEJBUUVGQUFPQ0FROEFNSUlCQ2dLQ0FRRUEyY0lpVUhwcTRVZzBhaGR6aXkycHY2cHQKSVIza2s5REd2eVRNY0syaWZQQ2dtUU5CdHV6VXNFN0l4c294eldITmU5RFpXRXJTSjVON3Ym1lTzJkMVd2NQp1aFhzbkFTbnkwY1N9xVDNQSlpDVk1MK0llZVFKdnhaVjZaWWU4V2FFL1NQSGJzczRjYy9wcG1mc3pxCnErUi83cXEyMm9ueHNHaE9vQ1h1TlQvMFF2WXVzMnNucGtueWRKRHUxelhGbDREYkFIZGMvamNVK0NPWWROeS8KT3Iza2JIV0FaTkR4OWxaZUREOTRmNXZLcUF2V0FkSVJZa2UrSmpNTHg0VHo2ZlM0VXoxbzdBSTVuSApPQ1ZMblV5U0JkaGVuWTNGNEdFU0wwbnorVlhFWjRWVjRucWNjRmo5cnJ0Q29pT1BBNlgvNGdxMEZJbi9Qd0lECkFRQUJvMUF3VGpBZEJnTlZIUTRFRmdRVWw1VnVpVDBDT0xGTzcxMFBlcHRxSC9DRWZyY3dId1lEVlIwakJCZ3cKRm9BVWw1VnVpVDBDT0xGTzcxMFBlcHRxSC9DRWZyY3dEQVlEVlIwVEJBVXdBd0VCL3pBTkJna3Foa2lHOXcwQgpBUXNGQUFPQ0FRRUFXdEN4b2ZmNGgrWXZEcVVpTFFtUnpqQkVBMHJCOUMwL1FWOG9JQzJ3d1hzYi9KaVNuMHdOCjNMdHppejc0aStEbk1yQytoNFQ3enRaSkc3NVluSGRKcmxQajgzVWdDLzhYTlFCSUNDbTFUa3RlVU1jWG0reG4KTEZEMHpReFhpVzV0N1FHcWtvK2FjeTlhUnUvN3JRMXlNSE9HdVVkTTZETzErNXF4cTdFNXFMamhyNEdKejV5OAoraW8zK25UcUVKMHFQOVRocG96RXhBMW80OEY0ZHJybWdqd3ROUldEQVpBYmYyV1JNMXFKWXhxTTJqdU1FQWNsCnFMek1TdEZUQ2o1UGFTQ0NUV1VEK3ZlSWtsRWRpaFdpRm02dzk3Y1diZ0lGMlhlNGk4L2szMmF1N2xUTDEvd28KU3Q2dHpsa20yV25uUFlVMzBnRURnVTQ4OU02Z1dybklpZz09Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K tls.key: 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 kind: Secret metadata: creationTimestamp: \u0026quot;2022-03-10T14:02:50Z\u0026quot; name: governancedomain-tls-cert namespace: oigns resourceVersion: \u0026quot;3319899\u0026quot; uid: 274cc960-281a-494c-a3e3-d93c3abd051f type: kubernetes.io/tls   Install NGINX Use helm to install NGINX.\nConfigure the repository   Add the Helm chart repository for installing NGINX using the following command:\n$ helm repo add stable https://kubernetes.github.io/ingress-nginx The output will look similar to the following:\n\u0026quot;stable\u0026quot; has been added to your repositories   Update the repository using the following command:\n$ helm repo update The output will look similar to the following:\nHang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \u0026quot;stable\u0026quot; chart repository Update Complete. Happy Helming!   Create a namespace   Create a Kubernetes namespace for NGINX:\n$ kubectl create namespace nginxssl The output will look similar to the following:\nnamespace/nginxssl created   Install NGINX using helm If you can connect directly to the master node IP address from a browser, then install NGINX with the --set controller.service.type=NodePort parameter.\nIf you are using a Managed Service for your Kubernetes cluster, for example Oracle Kubernetes Engine (OKE) on Oracle Cloud Infrastructure (OCI), and connect from a browser to the Load Balancer IP address, then use the --set controller.service.type=LoadBalancer parameter. This instructs the Managed Service to setup a Load Balancer to direct traffic to the NGINX ingress.\n  To install NGINX use the following helm command depending on if you are using NodePort or LoadBalancer:\na) Using NodePort\n$ helm install nginx-ingress -n nginxssl --set controller.extraArgs.default-ssl-certificate=oigns/governancedomain-tls-cert --set controller.service.type=NodePort --set controller.admissionWebhooks.enabled=false stable/ingress-nginx The output will look similar to the following:\n$ helm install nginx-ingress -n nginxssl --set controller.extraArgs.default-ssl-certificate=oigns/governancedomain-tls-cert --set controller.service.type=NodePort --set controller.admissionWebhooks.enabled=false stable/ingress-nginx NAME: nginx-ingress LAST DEPLOYED: Thu Mar 10 14:04:40 2022 NAMESPACE: nginxssl STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: The nginx-ingress controller has been installed. Get the application URL by running these commands: export HTTP_NODE_PORT=$(kubectl --namespace nginxssl get services -o jsonpath=\u0026quot;{.spec.ports[0].nodePort}\u0026quot; nginx-ingress-controller) export HTTPS_NODE_PORT=$(kubectl --namespace nginxssl get services -o jsonpath=\u0026quot;{.spec.ports[1].nodePort}\u0026quot; nginx-ingress-controller) export NODE_IP=$(kubectl --namespace nginxssl get nodes -o jsonpath=\u0026quot;{.items[0].status.addresses[1].address}\u0026quot;) echo \u0026quot;Visit http://$NODE_IP:$HTTP_NODE_PORT to access your application via HTTP.\u0026quot; echo \u0026quot;Visit https://$NODE_IP:$HTTPS_NODE_PORT to access your application via HTTPS.\u0026quot; An example Ingress that makes use of the controller: apiVersion: networking.k8s.io/v1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx name: example namespace: foo spec: ingressClassName: example-class rules: - host: www.example.com http: paths: - path: / pathType: Prefix backend: serviceName: exampleService servicePort: 80 path: / # This section is only required if TLS is to be enabled for the Ingress tls: - hosts: - www.example.com secretName: example-tls If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided: apiVersion: v1 kind: Secret metadata: name: example-tls namespace: foo data: tls.crt: \u0026lt;base64 encoded cert\u0026gt; tls.key: \u0026lt;base64 encoded key\u0026gt; type: kubernetes.io/tls b) Using LoadBalancer\n$ helm install nginx-ingress -n nginxssl --set controller.extraArgs.default-ssl-certificate=oigns/governancedomain-tls-cert --set controller.service.type=LoadBalancer --set controller.admissionWebhooks.enabled=false stable/ingress-nginx The output will look similar to the following:\nNAME: nginx-ingress LAST DEPLOYED: Thu Mar 10 14:06:42 2022 NAMESPACE: nginxssl STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: The ingress-nginx controller has been installed. It may take a few minutes for the LoadBalancer IP to be available. You can watch the status by running 'kubectl --namespace nginxssl get services -o wide -w nginx-ingress-ingress-nginx-controller' An example Ingress that makes use of the controller: apiVersion: networking.k8s.io/v1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx name: example namespace: foo spec: rules: - host: www.example.com http: paths: - path: / pathType: Prefix backend: service: name: exampleService port: 80 # This section is only required if TLS is to be enabled for the Ingress tls: - hosts: - www.example.com secretName: example-tls If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided: apiVersion: v1 kind: Secret metadata: name: example-tls namespace: foo data: tls.crt: \u0026lt;base64 encoded cert\u0026gt; tls.key: \u0026lt;base64 encoded key\u0026gt; type: kubernetes.io/tls   Setup routing rules for the domain   Setup routing rules by running the following commands:\n$ cd $WORKDIR/kubernetes/charts/ingress-per-domain Edit values.yaml and change the domainUID parameter to match your domainUID, for example domainUID: governancedomain. Change sslType to SSL and secretName to governancedomain-tls-cert. The file should look as follows:\n# Load balancer type. Supported values are: TRAEFIK, NGINX type: NGINX # Type of Configuration Supported Values are : NONSSL,SSL # tls: NONSSL tls: SSL # TLS secret name if the mode is SSL secretName: governancedomain-tls-cert # TimeOut value to be set for nginx parameters proxy-read-timeout and proxy-send-timeout nginxTimeOut: 180 # WLS domain as backend to the load balancer wlsDomain: domainUID: governancedomain adminServerName: AdminServer adminServerPort: 7001 soaClusterName: soa_cluster soaManagedServerPort: 8001 oimClusterName: oim_cluster oimManagedServerPort: 14000   Create an ingress for the domain   Create an Ingress for the domain (governancedomain-nginx), in the domain namespace by using the sample Helm chart:\n$ cd $WORKDIR $ helm install governancedomain-nginx kubernetes/charts/ingress-per-domain --namespace oigns --values kubernetes/charts/ingress-per-domain/values.yaml Note: The $WORKDIR/kubernetes/charts/ingress-per-domain/templates/nginx-ingress-k8s1.19.yaml and nginx-ingress.yaml has nginx.ingress.kubernetes.io/enable-access-log set to false. If you want to enable access logs then set this value to true before executing the command. Enabling access-logs can cause issues with disk space if not regularly maintained.\nFor example:\n$ cd $WORKDIR $ helm install governancedomain-nginx kubernetes/charts/ingress-per-domain --namespace oigns --values kubernetes/charts/ingress-per-domain/values.yaml The output will look similar to the following:\nNAME: governancedomain-nginx LAST DEPLOYED: Thu Mar 10 14:07:51 2022 NAMESPACE: oigns STATUS: deployed REVISION: 1 TEST SUITE: None   Run the following command to show the ingress is created successfully:\n$ kubectl get ing -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl get ing -n oigns The output will look similar to the following:\nNAME CLASS HOSTS ADDRESS PORTS AGE governancedomain-nginx \u0026lt;none\u0026gt; * x.x.x.x 80 49s   Find the node port of NGINX using the following command:\n$ kubectl get services -n nginxssl -o jsonpath=\u0026#34;{.spec.ports[1].nodePort}\u0026#34; nginx-ingress-ingress-nginx-controller The output will look similar to the following:\n32033   Run the following command to check the ingress:\n$ kubectl describe ing governancedomain-nginx -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl describe ing governancedomain-nginx -n oigns The output will look similar to the following:\nNamespace: oigns Address: 10.96.160.58 Default backend: default-http-backend:80 (\u0026lt;error: endpoints \u0026quot;default-http-backend\u0026quot; not found\u0026gt;) Rules: Host Path Backends ---- ---- -------- * /console governancedomain-adminserver:7001 (10.244.2.96:7001) /em governancedomain-adminserver:7001 (10.244.2.96:7001) /soa governancedomain-cluster-soa-cluster:8001 (10.244.2.97:8001) /integration governancedomain-cluster-soa-cluster:8001 (10.244.2.97:8001) /soa-infra governancedomain-cluster-soa-cluster:8001 (10.244.2.97:8001) /identity governancedomain-cluster-oim-cluster:14000 (10.244.2.98:14000) /admin governancedomain-cluster-oim-cluster:14000 (10.244.2.98:14000) /oim governancedomain-cluster-oim-cluster:14000 (10.244.2.98:14000) /sysadmin governancedomain-cluster-oim-cluster:14000 (10.244.2.98:14000) /workflowservice governancedomain-cluster-oim-cluster:14000 (10.244.2.98:14000) /xlWebApp governancedomain-cluster-oim-cluster:14000 (10.244.2.98:14000) /Nexaweb governancedomain-cluster-oim-cluster:14000 (10.244.2.98:14000) /callbackResponseService governancedomain-cluster-oim-cluster:14000 (10.244.2.98:14000) /spml-xsd governancedomain-cluster-oim-cluster:14000 (10.244.2.98:14000) /HTTPClnt governancedomain-cluster-oim-cluster:14000 (10.244.2.98:14000) /reqsvc governancedomain-cluster-oim-cluster:14000 (10.244.2.98:14000) /iam governancedomain-cluster-oim-cluster:14000 (10.244.2.98:14000) /provisioning-callback governancedomain-cluster-oim-cluster:14000 (10.244.2.98:14000) /CertificationCallbackService governancedomain-cluster-oim-cluster:14000 (10.244.2.98:14000) /ucs governancedomain-cluster-oim-cluster:14000 (10.244.2.98:14000) /FacadeWebApp governancedomain-cluster-oim-cluster:14000 (10.244.2.98:14000) /OIGUI governancedomain-cluster-oim-cluster:14000 (10.244.2.98:14000) /weblogic governancedomain-cluster-oim-cluster:14000 (10.244.2.98:14000) Annotations: kubernetes.io/ingress.class: nginx meta.helm.sh/release-name: governancedomain-nginx meta.helm.sh/release-namespace: oigns nginx.ingress.kubernetes.io/affinity: cookie nginx.ingress.kubernetes.io/configuration-snippet: more_set_input_headers \u0026quot;X-Forwarded-Proto: https\u0026quot;; more_set_input_headers \u0026quot;WL-Proxy-SSL: true\u0026quot;; nginx.ingress.kubernetes.io/enable-access-log: false nginx.ingress.kubernetes.io/ingress.allow-http: false nginx.ingress.kubernetes.io/proxy-buffer-size: 2000k Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Sync 17s (x2 over 28s) nginx-ingress-controller Scheduled for sync   To confirm that the new Ingress is successfully routing to the domain\u0026rsquo;s server pods, run the following command to send a request to the URL for the WebLogic ReadyApp framework:\nNote: If using a load balancer for your ingress replace ${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT} with ${LOADBALANCER-HOSTNAME}:${LOADBALANCER-PORT}.\n$ curl -v -k https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/weblogic/ready For example:\n$ curl -v -k https://masternode.example.com:32033/weblogic/ready The output will look similar to the following:\n$ curl -v -k https://masternode.example.com:32033/weblogic/ready * About to connect() to X.X.X.X port 32033 (#0) * Trying X.X.X.X... * Connected to masternode.example.com (X.X.X.X) port 32033 (#0) * Initializing NSS with certpath: sql:/etc/pki/nssdb * skipping SSL peer certificate verification * SSL connection using TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 * Server certificate: * subject: CN=masternode.example.com * start date: Nov 10 13:05:21 2021 GMT * expire date: Nov 10 13:05:21 2022 GMT * common name: masternode.example.com * issuer: CN=masternode.example.com \u0026gt; GET /weblogic/ready HTTP/1.1 \u0026gt; User-Agent: curl/7.29.0 \u0026gt; Host: X.X.X.X:32033 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; Server: nginx/1.19.1 \u0026lt; Date: Thu, 10 Mar 2022 14:09:57 GMT \u0026lt; Content-Length: 0 \u0026lt; Connection: keep-alive \u0026lt; Strict-Transport-Security: max-age=15724800; includeSubDomains \u0026lt; * Connection #0 to host X.X.X.X left intact   Verify that you can access the domain URL After setting up the NGINX ingress, verify that the domain applications are accessible through the NGINX ingress port (for example 32033) as per Validate Domain URLs \n"
},
{
	"uri": "/fmw-kubernetes/oam/manage-oam-domains/wlst-admin-operations/",
	"title": "b. WLST Administration Operations",
	"tags": [],
	"description": "Describes the steps for WLST administration using helper pod running in the same Kubernetes Cluster as OAM Domain.",
	"content": "To use WLST to administer the OAM domain, use the helper pod in the same Kubernetes cluster as the OAM Domain.\n  Check to see if the helper pod exists by running:\n$ kubectl get pods -n \u0026lt;domain_namespace\u0026gt; | grep helper For example:\n$ kubectl get pods -n oamns | grep helper The output should look similar to the following:\nhelper 1/1 Running 0 26h If the helper pod doesn\u0026rsquo;t exist then see Step 1 in Prepare your environment  to create it.\n  Run the following command to start a bash shell in the helper pod:\n$ kubectl exec -it helper -n \u0026lt;domain_namespace\u0026gt; -- /bin/bash For example:\n$ kubectl exec -it helper -n oamns -- /bin/bash This will take you into a bash shell in the running helper pod:\n[oracle@helper ~]$   Connect to WLST using the following command:\n$ cd $ORACLE_HOME/oracle_common/common/bin $ ./wlst.sh The output will look similar to the following:\nInitializing WebLogic Scripting Tool (WLST) ... Jython scans all the jar files it can find at first startup. Depending on the system, this process may take a few minutes to complete, and WLST may not return a prompt right away. Welcome to WebLogic Server Administration Scripting Shell Type help() for help on available commands wls:/offline\u0026gt;   To access t3 for the Administration Server connect as follows:\nwls:/offline\u0026gt; connect(\u0026#39;weblogic\u0026#39;,\u0026#39;\u0026lt;password\u0026gt;\u0026#39;,\u0026#39;t3://accessdomain-adminserver:7001\u0026#39;) The output will look similar to the following:\nConnecting to t3://accessdomain-adminserver:7001 with userid weblogic ... Successfully connected to Admin Server \u0026quot;AdminServer\u0026quot; that belongs to domain \u0026quot;accessdomain\u0026quot;. Warning: An insecure protocol was used to connect to the server. To ensure on-the-wire security, the SSL port or Admin port should be used instead. wls:/accessdomain/serverConfig/\u0026gt; Or to access t3 for the OAM Cluster service, connect as follows:\nconnect(\u0026#39;weblogic\u0026#39;,\u0026#39;\u0026lt;password\u0026gt;\u0026#39;,\u0026#39;t3://accessdomain-cluster-oam-cluster:14100\u0026#39;) The output will look similar to the following:\nConnecting to t3://accessdomain-cluster-oam-cluster:14100 with userid weblogic ... Successfully connected to managed Server \u0026quot;oam_server1\u0026quot; that belongs to domain \u0026quot;accessdomain\u0026quot;. Warning: An insecure protocol was used to connect to the server. To ensure on-the-wire security, the SSL port or Admin port should be used instead. wls:/accessdomain/serverConfig/\u0026gt;   Sample operations For a full list of WLST operations refer to WebLogic Server WLST Online and Offline Command Reference.\nDisplay servers wls:/accessdomain/serverConfig/\u0026gt; cd(\u0026#39;/Servers\u0026#39;) wls:/accessdomain/serverConfig/Servers\u0026gt; ls() dr-- AdminServer dr-- oam_policy_mgr1 dr-- oam_policy_mgr2 dr-- oam_policy_mgr3 dr-- oam_policy_mgr4 dr-- oam_policy_mgr5 dr-- oam_server1 dr-- oam_server2 dr-- oam_server3 dr-- oam_server4 dr-- oam_server5 wls:/accessdomain/serverConfig/Servers\u0026gt; Configure logging for managed servers Connect to the Administration Server and run the following:\nwls:/accessdomain/serverConfig/\u0026gt; domainRuntime() Location changed to domainRuntime tree. This is a read-only tree with DomainMBean as the root MBean. For more help, use help(\u0026#39;domainRuntime\u0026#39;) wls:/accessdomain/domainRuntime/\u0026gt; wls:/accessdomain/domainRuntime/\u0026gt; listLoggers(pattern=\u0026#34;oracle.oam.*\u0026#34;,target=\u0026#34;oam_server1\u0026#34;) ------------------------------------------+----------------- Logger | Level ------------------------------------------+----------------- oracle.oam | \u0026lt;Inherited\u0026gt; oracle.oam.admin.foundation.configuration | \u0026lt;Inherited\u0026gt; oracle.oam.admin.service.config | \u0026lt;Inherited\u0026gt; oracle.oam.agent | \u0026lt;Inherited\u0026gt; oracle.oam.agent-default | \u0026lt;Inherited\u0026gt; oracle.oam.audit | \u0026lt;Inherited\u0026gt; oracle.oam.binding | \u0026lt;Inherited\u0026gt; oracle.oam.certvalidation | \u0026lt;Inherited\u0026gt; oracle.oam.certvalidation.mbeans | \u0026lt;Inherited\u0026gt; oracle.oam.common.healthcheck | \u0026lt;Inherited\u0026gt; oracle.oam.common.runtimeent | \u0026lt;Inherited\u0026gt; oracle.oam.commonutil | \u0026lt;Inherited\u0026gt; oracle.oam.config | \u0026lt;Inherited\u0026gt; oracle.oam.controller | \u0026lt;Inherited\u0026gt; oracle.oam.default | \u0026lt;Inherited\u0026gt; oracle.oam.diagnostic | \u0026lt;Inherited\u0026gt; oracle.oam.engine.authn | \u0026lt;Inherited\u0026gt; oracle.oam.engine.authz | \u0026lt;Inherited\u0026gt; oracle.oam.engine.policy | \u0026lt;Inherited\u0026gt; oracle.oam.engine.ptmetadata | \u0026lt;Inherited\u0026gt; oracle.oam.engine.session | \u0026lt;Inherited\u0026gt; oracle.oam.engine.sso | \u0026lt;Inherited\u0026gt; oracle.oam.esso | \u0026lt;Inherited\u0026gt; oracle.oam.extensibility.lifecycle | \u0026lt;Inherited\u0026gt; oracle.oam.foundation.access | \u0026lt;Inherited\u0026gt; oracle.oam.idm | \u0026lt;Inherited\u0026gt; oracle.oam.install | \u0026lt;Inherited\u0026gt; oracle.oam.install.bootstrap | \u0026lt;Inherited\u0026gt; oracle.oam.install.mbeans | \u0026lt;Inherited\u0026gt; oracle.oam.ipf.rest.api | \u0026lt;Inherited\u0026gt; oracle.oam.oauth | \u0026lt;Inherited\u0026gt; oracle.oam.plugin | \u0026lt;Inherited\u0026gt; oracle.oam.proxy.oam | \u0026lt;Inherited\u0026gt; oracle.oam.proxy.oam.workmanager | \u0026lt;Inherited\u0026gt; oracle.oam.proxy.opensso | \u0026lt;Inherited\u0026gt; oracle.oam.pswd.service.provider | \u0026lt;Inherited\u0026gt; oracle.oam.replication | \u0026lt;Inherited\u0026gt; oracle.oam.user.identity.provider | \u0026lt;Inherited\u0026gt; wls:/accessdomain/domainRuntime/\u0026gt; Set the log level to TRACE:32:\nwls:/accessdomain/domainRuntime/\u0026gt; setLogLevel(target=\u0026#39;oam_server1\u0026#39;,logger=\u0026#39;oracle.oam\u0026#39;,level=\u0026#39;TRACE:32\u0026#39;,persist=\u0026#34;1\u0026#34;,addLogger=1) wls:/accessdomain/domainRuntime/\u0026gt; wls:/accessdomain/domainRuntime/\u0026gt; listLoggers(pattern=\u0026#34;oracle.oam.*\u0026#34;,target=\u0026#34;oam_server1\u0026#34;) ------------------------------------------+----------------- Logger | Level ------------------------------------------+----------------- oracle.oam | TRACE:32 oracle.oam.admin.foundation.configuration | \u0026lt;Inherited\u0026gt; oracle.oam.admin.service.config | \u0026lt;Inherited\u0026gt; oracle.oam.agent | \u0026lt;Inherited\u0026gt; oracle.oam.agent-default | \u0026lt;Inherited\u0026gt; oracle.oam.audit | \u0026lt;Inherited\u0026gt; oracle.oam.binding | \u0026lt;Inherited\u0026gt; oracle.oam.certvalidation | \u0026lt;Inherited\u0026gt; oracle.oam.certvalidation.mbeans | \u0026lt;Inherited\u0026gt; oracle.oam.common.healthcheck | \u0026lt;Inherited\u0026gt; oracle.oam.common.runtimeent | \u0026lt;Inherited\u0026gt; oracle.oam.commonutil | \u0026lt;Inherited\u0026gt; oracle.oam.config | \u0026lt;Inherited\u0026gt; oracle.oam.controller | \u0026lt;Inherited\u0026gt; oracle.oam.default | \u0026lt;Inherited\u0026gt; oracle.oam.diagnostic | \u0026lt;Inherited\u0026gt; oracle.oam.engine.authn | \u0026lt;Inherited\u0026gt; oracle.oam.engine.authz | \u0026lt;Inherited\u0026gt; oracle.oam.engine.policy | \u0026lt;Inherited\u0026gt; oracle.oam.engine.ptmetadata | \u0026lt;Inherited\u0026gt; oracle.oam.engine.session | \u0026lt;Inherited\u0026gt; oracle.oam.engine.sso | \u0026lt;Inherited\u0026gt; oracle.oam.esso | \u0026lt;Inherited\u0026gt; oracle.oam.extensibility.lifecycle | \u0026lt;Inherited\u0026gt; oracle.oam.foundation.access | \u0026lt;Inherited\u0026gt; oracle.oam.idm | \u0026lt;Inherited\u0026gt; oracle.oam.install | \u0026lt;Inherited\u0026gt; oracle.oam.install.bootstrap | \u0026lt;Inherited\u0026gt; oracle.oam.install.mbeans | \u0026lt;Inherited\u0026gt; oracle.oam.ipf.rest.api | \u0026lt;Inherited\u0026gt; oracle.oam.oauth | \u0026lt;Inherited\u0026gt; oracle.oam.plugin | \u0026lt;Inherited\u0026gt; oracle.oam.proxy.oam | \u0026lt;Inherited\u0026gt; oracle.oam.proxy.oam.workmanager | \u0026lt;Inherited\u0026gt; oracle.oam.proxy.opensso | \u0026lt;Inherited\u0026gt; oracle.oam.pswd.service.provider | \u0026lt;Inherited\u0026gt; oracle.oam.replication | \u0026lt;Inherited\u0026gt; oracle.oam.user.identity.provider | \u0026lt;Inherited\u0026gt; wls:/accessdomain/domainRuntime/\u0026gt; Verify that TRACE:32 log level is set by connecting to the Administration Server and viewing the logs:\n$ kubectl exec -it accessdomain-adminserver -n oamns -- /bin/bash [oracle@accessdomain-adminserver oracle]$ [oracle@accessdomain-adminserver oracle]$ cd /u01/oracle/user_projects/domains/accessdomain/servers/oam_server1/logs [oracle@accessdomain-adminserver logs]$ tail oam_server1-diagnostic.log 2022-03-07T10:26:14.793+00:00] [oam_server1] [TRACE:32] [] [oracle.oam.config] [tid: Configuration Store Observer] [userId: \u0026lt;anonymous\u0026gt;] [ecid: 8b3ac37b-c7cf-46dd-aeee-5ed67886be21-0000000b,0:1795] [APP: oam_server] [partition-name: DOMAIN] [tenant-name: GLOBAL] [SRC_CLASS: oracle.security.am.admin.config.util.observable.ObservableConfigStore$StoreWatcher] [SRC_METHOD: run] Start of run before start of detection at 1,635,848,774,793. Detector: oracle.security.am.admin.config.util.observable.DbStoreChangeDetector:Database configuration store:DSN:jdbc/oamds. Monitor: { StoreMonitor: { disabled: \u0026#39;false\u0026#39; } } [2022-03-07T10:26:14.793+00:00] [oam_server1] [TRACE] [] [oracle.oam.config] [tid: Configuration Store Observer] [userId: \u0026lt;anonymous\u0026gt;] [ecid: 8b3ac37b-c7cf-46dd-aeee-5ed67886be21-0000000b,0:1795] [APP: oam_server] [partition-name: DOMAIN] [tenant-name: GLOBAL] [SRC_CLASS: oracle.security.am.admin.config.util.store.StoreUtil] [SRC_METHOD: getContainerProperty] Configuration property CONFIG_HISTORY not specified [2022-03-07T10:26:14.793+00:00] [oam_server1] [TRACE] [] [oracle.oam.config] [tid: Configuration Store Observer] [userId: \u0026lt;anonymous\u0026gt;] [ecid: 8b3ac37b-c7cf-46dd-aeee-5ed67886be21-0000000b,0:1795] [APP: oam_server] [partition-name: DOMAIN] [tenant-name: GLOBAL] [SRC_CLASS: oracle.security.am.admin.config.util.store.StoreUtil] [SRC_METHOD: getContainerProperty] Configuration property CONFIG not specified [2022-03-07T10:26:14.795+00:00] [oam_server1] [TRACE:32] [] [oracle.oam.config] [tid: Configuration Store Observer] [userId: \u0026lt;anonymous\u0026gt;] [ecid: 8b3ac37b-c7cf-46dd-aeee-5ed67886be21-0000000b,0:1795] [APP: oam_server] [partition-name: DOMAIN] [tenant-name: GLOBAL] [SRC_CLASS: oracle.security.am.admin.config.util.store.DbStore] [SRC_METHOD: getSelectSQL] SELECT SQL:SELECT version from IDM_OBJECT_STORE where id = ? and version = (select max(version) from IDM_OBJECT_STORE where id = ?) [2022-03-07T10:26:14.797+00:00] [oam_server1] [TRACE] [] [oracle.oam.config] [tid: Configuration Store Observer] [userId: \u0026lt;anonymous\u0026gt;] [ecid: 8b3ac37b-c7cf-46dd-aeee-5ed67886be21-0000000b,0:1795] [APP: oam_server] [partition-name: DOMAIN] [tenant-name: GLOBAL] [SRC_CLASS: oracle.security.am.admin.config.util.store.DbStore] [SRC_METHOD: load] Time (ms) to load key CONFIG:-1{FIELD_TYPES=INT, SELECT_FIELDS=SELECT version from IDM_OBJECT_STORE }:4 Performing WLST Administration via SSL   By default the SSL port is not enabled for the Administration Server or OAM Managed Servers. To configure the SSL port for the Administration Server and Managed Servers login to WebLogic Administration console https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/console and navigate to Lock \u0026amp; Edit -\u0026gt; Environment -\u0026gt;Servers -\u0026gt; server_name -\u0026gt;Configuration -\u0026gt; General -\u0026gt; SSL Listen Port Enabled -\u0026gt; Provide SSL Port ( For Administration Server: 7002 and for OAM Managed Server (oam_server1): 14101) - \u0026gt; Save -\u0026gt; Activate Changes.\nNote: If configuring the OAM Managed Servers for SSL you must enable SSL on the same port for all servers (oam_server1 through oam_server5)\n  Create a myscripts directory as follows:\n$ cd $WORKDIR/kubernetes/ $ mkdir myscripts $ cd myscripts For example:\n$ cd $WORKDIR/kubernetes/ $ mkdir myscripts $ cd myscripts   Create a sample yaml template file in the myscripts directory called \u0026lt;domain_uid\u0026gt;-adminserver-ssl.yaml to create a Kubernetes service for the Administration Server:\nNote: Update the domainName, domainUID and namespace based on your environment. For example:\napiVersion: v1 kind: Service metadata: labels: serviceType: SERVER weblogic.domainName: accessdomain weblogic.domainUID: accessdomain weblogic.resourceVersion: domain-v2 weblogic.serverName: AdminServer name: accessdomain-adminserverssl namespace: oamns spec: clusterIP: None ports: - name: default port: 7002 protocol: TCP targetPort: 7002 selector: weblogic.createdByOperator: \u0026quot;true\u0026quot; weblogic.domainUID: accessdomain weblogic.serverName: AdminServer type: ClusterIP and the following sample yaml template file \u0026lt;domain_uid\u0026gt;-oamcluster-ssl.yaml for the OAM Managed Server:\napiVersion: v1 kind: Service metadata: labels: serviceType: SERVER weblogic.domainName: accessdomain weblogic.domainUID: accessdomain weblogic.resourceVersion: domain-v2 name: accessdomain-oamcluster-ssl namespace: oamns spec: clusterIP: None ports: - name: default port: 14101 protocol: TCP targetPort: 14101 selector: weblogic.clusterName: oam_cluster weblogic.createdByOperator: \u0026quot;true\u0026quot; weblogic.domainUID: accessdomain type: ClusterIP   Apply the template using the following command for the AdminServer:\n$ kubectl apply -f \u0026lt;domain_uid\u0026gt;-adminserver-ssl.yaml For example:\n$ kubectl apply -f accessdomain-adminserver-ssl.yaml service/accessdomain-adminserverssl created and using the following command for the OAM Managed Server:\n$ kubectl apply -f \u0026lt;domain_uid\u0026gt;-oamcluster-ssl.yaml For example:\n$ kubectl apply -f accessdomain-oamcluster-ssl.yaml service/accessdomain-oamcluster-ssl created   Validate that the Kubernetes Services to access SSL ports are created successfully:\n$ kubectl get svc -n \u0026lt;domain_namespace\u0026gt; |grep ssl For example:\n$ kubectl get svc -n oamns |grep ssl The output will look similar to the following:\naccessdomain-adminserverssl ClusterIP None \u0026lt;none\u0026gt; 7002/TCP 102s accessdomain-oamcluster-ssl ClusterIP None \u0026lt;none\u0026gt; 14101/TCP 35s   Inside the bash shell of the running helper pod, run the following:\n[oracle@helper bin]$ export WLST_PROPERTIES=\u0026#34;-Dweblogic.security.SSL.ignoreHostnameVerification=true -Dweblogic.security.TrustKeyStore=DemoTrust\u0026#34; [oracle@helper bin]$ cd /u01/oracle/oracle_common/common/bin [oracle@helper bin]$ ./wlst.sh Initializing WebLogic Scripting Tool (WLST) ... Welcome to WebLogic Server Administration Scripting Shell Type help() for help on available commands wls:/offline\u0026gt; To connect to the Administration Server t3s service:\nwls:/offline\u0026gt; connect(\u0026#39;weblogic\u0026#39;,\u0026#39;\u0026lt;password\u0026gt;\u0026#39;,\u0026#39;t3s://accessdomain-adminserverssl:7002\u0026#39;) Connecting to t3s://accessdomain-adminserverssl:7002 with userid weblogic ... \u0026lt;Mar 7, 2022 10:42:05 AM GMT\u0026gt; \u0026lt;Info\u0026gt; \u0026lt;Security\u0026gt; \u0026lt;BEA-090905\u0026gt; \u0026lt;Disabling the CryptoJ JCE Provider self-integrity check for better startup performance. To enable this check, specify -Dweblogic.security.allowCryptoJDefaultJCEVerification=true.\u0026gt; \u0026lt;Mar 7, 2022 10:42:05 AM GMT\u0026gt; \u0026lt;Info\u0026gt; \u0026lt;Security\u0026gt; \u0026lt;BEA-090906\u0026gt; \u0026lt;Changing the default Random Number Generator in RSA CryptoJ from ECDRBG128 to HMACDRBG. To disable this change, specify -Dweblogic.security.allowCryptoJDefaultPRNG=true.\u0026gt; \u0026lt;Mar 7, 2022 10:42:05 AM GMT\u0026gt; \u0026lt;Info\u0026gt; \u0026lt;Security\u0026gt; \u0026lt;BEA-090909\u0026gt; \u0026lt;Using the configured custom SSL Hostname Verifier implementation: weblogic.security.utils.SSLWLSHostnameVerifier$NullHostnameVerifier.\u0026gt; Successfully connected to Admin Server \u0026#34;AdminServer\u0026#34; that belongs to domain \u0026#34;accessdomain\u0026#34;. wls:/accessdomain/serverConfig/\u0026gt; To connect to the OAM Managed Server t3s service:\nwls:/offline\u0026gt; connect(\u0026#39;weblogic\u0026#39;,\u0026#39;\u0026lt;password\u0026gt;\u0026#39;,\u0026#39;t3s://accessdomain-oamcluster-ssl:14101\u0026#39;) Connecting to t3s://accessdomain-oamcluster-ssl:14101 with userid weblogic ... \u0026lt;Mar 7, 2022 10:43:16 AM GMT\u0026gt; \u0026lt;Info\u0026gt; \u0026lt;Security\u0026gt; \u0026lt;BEA-090905\u0026gt; \u0026lt;Disabling the CryptoJ JCE Provider self-integrity check for better startup performance. To enable this check, specify -Dweblogic.security.allowCryptoJDefaultJCEVerification=true.\u0026gt; \u0026lt;Mar 7, 2022 10:43:16 AM GMT\u0026gt; \u0026lt;Info\u0026gt; \u0026lt;Security\u0026gt; \u0026lt;BEA-090906\u0026gt; \u0026lt;Changing the default Random Number Generator in RSA CryptoJ from ECDRBG128 to HMACDRBG. To disable this change, specify -Dweblogic.security.allowCryptoJDefaultPRNG=true.\u0026gt; \u0026lt;Mar 7, 2022 10:43:16 AM GMT\u0026gt; \u0026lt;Info\u0026gt; \u0026lt;Security\u0026gt; \u0026lt;BEA-090909\u0026gt; \u0026lt;Using the configured custom SSL Hostname Verifier implementation: weblogic.security.utils.SSLWLSHostnameVerifier$NullHostnameVerifier.\u0026gt; Successfully connected to managed Server \u0026#34;oam_server1\u0026#34; that belongs to domain \u0026#34;accessdomain\u0026#34;.   "
},
{
	"uri": "/fmw-kubernetes/oid/manage-oid-containers/monitoring-oid-instance/",
	"title": "c) Monitoring an Oracle Internet Directory Instance",
	"tags": [],
	"description": "Describes the steps for Monitoring the Oracle Internet Directory environment.",
	"content": " Introduction Install Prometheus and Grafana  Create a Kubernetes namespace Add Prometheus and Grafana Helm repositories Install the Prometheus operator View Prometheus and Grafana objects created Add the NodePort   Verify using Grafana GUI  Introduction After the Oracle Internet Directory instance (OID) is set up you can monitor it using Prometheus and Grafana.\nInstall Prometheus and Grafana Create a Kubernetes namespace   Create a Kubernetes namespace to provide a scope for Prometheus and Grafana objects such as pods and services that you create in the environment. To create your namespace issue the following command:\n$ kubectl create namespace \u0026lt;namespace\u0026gt; For example:\n$ kubectl create namespace monitoring The output will look similar to the following:\nnamespace/monitoring created   Add Prometheus and Grafana Helm repositories   Add the Prometheus and Grafana Helm repositories by issuing the following command:\n$ helm repo add prometheus https://prometheus-community.github.io/helm-charts The output will look similar to the following:\n\u0026#34;prometheus\u0026#34; has been added to your repositories   Run the following command to update the repositories:\n$ helm repo update The output will look similar to the following:\nHang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \u0026#34;stable\u0026#34; chart repository ...Successfully got an update from the \u0026#34;prometheus\u0026#34; chart repository ...Successfully got an update from the \u0026#34;prometheus-community\u0026#34; chart repository Update Complete. Happy Helming!   Install the Prometheus operator   Install the Prometheus operator using the helm command:\n$ helm install \u0026lt;release_name\u0026gt; prometheus/kube-prometheus-stack -n \u0026lt;namespace\u0026gt; For example:\n$ helm install monitoring prometheus/kube-prometheus-stack -n monitoring The output should look similar to the following:\nNAME: monitoring LAST DEPLOYED: Fri Mar 18 09:57:54 2022 NAMESPACE: monitoring STATUS: deployed REVISION: 1 NOTES: kube-prometheus-stack has been installed. Check its status by running: kubectl --namespace monitoring get pods -l \u0026#34;release=monitoring\u0026#34; Visit https://github.com/prometheus-operator/kube-prometheus for instructions on how to create \u0026amp; configure Alertmanager and Prometheus instances using the Operator.   View Prometheus and Grafana Objects created View the objects created for Prometheus and Grafana by issuing the following command:\n$ kubectl get all,service,pod -o wide -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl get all,service,pod -o wide -n monitoring The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/alertmanager-monitoring-kube-prometheus-alertmanager-0 2/2 Running 0 36s 10.244.1.78 \u0026lt;worker-node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/monitoring-grafana-578f79599c-qc9gd 3/3 Running 0 47s 10.244.2.200 \u0026lt;worker-node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/monitoring-kube-prometheus-operator-65cdf7995-kndgg 1/1 Running 0 47s 10.244.2.199 \u0026lt;worker-node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/monitoring-kube-state-metrics-56bfd4f44f-85l4p 1/1 Running 0 47s 10.244.1.76 \u0026lt;worker-node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/monitoring-prometheus-node-exporter-g2x9g 1/1 Running 0 47s 100.102.48.121 \u0026lt;master-node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/monitoring-prometheus-node-exporter-p9kkq 1/1 Running 0 47s 100.102.48.84 \u0026lt;worker-node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/monitoring-prometheus-node-exporter-rzhrd 1/1 Running 0 47s 100.102.48.28 \u0026lt;worker-node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/prometheus-monitoring-kube-prometheus-prometheus-0 2/2 Running 0 35s 10.244.1.79 \u0026lt;worker-node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service/alertmanager-operated ClusterIP None \u0026lt;none\u0026gt; 9093/TCP,9094/TCP,9094/UDP 36s app.kubernetes.io/name=alertmanager service/monitoring-grafana ClusterIP 10.110.193.30 \u0026lt;none\u0026gt; 80/TCP 47s app.kubernetes.io/instance=monitoring,app.kubernetes.io/name=grafana service/monitoring-kube-prometheus-alertmanager ClusterIP 10.104.2.37 \u0026lt;none\u0026gt; 9093/TCP 47s alertmanager=monitoring-kube-prometheus-alertmanager,app.kubernetes.io/name=alertmanager service/monitoring-kube-prometheus-operator ClusterIP 10.99.162.229 \u0026lt;none\u0026gt; 443/TCP 47s app=kube-prometheus-stack-operator,release=monitoring service/monitoring-kube-prometheus-prometheus ClusterIP 10.108.161.46 \u0026lt;none\u0026gt; 9090/TCP 47s app.kubernetes.io/name=prometheus,prometheus=monitoring-kube-prometheus-prometheus service/monitoring-kube-state-metrics ClusterIP 10.111.162.185 \u0026lt;none\u0026gt; 8080/TCP 47s app.kubernetes.io/instance=monitoring,app.kubernetes.io/name=kube-state-metrics service/monitoring-prometheus-node-exporter ClusterIP 10.109.21.136 \u0026lt;none\u0026gt; 9100/TCP 47s app=prometheus-node-exporter,release=monitoring service/prometheus-operated ClusterIP None \u0026lt;none\u0026gt; 9090/TCP 35s app.kubernetes.io/name=prometheus NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE CONTAINERS IMAGES SELECTOR daemonset.apps/monitoring-prometheus-node-exporter 3 3 3 3 3 \u0026lt;none\u0026gt; 47s node-exporter quay.io/prometheus/node-exporter:v1.3.1 app=prometheus-node-exporter,release=monitoring NAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTOR deployment.apps/monitoring-grafana 1/1 1 1 47s grafana-sc-dashboard,grafana-sc-datasources,grafana quay.io/kiwigrid/k8s-sidecar:1.15.6,quay.io/kiwigrid/k8s-sidecar:1.15.6,grafana/grafana:8.4.2 app.kubernetes.io/instance=monitoring,app.kubernetes.io/name=grafana deployment.apps/monitoring-kube-prometheus-operator 1/1 1 1 47s kube-prometheus-stack quay.io/prometheus-operator/prometheus-operator:v0.55.0 app=kube-prometheus-stack-operator,release=monitoring deployment.apps/monitoring-kube-state-metrics 1/1 1 1 47s kube-state-metrics k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.4.1 app.kubernetes.io/instance=monitoring,app.kubernetes.io/name=kube-state-metrics NAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTOR replicaset.apps/monitoring-grafana-578f79599c 1 1 1 47s grafana-sc-dashboard,grafana-sc-datasources,grafana quay.io/kiwigrid/k8s-sidecar:1.15.6,quay.io/kiwigrid/k8s-sidecar:1.15.6,grafana/grafana:8.4.2 app.kubernetes.io/instance=monitoring,app.kubernetes.io/name=grafana,pod-template-hash=578f79599c replicaset.apps/monitoring-kube-prometheus-operator-65cdf7995 1 1 1 47s kube-prometheus-stack quay.io/prometheus-operator/prometheus-operator:v0.55.0 app=kube-prometheus-stack-operator,pod-template-hash=65cdf7995,release=monitoring replicaset.apps/monitoring-kube-state-metrics-56bfd4f44f 1 1 1 47s kube-state-metrics k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.4.1 app.kubernetes.io/instance=monitoring,app.kubernetes.io/name=kube-state-metrics,pod-template-hash=56bfd4f44f NAME READY AGE CONTAINERS IMAGES statefulset.apps/alertmanager-monitoring-kube-prometheus-alertmanager 1/1 36s alertmanager,config-reloader quay.io/prometheus/alertmanager:v0.23.0,quay.io/prometheus-operator/prometheus-config-reloader:v0.55.0 statefulset.apps/prometheus-monitoring-kube-prometheus-prometheus 1/1 35s prometheus,config-reloader quay.io/prometheus/prometheus:v2.33.5,quay.io/prometheus-operator/prometheus-config-reloader:v0.55.0 Add the NodePort   Edit the grafana service to add the NodePort:\n$ kubectl edit service/\u0026lt;deployment_name\u0026gt;-grafana -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl edit service/monitoring-grafana -n monitoring Note: This opens an edit session for the domain where parameters can be changed using standard vi commands.\nChange the ports entry and add nodePort: 30091 and type: NodePort:\n ports: - name: http-web nodePort: 30091 port: 80 protocol: TCP targetPort: 3000 selector: app.kubernetes.io/instance: monitoring app.kubernetes.io/name: grafana sessionAffinity: None type: NodePort   Save the file and exit (:wq).\n  Verify Using Grafana GUI   Access the Grafana GUI using http://\u0026lt;HostIP\u0026gt;:\u0026lt;nodeport\u0026gt; and login with admin/prom-operator. Change the password when prompted.\n  Download the K8 Cluster Detail Dashboard json file from: https://grafana.com/grafana/dashboards/10856.\n  Import the Grafana dashboard by navigating on the left hand menu to Create \u0026gt; Import. Click Upload JSON file and select the json downloaded file. In the Prometheus drop down box select Prometheus. Click Import. The dashboard should be displayed.\n  Verify your installation by viewing some of the customized dashboard views.\n  "
},
{
	"uri": "/fmw-kubernetes/oud/manage-oud-containers/monitoring-oud-instance/",
	"title": "c) Monitoring an Oracle Unified Directory Instance",
	"tags": [],
	"description": "Describes the steps for Monitoring the Oracle Unified Directory environment.",
	"content": " Introduction Install Prometheus and Grafana  Create a Kubernetes namespace Add Prometheus and Grafana Helm repositories Install the Prometheus operator View Prometheus and Grafana objects created Add the NodePort   Verify using Grafana GUI  Introduction After the Oracle Unified Directory instance (OUD) is set up you can monitor it using Prometheus and Grafana.\nInstall Prometheus and Grafana Create a Kubernetes namespace   Create a Kubernetes namespace to provide a scope for Prometheus and Grafana objects such as pods and services that you create in the environment. To create your namespace issue the following command:\n$ kubectl create namespace \u0026lt;namespace\u0026gt; For example:\n$ kubectl create namespace monitoring The output will look similar to the following:\nnamespace/monitoring created   Add Prometheus and Grafana Helm repositories   Add the Prometheus and Grafana Helm repositories by issuing the following command:\n$ helm repo add prometheus https://prometheus-community.github.io/helm-charts The output will look similar to the following:\n\u0026#34;prometheus\u0026#34; has been added to your repositories   Run the following command to update the repositories:\n$ helm repo update The output will look similar to the following:\nHang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \u0026#34;stable\u0026#34; chart repository ...Successfully got an update from the \u0026#34;prometheus\u0026#34; chart repository ...Successfully got an update from the \u0026#34;prometheus-community\u0026#34; chart repository Update Complete. Happy Helming!   Install the Prometheus operator   Install the Prometheus operator using the helm command:\n$ helm install \u0026lt;release_name\u0026gt; prometheus/kube-prometheus-stack -n \u0026lt;namespace\u0026gt; For example:\n$ helm install monitoring prometheus/kube-prometheus-stack -n monitoring The output should look similar to the following:\nNAME: monitoring LAST DEPLOYED: Fri Mar 18 09:57:54 2022 NAMESPACE: monitoring STATUS: deployed REVISION: 1 NOTES: kube-prometheus-stack has been installed. Check its status by running: kubectl --namespace monitoring get pods -l \u0026#34;release=monitoring\u0026#34; Visit https://github.com/prometheus-operator/kube-prometheus for instructions on how to create \u0026amp; configure Alertmanager and Prometheus instances using the Operator.   View Prometheus and Grafana Objects created View the objects created for Prometheus and Grafana by issuing the following command:\n$ kubectl get all,service,pod -o wide -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl get all,service,pod -o wide -n monitoring The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/alertmanager-monitoring-kube-prometheus-alertmanager-0 2/2 Running 0 36s 10.244.1.78 \u0026lt;worker-node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/monitoring-grafana-578f79599c-qc9gd 3/3 Running 0 47s 10.244.2.200 \u0026lt;worker-node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/monitoring-kube-prometheus-operator-65cdf7995-kndgg 1/1 Running 0 47s 10.244.2.199 \u0026lt;worker-node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/monitoring-kube-state-metrics-56bfd4f44f-85l4p 1/1 Running 0 47s 10.244.1.76 \u0026lt;worker-node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/monitoring-prometheus-node-exporter-g2x9g 1/1 Running 0 47s 100.102.48.121 \u0026lt;master-node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/monitoring-prometheus-node-exporter-p9kkq 1/1 Running 0 47s 100.102.48.84 \u0026lt;worker-node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/monitoring-prometheus-node-exporter-rzhrd 1/1 Running 0 47s 100.102.48.28 \u0026lt;worker-node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/prometheus-monitoring-kube-prometheus-prometheus-0 2/2 Running 0 35s 10.244.1.79 \u0026lt;worker-node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service/alertmanager-operated ClusterIP None \u0026lt;none\u0026gt; 9093/TCP,9094/TCP,9094/UDP 36s app.kubernetes.io/name=alertmanager service/monitoring-grafana ClusterIP 10.110.193.30 \u0026lt;none\u0026gt; 80/TCP 47s app.kubernetes.io/instance=monitoring,app.kubernetes.io/name=grafana service/monitoring-kube-prometheus-alertmanager ClusterIP 10.104.2.37 \u0026lt;none\u0026gt; 9093/TCP 47s alertmanager=monitoring-kube-prometheus-alertmanager,app.kubernetes.io/name=alertmanager service/monitoring-kube-prometheus-operator ClusterIP 10.99.162.229 \u0026lt;none\u0026gt; 443/TCP 47s app=kube-prometheus-stack-operator,release=monitoring service/monitoring-kube-prometheus-prometheus ClusterIP 10.108.161.46 \u0026lt;none\u0026gt; 9090/TCP 47s app.kubernetes.io/name=prometheus,prometheus=monitoring-kube-prometheus-prometheus service/monitoring-kube-state-metrics ClusterIP 10.111.162.185 \u0026lt;none\u0026gt; 8080/TCP 47s app.kubernetes.io/instance=monitoring,app.kubernetes.io/name=kube-state-metrics service/monitoring-prometheus-node-exporter ClusterIP 10.109.21.136 \u0026lt;none\u0026gt; 9100/TCP 47s app=prometheus-node-exporter,release=monitoring service/prometheus-operated ClusterIP None \u0026lt;none\u0026gt; 9090/TCP 35s app.kubernetes.io/name=prometheus NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE CONTAINERS IMAGES SELECTOR daemonset.apps/monitoring-prometheus-node-exporter 3 3 3 3 3 \u0026lt;none\u0026gt; 47s node-exporter quay.io/prometheus/node-exporter:v1.3.1 app=prometheus-node-exporter,release=monitoring NAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTOR deployment.apps/monitoring-grafana 1/1 1 1 47s grafana-sc-dashboard,grafana-sc-datasources,grafana quay.io/kiwigrid/k8s-sidecar:1.15.6,quay.io/kiwigrid/k8s-sidecar:1.15.6,grafana/grafana:8.4.2 app.kubernetes.io/instance=monitoring,app.kubernetes.io/name=grafana deployment.apps/monitoring-kube-prometheus-operator 1/1 1 1 47s kube-prometheus-stack quay.io/prometheus-operator/prometheus-operator:v0.55.0 app=kube-prometheus-stack-operator,release=monitoring deployment.apps/monitoring-kube-state-metrics 1/1 1 1 47s kube-state-metrics k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.4.1 app.kubernetes.io/instance=monitoring,app.kubernetes.io/name=kube-state-metrics NAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTOR replicaset.apps/monitoring-grafana-578f79599c 1 1 1 47s grafana-sc-dashboard,grafana-sc-datasources,grafana quay.io/kiwigrid/k8s-sidecar:1.15.6,quay.io/kiwigrid/k8s-sidecar:1.15.6,grafana/grafana:8.4.2 app.kubernetes.io/instance=monitoring,app.kubernetes.io/name=grafana,pod-template-hash=578f79599c replicaset.apps/monitoring-kube-prometheus-operator-65cdf7995 1 1 1 47s kube-prometheus-stack quay.io/prometheus-operator/prometheus-operator:v0.55.0 app=kube-prometheus-stack-operator,pod-template-hash=65cdf7995,release=monitoring replicaset.apps/monitoring-kube-state-metrics-56bfd4f44f 1 1 1 47s kube-state-metrics k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.4.1 app.kubernetes.io/instance=monitoring,app.kubernetes.io/name=kube-state-metrics,pod-template-hash=56bfd4f44f NAME READY AGE CONTAINERS IMAGES statefulset.apps/alertmanager-monitoring-kube-prometheus-alertmanager 1/1 36s alertmanager,config-reloader quay.io/prometheus/alertmanager:v0.23.0,quay.io/prometheus-operator/prometheus-config-reloader:v0.55.0 statefulset.apps/prometheus-monitoring-kube-prometheus-prometheus 1/1 35s prometheus,config-reloader quay.io/prometheus/prometheus:v2.33.5,quay.io/prometheus-operator/prometheus-config-reloader:v0.55.0 Add the NodePort   Edit the grafana service to add the NodePort:\n$ kubectl edit service/\u0026lt;deployment_name\u0026gt;-grafana -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl edit service/monitoring-grafana -n monitoring Note: This opens an edit session for the domain where parameters can be changed using standard vi commands.\nChange the ports entry and add nodePort: 30091 and type: NodePort:\n ports: - name: http-web nodePort: 30091 port: 80 protocol: TCP targetPort: 3000 selector: app.kubernetes.io/instance: monitoring app.kubernetes.io/name: grafana sessionAffinity: None type: NodePort   Save the file and exit (:wq).\n  Verify Using Grafana GUI   Access the Grafana GUI using http://\u0026lt;HostIP\u0026gt;:\u0026lt;nodeport\u0026gt; and login with admin/prom-operator. Change the password when prompted.\n  Download the K8 Cluster Detail Dashboard json file from: https://grafana.com/grafana/dashboards/10856.\n  Import the Grafana dashboard by navigating on the left hand menu to Create \u0026gt; Import. Click Upload JSON file and select the json downloaded file. In the Prometheus drop down box select Prometheus. Click Import. The dashboard should be displayed.\n  Verify your installation by viewing some of the customized dashboard views.\n  "
},
{
	"uri": "/fmw-kubernetes/oudsm/manage-oudsm-containers/monitoring-oudsm-instance/",
	"title": "c) Monitoring an Oracle Unified Directory Services Manager Instance",
	"tags": [],
	"description": "Describes the steps for Monitoring the Oracle Unified Directory Services Manager environment.",
	"content": " Introduction Install Prometheus and Grafana  Create a Kubernetes namespace Add Prometheus and Grafana Helm repositories Install the Prometheus operator View Prometheus and Grafana Objects Created Add the NodePort   Verify Using Grafana GUI  Introduction After the Oracle Unified Directory Services Manager instance is set up you can monitor it using Prometheus and Grafana.\nInstall Prometheus and Grafana Create a Kubernetes namespace   Create a Kubernetes namespace to provide a scope for Prometheus and Grafana objects such as pods and services that you create in the environment. To create your namespace issue the following command:\n$ kubectl create namespace \u0026lt;namespace\u0026gt; For example:\n$ kubectl create namespace monitoring The output will look similar to the following:\nnamespace/monitoring created   Add Prometheus and Grafana Helm repositories   Add the Prometheus and Grafana Helm repositories by issuing the following command:\n$ helm repo add prometheus https://prometheus-community.github.io/helm-charts The output will look similar to the following:\n\u0026#34;prometheus\u0026#34; has been added to your repositories   Run the following command to update the repositories:\n$ helm repo update The output will look similar to the following:\nHang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \u0026#34;stable\u0026#34; chart repository ...Successfully got an update from the \u0026#34;prometheus\u0026#34; chart repository ...Successfully got an update from the \u0026#34;prometheus-community\u0026#34; chart repository Update Complete. Happy Helming!   Install the Prometheus operator   Install the Prometheus operator using the helm command:\n$ helm install \u0026lt;release_name\u0026gt; prometheus/kube-prometheus-stack -n \u0026lt;namespace\u0026gt; For example:\n$ helm install monitoring prometheus/kube-prometheus-stack -n monitoring The output should look similar to the following:\nNAME: monitoring LAST DEPLOYED: Thu Mar 24 16:29:23 2022 NAMESPACE: monitoring STATUS: deployed REVISION: 1 NOTES: kube-prometheus-stack has been installed. Check its status by running: kubectl --namespace monitoring get pods -l \u0026#34;release=monitoring\u0026#34; Visit https://github.com/prometheus-operator/kube-prometheus for instructions on how to create \u0026amp; configure Alertmanager and Prometheus instances using the Operator.   View Prometheus and Grafana Objects created View the objects created for Prometheus and Grafana by issuing the following command:\n$ kubectl get all,service,pod -o wide -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl get all,service,pod -o wide -n monitoring The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/alertmanager-monitoring-kube-prometheus-alertmanager-0 2/2 Running 0 27s 10.244.2.141 \u0026lt;worker-node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/monitoring-grafana-578f79599c-qqdfb 2/3 Running 0 34s 10.244.1.127 \u0026lt;worker-node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/monitoring-kube-prometheus-operator-65cdf7995-w6btr 1/1 Running 0 34s 10.244.1.126 \u0026lt;worker-node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/monitoring-kube-state-metrics-56bfd4f44f-5ls8t 1/1 Running 0 34s 10.244.2.139 \u0026lt;worker-node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/monitoring-prometheus-node-exporter-5b2f6 1/1 Running 0 34s 100.102.48.84 \u0026lt;worker-node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/monitoring-prometheus-node-exporter-fw9xh 1/1 Running 0 34s 100.102.48.28 \u0026lt;worker-node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/monitoring-prometheus-node-exporter-s5n9g 1/1 Running 0 34s 100.102.48.121 \u0026lt;master-node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/prometheus-monitoring-kube-prometheus-prometheus-0 2/2 Running 0 26s 10.244.1.128 \u0026lt;worker-node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service/alertmanager-operated ClusterIP None \u0026lt;none\u0026gt; 9093/TCP,9094/TCP,9094/UDP 27s app.kubernetes.io/name=alertmanager service/monitoring-grafana ClusterIP 10.110.97.252 \u0026lt;none\u0026gt; 80/TCP 34s app.kubernetes.io/instance=monitoring,app.kubernetes.io/name=grafana service/monitoring-kube-prometheus-alertmanager ClusterIP 10.110.82.176 \u0026lt;none\u0026gt; 9093/TCP 34s alertmanager=monitoring-kube-prometheus-alertmanager,app.kubernetes.io/name=alertmanager service/monitoring-kube-prometheus-operator ClusterIP 10.104.147.173 \u0026lt;none\u0026gt; 443/TCP 34s app=kube-prometheus-stack-operator,release=monitoring service/monitoring-kube-prometheus-prometheus ClusterIP 10.110.109.245 \u0026lt;none\u0026gt; 9090/TCP 34s app.kubernetes.io/name=prometheus,prometheus=monitoring-kube-prometheus-prometheus service/monitoring-kube-state-metrics ClusterIP 10.107.111.214 \u0026lt;none\u0026gt; 8080/TCP 34s app.kubernetes.io/instance=monitoring,app.kubernetes.io/name=kube-state-metrics service/monitoring-prometheus-node-exporter ClusterIP 10.108.97.196 \u0026lt;none\u0026gt; 9100/TCP 34s app=prometheus-node-exporter,release=monitoring service/prometheus-operated ClusterIP None \u0026lt;none\u0026gt; 9090/TCP 26s app.kubernetes.io/name=prometheus NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE CONTAINERS IMAGES SELECTOR daemonset.apps/monitoring-prometheus-node-exporter 3 3 3 3 3 \u0026lt;none\u0026gt; 34s node-exporter quay.io/prometheus/node-exporter:v1.3.1 app=prometheus-node-exporter,release=monitoring NAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTOR deployment.apps/monitoring-grafana 0/1 1 0 34s grafana-sc-dashboard,grafana-sc-datasources,grafana quay.io/kiwigrid/k8s-sidecar:1.15.6,quay.io/kiwigrid/k8s-sidecar:1.15.6,grafana/grafana:8.4.2 app.kubernetes.io/instance=monitoring,app.kubernetes.io/name=grafana deployment.apps/monitoring-kube-prometheus-operator 1/1 1 1 34s kube-prometheus-stack quay.io/prometheus-operator/prometheus-operator:v0.55.0 app=kube-prometheus-stack-operator,release=monitoring deployment.apps/monitoring-kube-state-metrics 1/1 1 1 34s kube-state-metrics k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.4.1 app.kubernetes.io/instance=monitoring,app.kubernetes.io/name=kube-state-metrics NAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTOR replicaset.apps/monitoring-grafana-578f79599c 1 1 0 34s grafana-sc-dashboard,grafana-sc-datasources,grafana quay.io/kiwigrid/k8s-sidecar:1.15.6,quay.io/kiwigrid/k8s-sidecar:1.15.6,grafana/grafana:8.4.2 app.kubernetes.io/instance=monitoring,app.kubernetes.io/name=grafana,pod-template-hash=578f79599c replicaset.apps/monitoring-kube-prometheus-operator-65cdf7995 1 1 1 34s kube-prometheus-stack quay.io/prometheus-operator/prometheus-operator:v0.55.0 app=kube-prometheus-stack-operator,pod-template-hash=65cdf7995,release=monitoring replicaset.apps/monitoring-kube-state-metrics-56bfd4f44f 1 1 1 34s kube-state-metrics k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.4.1 app.kubernetes.io/instance=monitoring,app.kubernetes.io/name=kube-state-metrics,pod-template-hash=56bfd4f44f NAME READY AGE CONTAINERS IMAGES statefulset.apps/alertmanager-monitoring-kube-prometheus-alertmanager 1/1 27s alertmanager,config-reloader quay.io/prometheus/alertmanager:v0.23.0,quay.io/prometheus-operator/prometheus-config-reloader:v0.55.0 statefulset.apps/prometheus-monitoring-kube-prometheus-prometheus 1/1 26s prometheus,config-reloader quay.io/prometheus/prometheus:v2.33.5,quay.io/prometheus-operator/prometheus-config-reloader:v0.55.0 Add the NodePort   Edit the grafana service to add the NodePort:\n$ kubectl edit service/\u0026lt;deployment_name\u0026gt;-grafana -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl edit service/monitoring-grafana -n monitoring Note: This opens an edit session for the domain where parameters can be changed using standard vi commands.\nChange the ports entry and add nodePort: 30091 and type: NodePort:\n ports: - name: http-web nodePort: 30091 port: 80 protocol: TCP targetPort: 3000 selector: app.kubernetes.io/instance: monitoring app.kubernetes.io/name: grafana sessionAffinity: None type: NodePort   Save the file and exit (:wq).\n  Verify Using Grafana GUI   Access the Grafana GUI using http://\u0026lt;HostIP\u0026gt;:\u0026lt;nodeport\u0026gt; and login with admin/prom-operator. Change the password when prompted.\n  Download the K8 Cluster Detail Dashboard json file from: https://grafana.com/grafana/dashboards/10856.\n  Import the Grafana dashboard by navigating on the left hand menu to Create \u0026gt; Import. Click Upload JSON file and select the json downloaded file. In the Prometheus drop down box select Prometheus. Click Import. The dashboard should be displayed.\n  Verify your installation by viewing some of the customized dashboard views.\n  "
},
{
	"uri": "/fmw-kubernetes/oam/manage-oam-domains/logging-and-visualization/",
	"title": "c. Logging and Visualization",
	"tags": [],
	"description": "Describes the steps for logging and visualization with Elasticsearch and Kibana.",
	"content": "After the OAM domain is set up you can publish operator and WebLogic Server logs into Elasticsearch and interact with them in Kibana.\nInstall Elasticsearch and Kibana   If your domain namespace is anything other than oamns, edit the $WORKDIR/kubernetes/elasticsearch-and-kibana/elasticsearch_and_kibana.yaml and change all instances of oamns to your domain namespace.\n  Create a Kubernetes secret to access the Elasticsearch and Kibana container images:\nNote: You must first have a user account on hub.docker.com.\n$ kubectl create secret docker-registry \u0026#34;dockercred\u0026#34; --docker-server=\u0026#34;https://index.docker.io/v1/\u0026#34; --docker-username=\u0026#34;\u0026lt;docker_username\u0026gt;\u0026#34; --docker-password=\u0026lt;password\u0026gt; --docker-email=\u0026lt;docker_email_credentials\u0026gt; --namespace=\u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl create secret docker-registry \u0026quot;dockercred\u0026quot; --docker-server=\u0026quot;https://index.docker.io/v1/\u0026quot; --docker-username=\u0026quot;username\u0026quot; --docker-password=\u0026lt;password\u0026gt; --docker-email=user@example.com --namespace=oamns The output will look similar to the following:\nsecret/dockercred created   Create the Kubernetes resource using the following command:\n$ kubectl apply -f $WORKDIR/kubernetes/elasticsearch-and-kibana/elasticsearch_and_kibana.yaml The output will look similar to the following:\ndeployment.apps/elasticsearch created service/elasticsearch created deployment.apps/kibana created service/kibana created   Run the following command to ensure Elasticsearch is used by the operator:\n$ helm get values --all weblogic-kubernetes-operator -n opns The output will look similar to the following:\nCOMPUTED VALUES: clusterSizePaddingValidationEnabled: true domainNamespaceLabelSelector: weblogic-operator=enabled domainNamespaceSelectionStrategy: LabelSelector domainNamespaces: - default elasticSearchHost: elasticsearch.default.svc.cluster.local elasticSearchPort: 9200 elkIntegrationEnabled: true enableClusterRoleBinding: true externalDebugHttpPort: 30999 externalRestEnabled: false externalRestHttpsPort: 31001 externalServiceNameSuffix: -ext image: ghcr.io/oracle/weblogic-kubernetes-operator:3.3.0 imagePullPolicy: IfNotPresent internalDebugHttpPort: 30999 introspectorJobNameSuffix: -introspector javaLoggingFileCount: 10 javaLoggingFileSizeLimit: 20000000 javaLoggingLevel: FINE logStashImage: logstash:6.6.0 remoteDebugNodePortEnabled: false serviceAccount: op-sa suspendOnDebugStartup: false   To check that Elasticsearch and Kibana are deployed in the Kubernetes cluster, run the following command:\n$ kubectl get pods -n \u0026lt;namespace\u0026gt; | grep 'elasticsearch\\|kibana' For example:\n$ kubectl get pods -n oamns | grep 'elasticsearch\\|kibana' The output will look similar to the following:\nelasticsearch-f7b7c4c4-tb4pp 1/1 Running 0 85s kibana-57f6685789-mgwdl 1/1 Running 0 85s   Create the logstash pod OAM Server logs can be pushed to the Elasticsearch server using the logstash pod. The logstash pod needs access to the persistent volume of the OAM domain created previously, for example accessdomain-domain-pv. The steps to create the logstash pod are as follows:\n  Obtain the OAM domain persistence volume details:\n$ kubectl get pv -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get pv -n oamns The output will look similar to the following:\nNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE accessdomain-domain-pv 10Gi RWX Retain Bound oamns/accessdomain-domain-pvc accessdomain-domain-storage-class 23h Make note of the CLAIM value, for example in this case accessdomain-domain-pvc\n  Run the following command to get the mountPath of your domain:\n$ kubectl describe domains \u0026lt;domain_uid\u0026gt; -n \u0026lt;domain_namespace\u0026gt; | grep \u0026#34;Mount Path\u0026#34; For example:\n$ kubectl describe domains accessdomain -n oamns | grep \u0026#34;Mount Path\u0026#34; The output will look similar to the following:\nMount Path: /u01/oracle/user_projects/domains   Navigate to the $WORKDIR/kubernetes/elasticsearch-and-kibana directory and create a logstash.yaml file as follows. Change the claimName and mountPath values to match the values returned in the previous commands. Change namespace to your domain namespace e.g oamns:\napiVersion: apps/v1 kind: Deployment metadata: name: logstash-wls namespace: oamns spec: selector: matchLabels: k8s-app: logstash-wls template: # create pods using pod definition in this template metadata: labels: k8s-app: logstash-wls spec: volumes: - name: weblogic-domain-storage-volume persistentVolumeClaim: claimName: accessdomain-domain-pvc - name: shared-logs emptyDir: {} imagePullSecrets: - name: dockercred containers: - name: logstash image: logstash:6.6.0 command: [\u0026quot;/bin/sh\u0026quot;] args: [\u0026quot;/usr/share/logstash/bin/logstash\u0026quot;, \u0026quot;-f\u0026quot;, \u0026quot;/u01/oracle/user_projects/domains/logstash/logstash.conf\u0026quot;] imagePullPolicy: IfNotPresent volumeMounts: - mountPath: /u01/oracle/user_projects/domains name: weblogic-domain-storage-volume - name: shared-logs mountPath: /shared-logs ports: - containerPort: 5044 name: logstash   In the NFS persistent volume directory that corresponds to the mountPath /u01/oracle/user_projects/domains, create a logstash directory. For example:\n$ mkdir -p /scratch/shared/accessdomainpv/logstash   Create a logstash.conf in the newly created logstash directory that contains the following. Make sure the paths correspond to your mountPath and domain name. Also, if your namespace is anything other than oamns change \u0026quot;elasticsearch.oamns.svc.cluster.local:9200\u0026quot; to \u0026quot;elasticsearch.\u0026lt;namespace\u0026gt;.svc.cluster.local:9200\u0026quot;:\ninput { file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/logs/accessdomain/AdminServer*.log\u0026quot; tags =\u0026gt; \u0026quot;Adminserver_log\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/logs/accessdomain/oam_policy_mgr*.log\u0026quot; tags =\u0026gt; \u0026quot;Policymanager_log\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/logs/accessdomain/oam_server*.log\u0026quot; tags =\u0026gt; \u0026quot;Oamserver_log\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/accessdomain/servers/AdminServer/logs/AdminServer-diagnostic.log\u0026quot; tags =\u0026gt; \u0026quot;Adminserver_diagnostic\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/accessdomain/servers/**/logs/oam_policy_mgr*-diagnostic.log\u0026quot; tags =\u0026gt; \u0026quot;Policy_diagnostic\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/accessdomain/servers/**/logs/oam_server*-diagnostic.log\u0026quot; tags =\u0026gt; \u0026quot;Oamserver_diagnostic\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/accessdomain/servers/**/logs/access*.log\u0026quot; tags =\u0026gt; \u0026quot;Access_logs\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/accessdomain/servers/AdminServer/logs/auditlogs/OAM/audit.log\u0026quot; tags =\u0026gt; \u0026quot;Audit_logs\u0026quot; start_position =\u0026gt; beginning } } filter { grok { match =\u0026gt; [ \u0026quot;message\u0026quot;, \u0026quot;\u0026lt;%{DATA:log_timestamp}\u0026gt; \u0026lt;%{WORD:log_level}\u0026gt; \u0026lt;%{WORD:thread}\u0026gt; \u0026lt;%{HOSTNAME:hostname}\u0026gt; \u0026lt;%{HOSTNAME:servername}\u0026gt; \u0026lt;%{DATA:timer}\u0026gt; \u0026lt;\u0026lt;%{DATA:kernel}\u0026gt;\u0026gt; \u0026lt;\u0026gt; \u0026lt;%{DATA:uuid}\u0026gt; \u0026lt;%{NUMBER:timestamp}\u0026gt; \u0026lt;%{DATA:misc}\u0026gt; \u0026lt;%{DATA:log_number}\u0026gt; \u0026lt;%{DATA:log_message}\u0026gt;\u0026quot; ] } if \u0026quot;_grokparsefailure\u0026quot; in [tags] { mutate { remove_tag =\u0026gt; [ \u0026quot;_grokparsefailure\u0026quot; ] } } } output { elasticsearch { hosts =\u0026gt; [\u0026quot;elasticsearch.oamns.svc.cluster.local:9200\u0026quot;] } }   Deploy the logstash pod by executing the following command:\n$ kubectl create -f $WORKDIR/kubernetes/elasticsearch-and-kibana/logstash.yaml The output will look similar to the following:\ndeployment.apps/logstash-wls created   Run the following command to check the logstash pod is created correctly:\n$ kubectl get pods -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl get pods -n oamns The output should look similar to the following:\nNAME READY STATUS RESTARTS AGE accessdomain-adminserver 1/1 Running 0 18h accessdomain-create-oam-infra-domain-job-7c9r9 0/1 Completed 0 23h accessdomain-oam-policy-mgr1 1/1 Running 0 18h accessdomain-oam-policy-mgr2 1/1 Running 0 18h accessdomain-oam-server1 1/1 Running 1 18h accessdomain-oam-server2 1/1 Running 1 18h elasticsearch-f7b7c4c4-tb4pp 1/1 Running 0 5m helper 1/1 Running 0 23h kibana-57f6685789-mgwdl 1/1 Running 0 5m logstash-wls-6687c5bf6-jmmdp 1/1 Running 0 12s nginx-ingress-ingress-nginx-controller-76fb7678f-k8rhq 1/1 Running 0 20h   Verify and access the Kibana console   Check if the indices are created correctly in the elasticsearch pod shown above:\n$ kubectl exec -it \u0026lt;elasticsearch-pod\u0026gt; -n \u0026lt;namespace\u0026gt; -- /bin/bash For example:\n$ kubectl exec -it elasticsearch-f7b7c4c4-tb4pp -n oamns -- /bin/bash This will take you into a bash shell in the elasticsearch pod:\n[root@elasticsearch-f7b7c4c4-tb4pp elasticsearch]#   In the elasticsearch bash shell, run the following to check the indices:\n[root@elasticsearch-f7b7c4c4-tb4pp elasticsearch]# curl -i \u0026#34;127.0.0.1:9200/_cat/indices?v\u0026#34; The output will look similar to the following:\nHTTP/1.1 200 OK content-type: text/plain; charset=UTF-8 content-length: 696 health status index uuid pri rep docs.count docs.deleted store.size pri.store.size green open .kibana_task_manager -IPDdiajTSyIRjelI2QJIg 1 0 2 0 12.6kb 12.6kb green open .kibana_1 YI9CZAjsTsCCuAyBb1ho3A 1 0 2 0 7.6kb 7.6kb yellow open logstash-2022.03.08 4pDJSTGVR3-oOwTtHnnTkQ 5 1 148 0 173.9kb 173.9kb Exit the bash shell by typing exit.\n  Find the Kibana port by running the following command:\n$ kubectl get svc -n \u0026lt;namespace\u0026gt; | grep kibana For example:\n$ kubectl get svc -n oamns | grep kibana The output will look similar to the following:\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kibana NodePort 10.104.248.203 \u0026lt;none\u0026gt; 5601:31394/TCP 11m In the example above the Kibana port is 31394.\n  Access the Kibana console with http://${MASTERNODE-HOSTNAME}:${KIBANA-PORT}/app/kibana.\n  Click Dashboard and in the Create index pattern page enter logstash*. Click Next Step.\n  From the Time Filter field name drop down menu select @timestamp and click Create index pattern.\n  Once the index pattern is created click on Discover in the navigation menu to view the logs.\n  For more details on how to use the Kibana console see the Kibana Guide\nCleanup To clean up the Elasticsearch and Kibana install:\n  Run the following command to delete logstash:\n$ kubectl delete -f $WORKDIR/kubernetes/elasticsearch-and-kibana/logstash.yaml The output will look similar to the following:\ndeployment.apps \u0026quot;logstash-wls\u0026quot; deleted   Run the following command to delete Elasticsearch and Kibana:\n$ kubectl delete -f $WORKDIR/kubernetes/elasticsearch-and-kibana/elasticsearch_and_kibana.yaml The output will look similar to the following:\ndeployment.apps \u0026quot;elasticsearch\u0026quot; deleted service \u0026quot;elasticsearch\u0026quot; deleted deployment.apps \u0026quot;kibana\u0026quot; deleted service \u0026quot;kibana\u0026quot; deleted   "
},
{
	"uri": "/fmw-kubernetes/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/fmw-kubernetes/oam/manage-oam-domains/monitoring-oam-domains/",
	"title": "d. Monitoring an OAM domain",
	"tags": [],
	"description": "Describes the steps for Monitoring the OAM domain.",
	"content": "After the OAM domain is set up you can monitor the OAM instance using Prometheus and Grafana. See Monitoring a domain.\nThe WebLogic Monitoring Exporter uses the WLS RESTful Management API to scrape runtime information and then exports Prometheus-compatible metrics. It is deployed as a web application in a WebLogic Server (WLS) instance, version 12.2.1 or later, typically, in the instance from which you want to get metrics.\nThere are two ways to setup monitoring and you should choose one method or the other:\n Setup automatically using setup-monitoring.sh Setup using manual configuration  Setup automatically using setup-monitoring.sh The $WORKDIR/kubernetes/monitoring-service/setup-monitoring.sh sets up the monitoring for the OAM domain. It installs Prometheus, Grafana, WebLogic Monitoring Exporter and deploys the web applications to the OAM domain. It also deploys the WebLogic Server Grafana dashboard.\nFor usage details execute ./setup-monitoring.sh -h.\n  Edit the $WORKDIR/kubernetes/monitoring-service/monitoring-inputs.yaml and change the domainUID, domainNamespace, and weblogicCredentialsSecretName to correspond to your deployment. For example:\nversion: create-accessdomain-monitoring-inputs-v1 # Unique ID identifying your domain. # This ID must not contain an underscope (\u0026quot;_\u0026quot;), and must be lowercase and unique across all domains in a Kubernetes cluster. domainUID: accessdomain # Name of the domain namespace domainNamespace: oamns # Boolean value indicating whether to install kube-prometheus-stack setupKubePrometheusStack: true # Additional parameters for helm install kube-prometheus-stack # Refer https://github.com/prometheus-community/helm-charts/blob/main/charts/kube-prometheus-stack/values.yaml for additional parameters # Sample : # additionalParamForKubePrometheusStack: --set nodeExporter.enabled=false --set prometheusOperator.tls.enabled=false --set prometheusOperator.admissionWebhooks.enabled=false additionalParamForKubePrometheusStack: # Name of the monitoring namespace monitoringNamespace: monitoring # Name of the Admin Server adminServerName: AdminServer # # Port number for admin server adminServerPort: 7001 # Cluster name oamClusterName: oam_cluster # Port number for managed server oamManagedServerPort: 14100 # WebLogic Monitoring Exporter to Cluster wlsMonitoringExporterTooamCluster: true # Cluster name policyClusterName: policy_cluster # Port number for managed server policyManagedServerPort: 15100 # WebLogic Monitoring Exporter to Cluster wlsMonitoringExporterTopolicyCluster: true # Boolean to indicate if the adminNodePort will be exposed exposeMonitoringNodePort: true # NodePort to expose Prometheus prometheusNodePort: 32101 # NodePort to expose Grafana grafanaNodePort: 32100 # NodePort to expose Alertmanager alertmanagerNodePort: 32102 # Name of the Kubernetes secret for the Admin Server's username and password weblogicCredentialsSecretName: accessdomain-credentials   Run the following command to setup monitoring.\n$ cd $WORKDIR/kubernetes/monitoring-service $ ./setup-monitoring.sh -i monitoring-inputs.yaml The output should be similar to the following:\nMonitoring setup in monitoring in progress node/worker-node1 not labeled node/worker-node2 not labeled node/master-node not labeled Setup prometheus-community/kube-prometheus-stack started \u0026quot;prometheus-community\u0026quot; has been added to your repositories Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \u0026quot;stable\u0026quot; chart repository ...Successfully got an update from the \u0026quot;prometheus\u0026quot; chart repository ...Successfully got an update from the \u0026quot;prometheus-community\u0026quot; chart repository ...Successfully got an update from the \u0026quot;appscode\u0026quot; chart repository Update Complete. ⎈ Happy Helming!⎈ Setup prometheus-community/kube-prometheus-stack in progress NAME: monitoring LAST DEPLOYED: Mon Mar 7 14:13:49 2022 NAMESPACE: monitoring STATUS: deployed REVISION: 1 NOTES: kube-prometheus-stack has been installed. Check its status by running: kubectl --namespace monitoring get pods -l \u0026quot;release=monitoring\u0026quot; Visit https://github.com/prometheus-operator/kube-prometheus for instructions on how to create \u0026amp; configure Alertmanager and Prometheus instances using the Operator. Setup prometheus-community/kube-prometheus-stack completed Deploy WebLogic Monitoring Exporter started Deploying WebLogic Monitoring Exporter with domainNamespace[oamns], domainUID[accessdomain], adminServerPodName[accessdomain-adminserver] % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 655 100 655 0 0 1564 0 --:--:-- --:--:-- --:--:-- 1566 100 2196k 100 2196k 0 0 2025k 0 0:00:01 0:00:01 --:--:-- 5951k created $WORKDIR/kubernetes/monitoring-service/scripts/wls-exporter-deploy dir created /tmp/ci-EHhB7bP847 /tmp/ci-EHhB7bP847 $WORKDIR/kubernetes/monitoring-service in temp dir adding: WEB-INF/weblogic.xml (deflated 61%) adding: config.yml (deflated 60%) $WORKDIR/kubernetes/monitoring-service created /tmp/ci-e7wPrlLlud 14:26 /tmp/ci-e7wPrlLlud $WORKDIR/kubernetes/monitoring-service in temp dir adding: WEB-INF/weblogic.xml (deflated 61%) adding: config.yml (deflated 60%) $WORKDIR/kubernetes/monitoring-service created /tmp/ci-U38XXs6d06 /tmp/ci-U38XXs6d06 $WORKDIR/kubernetes/monitoring-service in temp dir adding: WEB-INF/weblogic.xml (deflated 61%) adding: config.yml (deflated 60%) $WORKDIR/kubernetes/monitoring-service Initializing WebLogic Scripting Tool (WLST) ... Welcome to WebLogic Server Administration Scripting Shell Type help() for help on available commands Connecting to t3://accessdomain-adminserver:7001 with userid weblogic ... Successfully connected to Admin Server \u0026quot;AdminServer\u0026quot; that belongs to domain \u0026quot;accessdomain\u0026quot;. Warning: An insecure protocol was used to connect to the server. To ensure on-the-wire security, the SSL port or Admin port should be used instead. Deploying ......... Deploying application from /u01/oracle/wls-exporter-deploy/wls-exporter-adminserver.war to targets AdminServer (upload=true) ... \u0026lt;Mar 7, 2022 2:14:31 PM GMT\u0026gt; \u0026lt;Info\u0026gt; \u0026lt;J2EE Deployment SPI\u0026gt; \u0026lt;BEA-260121\u0026gt; \u0026lt;Initiating deploy operation for application, wls-exporter-adminserver [archive: /u01/oracle/wls-exporter-deploy/wls-exporter-adminserver.war], to AdminServer .\u0026gt; .Completed the deployment of Application with status completed Current Status of your Deployment: Deployment command type: deploy Deployment State : completed Deployment Message : no message Starting application wls-exporter-adminserver. \u0026lt;Mar 7, 2022 2:14:36 PM GMT\u0026gt; \u0026lt;Info\u0026gt; \u0026lt;J2EE Deployment SPI\u0026gt; \u0026lt;BEA-260121\u0026gt; \u0026lt;Initiating start operation for application, wls-exporter-adminserver [archive: null], to AdminServer .\u0026gt; Completed the start of Application with status completed Current Status of your Deployment: Deployment command type: start Deployment State : completed 14:27 Deployment command type: start Deployment State : completed Deployment Message : no message Deploying ......... Deploying application from /u01/oracle/wls-exporter-deploy/wls-exporter-oam.war to targets oam_cluster (upload=true) ... \u0026lt;Mar 7, 2022 2:14:37 PM GMT\u0026gt; \u0026lt;Info\u0026gt; \u0026lt;J2EE Deployment SPI\u0026gt; \u0026lt;BEA-260121\u0026gt; \u0026lt;Initiating deploy operation for application, wls-exporter-oam [archive: /u01/oracle/wls-exporter-deploy/wls-exporter-oam.war], to oam_cluster .\u0026gt; .Completed the deployment of Application with status completed Current Status of your Deployment: Deployment command type: deploy Deployment State : completed Deployment Message : no message Starting application wls-exporter-oam. \u0026lt;Mar 7, 2022 2:14:41 PM GMT\u0026gt; \u0026lt;Info\u0026gt; \u0026lt;J2EE Deployment SPI\u0026gt; \u0026lt;BEA-260121\u0026gt; \u0026lt;Initiating start operation for application, wls-exporter-oam [archive: null], to oam_cluster .\u0026gt; .Completed the start of Application with status completed Current Status of your Deployment: Deployment command type: start Deployment State : completed Deployment Message : no message Deploying ......... Deploying application from /u01/oracle/wls-exporter-deploy/wls-exporter-policy.war to targets policy_cluster (upload=true) ... \u0026lt;Mar 7, 2022 2:14:44 PM GMT\u0026gt; \u0026lt;Info\u0026gt; \u0026lt;J2EE Deployment SPI\u0026gt; \u0026lt;BEA-260121\u0026gt; \u0026lt;Initiating deploy operation for application, wls-exporter-policy [archive: /u01/oracle/wls-exporter-deploy/wls-exporter-policy.war], to policy_cluster .\u0026gt; .Completed the deployment of Application with status completed Current Status of your Deployment: Deployment command type: deploy Deployment State : completed Deployment Message : no message Starting application wls-exporter-policy. \u0026lt;Mar 7, 2022 2:14:49 PM GMT\u0026gt; \u0026lt;Info\u0026gt; \u0026lt;J2EE Deployment SPI\u0026gt; \u0026lt;BEA-260121\u0026gt; \u0026lt;Initiating start operation for application, wls-exporter-policy [archive: null], to policy_cluster .\u0026gt; .Completed the start of Application with status completed Current Status of your Deployment: Deployment command type: start Deployment State : completed Deployment Message : no message Disconnected from weblogic server: AdminServer Exiting WebLogic Scripting Tool. \u0026lt;Mar 7, 2022 2:14:52 PM GMT\u0026gt; \u0026lt;Warning\u0026gt; \u0026lt;JNDI\u0026gt; \u0026lt;BEA-050001\u0026gt; \u0026lt;WLContext.close() was called in a different thread than the one in which it was created.\u0026gt; 14:27 Deploy WebLogic Monitoring Exporter completed secret/basic-auth created servicemonitor.monitoring.coreos.com/wls-exporter created Deploying WebLogic Server Grafana Dashboard.... {\u0026quot;id\u0026quot;:25,\u0026quot;slug\u0026quot;:\u0026quot;weblogic-server-dashboard\u0026quot;,\u0026quot;status\u0026quot;:\u0026quot;success\u0026quot;,\u0026quot;uid\u0026quot;:\u0026quot;5yUwzbZWz\u0026quot;,\u0026quot;url\u0026quot;:\u0026quot;/d/5yUwzbZWz/weblogic-server-dashboard\u0026quot;,\u0026quot;version\u0026quot;:1} Deployed WebLogic Server Grafana Dashboard successfully Grafana is available at NodePort: 32100 Prometheus is available at NodePort: 32101 Altermanager is available at NodePort: 32102 ==============================================================   Prometheus service discovery After the ServiceMonitor is deployed, the wls-exporter should be discovered by Prometheus and be able to collect metrics.\n  Access the following URL to view Prometheus service discovery: http://${MASTERNODE-HOSTNAME}:32101/service-discovery\n  Click on serviceMonitor/oamns/wls-exporter/0 and then show more. Verify all the targets are mentioned.\n  Note : It may take several minutes for serviceMonitor/oamns/wls-exporter/0 to appear, so refresh the page until it does.\nGrafana dashboard   Access the Grafana dashboard with the following URL: http://${MASTERNODE-HOSTNAME}:32100 and login with admin/admin. Change your password when prompted.\n  In the Dashboards panel, click on WebLogic Server Dashboard. The dashboard for your OAM domain should be displayed. If it is not displayed, click the Search icon in the left hand menu and search for WebLogic Server Dashboard.\n  Cleanup To uninstall the Prometheus, Grafana, WebLogic Monitoring Exporter and the deployments, you can run the $WORKDIR/monitoring-service/kubernetes/delete-monitoring.sh script. For usage details execute ./delete-monitoring.sh -h.\n  To uninstall run the following command:\nFor example:\n$ cd $WORKDIR/kubernetes/monitoring-service $ ./delete-monitoring.sh -i monitoring-inputs.yaml $ kubectl delete namespace monitoring   Setup using manual configuration Install Prometheus, Grafana and WebLogic Monitoring Exporter manually. Create the web applications and deploy to the OAM domain.\nDeploy the Prometheus operator   Kube-Prometheus requires all nodes to be labelled with kubernetes.io/os=linux. To check if your nodes are labelled, run the following:\n$ kubectl get nodes --show-labels If the nodes are labelled the output will look similar to the following:\nNAME STATUS ROLES AGE VERSION LABELS worker-node1 Ready \u0026lt;none\u0026gt; 42d v1.20.10 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=worker-node1,kubernetes.io/os=linux worker-node2 Ready \u0026lt;none\u0026gt; 42d v1.20.10 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=worker-node2,kubernetes.io/os=linux master-node Ready master 42d v1.20.10 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=master-node,kubernetes.io/os=linux,node-role.kubernetes.io/master= If the nodes are not labelled, run the following command:\n$ kubectl label nodes --all kubernetes.io/os=linux   Clone Prometheus by running the following commands:\n$ cd $WORKDIR/kubernetes/monitoring-service $ git clone https://github.com/coreos/kube-prometheus.git -b v0.7.0 Note: Please refer the compatibility matrix of Kube Prometheus. Please download the release of the repository according to the Kubernetes version of your cluster.\n  Run the following command to create the namespace and custom resource definitions:\n$ cd kube-prometheus $ kubectl create -f manifests/setup The output will look similar to the following:\nnamespace/monitoring created customresourcedefinition.apiextensions.k8s.io/alertmanagerconfigs.monitoring.coreos.com created customresourcedefinition.apiextensions.k8s.io/alertmanagers.monitoring.coreos.com created customresourcedefinition.apiextensions.k8s.io/podmonitors.monitoring.coreos.com created customresourcedefinition.apiextensions.k8s.io/probes.monitoring.coreos.com created customresourcedefinition.apiextensions.k8s.io/prometheuses.monitoring.coreos.com created customresourcedefinition.apiextensions.k8s.io/prometheusrules.monitoring.coreos.com created customresourcedefinition.apiextensions.k8s.io/servicemonitors.monitoring.coreos.com created customresourcedefinition.apiextensions.k8s.io/thanosrulers.monitoring.coreos.com created clusterrole.rbac.authorization.k8s.io/prometheus-operator created clusterrolebinding.rbac.authorization.k8s.io/prometheus-operator created deployment.apps/prometheus-operator created service/prometheus-operator created serviceaccount/prometheus-operator created   Run the following command to created the rest of the resources:\n$ kubectl create -f manifests/ The output will look similar to the following:\nalertmanager.monitoring.coreos.com/main created prometheusrule.monitoring.coreos.com/alertmanager-main-rules created secret/alertmanager-main created service/alertmanager-main created serviceaccount/alertmanager-main created servicemonitor.monitoring.coreos.com/alertmanager-main created clusterrole.rbac.authorization.k8s.io/blackbox-exporter created clusterrolebinding.rbac.authorization.k8s.io/blackbox-exporter created configmap/blackbox-exporter-configuration created deployment.apps/blackbox-exporter created service/blackbox-exporter created serviceaccount/blackbox-exporter created servicemonitor.monitoring.coreos.com/blackbox-exporter created secret/grafana-config created secret/grafana-datasources created configmap/grafana-dashboard-alertmanager-overview created configmap/grafana-dashboard-apiserver created configmap/grafana-dashboard-cluster-total created configmap/grafana-dashboard-controller-manager created configmap/grafana-dashboard-k8s-resources-cluster created configmap/grafana-dashboard-k8s-resources-namespace created configmap/grafana-dashboard-k8s-resources-node created configmap/grafana-dashboard-k8s-resources-pod created configmap/grafana-dashboard-k8s-resources-workload created configmap/grafana-dashboard-k8s-resources-workloads-namespace created configmap/grafana-dashboard-kubelet created configmap/grafana-dashboard-namespace-by-pod created configmap/grafana-dashboard-namespace-by-workload created configmap/grafana-dashboard-node-cluster-rsrc-use created configmap/grafana-dashboard-node-rsrc-use created configmap/grafana-dashboard-nodes created configmap/grafana-dashboard-persistentvolumesusage created configmap/grafana-dashboard-pod-total created configmap/grafana-dashboard-prometheus-remote-write created configmap/grafana-dashboard-prometheus created configmap/grafana-dashboard-proxy created configmap/grafana-dashboard-scheduler created configmap/grafana-dashboard-workload-total created configmap/grafana-dashboards created deployment.apps/grafana created service/grafana created serviceaccount/grafana created servicemonitor.monitoring.coreos.com/grafana created prometheusrule.monitoring.coreos.com/kube-prometheus-rules created clusterrole.rbac.authorization.k8s.io/kube-state-metrics created clusterrolebinding.rbac.authorization.k8s.io/kube-state-metrics created deployment.apps/kube-state-metrics created prometheusrule.monitoring.coreos.com/kube-state-metrics-rules created service/kube-state-metrics created serviceaccount/kube-state-metrics created servicemonitor.monitoring.coreos.com/kube-state-metrics created prometheusrule.monitoring.coreos.com/kubernetes-monitoring-rules created servicemonitor.monitoring.coreos.com/kube-apiserver created servicemonitor.monitoring.coreos.com/coredns created servicemonitor.monitoring.coreos.com/kube-controller-manager created servicemonitor.monitoring.coreos.com/kube-scheduler created servicemonitor.monitoring.coreos.com/kubelet created clusterrole.rbac.authorization.k8s.io/node-exporter created clusterrolebinding.rbac.authorization.k8s.io/node-exporter created daemonset.apps/node-exporter created prometheusrule.monitoring.coreos.com/node-exporter-rules created service/node-exporter created serviceaccount/node-exporter created servicemonitor.monitoring.coreos.com/node-exporter created apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created clusterrole.rbac.authorization.k8s.io/prometheus-adapter created clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created clusterrolebinding.rbac.authorization.k8s.io/prometheus-adapter created clusterrolebinding.rbac.authorization.k8s.io/resource-metrics:system:auth-delegator created clusterrole.rbac.authorization.k8s.io/resource-metrics-server-resources created configmap/adapter-config created deployment.apps/prometheus-adapter created rolebinding.rbac.authorization.k8s.io/resource-metrics-auth-reader created service/prometheus-adapter created serviceaccount/prometheus-adapter created servicemonitor.monitoring.coreos.com/prometheus-adapter created clusterrole.rbac.authorization.k8s.io/prometheus-k8s created clusterrolebinding.rbac.authorization.k8s.io/prometheus-k8s created prometheusrule.monitoring.coreos.com/prometheus-operator-rules created servicemonitor.monitoring.coreos.com/prometheus-operator created prometheus.monitoring.coreos.com/k8s created prometheusrule.monitoring.coreos.com/prometheus-k8s-prometheus-rules created rolebinding.rbac.authorization.k8s.io/prometheus-k8s-config created rolebinding.rbac.authorization.k8s.io/prometheus-k8s created rolebinding.rbac.authorization.k8s.io/prometheus-k8s created rolebinding.rbac.authorization.k8s.io/prometheus-k8s created role.rbac.authorization.k8s.io/prometheus-k8s-config created role.rbac.authorization.k8s.io/prometheus-k8s created role.rbac.authorization.k8s.io/prometheus-k8s created role.rbac.authorization.k8s.io/prometheus-k8s created service/prometheus-k8s created serviceaccount/prometheus-k8s created servicemonitor.monitoring.coreos.com/prometheus-k8s created unable to recognize \u0026quot;manifests/alertmanager-podDisruptionBudget.yaml\u0026quot;: no matches for kind \u0026quot;PodDisruptionBudget\u0026quot; in version \u0026quot;policy/v1\u0026quot; unable to recognize \u0026quot;manifests/prometheus-adapter-podDisruptionBudget.yaml\u0026quot;: no matches for kind \u0026quot;PodDisruptionBudget\u0026quot; in version \u0026quot;policy/v1\u0026quot; unable to recognize \u0026quot;manifests/prometheus-podDisruptionBudget.yaml\u0026quot;: no matches for kind \u0026quot;PodDisruptionBudget\u0026quot; in version \u0026quot;policy/v1\u0026quot;   Provide external access for Grafana, Prometheus, and Alertmanager, by running the following commands:\n$ kubectl patch svc grafana -n monitoring --type=json -p \u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/type\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;NodePort\u0026#34; },{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/ports/0/nodePort\u0026#34;, \u0026#34;value\u0026#34;: 32100 }]\u0026#39; $ kubectl patch svc prometheus-k8s -n monitoring --type=json -p \u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/type\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;NodePort\u0026#34; },{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/ports/0/nodePort\u0026#34;, \u0026#34;value\u0026#34;: 32101 }]\u0026#39; $ kubectl patch svc alertmanager-main -n monitoring --type=json -p \u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/type\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;NodePort\u0026#34; },{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/ports/0/nodePort\u0026#34;, \u0026#34;value\u0026#34;: 32102 }]\u0026#39; Note: This assigns port 32100 to Grafana, 32101 to Prometheus, and 32102 to Alertmanager.\nThe output will look similar to the following:\nservice/grafana patched service/prometheus-k8s patched service/alertmanager-main patched   Verify that the Prometheus, Grafana, and Alertmanager pods are running in the monitoring namespace and the respective services have the exports configured correctly:\n$ kubectl get pods,services -o wide -n monitoring The output should look similar to the following:\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/alertmanager-main-0 2/2 Running 0 67s 10.244.1.7 worker-node1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/alertmanager-main-1 2/2 Running 0 67s 10.244.2.26 worker-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/alertmanager-main-2 2/2 Running 0 67s 10.244.1.8 worker-node1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/grafana-f8cd57fcf-tmlqt 1/1 Running 0 65s 10.244.2.28 worker-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/kube-state-metrics-587bfd4f97-l8knh 3/3 Running 0 65s 10.244.1.9 worker-node1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/node-exporter-2ztpd 2/2 Running 0 65s 10.247.95.26 worker-node1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/node-exporter-92sxb 2/2 Running 0 65s 10.250.40.59 worker-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/node-exporter-d77tl 2/2 Running 0 65s 10.196.54.36 master-node \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/prometheus-adapter-69b8496df6-6gqrz 1/1 Running 0 65s 10.244.2.29 worker-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/prometheus-k8s-0 2/2 Running 1 66s 10.244.2.27 worker-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/prometheus-k8s-1 2/2 Running 1 66s 10.244.1.10 worker-node1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/prometheus-operator-7649c7454f-9p747 2/2 Running 0 2m 10.244.2.25 worker-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service/alertmanager-main NodePort 10.104.92.62 \u0026lt;none\u0026gt; 9093:32102/TCP 67s alertmanager=main,app=alertmanager service/alertmanager-operated ClusterIP None \u0026lt;none\u0026gt; 9093/TCP,9094/TCP,9094/UDP 67s app=alertmanager service/grafana NodePort 10.100.171.3 \u0026lt;none\u0026gt; 3000:32100/TCP 66s app=grafana service/kube-state-metrics ClusterIP None \u0026lt;none\u0026gt; 8443/TCP,9443/TCP 66s app.kubernetes.io/name=kube-state-metrics service/node-exporter ClusterIP None \u0026lt;none\u0026gt; 9100/TCP 66s app.kubernetes.io/name=node-exporter service/prometheus-adapter ClusterIP 10.109.248.92 \u0026lt;none\u0026gt; 443/TCP 66s name=prometheus-adapter service/prometheus-k8s NodePort 10.98.212.247 \u0026lt;none\u0026gt; 9090:32101/TCP 66s app=prometheus,prometheus=k8s service/prometheus-operated ClusterIP None \u0026lt;none\u0026gt; 9090/TCP 66s app=prometheus service/prometheus-operator ClusterIP None \u0026lt;none\u0026gt; 8443/TCP 2m1s app.kubernetes.io/component=controller,app.kubernetes.io/name=prometheus-operator   Deploy WebLogic Monitoring Exporter   Generate the WebLogic Monitoring Exporter deployment package. The wls-exporter.war package need to be updated and created for each listening port (Administration Server and Managed Servers) in the domain. Set the below environment values and run the script get-wls-exporter.sh to generate the required WAR files at ${WORKDIR}/kubernetes/monitoring-service/scripts/wls-exporter-deploy:\n$ cd $WORKDIR/kubernetes/monitoring-service/scripts $ export adminServerPort=7001 $ export wlsMonitoringExporterTopolicyCluster=true $ export policyManagedServerPort=15100 $ export wlsMonitoringExporterTooamCluster=true $ export oamManagedServerPort=14100 $ sh get-wls-exporter.sh The output will look similar to the following:\n % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 655 100 655 0 0 1107 0 --:--:-- --:--:-- --:--:-- 1108 100 2196k 100 2196k 0 0 1787k 0 0:00:01 0:00:01 --:--:-- 9248k created $WORKDIR/kubernetes/monitoring-service/scripts/wls-exporter-deploy dir domainNamespace is empty, setting to default oamns domainUID is empty, setting to default accessdomain weblogicCredentialsSecretName is empty, setting to default \u0026quot;accessdomain-domain-credentials\u0026quot; adminServerName is empty, setting to default \u0026quot;AdminServer\u0026quot; oamClusterName is empty, setting to default \u0026quot;oam_cluster\u0026quot; policyClusterName is empty, setting to default \u0026quot;policy_cluster\u0026quot; created /tmp/ci-Bu74rCBxwu /tmp/ci-Bu74rCBxwu $WORKDIR/kubernetes/monitoring-service/scripts in temp dir adding: WEB-INF/weblogic.xml (deflated 61%) adding: config.yml (deflated 60%) $WORKDIR/kubernetes/monitoring-service/scripts created /tmp/ci-RQv3rLbLsX /tmp/ci-RQv3rLbLsX $WORKDIR/kubernetes/monitoring-service/scripts in temp dir adding: WEB-INF/weblogic.xml (deflated 61%) adding: config.yml (deflated 60%) $WORKDIR/kubernetes/monitoring-service/scripts created /tmp/ci-DWIYlocP5e /tmp/ci-DWIYlocP5e $WORKDIR/kubernetes/monitoring-service/scripts in temp dir adding: WEB-INF/weblogic.xml (deflated 61%) adding: config.yml (deflated 60%) $WORKDIR/kubernetes/monitoring-service/scripts   Deploy the WebLogic Monitoring Exporter WAR files into the Oracle Access Management domain:\n$ cd $WORKDIR/kubernetes/monitoring-service/scripts $ kubectl cp wls-exporter-deploy \u0026lt;domain_namespace\u0026gt;/\u0026lt;domain_uid\u0026gt;-adminserver:/u01/oracle $ kubectl cp deploy-weblogic-monitoring-exporter.py \u0026lt;domain_namespace\u0026gt;/\u0026lt;domain_uid\u0026gt;-adminserver:/u01/oracle/wls-exporter-deploy $ kubectl exec -it -n \u0026lt;domain_namespace\u0026gt; \u0026lt;domain_uid\u0026gt;-adminserver -- /u01/oracle/oracle_common/common/bin/wlst.sh /u01/oracle/wls-exporter-deploy/deploy-weblogic-monitoring-exporter.py -domainName \u0026lt;domain_uid\u0026gt; -adminServerName AdminServer -adminURL \u0026lt;domain_uid\u0026gt;-adminserver:7001 -username weblogic -password \u0026lt;password\u0026gt; -oamClusterName oam_cluster -wlsMonitoringExporterTooamCluster true -policyClusterName policy_cluster -wlsMonitoringExporterTopolicyCluster true For example:\n$ cd $WORKDIR/kubernetes/monitoring-service/scripts $ kubectl cp wls-exporter-deploy oamns/accessdomain-adminserver:/u01/oracle $ kubectl cp deploy-weblogic-monitoring-exporter.py oamns/accessdomain-adminserver:/u01/oracle/wls-exporter-deploy $ kubectl exec -it -n oamns accessdomain-adminserver -- /u01/oracle/oracle_common/common/bin/wlst.sh /u01/oracle/wls-exporter-deploy/deploy-weblogic-monitoring-exporter.py -domainName accessdomain -adminServerName AdminServer -adminURL accessdomain-adminserver:7001 -username weblogic -password \u0026lt;password\u0026gt; -oamClusterName oam_cluster -wlsMonitoringExporterTooamCluster true -policyClusterName policy_cluster -wlsMonitoringExporterTopolicyCluster true The output will look similar to the following:\nInitializing WebLogic Scripting Tool (WLST) ... Welcome to WebLogic Server Administration Scripting Shell Type help() for help on available commands Connecting to t3://accessdomain-adminserver:7001 with userid weblogic ... Successfully connected to Admin Server \u0026quot;AdminServer\u0026quot; that belongs to domain \u0026quot;accessdomain\u0026quot;. Warning: An insecure protocol was used to connect to the server. To ensure on-the-wire security, the SSL port or Admin port should be used instead. Deploying ......... Deploying application from /u01/oracle/wls-exporter-deploy/wls-exporter-adminserver.war to targets AdminServer (upload=true) ... \u0026lt;Mar 7, 2022 3:38:15 PM GMT\u0026gt; \u0026lt;Info\u0026gt; \u0026lt;J2EE Deployment SPI\u0026gt; \u0026lt;BEA-260121\u0026gt; \u0026lt;Initiating deploy operation for application, wls-exporter-adminserver [archive: /u01/oracle/wls-exporter-deploy/wls-exporter-adminserver.war], to AdminServer .\u0026gt; ..Completed the deployment of Application with status completed Current Status of your Deployment: Deployment command type: deploy Deployment State : completed Deployment Message : no message Starting application wls-exporter-adminserver. \u0026lt;Mar 7, 2022 3:38:25 PM GMT\u0026gt; \u0026lt;Info\u0026gt; \u0026lt;J2EE Deployment SPI\u0026gt; \u0026lt;BEA-260121\u0026gt; \u0026lt;Initiating start operation for application, wls-exporter-adminserver [archive: null], to AdminServer .\u0026gt; .Completed the start of Application with status completed Current Status of your Deployment: Deployment command type: start Deployment State : completed Deployment Message : no message Deploying ......... Deploying application from /u01/oracle/wls-exporter-deploy/wls-exporter-oam.war to targets oam_cluster (upload=true) ... \u0026lt;Mar 7, 2022 3:38:28 PM GMT\u0026gt; \u0026lt;Info\u0026gt; \u0026lt;J2EE Deployment SPI\u0026gt; \u0026lt;BEA-260121\u0026gt; \u0026lt;Initiating deploy operation for application, wls-exporter-oam [archive: /u01/oracle/wls-exporter-deploy/wls-exporter-oam.war], to oam_cluster .\u0026gt; .Completed the deployment of Application with status completed Current Status of your Deployment: Deployment command type: deploy Deployment State : completed Deployment Message : no message Starting application wls-exporter-oam. \u0026lt;Mar 7, 2022 3:38:34 PM GMT\u0026gt; \u0026lt;Info\u0026gt; \u0026lt;J2EE Deployment SPI\u0026gt; \u0026lt;BEA-260121\u0026gt; \u0026lt;Initiating start operation for application, wls-exporter-oam [archive: null], to oam_cluster .\u0026gt; .Completed the start of Application with status completed Current Status of your Deployment: Deployment command type: start Deployment State : completed Deployment Message : no message Deploying ......... Deploying application from /u01/oracle/wls-exporter-deploy/wls-exporter-policy.war to targets policy_cluster (upload=true) ... \u0026lt;Mar 7, 2022 3:38:38 PM GMT\u0026gt; \u0026lt;Info\u0026gt; \u0026lt;J2EE Deployment SPI\u0026gt; \u0026lt;BEA-260121\u0026gt; \u0026lt;Initiating deploy operation for application, wls-exporter-policy [archive: /u01/oracle/wls-exporter-deploy/wls-exporter-policy.war], to policy_cluster .\u0026gt; .Completed the deployment of Application with status completed Current Status of your Deployment: Deployment command type: deploy Deployment State : completed Deployment Message : no message Starting application wls-exporter-policy. \u0026lt;Mar 7, 2022 3:38:44 PM GMT\u0026gt; \u0026lt;Info\u0026gt; \u0026lt;J2EE Deployment SPI\u0026gt; \u0026lt;BEA-260121\u0026gt; \u0026lt;Initiating start operation for application, wls-exporter-policy [archive: null], to policy_cluster .\u0026gt; .Completed the start of Application with status completed Current Status of your Deployment: Deployment command type: start Deployment State : completed Deployment Message : no message Disconnected from weblogic server: AdminServer Exiting WebLogic Scripting Tool. \u0026lt;Mar 7, 2022 3:38:47 PM GMT\u0026gt; \u0026lt;Warning\u0026gt; \u0026lt;JNDI\u0026gt; \u0026lt;BEA-050001\u0026gt; \u0026lt;WLContext.close() was called in a different thread than the one in which it was created.\u0026gt;   Configure Prometheus Operator Prometheus enables you to collect metrics from the WebLogic Monitoring Exporter. The Prometheus Operator identifies the targets using service discovery. To get the WebLogic Monitoring Exporter end point discovered as a target, you must create a service monitor pointing to the service.\nThe exporting of metrics from wls-exporter requires basicAuth, so a Kubernetes Secret is created with the user name and password that are base64 encoded. This Secret is used in the ServiceMonitor deployment. The wls-exporter-ServiceMonitor.yaml has basicAuth with credentials as username: weblogic and password: \u0026lt;password\u0026gt; in base64 encoded.\n  Run the following command to get the base64 encoded version of the weblogic password:\n$ echo -n \u0026#34;\u0026lt;password\u0026gt;\u0026#34; | base64 The output will look similar to the following:\nV2VsY29tZTE=   Update the $WORKDIR/kubernetes/monitoring-service/manifests/wls-exporter-ServiceMonitor.yaml and change the password: value to the value returned above. Also change the namespace: and weblogic.domainName: values to match your OAM namespace and domain name:\napiVersion: v1 kind: Secret metadata: name: basic-auth namespace: oamns data: password: V2VsY29tZTE= user: d2VibG9naWM= type: Opaque --- apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: wls-exporter namespace: oamns labels: k8s-app: wls-exporter release: monitoring spec: namespaceSelector: matchNames: - oamns selector: matchLabels: weblogic.domainName: accessdomain endpoints: - basicAuth: password: name: basic-auth key: password username: name: basic-auth key: user port: default relabelings: - action: labelmap regex: __meta_kubernetes_service_label_(.+) interval: 10s honorLabels: true path: /wls-exporter/metrics   Update the $WORKDIR/kubernetes/monitoring-service/manifests/prometheus-roleSpecific-domain-namespace.yaml and change the namespace to match your OAM namespace. For example:\napiVersion: rbac.authorization.k8s.io/v1 items: - apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: prometheus-k8s namespace: oamns rules: - apiGroups: - \u0026quot;\u0026quot; resources: - services - endpoints - pods verbs: - get - list - watch kind: RoleList   Update the $WORKDIR/kubernetes/monitoring-service/manifests/prometheus-roleBinding-domain-namespace.yaml and change the namespace` to match your OAM namespace. For example:\napiVersion: rbac.authorization.k8s.io/v1 items: - apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: prometheus-k8s namespace: oamns roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: prometheus-k8s subjects: - kind: ServiceAccount name: prometheus-k8s namespace: monitoring kind: RoleBindingList   Run the following command to enable Prometheus:\n$ kubectl apply -f . The output will look similar to the following:\nrolebinding.rbac.authorization.k8s.io/prometheus-k8s created role.rbac.authorization.k8s.io/prometheus-k8s created secret/basic-auth created servicemonitor.monitoring.coreos.com/wls-exporter created   Prometheus Service Discovery After the ServiceMonitor is deployed, the wls-exporter should be discovered by Prometheus and be able to collect metrics.\n  Access the following URL to view Prometheus service discovery: http://${MASTERNODE-HOSTNAME}:32101/service-discovery\n  Click on oamns/wls-exporter/0  and then show more. Verify all the targets are mentioned.\n  Grafana Dashboard   Access the Grafana dashboard with the following URL: http://${MASTERNODE-HOSTNAME}:32100 and login with admin/admin. Change your password when prompted.\n  Import the Grafana dashboard by navigating on the left hand menu to Create \u0026gt; Import. Copy the content from $WORKDIR/kubernetes/monitoring-service/config/weblogic-server-dashboard-import.json and paste. Then click Load and Import. The dashboard should be displayed in the Dashboards panel.\n  Cleanup To clean up a manual installation:\n  Run the following commands:\n$ cd $WORKDIR/kubernetes/monitoring-service/manifests/ $ kubectl delete -f .   Delete the deployments:\n$ cd $WORKDIR/kubernetes/monitoring-service/scripts/ $ kubectl cp undeploy-weblogic-monitoring-exporter.py \u0026lt;domain_namespace\u0026gt;/\u0026lt;domain_uid\u0026gt;-adminserver:/u01/oracle/wls-exporter-deploy $ kubectl exec -it -n \u0026lt;domain_namespace\u0026gt; \u0026lt;domain_uid\u0026gt;-adminserver -- /u01/oracle/oracle_common/common/bin/wlst.sh /u01/oracle/wls-exporter-deploy/undeploy-weblogic-monitoring-exporter.py -domainName \u0026lt;domain_uid\u0026gt; -adminServerName AdminServer -adminURL \u0026lt;domain_uid\u0026gt;-adminserver:7001 -username weblogic -password \u0026lt;password\u0026gt; -oamClusterName oam_cluster -wlsMonitoringExporterTooamCluster true -policyClusterName policy_cluster -wlsMonitoringExporterTopolicyCluster true   Delete Prometheus:\n$ cd $WORKDIR/kubernetes/monitoring-service/kube-prometheus $ kubectl delete -f manifests $ kubectl delete -f manifests/setup   Delete the monitoring namespace:\n$ kubectl delete namespace monitoring   "
},
{
	"uri": "/fmw-kubernetes/oam/manage-oam-domains/delete-domain-home/",
	"title": "e. Delete the OAM domain home",
	"tags": [],
	"description": "Learn about the steps to cleanup the OAM domain home.",
	"content": "Sometimes in production, but most likely in testing environments, you might want to remove the domain home that is generated using the create-domain.sh script.\n  Run the following command to delete the domain:\n$ cd $WORKDIR/kubernetes/delete-domain $ ./delete-weblogic-domain-resources.sh -d \u0026lt;domain_uid\u0026gt; For example:\n$ cd $WORKDIR/kubernetes/delete-domain $ ./delete-weblogic-domain-resources.sh -d accessdomain   Drop the RCU schemas as follows:\n$ kubectl exec -it helper -n \u0026lt;domain_namespace\u0026gt; -- /bin/bash [oracle@helper ~]$ [oracle@helper ~]$ export CONNECTION_STRING=\u0026lt;db_host.domain\u0026gt;:\u0026lt;db_port\u0026gt;/\u0026lt;service_name\u0026gt; [oracle@helper ~]$ export RCUPREFIX=\u0026lt;rcu_schema_prefix\u0026gt; /u01/oracle/oracle_common/bin/rcu -silent -dropRepository -databaseType ORACLE -connectString $CONNECTION_STRING \\ -dbUser sys -dbRole sysdba -selectDependentsForComponents true -schemaPrefix $RCUPREFIX \\ -component MDS -component IAU -component IAU_APPEND -component IAU_VIEWER -component OPSS \\ -component WLS -component STB -component OAM -f \u0026lt; /tmp/pwd.txt For example:\n$ kubectl exec -it helper -n oamns -- /bin/bash [oracle@helper ~]$ export CONNECTION_STRING=mydatabasehost.example.com:1521/orcl.example.com [oracle@helper ~]$ export RCUPREFIX=OAMK8S /u01/oracle/oracle_common/bin/rcu -silent -dropRepository -databaseType ORACLE -connectString $CONNECTION_STRING \\ -dbUser sys -dbRole sysdba -selectDependentsForComponents true -schemaPrefix $RCUPREFIX \\ -component MDS -component IAU -component IAU_APPEND -component IAU_VIEWER -component OPSS \\ -component WLS -component STB -component OAM -f \u0026lt; /tmp/pwd.txt   Delete the contents of the persistent volume, for example:\n$ rm -rf \u0026lt;persistent_volume\u0026gt;/accessdomainpv/* For example:\n$ rm -rf /scratch/shared/accessdomainpv/*   Delete the WebLogic Kubernetes Operator, by running the following command:\n$ helm delete weblogic-kubernetes-operator -n opns   Delete the label from the OAM namespace:\n$ kubectl label namespaces \u0026lt;domain_namespace\u0026gt; weblogic-operator- For example:\n$ kubectl label namespaces oamns weblogic-operator-   Delete the service account for the operator:\n$ kubectl delete serviceaccount \u0026lt;sample-kubernetes-operator-sa\u0026gt; -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl delete serviceaccount op-sa -n opns   Delete the operator namespace:\n$ kubectl delete namespace \u0026lt;sample-kubernetes-operator-ns\u0026gt; For example:\n$ kubectl delete namespace opns   To delete NGINX:\n$ helm delete oam-nginx -n \u0026lt;domain_namespace\u0026gt; For example:\n$ helm delete oam-nginx -n oamns Then run:\n$ helm delete nginx-ingress -n \u0026lt;domain_namespace\u0026gt; For example:\n$ helm delete nginx-ingress -n oamns   Delete the OAM namespace:\n$ kubectl delete namespace \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl delete namespace oamns   "
},
{
	"uri": "/fmw-kubernetes/oid/",
	"title": "Oracle Internet Directory",
	"tags": [],
	"description": "Oracle Internet Directory provides a comprehensive Directory Solution for robust Identity Management",
	"content": "Oracle Internet Directory provides a comprehensive Directory Solution for robust Identity Management. Oracle Internet Directory is an all-in-one directory solution with storage, proxy, synchronization and virtualization capabilities. While unifying the approach, it provides all the services required for high-performance Enterprise and carrier-grade environments. Oracle Internet Directory ensures scalability to billions of entries, ease of installation, elastic deployments, enterprise manageability and effective monitoring.\nThis project supports deployment of Oracle Internet Directory (OID) container images based on the 12cPS4 (12.2.1.4.0) release within a Kubernetes environment. The OID container image refers to binaries for OID Release 12.2.1.4.0.\nThis project has several key features to assist you with deploying and managing Oracle Internet Directory in a Kubernetes environment. You can:\n Create Oracle Internet Directory instances in a Kubernetes persistent volume (PV). This PV can reside in an NFS file system or other Kubernetes volume types. Start servers based on declarative startup parameters and desired states. Expose the Oracle Internet Directory services for external access. Scale Oracle Internet Directory by starting and stopping servers on demand. Monitor the Oracle Internet Directory instance using Prometheus and Grafana.  Follow the instructions in this guide to set up Oracle Internet Directory on Kubernetes.\nCurrent production release The current production release for the Oracle Internet Directory 12c PS4 (12.2.1.4.0) deployment on Kubernetes is 22.2.1.\nRecent changes and known issues See the Release Notes for recent changes and known issues for Oracle Internet Directory deployment on Kubernetes.\nGetting started This documentation explains how to configure OID on a Kubernetes cluster where no other Oracle Identity Management products will be deployed. For detailed information about this type of deployment, start at Prerequisites and follow this documentation sequentially.\nIf performing an Enterprise Deployment, refer to the Enterprise Deployment Guide for Oracle Identity and Access Management in a Kubernetes Cluster instead.\nDocumentation for earlier releases To view documentation for an earlier release, see:\n Version 21.4.1  "
},
{
	"uri": "/fmw-kubernetes/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]