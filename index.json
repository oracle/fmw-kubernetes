[
{
	"uri": "/fmw-kubernetes/oud/prerequisites/",
	"title": "Prerequisites",
	"tags": [],
	"description": "Oracle Unified Directory Prerequisites.",
	"content": "Introduction This document provides information about the system requirements for deploying and running Oracle Unified Directory 12c PS4 (12.2.1.4.0) in a Kubernetes environment.\nSystem Requirements for Oracle Unified Directory on Kubernetes  Kubernetes 1.16.0 or higher (check with kubectl version). Docker 18.03 or higher (check with docker version) Helm 3.0.2+ or higher (check with helm version)  "
},
{
	"uri": "/fmw-kubernetes/oudsm/prerequisites/",
	"title": "Prerequisites",
	"tags": [],
	"description": "Oracle Unified Directory Services Manager Prerequisites.",
	"content": "Introduction This document provides information about the system requirements for deploying and running Oracle Unified Directory Services Manager 12c PS4 (12.2.1.4.0) in a Kubernetes environment.\nSystem Requirements for Oracle Unified Directory Services Manager on Kubernetes  Kubernetes 1.16.0 or higher (check with kubectl version). Docker 18.03 or higher (check with docker version) Helm 3.0.2+ or higher (check with helm version)  "
},
{
	"uri": "/fmw-kubernetes/soa-domains/installguide/prerequisites/",
	"title": "Requirements and limitations",
	"tags": [],
	"description": "Understand the system requirements and limitations for deploying and running Oracle SOA Suite domains with the WebLogic Kubernetes operator, including the SOA cluster sizing recommendations.",
	"content": "This section provides information about the system requirements and limitations for deploying and running Oracle SOA Suite domains with the WebLogic Kubernetes operator.\nSystem requirements for Oracle SOA Suite domains For the current production release 20.4.2:\n Oracle Linux 7 (UL6+) and Red Hat Enterprise Linux 7 (UL3+ only with standalone Kubernetes) are supported. Oracle Linux Cloud Native Environment (OLCNE) version 1.1.2 is supported. Kubernetes 1.14.8+, 1.15.7+, 1.16.0+, 1.17.0+, and 1.18.0+ (check with kubectl version). Docker 18.09.1ce+, 19.03.1+ (check with docker version) or CRI-O 1.14.7 (check with crictl version | grep RuntimeVersion). Flannel networking v0.9.1-amd64 or later (check with docker images | grep flannel). Helm 3.1.3+ (check with helm version --client --short). Oracle WebLogic Kubernetes operator 3.0.1 (see operator releases page). Oracle SOA Suite 12.2.1.4 Docker image downloaded from My Oracle Support (MOS patch 32215749). This image contains the latest bundle patch and one-off patches for Oracle SOA Suite. You must have the cluster-admin role to install the operator. The operator does not need the cluster-admin role at runtime. We do not currently support running SOA in non-Linux containers. Additionally, see the Oracle SOA Suite documentation for other requirements such as database version.  See here for resourse sizing information for Oracle SOA Suite domains setup on Kubernetes cluster.\nLimitations Compared to running a WebLogic Server domain in Kubernetes using the operator, the following limitations currently exist for Oracle SOA Suite domains:\n In this release, Oracle SOA Suite domains are supported using the domain on a persistent volume model only, where the domain home is located in a persistent volume (PV). The \u0026ldquo;domain in image\u0026rdquo; and \u0026ldquo;model in image\u0026rdquo; models are not supported. Also, \u0026ldquo;WebLogic Deploy Tooling (WDT)\u0026rdquo; based deployments are currently not supported. Only configured clusters are supported. Dynamic clusters are not supported for Oracle SOA Suite domains. Note that you can still use all of the scaling features, but you need to define the maximum size of your cluster at domain creation time. Mixed clusters (configured servers targeted to a dynamic cluster) are not supported. The WebLogic Logging Exporter currently supports WebLogic Server logs only. Other logs will not be sent to Elasticsearch. Note, however, that you can use a sidecar with a log handling tool like Logstash or Fluentd to get logs. The WebLogic Monitoring Exporter currently supports WebLogic MBean trees only. Support for JRF and Oracle SOA Suite MBeans is not available. Also, a metrics dashboard specific to Oracle SOA Suite is not available. Instead, use the WebLogic Server dashboard to monitor the Oracle SOA Suite server metrics in Grafana. Some features such as multicast, multitenancy, production redeployment, and Node Manager (although it is used internally for the liveness probe and to start WebLogic Server instances) are not supported in this release. Features such as Java Messaging Service whole server migration, consensus leasing, and maximum availability architecture (Oracle SOA Suite EDG setup) are not supported in this release. Enabling or disabling the memory resiliency for Oracle Service Bus using the Enterprise Manager Console is not supported in this release. Zero downtime upgrade (ZDT) of the domain is not supported.  For up-to-date information about the features of WebLogic Server that are supported in Kubernetes environments, see My Oracle Support Doc ID 2349228.1.\n"
},
{
	"uri": "/fmw-kubernetes/",
	"title": "Oracle Fusion Middleware on Kubernetes",
	"tags": [],
	"description": "This document lists all the Oracle Fusion Middleware products deployment supported on Kubernetes.",
	"content": "Oracle Fusion Middleware on Kubernetes Oracle supports the deployment of the following Oracle Fusion Middleware products on Kubernetes. Click on the appropriate document link below to get started on setting up the product.\n Oracle Access Management  The Oracle WebLogic Server Kubernetes Operator supports deployment of Oracle Access Management (OAM). Follow the instructions in this guide to set up these Oracle Access Management domains on Kubernetes.\n Oracle Identity Governance  The Oracle WebLogic Kubernetes Operator supports deployment of Oracle Identity Governance. Follow the instructions in this guide to set up Oracle Identity Governance domains on Kubernetes.\n Oracle SOA Suite  The Oracle WebLogic Server Kubernetes Operator (the “operator”) supports deployment of Oracle SOA Suite components such as Oracle Service-Oriented Architecture (SOA), Oracle Service Bus (OSB), and Oracle Enterprise Scheduler (ESS). Follow the instructions in this guide to set up these Oracle SOA Suite domains on Kubernetes.\n Oracle Unified Directory  Oracle Unified Directory provides a comprehensive Directory Solution for robust Identity Management\n Oracle Unified Directory Services Manager  Oracle Unified Directory Services Manager provides an interface for managing instances of Oracle Unified Directory\n Oracle WebCenter Sites  The WebLogic Kubernetes operator supports deployment of Oracle WebCenter Sites. Follow the instructions in this guide to set up Oracle WebCenter Sites domains on Kubernetes.\n "
},
{
	"uri": "/fmw-kubernetes/soa-domains/release-notes/",
	"title": "Release Notes",
	"tags": [],
	"description": "",
	"content": "Review the latest changes and known issues for Oracle SOA Suite on Kubernetes.\nRecent changes    Date Version Introduces backward incompatibilities Change     November 30, 2020 20.4.2 no Supports Oracle SOA Suite 12.2.1.4 domains deployment using October 2020 PSU and known bug fixes. Added HEALTHCHECK support for Oracle SOA Suite docker image. Certified Oracle WebLogic Kubernetes operator version 3.0.1.   October 3, 2020 20.3.3 no Certified Oracle WebLogic Kubernetes operator version 3.0.1. Kubernetes 1.14.8+, 1.15.7+, 1.16.0+, 1.17.0+, and 1.18.0+ support. Flannel is the only supported CNI in this release. SSL enabling for the Administration Server and Managed Servers is supported. Only Oracle SOA Suite 12.2.1.4 is supported.    Known issues  Overriding tuning parameters is not supported using configuration overrides Deployments in WebLogic administration console display unexpected error Enterprise Manager console may display ADF_FACES-30200 error Configure the external URL access for Oracle SOA Suite composite applications Configure the external access for the Oracle Enterprise Scheduler WebServices WSDL URLs  "
},
{
	"uri": "/fmw-kubernetes/soa-domains/adminguide/deploying-composites/supportjdev/",
	"title": "Deploy using JDeveloper",
	"tags": [],
	"description": "Deploy Oracle SOA Suite and Oracle Service Bus composite applications from Oracle JDeveloper to Oracle SOA Suite in the WebLogic Kubernetes operator environment.",
	"content": "Learn how to deploy Oracle SOA Suite and Oracle Service Bus composite applications from Oracle JDeveloper (running outside the Kubernetes network) to an Oracle SOA Suite instance in the WebLogic Kubernetes operator environment.\nUse JDeveloper for development and test environments only. For a production environment, you should deploy using Application Control and WLST methods.\n Deploy Oracle SOA Suite and Oracle Service Bus composite applications to Oracle SOA Suite from JDeveloper To deploy Oracle SOA Suite and Oracle Service Bus composite applications from Oracle JDeveloper, the Administration Server must be configured to expose a T3 channel. The WebLogic Kubernetes operator provides an option to expose a T3 channel for the Administration Server using the exposeAdminT3Channel setting during domain creation, then the matching T3 service can be used to connect. By default, when exposeAdminT3Channel is set, the WebLogic Kubernetes operator environment exposes the NodePort for the T3 channel of the NetworkAccessPoint at 30012 (use t3ChannelPort to configure the port to a different value).\nIf you miss enabling exposeAdminT3Channel during domain creation, follow Expose a T3/T3S Channel for the Administration Server to expose a T3 channel manually.\nPrerequisites   Get the Kubernetes cluster master address and verify the T3 port that will be used for creating application server connections. Use the following command to get the T3 port:\n$ kubectl get service \u0026lt;domainUID\u0026gt;-\u0026lt;AdministrationServerName\u0026gt;-external -n \u0026lt;namespace\u0026gt;-o jsonpath='{.spec.ports[0].nodePort}' For example:\n$ kubectl get service soainfra-adminserver-external -n soana-o jsonpath='{.spec.ports[0].nodePort}'   Oracle SOA Suite in the WebLogic Kubernetes operator environment is deployed in a Reference Configuration domain. If a SOA project is developed in Classic mode JDeveloper displays a Mismatch notification in the Deploy Composite Wizard. By default, JDeveloper is in Classic mode. To develop SOA projects in Reference Configuration mode, you must manually enable this feature in JDeveloper: a. From the File menu, select Tools, then Preferences. b. Select Reference Configuration Settings. c. Select Enable Reference Configuration settings in adapters.\n  JDeveloper needs to access the Servers during deployment. In the WebLogic Kubernetes operator environment, Administration and Managed Servers are pods and cannot be accessed directly by JDeveloper. As a workaround, you must configure the reachability of the Managed Servers:\nThe Managed Server T3 port is not exposed by default and opening this will have a security risk as the authentication method here is based on a userid/password. It is not recommended to do this on production instances.\n   Decide on an external IP address to be used to configure access to the Managed Servers. Master or worker node IP address can be used to configure Managed Server reachability. In these steps, the Kubernetes cluster master IP is used for demonstration.\n  Get the pod names of the Administration Server and Managed Servers (that is, \u0026lt;domainUID\u0026gt;-\u0026lt;server name\u0026gt;), which will be used to map in /etc/hosts.\n  Update /etc/hosts (or in Windows, C:\\Windows\\System32\\Drivers\\etc\\hosts) on the host where JDeveloper is running with the entries below, where\n\u0026lt;Master IP\u0026gt; \u0026lt;Administration Server pod name\u0026gt; \u0026lt;Master IP\u0026gt; \u0026lt;Managed Server1 pod name\u0026gt; \u0026lt;Master IP\u0026gt; \u0026lt;Managed Server2 pod name\u0026gt; Sample /etc/hosts entries looks as follows, where X.X.X.X is the master node IP address:\nX.X.X.X soainfra-adminserver X.X.X.X soainfra-soa-server1 X.X.X.X soainfra-soa-server2   Get the Kubernetes service name of the Oracle SOA Suite cluster to access externally with the master IP (or external IP):\n$ kubectl get service \u0026lt;domainUID\u0026gt;-cluster-\u0026lt;soa-cluster\u0026gt; -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl get service soainfra-cluster-soa-cluster -n soans   Create a Kubernetes service to expose the Oracle SOA Suite cluster service (\u0026lt;domainUID\u0026gt;-cluster-\u0026lt;soa-cluster\u0026gt;) externally with same port as the Managed Server:\n$ kubectl expose service \u0026lt;domainUID\u0026gt;-cluster-\u0026lt;soa-cluster\u0026gt; --name \u0026lt;domainUID\u0026gt;-\u0026lt;soa-cluster\u0026gt;-ext --external-ip=\u0026lt;Master IP\u0026gt; -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl expose service soainfra-cluster-soa-cluster --name soainfra-cluster-soa-cluster-ext --external-ip=X.X.X.X -n soans  In a production environment, exposing the SOA cluster service with an external IP address is not recommended, as it can cause message drops on the SOA Managed Servers.\n     Create an application server connection in JDeveloper   Create a new application server connection (for example wls-k8s-op-connection) in JDeveloper:   In the configuration page, provide the WebLogic Hostname as the Kubernetes Master Address.\n  Update the Port as the T3 port (default is 30012) obtained in Prerequisites.\n  Enter the WebLogic Domain value (domainUID).\n  Test the connection to verify it is successful.   Deploy SOA composite applications using JDeveloper   In JDeveloper, right-click the SOA project you want to deploy and select Deploy to display the deployment wizard.   In the Deployment Action page, select Deploy to Application Server and click Next.   In the Deployment Configuration page, select the appropriate options and click Next.   In the Select server page, select the application server connection (wls-k8s-op-connection) that was created earlier and click Next.   If the Prerequisites were configured correctly, the lookup discovers the Managed Servers for deploying the composite.   Using the application server connection, the Managed Servers (Oracle SOA Suite cluster) are listed on the SOA Servers page. Select the Oracle SOA Suite cluster and click Next.   On the Summary page, click Finish to start deploying the composites to the Oracle SOA Suite cluster.   Verify logs on JDeveloper to confirm successful deployment.   Enter the soa-infra URLs in a browser to confirm the composites are deployed on both servers of the Oracle SOA Suite cluster.   Deploy Oracle Service Bus composite applications using JDeveloper   In JDeveloper, right-click the Oracle Service Bus project you want to deploy and select Deploy to display the deployment wizard.   In the Deployment Action page, select Deploy to Application Server and click Next.   In the Select Server page, select the application server connection (wls-k8s-op-connection) that was created earlier and click Next.   On the Summary page, click Finish to start deploying the composites to the Oracle Service Bus cluster.   In JDeveloper, verify logs to confirm successful deployment.   In the Oracle Service Bus Console, click Launch Test Console to verify that the Oracle Service Bus composite application is deployed successfully.   "
},
{
	"uri": "/fmw-kubernetes/soa-domains/appendix/soa-cluster-sizing-info/",
	"title": "Domain resource sizing",
	"tags": [],
	"description": "Describes the resourse sizing information for Oracle SOA Suite domains setup on Kubernetes cluster.",
	"content": "Oracle SOA cluster sizing recommendations    Oracle SOA Normal Usage Moderate Usage High Usage     Administration Server No of CPU core(s) : 1, Memory : 4GB No of CPU core(s) : 1, Memory : 4GB No of CPU core(s) : 1, Memory : 4GB   Managed Server No of Servers : 2, No of CPU core(s) : 2, Memory : 16GB No of Servers : 2, No of CPU core(s) : 4, Memory : 16GB No of Servers : 3, No of CPU core(s) : 6, Memory : 16-32GB   PV Storage Minimum 250GB Minimum 250GB Minimum 500GB    "
},
{
	"uri": "/fmw-kubernetes/soa-domains/patch_and_upgrade/patch-an-image/",
	"title": "Patch an image",
	"tags": [],
	"description": "Create a patched Oracle SOA Suite image using the WebLogic Image Tool.",
	"content": "Oracle releases Oracle SOA Suite images regularly with latest bundle and recommended interim patches in My Oracle Support (MOS). However, if there is a need to create images with new bundle and interim patches, you can build these images using WebLogic Image Tool.\nIf you have access to the Oracle SOA Suite patches, you can patch an existing Oracle SOA Suite image with a bundle patch and interim patches. It is recommended to use the WebLogic Image Tool to patch the Oracle SOA Suite image.\n Recommendations:\n Use the WebLogic Image Tool create feature for patching the Oracle SOA Suite Docker image with a bundle patch and multiple interim patches. This is the recommended approach because it optimizes the size of the image. Use the WebLogic Image Tool update feature for patching the Oracle SOA Suite Docker image with a single interim patch. Note that the patched image size may increase considerably due to additional image layers introduced by the patch application tool.   Apply the patched image   Shut down the domain, by updating the spec.serverStartPolicy: field value to NEVER in domain.yaml configuration file and apply the updated domain.yaml file.\n$ kubectl apply -f domain.yaml   In domain.yaml configuration file, update the image: field value with the patched image and also update the spec.serverStartPolicy: field value to IF_NEEDED.\n  Apply the updated domain.yaml configuration file, to start up the domain.\n$ kubectl apply -f domain.yaml   "
},
{
	"uri": "/fmw-kubernetes/soa-domains/adminguide/configure-load-balancer/",
	"title": "Set up a load balancer",
	"tags": [],
	"description": "Configure different load balancers for Oracle SOA Suite domains.",
	"content": "The Oracle WebLogic Server Kubernetes operator supports ingress-based load balancers such as Traefik, NGINX (kubernetes/ingress-nginx) and Voyager. It also supports Apache webtier load balancer.\n Traefik  Configure the ingress-based Traefik load balancer for Oracle SOA Suite domains.\n NGINX  Configure the ingress-based NGINX load balancer for Oracle SOA Suite domains.\n Voyager  Configure the ingress-based Voyager load balancer for Oracle SOA Suite domains.\n Apache webtier  Configure the Apache webtier load balancer for Oracle SOA Suite domains.\n "
},
{
	"uri": "/fmw-kubernetes/soa-domains/adminguide/configure-load-balancer/traefik/",
	"title": "Traefik",
	"tags": [],
	"description": "Configure the ingress-based Traefik load balancer for Oracle SOA Suite domains.",
	"content": "This section provides information about how to install and configure the ingress-based Traefik load balancer (version 2.2.1 or later for production deployments) to load balance Oracle SOA Suite domain clusters. You can configure Traefik for non-SSL and SSL termination access of the application URL.\nFollow these steps to set up Traefik as a load balancer for an Oracle SOA Suite domain in a Kubernetes cluster:\n  Non-SSL and SSL termination\n Install the Traefik (ingress-based) load balancer Configure Traefik to manage ingresses Create an Ingress for the domain Verify domain application URL access Uninstall the Traefik ingress    End-to-end SSL configuration\n Install the Traefik load balancer for End-to-end SSL Configure Traefik to manage domain Create IngressRouteTCP Verify end-to-end SSL access Uninstall Traefik    Non-SSL and SSL termination Install the Traefik (ingress-based) load balancer   Use Helm to install the Traefik (ingress-based) load balancer. Use the values.yaml file in the sample but set kubernetes.namespaces specifically.\n$ cd ${WORKDIR}/weblogic-kubernetes-operator $ kubectl create namespace traefik $ helm repo add traefik https://containous.github.io/traefik-helm-chart Sample output:\n\u0026#34;traefik\u0026#34; has been added to your repositories   Install Traefik:\n$ helm install traefik traefik/traefik \\  --namespace traefik \\  --values kubernetes/samples/scripts/charts/traefik/values.yaml \\  --set \u0026#34;kubernetes.namespaces={traefik}\u0026#34; \\  --set \u0026#34;service.type=NodePort\u0026#34; --wait    Click here to see the sample output.   LAST DEPLOYED: Sun Sep 13 21:32:00 2020 NAMESPACE: traefik STATUS: deployed REVISION: 1 TEST SUITE: None    A sample values.yaml for deployment of Traefik 2.2.x:\nimage: name: traefik tag: 2.2.8 pullPolicy: IfNotPresent ingressRoute: dashboard: enabled: true # Additional ingressRoute annotations (e.g. for kubernetes.io/ingress.class) annotations: {} # Additional ingressRoute labels (e.g. for filtering IngressRoute by custom labels) labels: {} providers: kubernetesCRD: enabled: true kubernetesIngress: enabled: true # IP used for Kubernetes Ingress endpoints ports: traefik: port: 9000 expose: true # The exposed port for this service exposedPort: 9000 # The port protocol (TCP/UDP) protocol: TCP web: port: 8000 # hostPort: 8000 expose: true exposedPort: 30305 nodePort: 30305 # The port protocol (TCP/UDP) protocol: TCP # Use nodeport if set. This is useful if you have configured Traefik in a # LoadBalancer # nodePort: 32080 # Port Redirections # Added in 2.2, you can make permanent redirects via entrypoints. # https://docs.traefik.io/routing/entrypoints/#redirection # redirectTo: websecure websecure: port: 8443 # # hostPort: 8443 expose: true exposedPort: 30443 # The port protocol (TCP/UDP) protocol: TCP nodePort: 30443   Verify the Traefik status and find the port number of the SSL and non-SSL services:\n$ kubectl get all -n traefik    Click here to see the sample output.   NAME READY STATUS RESTARTS AGE pod/traefik-845f5d6dbb-swb96 1/1 Running 0 32s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/traefik LoadBalancer 10.99.52.249 \u0026lt;pending\u0026gt; 9000:31288/TCP,30305:30305/TCP,30443:30443/TCP 32s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/traefik 1/1 1 1 33s NAME DESIRED CURRENT READY AGE replicaset.apps/traefik-845f5d6dbb 1 1 1 33s      Access the Traefik dashboard through the URL http://$(hostname -f):31288, with the HTTP host traefik.example.com:\n$ curl -H \u0026#34;host: $(hostname -f)\u0026#34; http://$(hostname -f):31288/dashboard/  Note: Make sure that you specify a fully qualified node name for $(hostname -f)\n   Configure Traefik to manage ingresses Configure Traefik to manage ingresses created in this namespace, where traefik is the Traefik namespace and soans is the namespace of the domain:\n$ helm upgrade traefik traefik/traefik --namespace traefik --reuse-values \\  --set \u0026#34;kubernetes.namespaces={traefik,soans}\u0026#34;    Click here to see the sample output.   Release \u0026#34;traefik\u0026#34; has been upgraded. Happy Helming! NAME: traefik LAST DEPLOYED: Sun Sep 13 21:32:12 2020 NAMESPACE: traefik STATUS: deployed REVISION: 2 TEST SUITE: None    Create an ingress for the domain Create an ingress for the domain in the domain namespace by using the sample Helm chart. Here path-based routing is used for ingress. Sample values for default configuration are shown in the file ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain/values.yaml. By default, type is TRAEFIK , tls is Non-SSL, and domainType is soa. These values can be overridden by passing values through the command line or can be edited in the sample file values.yaml based on the type of configuration (non-SSL or SSL). If needed, you can update the ingress YAML file to define more path rules (in section spec.rules.host.http.paths) based on the domain application URLs that need to be accessed. The template YAML file for the Traefik (ingress-based) load balancer is located at ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain/templates/traefik-ingress.yaml\n  Install ingress-per-domain using Helm for non-SSL configuration:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator $ helm install soa-traefik-ingress \\  kubernetes/samples/charts/ingress-per-domain \\  --namespace soans \\  --values kubernetes/samples/charts/ingress-per-domain/values.yaml \\  --set \u0026#34;traefik.hostname=$(hostname -f)\u0026#34; Sample output:\nNAME: soa-traefik-ingress LAST DEPLOYED: Mon Jul 20 11:44:13 2020 NAMESPACE: soans STATUS: deployed REVISION: 1 TEST SUITE: None   For secured access (SSL) to the Oracle SOA Suite application, create a certificate and generate a Kubernetes secret:\n$ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /tmp/tls1.key -out /tmp/tls1.crt -subj \u0026#34;/CN=*\u0026#34; $ kubectl -n soans create secret tls soainfra-tls-cert --key /tmp/tls1.key --cert /tmp/tls1.crt   Create Traefik Middleware custom resource\nIn case of SSL termination, Traefik must pass a custom header WL-Proxy-SSL:true to the WebLogic Server endpoints. Create the Middleware using the following command:\n$ cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: traefik.containo.us/v1alpha1 kind: Middleware metadata: name: wls-proxy-ssl namespace: soans spec: headers: customRequestHeaders: WL-Proxy-SSL: \u0026#34;true\u0026#34; EOF   Create the Traefik TLSStore custom resource.\nIn case of SSL termination, Traefik should be configured to use the user-defined SSL certificate. If the user-defined SSL certificate is not configured, Traefik will create a default SSL certificate. To configure a user-defined SSL certificate for Traefik, use the TLSStore custom resource. The Kubernetes secret created with the SSL certificate should be referenced in the TLSStore object. Run the following command to create the TLSStore:\n$ cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: traefik.containo.us/v1alpha1 kind: TLSStore metadata: name: default namespace: soans spec: defaultCertificate: secretName: soainfra-tls-cert EOF   Install ingress-per-domain using Helm for SSL configuration.\nThe Kubernetes secret name should be updated in the template file.\nThe template file also contains the following annotations:\ntraefik.ingress.kubernetes.io/router.entrypoints: websecure traefik.ingress.kubernetes.io/router.tls: \u0026#34;true\u0026#34; traefik.ingress.kubernetes.io/router.middlewares: soans-wls-proxy-ssl@kubernetescrd The entry point for SSL access and the Middleware name should be updated in the annotation. The Middleware name should be in the form \u0026lt;namespace\u0026gt;-\u0026lt;middleware name\u0026gt;@kubernetescrd.\n$ cd ${WORKDIR}/weblogic-kubernetes-operator $ helm install soa-traefik-ingress \\  kubernetes/samples/charts/ingress-per-domain \\  --namespace soans \\  --values kubernetes/samples/charts/ingress-per-domain/values.yaml \\  --set \u0026#34;traefik.hostname=$(hostname -f)\u0026#34; \\  --set tls=SSL Sample output:\nNAME: soa-traefik-ingress LAST DEPLOYED: Mon Jul 20 11:44:13 2020 NAMESPACE: soans STATUS: deployed REVISION: 1 TEST SUITE: None   For non-SSL access to the Oracle SOA Suite application, get the details of the services by the ingress:\n$ kubectl describe ingress soainfra-traefik -n soans    Click here to see all services supported by the above deployed ingress.   Name: soainfra-traefik Namespace: soans Address: Default backend: default-http-backend:80 (\u0026lt;error: endpoints \u0026#34;default-http-backend\u0026#34; not found\u0026gt;) Rules: Host Path Backends ---- ---- -------- www.example.com /console soainfra-adminserver:7001 (10.244.0.45:7001) /em soainfra-adminserver:7001 (10.244.0.45:7001) /weblogic/ready soainfra-adminserver:7001 (10.244.0.45:7001) soainfra-cluster-soa-cluster:8001 (10.244.0.46:8001,10.244.0.47:8001) /soa-infra soainfra-cluster-soa-cluster:8001 (10.244.0.46:8001,10.244.0.47:8001) /soa/composer soainfra-cluster-soa-cluster:8001 (10.244.0.46:8001,10.244.0.47:8001) /integration/worklistapp soainfra-cluster-soa-cluster:8001 (10.244.0.46:8001,10.244.0.47:8001) Annotations: kubernetes.io/ingress.class: traefik Events: \u0026lt;none\u0026gt;      For SSL access to the Oracle SOA Suite application, get the details of the services by the above deployed ingress:\n$ kubectl describe ingress soainfra-traefik -n soans    Click here to see all services supported by the above deployed ingress.    ``` Name: soainfra-traefik Namespace: soans Address: Default backend: default-http-backend:80 (\u0026lt;error: endpoints \u0026quot;default-http-backend\u0026quot; not found\u0026gt;) TLS: soainfra-tls-cert terminates www.example.com Rules: Host Path Backends ---- ---- -------- www.example.com /console soainfra-adminserver:7001 () /em soainfra-adminserver:7001 () /weblogic/ready soainfra-adminserver:7001 () soainfra-cluster-soa-cluster:8001 () /soa-infra soainfra-cluster-soa-cluster:8001 () /soa/composer soainfra-cluster-soa-cluster:8001 () /integration/worklistapp soainfra-cluster-soa-cluster:8001 () Annotations: kubernetes.io/ingress.class: traefik meta.helm.sh/release-name: soa-traefik-ingress meta.helm.sh/release-namespace: soans traefik.ingress.kubernetes.io/router.entrypoints: websecure traefik.ingress.kubernetes.io/router.middlewares: soans-wls-proxy-ssl@kubernetescrd traefik.ingress.kubernetes.io/router.tls: true Events: \u0026lt;none\u0026gt; ```      To confirm that the load balancer noticed the new ingress and is successfully routing to the domain server pods, you can send a request to the URL for the \u0026ldquo;WebLogic ReadyApp framework\u0026rdquo;, which should return an HTTP 200 status code, as follows:\n$ curl -v http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER_PORT}/weblogic/ready * Trying 149.87.129.203... \u0026gt; GET http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER_PORT}/weblogic/ready HTTP/1.1 \u0026gt; User-Agent: curl/7.29.0 \u0026gt; Accept: */* \u0026gt; Proxy-Connection: Keep-Alive \u0026gt; host: $(hostname -f) \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; Date: Sat, 14 Mar 2020 08:35:03 GMT \u0026lt; Vary: Accept-Encoding \u0026lt; Content-Length: 0 \u0026lt; Proxy-Connection: Keep-Alive \u0026lt; * Connection #0 to host localhost left intact   Verify domain application URL access For non-SSL configuration After setting up the Traefik (ingress-based) load balancer, verify that the domain application URLs are accessible through the non-SSL load balancer port 30305 for HTTP access. The sample URLs for Oracle SOA Suite domain of type soa are:\nhttp://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/weblogic/ready http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/console http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/em http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/soa-infra http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/soa/composer http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/integration/worklistapp For SSL configuration After setting up the Traefik (ingress-based) load balancer, verify that the domain applications are accessible through the SSL load balancer port 30443 for HTTPS access. The sample URLs for Oracle SOA Suite domain of type soa are:\nhttps://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/weblogic/ready https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/console https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/em https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/soa-infra https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/soa/composer https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/integration/worklistapp Uninstall the Traefik ingress Uninstall and delete the ingress deployment:\n$ helm delete soa-traefik-ingress -n soans End-to-end SSL configuration Install the Traefik load balancer for end-to-end SSL   Use Helm to install the Traefik (ingress-based) load balancer. Use the values.yaml file in the sample but set kubernetes.namespaces specifically.\n$ cd ${WORKDIR}/weblogic-kubernetes-operator $ kubectl create namespace traefik $ helm repo add traefik https://containous.github.io/traefik-helm-chart Sample output:\n\u0026#34;traefik\u0026#34; has been added to your repositories   Install Traefik:\n$ helm install traefik traefik/traefik \\  --namespace traefik \\  --values kubernetes/samples/charts/traefik/values.yaml \\  --set \u0026#34;kubernetes.namespaces={traefik}\u0026#34; \\  --set \u0026#34;service.type=NodePort\u0026#34; --wait    Click here to see the sample output.   LAST DEPLOYED: Sun Sep 13 21:32:00 2020 NAMESPACE: traefik STATUS: deployed REVISION: 1 TEST SUITE: None      Verify the Traefik operator status and find the port number of the SSL and non-SSL services:\n$ kubectl get all -n traefik    Click here to see the sample output.   NAME READY STATUS RESTARTS AGE pod/traefik-845f5d6dbb-swb96 1/1 Running 0 32s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/traefik LoadBalancer 10.99.52.249 \u0026lt;pending\u0026gt; 9000:31288/TCP,30305:30305/TCP,30443:30443/TCP 32s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/traefik 1/1 1 1 33s NAME DESIRED CURRENT READY AGE replicaset.apps/traefik-845f5d6dbb 1 1 1 33s      Access the Traefik dashboard through the URL http://$(hostname -f):31288, with the HTTP host traefik.example.com:\n$ curl -H \u0026#34;host: $(hostname -f)\u0026#34; http://$(hostname -f):31288/dashboard/  Note: Make sure that you specify a fully qualified node name for $(hostname -f).\n   Configure Traefik to manage the domain Configure Traefik to manage the domain application service created in this namespace, where traefik is the Traefik namespace and soans is the namespace of the domain:\n$ helm upgrade traefik traefik/traefik --namespace traefik --reuse-values \\  --set \u0026#34;kubernetes.namespaces={traefik,soans}\u0026#34;    Click here to see the sample output.   Release \u0026#34;traefik\u0026#34; has been upgraded. Happy Helming! NAME: traefik LAST DEPLOYED: Sun Sep 13 21:32:12 2020 NAMESPACE: traefik STATUS: deployed REVISION: 2 TEST SUITE: None    Create IngressRouteTCP   To enable SSL passthrough in Traefik, you can configure a TCP router. A sample YAML for IngressRouteTCP is available at ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain/tls/traefik-tls.yaml. The following should be updated in traefik-tls.yaml:\n The service name and the SSL port should be updated in the Services. The load balancer hostname should be updated in the HostSNI rule.  Sample traefik-tls.yaml:\napiVersion: traefik.containo.us/v1alpha1 kind: IngressRouteTCP metadata: name: soa-cluster-routetcp namespace: soans spec: entryPoints: - websecure routes: - match: HostSNI(`${LOADBALANCER_HOSTNAME}`) services: - name: soainfra-cluster-soa-cluster port: 8002 weight: 3 TerminationDelay: 400 tls: passthrough: true   Create the IngressRouteTCP:\n$ kubectl apply -f traefik-tls.yaml   Verify end-to-end SSL access Verify the access to application URLs exposed through the configured service. Because the SOA cluster service is configured, you should be able to access the following SOA domain URLs:\nhttps://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/soa-infra/ https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/soa/composer https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/integration/worklistapp Uninstall Traefik $ helm delete traefik -n traefik "
},
{
	"uri": "/fmw-kubernetes/oam/manage-oam-domains/domain-lifecycle/",
	"title": "Domain Life Cycle",
	"tags": [],
	"description": "Learn about the domain life cyle of an OAM domain.",
	"content": "As OAM domains use the Oracle WebLogic Server Kubernetes Operator, domain lifecyle operations are managed using the Oracle WebLogic Server Kubernetes Operator itself.\nThis document shows the basic operations for starting, stopping and scaling servers in the OAM domain.\nFor more detailed information refer to Domain Life Cycle in the Oracle WebLogic Server Kubernetes Operator documentation.\nDo not use the WebLogic Server Administration Console or Oracle Enterprise Manager Console to start or stop servers.\n View existing OAM servers The default OAM deployment starts the AdminServer (AdminServer), two OAM Managed Servers (oam_server1 and oam_server2) and one OAM Policy Manager server (oam_policy_mgr1).\nThe deployment also creates, but doesn\u0026rsquo;t start, three extra OAM Managed Servers (oam-server3 to oam-server5) and four more OAM Policy Manager servers (oam_policy_mgr2 to oam_policy_mgr5).\nAll these servers are visible in the WebLogic Server Console https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/console by navigating to Domain Structure \u0026gt; oamcluster \u0026gt; Environment \u0026gt; Servers.\nTo view the running servers using kubectl, run the following command:\n$ kubectl get pods -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get pods -n accessns The output should look similar to the following:\nNAME READY STATUS RESTARTS AGE accessinfra-adminserver 1/1 Running 0 18h accessinfra-create-oam-infra-domain-job-vj69h 0/1 Completed 0 23h accessinfra-oam-policy-mgr1 1/1 Running 0 18h accessinfra-oam-server1 1/1 Running 0 18h accessinfra-oam-server2 1/1 Running 0 18h helper 1/1 Running 0 40h voyager-accessinfra-voyager-698764d6d-w8pbt 1/1 Running 0 8m47s bash-4.2$ Starting/Scaling up OAM Managed Servers The number of OAM Managed Servers running is dependent on the replicas parameter configured for the cluster. To start more OAM Managed Servers perform the following steps:\n  Run the following kubectl command to edit the domain:\n$ kubectl edit domain \u0026lt;domain_uid\u0026gt; -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl edit domain accessinfra -n accessns Note: This opens an edit session for the domain where parameters can be changed using standard vi commands.\n  In the edit session search for \u0026ldquo;clusterName: oam_cluster\u0026rdquo; and look for the replicas parameter. By default the replicas parameter is set to \u0026ldquo;2\u0026rdquo; hence two OAM Managed Servers are started (oam_server1 and oam_server2):\n clusters: - clusterName: oam_cluster replicas: 2 serverPod: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - podAffinityTerm: labelSelector: matchExpressions: - key: weblogic.clusterName operator: In values: - $(CLUSTER_NAME)   To start more OAM Managed Servers, increase the replicas value as desired. In the example below, two more managed servers will be started by setting replicas to \u0026ldquo;4\u0026rdquo;:\n clusters: - clusterName: oam_cluster replicas: 4 serverPod: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - podAffinityTerm: labelSelector: matchExpressions: - key: weblogic.clusterName operator: In values: - $(CLUSTER_NAME)   Save the file and exit (:wq!)\nThe output will look similar to the following:\ndomain.weblogic.oracle/accessinfra edited   Run the following kubectl command to view the pods:\n$ kubectl get pods -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get pods -n accessns The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE accessinfra-adminserver 1/1 Running 0 18h accessinfra-create-oam-infra-domain-job-vj69h 0/1 Completed 0 23h accessinfra-oam-policy-mgr1 1/1 Running 0 18h accessinfra-oam-server1 1/1 Running 0 18h accessinfra-oam-server2 1/1 Running 0 18h accessinfra-oam-server3 0/1 Running 0 6s accessinfra-oam-server4 0/1 Running 0 6s helper 1/1 Running 0 40h voyager-accessinfra-voyager-698764d6d-w8pbt 1/1 Running 0 10m Two new pods (accessinfra-oam-server3 and accessinfra-oam-server4) are started, but currently have a READY status of 0/1. This means oam_server3 and oam_server4 are not currently running but are in the process of starting. The servers will take several minutes to start so keep executing the command until READY shows 1/1:\nNAME READY STATUS RESTARTS AGE accessinfra-adminserver 1/1 Running 0 18h accessinfra-create-oam-infra-domain-job-vj69h 0/1 Completed 0 24h accessinfra-oam-policy-mgr1 1/1 Running 0 18h accessinfra-oam-server1 1/1 Running 0 18h accessinfra-oam-server2 1/1 Running 0 18h accessinfra-oam-server3 1/1 Running 0 5m5s accessinfra-oam-server4 1/1 Running 0 5m5s helper 1/1 Running 0 41h voyager-accessinfra-voyager-698764d6d-w8pbt 1/1 Running 0 15m Note: To check what is happening during server startup when READY is 0/1, run the following command to view the log of the pod that is starting:\n$ kubectl logs \u0026lt;pod\u0026gt; -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl logs accessinfra-oam-server3 -n accessns   To start more OAM Policy Manager servers, repeat the previous commands but change the replicas parameter for the policy_cluster. In the example below replicas has been increased to \u0026ldquo;2\u0026rdquo;:\n- clusterName: policy_cluster replicas: 2 serverPod: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - podAffinityTerm: labelSelector: matchExpressions: - key: weblogic.clusterName operator: In values: - $(CLUSTER_NAME) After saving the changes a new pod will be started. After a few minutes it will have a READY status of 1/1. In the example below accessinfra-oam-policy-mgr2 is started:\nNAME READY STATUS RESTARTS AGE accessinfra-adminserver 1/1 Running 0 18h accessinfra-create-oam-infra-domain-job-vj69h 0/1 Completed 0 24h accessinfra-oam-policy-mgr1 1/1 Running 0 18h accessinfra-oam-policy-mgr2 1/1 Running 0 4m3s accessinfra-oam-server1 1/1 Running 0 18h accessinfra-oam-server2 1/1 Running 0 18h accessinfra-oam-server3 1/1 Running 0 10m accessinfra-oam-server4 1/1 Running 0 10m helper 1/1 Running 0 41h voyager-accessinfra-voyager-698764d6d-w8pbt 1/1 Running 0 21m   Stopping/Scaling down OAM Managed Servers As mentioned in the previous section, the number of OAM Managed Servers running is dependent on the replicas parameter configured for the cluster. To stop one or more OAM Managed Servers, perform the following:\n  Run the following kubectl command to edit the domain:\n$ kubectl edit domain \u0026lt;domain_uid\u0026gt; -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl edit domain accessinfra -n accessns   In the edit session search for \u0026ldquo;clusterName: oam_cluster\u0026rdquo; and look for the replicas parameter. In the example below replicas is set to \u0026ldquo;4\u0026rdquo;, hence four OAM Managed Servers are started (oam_server1 - oam_server4):\nclusters: - clusterName: oam_cluster replicas: 4 serverPod: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - podAffinityTerm: labelSelector: matchExpressions: - key: weblogic.clusterName operator: In values: - $(CLUSTER_NAME)   To stop OAM Managed Servers, decrease the replicas value as desired. In the example below, we will stop two managed servers by setting replicas to \u0026ldquo;2\u0026rdquo;:\n clusters: - clusterName: oam_cluster replicas: 2 serverPod: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - podAffinityTerm: labelSelector: matchExpressions: - key: weblogic.clusterName operator: In values: - $(CLUSTER_NAME)   Save the file and exit (:wq!)\n  Run the following kubectl command to view the pods:\n$ kubectl get pods -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get pods -n accessns The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE accessinfra-adminserver 1/1 Running 0 18h accessinfra-create-oam-infra-domain-job-vj69h 0/1 Completed 0 24h accessinfra-oam-policy-mgr1 1/1 Running 0 18h accessinfra-oam-policy-mgr2 1/1 Running 0 5m21s accessinfra-oam-server1 1/1 Running 0 18h accessinfra-oam-server2 1/1 Running 0 18h accessinfra-oam-server3 1/1 Terminating 0 12m accessinfra-oam-server4 1/1 Terminating 0 12m helper 1/1 Running 0 41h voyager-accessinfra-voyager-698764d6d-w8pbt 1/1 Running 0 23m Two pods now have a STATUS of Terminating (accessinfra-oam-server3 and accessinfra-oam-server4). The servers will take a minute or two to stop, so keep executing the command until the pods have disappeared:\nNAME READY STATUS RESTARTS AGE accessinfra-adminserver 1/1 Running 0 18h accessinfra-create-oam-infra-domain-job-vj69h 0/1 Completed 0 24h accessinfra-oam-policy-mgr1 1/1 Running 0 18h accessinfra-oam-policy-mgr2 1/1 Running 0 6m3s accessinfra-oam-server1 1/1 Running 0 18h accessinfra-oam-server2 1/1 Running 0 18h helper 1/1 Running 0 41h voyager-accessinfra-voyager-698764d6d-w8pbt 1/1 Running 0 23m   To stop OAM Policy Manager servers, repeat the previous commands but change the replicas parameter for the policy_cluster. In the example below replicas has been decreased from \u0026ldquo;2\u0026rdquo; to \u0026ldquo;1\u0026rdquo;:\n- clusterName: policy_cluster replicas: 1 serverPod: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - podAffinityTerm: labelSelector: matchExpressions: - key: weblogic.clusterName operator: In values: - $(CLUSTER_NAME) After saving the changes one pod will move to a STATUS of Terminating (accessinfra-oam-policy-mgr2).\nNAME READY STATUS RESTARTS AGE accessinfra-adminserver 1/1 Running 0 18h accessinfra-create-oam-infra-domain-job-vj69h 0/1 Completed 0 24h accessinfra-oam-policy-mgr1 1/1 Running 0 18h accessinfra-oam-policy-mgr2 1/1 Terminating 0 7m12s accessinfra-oam-server1 1/1 Running 0 18h accessinfra-oam-server2 1/1 Running 0 18h helper 1/1 Running 0 41h voyager-accessinfra-voyager-698764d6d-w8pbt 1/1 Running 0 24m The server will take a minute or two to stop, so keep executing the command until the pod has disappeared:\nNAME READY STATUS RESTARTS AGE accessinfra-adminserver 1/1 Running 0 18h accessinfra-create-oam-infra-domain-job-vj69h 0/1 Completed 0 24h accessinfra-oam-policy-mgr1 1/1 Running 0 18h accessinfra-oam-server1 1/1 Running 0 18h accessinfra-oam-server2 1/1 Running 0 18h helper 1/1 Running 0 41h voyager-accessinfra-voyager-698764d6d-w8pbt 1/1 Running 0 25m   Stopping and Starting the AdminServer and Managed Servers To stop all the OAM Managed Servers and the AdminServer in one operation:\n  Run the following kubectl command to edit the domain:\n$ kubectl edit domain \u0026lt;domain_uid\u0026gt; -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl edit domain accessinfra -n accessns   In the edit session search for serverStartPolicy: IF_NEEDED:\n volumes: - name: weblogic-domain-storage-volume persistentVolumeClaim: claimName: accessinfra-domain-pvc serverStartPolicy: IF_NEEDED webLogicCredentialsSecret: name: accessinfra-domain-credentials   Change serverStartPolicy: IF_NEEDED to NEVER as follows:\n volumes: - name: weblogic-domain-storage-volume persistentVolumeClaim: claimName: accessinfra-domain-pvc serverStartPolicy: NEVER webLogicCredentialsSecret: name: accessinfra-domain-credentials   Save the file and exit (:wq!).\n  Run the following kubectl command to view the pods:\n$ kubectl get pods -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get pods -n accessns The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE accessinfra-adminserver 1/1 Terminating 0 18h accessinfra-create-oam-infra-domain-job-vj69h 0/1 Completed 0 24h accessinfra-oam-policy-mgr1 1/1 Terminating 0 18h accessinfra-oam-server1 1/1 Terminating 0 18h accessinfra-oam-server2 1/1 Terminating 0 18h helper 1/1 Running 0 41h voyager-accessinfra-voyager-698764d6d-w8pbt 1/1 Running 0 27m The AdminServer pods and Managed Server pods will move to a STATUS of Terminating. After a few minutes, run the command again and the pods should have disappeared:\nNAME READY STATUS RESTARTS AGE accessinfra-create-oam-infra-domain-job-vj69h 0/1 Completed 0 24h helper 1/1 Running 0 41h voyager-accessinfra-voyager-698764d6d-w8pbt 1/1 Running 0 28m   To start the AdminServer and Managed Servers up again, repeat the previous steps but change serverStartPolicy: NEVER to IF_NEEDED as follows:\n volumes: - name: weblogic-domain-storage-volume persistentVolumeClaim: claimName: accessinfra-domain-pvc serverStartPolicy: IF_NEEDED webLogicCredentialsSecret: name: accessinfra-domain-credentials   Run the following kubectl command to view the pods:\n$ kubectl get pods -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get pods -n accessns The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE accessinfra-create-oam-infra-domain-job-vj69h 0/1 Completed 0 24h accessinfra-introspect-domain-job-7qx29 1/1 Running 0 8s helper 1/1 Running 0 41h voyager-accessinfra-voyager-698764d6d-w8pbt 1/1 Running 0 29m The AdminServer pod will start followed by the OAM Managed Servers pods. This process will take several minutes, so keep executing the command until all the pods are running with READY status 1/1 :\nNAME READY STATUS RESTARTS AGE accessinfra-adminserver 1/1 Running 0 6m4s accessinfra-create-oam-infra-domain-job-vj69h 0/1 Completed 0 24h accessinfra-oam-policy-mgr1 1/1 Running 0 3m5s accessinfra-oam-server1 1/1 Running 0 3m5s accessinfra-oam-server2 1/1 Running 0 3m5s helper 1/1 Running 0 41h voyager-accessinfra-voyager-698764d6d-w8pbt 1/1 Running 0 36m   "
},
{
	"uri": "/fmw-kubernetes/oig/manage-oig-domains/domain-lifecycle/",
	"title": "Domain Life Cycle",
	"tags": [],
	"description": "Learn about the domain life cyle of an OIG domain.",
	"content": " View Existing OIG Servers Starting/Scaling up OIG Managed Servers Stopping/Scaling down OIG Managed Servers Stopping and Starting the AdminServer and Managed Servers  As OIG domains use the Oracle WebLogic Kubernetes Operator, domain lifecyle operations are managed using the Oracle WebLogic Kubernetes Operator itself.\nThis document shows the basic operations for starting, stopping and scaling servers in the OIG domain.\nFor more detailed information refer to Domain Life Cycle in the Oracle WebLogic Kubernetes Operator documentation.\nDo not use the WebLogic Server Administration Console or Oracle Enterprise Manager Console to start or stop servers.\n View Existing OIG Servers The default OIG deployment starts the AdminServer (AdminServer), one OIG Managed Server (oim_server1) and one SOA Managed Server (soa_server1).\nThe deployment also creates, but doesn\u0026rsquo;t start, four extra OIG Managed Servers (oim-server2 to oim-server5) and four more SOA Managed Servers (soa_server2 to soa_server5).\nAll these servers are visible in the WebLogic Server Administration Console https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/console by navigating to Domain Structure \u0026gt; oimcluster \u0026gt; Environment \u0026gt; Servers.\nTo view the running servers using kubectl, run the following command:\n$ kubectl get pods -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get pods -n oimcluster The output should look similar to the following:\n$ kubectl get pods -n oimcluster oimcluster-adminserver 1/1 Running 0 23h oimcluster-create-fmw-infra-sample-domain-job-dktkk 0/1 Completed 0 24h oimcluster-oim-server1 1/1 Running 0 23h oimcluster-soa-server1 1/1 Running 0 23h Starting/Scaling up OIG Managed Servers The number of OIG Managed Servers running is dependent on the replicas parameter configured for the cluster. To start more OIG Managed Servers perform the following steps:\n  Run the following kubectl command to edit the domain:\n$ kubectl edit domain \u0026lt;domain_uid\u0026gt; -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl edit domain oimcluster -n oimcluster Note: This opens an edit session for the domain where parameters can be changed using standard vi commands.\n  In the edit session search for \u0026ldquo;clusterName: oim_cluster\u0026rdquo; and look for the replicas parameter. By default the replicas parameter is set to \u0026ldquo;1\u0026rdquo; hence a single OIG Managed Server is started (oim_server1):\n - clusterName: oim_cluster clusterService: annotations: traefik.ingress.kubernetes.io/affinity: \u0026quot;true\u0026quot; replicas: 1 serverPod: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - podAffinityTerm: labelSelector: matchExpressions: - key: weblogic.clusterName operator: In values: - $(CLUSTER_NAME)   To start more OIG Managed Servers, increase the replicas value as desired. In the example below, one more Managed Server will be started by setting replicas to \u0026ldquo;2\u0026rdquo;:\n - clusterName: oim_cluster clusterService: annotations: traefik.ingress.kubernetes.io/affinity: \u0026quot;true\u0026quot; replicas: 2 serverPod: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - podAffinityTerm: labelSelector: matchExpressions: - key: weblogic.clusterName operator: In values: - $(CLUSTER_NAME)   Save the file and exit (:wq)\nThe output will look similar to the following:\ndomain.weblogic.oracle/oimcluster edited   Run the following kubectl command to view the pods:\n$ kubectl get pods -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get pods -n oimcluster The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE oimcluster-adminserver 1/1 Running 0 23h oimcluster-create-fmw-infra-sample-domain-job-dktkk 0/1 Completed 0 24h oimcluster-oim-server1 1/1 Running 0 23h oimcluster-oim-server2 0/1 Running 0 7s oimcluster-soa-server1 1/1 Running 0 23h One new pod (oimcluster-oim-server2) is started, but currently has a READY status of 0/1. This means oim_server2 is not currently running but is in the process of starting. The server will take several minutes to start so keep executing the command until READY shows 1/1:\nNAME READY STATUS RESTARTS AGE oimcluster-adminserver 1/1 Running 0 23h oimcluster-create-fmw-infra-sample-domain-job-dktkk 0/1 Completed 0 24h oimcluster-oim-server1 1/1 Running 0 23h oimcluster-oim-server2 1/1 Running 0 5m27s oimcluster-soa-server1 1/1 Running 0 23h Note: To check what is happening during server startup when READY is 0/1, run the following command to view the log of the pod that is starting:\n$ kubectl logs \u0026lt;pod\u0026gt; -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl logs oimcluster-oim-server2 -n oimcluster   Stopping/Scaling down OIG Managed Servers As mentioned in the previous section, the number of OIG Managed Servers running is dependent on the replicas parameter configured for the cluster. To stop one or more OIG Managed Servers, perform the following:\n  Run the following kubectl command to edit the domain:\n$ kubectl edit domain \u0026lt;domain_uid\u0026gt; -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl edit domain oimcluster -n oimcluster   In the edit session search for \u0026ldquo;clusterName: oim_cluster\u0026rdquo; and look for the replicas parameter. In the example below replicas is set to \u0026ldquo;2\u0026rdquo; hence two OIG Managed Servers are started (oim_server1 and oim_server2):\n - clusterName: oim_cluster clusterService: annotations: traefik.ingress.kubernetes.io/affinity: \u0026quot;true\u0026quot; replicas: 2 serverPod: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - podAffinityTerm: labelSelector: matchExpressions: - key: weblogic.clusterName operator: In values: - $(CLUSTER_NAME)   To stop OIG Managed Servers, decrease the replicas value as desired. In the example below, we will stop one Managed Server by setting replicas to \u0026ldquo;1\u0026rdquo;:\n - clusterName: oim_cluster clusterService: annotations: traefik.ingress.kubernetes.io/affinity: \u0026quot;true\u0026quot; replicas: 1 serverPod: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - podAffinityTerm: labelSelector: matchExpressions: - key: weblogic.clusterName operator: In values: - $(CLUSTER_NAME)   Save the file and exit (:wq)\n  Run the following kubectl command to view the pods:\n$ kubectl get pods -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get pods -n oimcluster The output will look similar to the following:\n$ kubectl get pods -n oimcluster NAME READY STATUS RESTARTS AGE oimcluster-adminserver 1/1 Running 0 23h oimcluster-create-fmw-infra-sample-domain-job-dktkk 0/1 Completed 0 24h oimcluster-oim-server1 1/1 Running 0 23h oimcluster-oim-server2 1/1 Terminating 0 7m30s oimcluster-soa-server1 1/1 Running 0 23h The exiting pod shows a STATUS of Terminating (oimcluster-oim-server2). The server may take a minute or two to stop, so keep executing the command until the pod has disappeared:\n$ kubectl get pods -n oimcluster NAME READY STATUS RESTARTS AGE oimcluster-adminserver 1/1 Running 0 23h oimcluster-create-fmw-infra-sample-domain-job-dktkk 0/1 Completed 0 24h oimcluster-oim-server1 1/1 Running 0 23h oimcluster-soa-server1 1/1 Running 0 23h   Stopping and Starting the AdminServer and Managed Servers To stop all the OIG Managed Servers and the AdminServer in one operation:\n  Run the following kubectl command to edit the domain:\n$ kubectl edit domain \u0026lt;domain_uid\u0026gt; -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl edit domain oimcluster -n oimcluster   In the edit session search for serverStartPolicy: IF_NEEDED:\n volumeMounts: - mountPath: /u01/oracle/user_projects/domains name: weblogic-domain-storage-volume volumes: - name: weblogic-domain-storage-volume persistentVolumeClaim: claimName: oimcluster-oim-pvc serverStartPolicy: IF_NEEDED   Change serverStartPolicy: IF_NEEDED to NEVER as follows:\n volumeMounts: - mountPath: /u01/oracle/user_projects/domains name: weblogic-domain-storage-volume volumes: - name: weblogic-domain-storage-volume persistentVolumeClaim: claimName: oimcluster-oim-pvc serverStartPolicy: NEVER   Save the file and exit (:wq).\n  Run the following kubectl command to view the pods:\n$ kubectl get pods -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get pods -n oimcluster The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE oimcluster-adminserver 1/1 Terminating 0 23h oimcluster-create-fmw-infra-sample-domain-job-dktkk 0/1 Completed 0 24h oimcluster-oim-server1 1/1 Terminating 0 23h oimcluster-soa-server1 1/1 Terminating 0 23h The AdminServer pod and Managed Server pods will move to a STATUS of Terminating. After a few minutes, run the command again and the pods should have disappeared:\nNAME READY STATUS RESTARTS AGE oimcluster-create-fmw-infra-sample-domain-job-dktkk 0/1 Completed 0 24h $   To start the AdminServer and Managed Servers up again, repeat the previous steps but change serverStartPolicy: NEVER to IF_NEEDED as follows:\n volumeMounts: - mountPath: /u01/oracle/user_projects/domains name: weblogic-domain-storage-volume volumes: - name: weblogic-domain-storage-volume persistentVolumeClaim: claimName: oimcluster-oim-pvc serverStartPolicy: IF_NEEDED   Run the following kubectl command to view the pods:\n$ kubectl get pods -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get pods -n oimcluster The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE oimcluster-adminserver 0/1 Running 0 22s oimcluster-create-fmw-infra-sample-domain-job-dktkk 0/1 Completed 0 24h The AdminServer pod will start followed by the OIG Managed Servers pods. This process will take several minutes, so keep executing the command until all the pods are running with READY status 1/1 :\nNAME READY STATUS RESTARTS AGE oimcluster-adminserver 1/1 Running 0 6m57s oimcluster-create-fmw-infra-sample-domain-job-dktkk 0/1 Completed 0 24h oimcluster-oim-server1 1/1 Running 0 4m33s oimcluster-soa-server1 1/1 Running 0 4m33s   "
},
{
	"uri": "/fmw-kubernetes/oam/",
	"title": "Oracle Access Management",
	"tags": [],
	"description": "The Oracle WebLogic Server Kubernetes Operator supports deployment of Oracle Access Management (OAM). Follow the instructions in this guide to set up these Oracle Access Management domains on Kubernetes.",
	"content": "The Oracle WebLogic Server Kubernetes Operator supports deployment of Oracle Access Management (OAM).\nIn this release, OAM domains are supported using the “domain on a persistent volume” model only, where the domain home is located in a persistent volume (PV).\nThe Oracle WebLogic Server Kubernetes Operator has several key features to assist you with deploying and managing Oracle Access Management domains in a Kubernetes environment. You can:\n Create OAM instances in a Kubernetes persistent volume. This persistent volume can reside in an NFS file system or other Kubernetes volume types. Start servers based on declarative startup parameters and desired states. Expose the OAM Services through external access. Scale OAM domains by starting and stopping Managed Servers on demand. Publish operator and WebLogic Server logs into Elasticsearch and interact with them in Kibana. Monitor the OAM instance using Prometheus and Grafana.  Limitations See here for limitations in this release.\nGetting started For detailed information about deploying Oracle Access Management domains, start at Prerequisites and follow this documentation sequentially.\nCurrent release The current supported release of the Oracle WebLogic Server Kubernetes Operator, for Oracle Access Management domains deployment is 3.0.1.\n"
},
{
	"uri": "/fmw-kubernetes/wcsites-domains/pre-requisites/",
	"title": "Pre-requisites ",
	"tags": [],
	"description": "Pre-requisites for setting up WebCenter Sites domains with WebLogic Kubernetes Operator",
	"content": "Contents  Introduction System Requirements Limitations WebCenter Sites Cluster Sizing Recommendations  Introduction This document describes the special considerations for deploying and running a WebCenter Sites domain with the WebLogic Kubernetes Operator. Other than those considerations listed here, WebCenter Sites domains work in the same way as Fusion Middleware Infrastructure domains and WebLogic Server domains.\nIn this release, WebCenter Sites domains are supported using the domain on a persistent volume model only where a WebCenter Sites domain is located in a persistent volume (PV).\nSystem Requirements  Kubernetes 1.13.0+, 1.14.0+, and 1.15.0+ (check with kubectl version). Flannel networking v0.9.1-amd64 (check with docker images | grep flannel). Docker 18.9.1 (check with docker version) Helm 2.14.3+ (check with helm version). Oracle Fusion Middleware Infrastructure 12.2.1.4.0 image. You must have the cluster-admin role to install the operator. These proxy setup are used for pulling the required binaries and source code from the respective repositories:  export NO_PROXY=\u0026quot;localhost,127.0.0.0/8,$(hostname -i),.your-company.com,/var/run/docker.sock\u0026rdquo; export no_proxy=\u0026quot;localhost,127.0.0.0/8,$(hostname -i),.your-company.com,/var/run/docker.sock\u0026rdquo; export http_proxy=http://www-proxy-your-company.com:80 export https_proxy=http://www-proxy-your-company.com:80 export HTTP_PROXY=http://www-proxy-your-company.com:80 export HTTPS_PROXY=http://www-proxy-your-company.com:80    NOTE: Add your host IP by using hostname -i and also nslookup IP addresses to the no_proxy, NO_PROXY list above.\nLimitations Compared to running a WebLogic Server domain in Kubernetes using the Operator, the following limitations currently exist for WebCenter Sites domain:\n Domain in image model is not supported in this version of the Operator. Only configured clusters are supported. Dynamic clusters are not supported for WebCenter Sites domains. Note that you can still use all of the scaling features. You just need to define the maximum size of your cluster at domain creation time. We do not currently support running WebCenter Sites in non-Linux containers. Deploying and running a WebCenter Sites domain is supported only in Operator versions 2.4.0 and later. The WebLogic Logging Exporter currently supports WebLogic Server logs only. Other logs will not be sent to Elasticsearch. Note, however, that you can use a sidecar with a log handling tool like Logstash or Fluentd to get logs. The WebLogic Monitoring Exporter currently supports the WebLogic MBean trees only. Support for JRF MBeans has not been added yet.  WebCenter Sites Cluster Sizing Recommendations    WebCenter Sites Normal Usage Moderate Usage High Usage     Admin Server No of CPU(s) : 1, Memory : 4GB No of CPU(s) : 1, Memory : 4GB No of CPU(s) : 1, Memory : 4GB   Managed Server No of Servers : 2, No of CPU(s) : 2, Memory : 16GB No of Servers : 2, No of CPU(s) : 4, Memory : 16GB No of Servers : 3, No of CPU(s) : 6, Memory : 16-32GB   PV Storage Minimum 250GB Minimum 250GB Minimum 500GB    "
},
{
	"uri": "/fmw-kubernetes/oam/prerequisites/",
	"title": "Prerequisites",
	"tags": [],
	"description": "System requirements and limitations for deploying and running an OAM domain home",
	"content": "Introduction This document provides information about the system requirements and limitations for deploying and running OAM domains with the Oracle WebLogic Server Kubernetes Operator 3.0.1.\nIn this release, OAM domains are supported using the “domain on a persistent volume” model only, where the domain home is located in a persistent volume (PV).\nSystem requirements for oam domains  Kubernetes 1.16.0+, 1.17.0+, and 1.18.0+ (check with kubectl version). Flannel networking v0.9.1-amd64 or later (check with docker images | grep flannel). Docker 18.09.1+ or 19.03.1+ (check with docker version) Helm 3.1.3+ (check with helm version). You must have the cluster-admin role to install the operator. We do not currently support running OAM in non-Linux containers. A running Oracle Database 12.2.0.1 or later. The database must be a supported version for OAM as outlined in Oracle Fusion Middleware 12c certifications.  Limitations Compared to running a WebLogic Server domain in Kubernetes using the operator, the following limitations currently exist for OAM domains:\n The \u0026ldquo;domain in image\u0026rdquo; model is not supported. Only configured clusters are supported. Dynamic clusters are not supported for OAM domains. Note that you can still use all of the scaling features, you just need to define the maximum size of your cluster at domain creation time. Deploying and running OAM domains is supported only with Oracle WebLogic Server Kubernetes Operator version 3.0.1 The WebLogic Monitoring Exporter currently supports the WebLogic MBean trees only. Support for JRF MBeans has not been added yet.  "
},
{
	"uri": "/fmw-kubernetes/oig/prerequisites/",
	"title": "Prerequisites",
	"tags": [],
	"description": "Sample for creating an OIG Suite domain home on an existing PV or PVC, and the domain resource YAML file for deploying the generated OIG domain.",
	"content": "Introduction This document provides information about the system requirements and limitations for deploying and running OIG domains with the Oracle WebLogic Kubernetes Operator 3.0.1.\nIn this release, OIG domains are supported using the “domain on a persistent volume” model only, where the domain home is located in a persistent volume (PV).\nSystem requirements for OIG domains  Kubernetes 1.14.8+, 1.15.7+, 1.16.0+, 1.17.0+, and 1.18.0+ (check with kubectl version). Flannel networking v0.9.1-amd64 or later (check with docker images | grep flannel). Docker 18.9.1 or 19.03.1 (check with docker version). Helm 3.1.3+ (check with helm version). You must have the cluster-admin role to install the operator. We do not currently support running OIG in non-Linux containers. A running Oracle Database 12.2.0.1 or later. The database must be a supported version for OIG as outlined in Oracle Fusion Middleware 12c certifications, and must meet the requirements as outlined in About Database Requirements for an Oracle Fusion Middleware Installation. Java Developer Kit (11.0.3 or later recommended)  Limitations Compared to running a WebLogic Server domain in Kubernetes using the operator, the following limitations currently exist for OIG domains:\n The \u0026ldquo;domain in image\u0026rdquo; model is not supported. Only configured clusters are supported. Dynamic clusters are not supported for OIG domains. Note that you can still use all of the scaling features, you just need to define the maximum size of your cluster at domain creation time. Deploying and running OIG domains is supported only with Oracle WebLogic Kubernetes Operator version 3.0.1 currently supports the WebLogic MBean trees only. The WebLogic Monitoring Exporter currently supports the WebLogic MBean trees only. Support for JRF MBeans has not been added yet.  "
},
{
	"uri": "/fmw-kubernetes/soa-domains/installguide/",
	"title": "Install Guide",
	"tags": [],
	"description": "",
	"content": "Install the WebLogic Kubernetes operator and prepare and deploy Oracle SOA Suite domains.\n Requirements and limitations  Understand the system requirements and limitations for deploying and running Oracle SOA Suite domains with the WebLogic Kubernetes operator, including the SOA cluster sizing recommendations.\n Prepare your environment  Prepare for creating Oracle SOA Suite domains, including required secrets creation, persistent volume and volume claim creation, database creation, and database schema creation.\n Create Oracle SOA Suite domains  Create an Oracle SOA Suite domain home on an existing PV or PVC, and create the domain resource YAML file for deploying the generated Oracle SOA Suite domain.\n "
},
{
	"uri": "/fmw-kubernetes/soa-domains/appendix/quickstart-deployment-on-prem/",
	"title": "Quick start deployment on-premise",
	"tags": [],
	"description": "Describes how to quickly get an Oracle SOA Suite domain instance running (using the defaults, nothing special) for development and test purposes.",
	"content": "Use this Quick Start to create an Oracle SOA Suite domain deployment in a Kubernetes cluster (on-premise environments) with the Oracle WebLogic Server Kubernetes operator. Note that this walkthrough is for demonstration purposes only, not for use in production. These instructions assume that you are already familiar with Kubernetes. If you need more detailed instructions, refer to the Install Guide.\nHardware requirements Supported Linux kernel for deploying and running Oracle SOA Suite domains with the operator is Oracle Linux 7 (UL6+) and Red Hat Enterprise Linux 7 (UL3+ only with standalone Kubernetes). Refer to the prerequisites for more details.\nFor this exercise the minimum hardware requirement to create a single node Kubernetes cluster and then deploy soaessosb (domain with SOA, OSB, and ESS) domain type with one managed server for SOA and one for OSB Cluster running along with Oracle Database running as a container\n   Hardware Size     RAM 32GB   Disk Space 250GB+   CPU core(s) 6    See here for resourse sizing information for Oracle SOA Suite domains setup on Kubernetes cluster.\nSet up Oracle SOA Suite in an on-premise environment Perform the steps in this topic to create a single instance on-premise Kubernetes cluster and then create an Oracle SOA Suite soaessosb domain type, which deploys a domain with Oracle Service-Oriented Architecture (SOA), Oracle Service Bus (OSB), and Oracle Enterprise Scheduler (ESS).\n Step 1 - Prepare a virtual machine for the Kubernetes cluster Step 2 - Set up a single instance Kubernetes cluster Step 3 - Get scripts and images Step 4 - Install the WebLogic Kubernetes Operator Step 5 - Install the Traefik (ingress-based) load balancer Step 6 - Create and configure an Oracle SOA Suite Domain  1. Prepare a virtual machine for the Kubernetes cluster For illustration purposes, these instructions are for Oracle Linux 7u6+. If you are using a different flavor of Linux, you will need to adjust the steps accordingly.\nThese steps must be run with the root user, unless specified otherwise. Any time you see YOUR_USERID in a command, you should replace it with your actual userid.\n 1.1 Prerequisites   Choose the directories where your Docker and Kubernetes files will be stored. The Docker directory should be on a disk with a lot of free space (more than 100GB) because it will be used for the Docker file system, which contains all of your images and containers. The Kubernetes directory is used for the /var/lib/kubelet file system and persistent volume storage.\n$ export docker_dir=/u01/docker $ export kubelet_dir=/u01/kubelet $ mkdir -p $docker_dir $kubelet_dir $ ln -s $kubelet_dir /var/lib/kubelet   Verify that IPv4 forwarding is enabled on your host.\nNote: Replace eth0 with the ethernet interface name of your compute resource if it is different.\n$ /sbin/sysctl -a 2\u0026gt;\u0026amp;1|grep -s 'net.ipv4.conf.docker0.forwarding' $ /sbin/sysctl -a 2\u0026gt;\u0026amp;1|grep -s 'net.ipv4.conf.eth0.forwarding' $ /sbin/sysctl -a 2\u0026gt;\u0026amp;1|grep -s 'net.ipv4.conf.lo.forwarding' $ /sbin/sysctl -a 2\u0026gt;\u0026amp;1|grep -s 'net.ipv4.ip_nonlocal_bind' For example: Verify that all are set to 1\n$ net.ipv4.conf.docker0.forwarding = 1 $ net.ipv4.conf.eth0.forwarding = 1 $ net.ipv4.conf.lo.forwarding = 1 $ net.ipv4.ip_nonlocal_bind = 1 Solution: Set all values to 1 immediately with the following commands:\n$ /sbin/sysctl net.ipv4.conf.docker0.forwarding=1 $ /sbin/sysctl net.ipv4.conf.eth0.forwarding=1 $ /sbin/sysctl net.ipv4.conf.lo.forwarding=1 $ /sbin/sysctl net.ipv4.ip_nonlocal_bind=1 To preserve the settings post-reboot: Update the above values to 1 in files in /usr/lib/sysctl.d/, /run/sysctl.d/, and /etc/sysctl.d/\n  Verify the iptables rule for forwarding.\nKubernetes uses iptables to handle many networking and port forwarding rules. A standard Docker installation may create a firewall rule that prevents forwarding.\nVerify if the iptables rule to accept forwarding traffic is set:\n$ /sbin/iptables -L -n | awk '/Chain FORWARD / {print $4}' | tr -d \u0026quot;)\u0026quot; If the output is \u0026ldquo;DROP\u0026rdquo;, then run the following command:\n$ /sbin/iptables -P FORWARD ACCEPT Verify if the iptables rule is set properly to \u0026ldquo;ACCEPT\u0026rdquo;:\n$ /sbin/iptables -L -n | awk '/Chain FORWARD / {print $4}' | tr -d \u0026quot;)\u0026quot;   Disable and stop firewalld:\n$ systemctl disable firewalld $ systemctl stop firewalld   1.2 Install and configure Docker  Note : If you have already installed Docker with version 18.03+ and configured Docker daemon root to sufficient disk space along with proxy settings, continue to Install and configure Kubernetes\n   Make sure that you have the right operating system version:\n$ uname -a $ more /etc/oracle-release For example:\nLinux xxxxxxx 4.1.12-124.27.1.el7uek.x86_64 #2 SMP Mon May 13 08:56:17 PDT 2019 x86_64 x86_64 x86_64 GNU/Linux Oracle Linux Server release 7.6   Install the latest docker-engine and start the Docker service:\n$ yum-config-manager --enable ol7_addons $ yum install docker-engine $ systemctl enable docker $ systemctl start docker   Add your userid to the Docker group. This will allow you to run the Docker commands without root access:\n$ /sbin/usermod -a -G docker \u0026lt;YOUR_USERID\u0026gt;   Check your Docker version. It must be at least 18.03.\n$ docker version For example:\nClient: Docker Engine - Community Version: 19.03.1-ol API version: 1.40 Go version: go1.12.5 Git commit: ead9442 Built: Wed Sep 11 06:40:28 2019 OS/Arch: linux/amd64 Experimental: false Server: Docker Engine - Community Engine: Version: 19.03.1-ol API version: 1.40 (minimum version 1.12) Go version: go1.12.5 Git commit: ead9442 Built: Wed Sep 11 06:38:43 2019 OS/Arch: linux/amd64 Experimental: false Default Registry: docker.io containerd: Version: v1.2.0-rc.0-108-gc444666 GitCommit: c4446665cb9c30056f4998ed953e6d4ff22c7c39 runc: Version: 1.0.0-rc5+dev GitCommit: 4bb1fe4ace1a32d3676bb98f5d3b6a4e32bf6c58 docker-init: Version: 0.18.0 GitCommit: fec3683   Update the Docker engine configuration:\n$ mkdir -p /etc/docker $ cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/docker/daemon.json { \u0026quot;group\u0026quot;: \u0026quot;docker\u0026quot;, \u0026quot;data-root\u0026quot;: \u0026quot;/u01/docker\u0026quot; } EOF   Configure proxy settings if you are behind an HTTP proxy. On some hosts /etc/systemd/system/docker.service.d may not be available. Create this directory if it is not available.\n ### Create the drop-in file /etc/systemd/system/docker.service.d/http-proxy.conf that contains proxy details: $ cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/systemd/system/docker.service.d/http-proxy.conf [Service] Environment=\u0026quot;HTTP_PROXY=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT\u0026quot; Environment=\u0026quot;HTTPS_PROXY=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT\u0026quot; Environment=\u0026quot;NO_PROXY=localhost,127.0.0.0/8,ADD-YOUR-INTERNAL-NO-PROXY-LIST,/var/run/docker.sock\u0026quot; EOF   Restart the Docker daemon to load the latest changes:\n$ systemctl daemon-reload $ systemctl restart docker   Verify that the proxy is configured with Docker:\n$ docker info|grep -i proxy For example:\nHTTP Proxy: http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT HTTPS Proxy: http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT No Proxy: localhost,127.0.0.0/8,ADD-YOUR-INTERNAL-NO-PROXY-LIST,/var/run/docker.sock   Verify Docker installation:\n$ docker run hello-world For example:\nHello from Docker! This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the \u0026quot;hello-world\u0026quot; image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal. To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bash Share images, automate workflows, and more with a free Docker ID: https://hub.docker.com/ For more examples and ideas, visit: https://docs.docker.com/get-started/   1.3 Install and configure Kubernetes   Add the external Kubernetes repository:\n$ cat \u0026lt;\u0026lt;EOF | sudo tee /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\\$basearch enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg exclude=kubelet kubeadm kubectl EOF   Set SELinux in permissive mode (effectively disabling it):\n$ export PATH=/sbin:$PATH $ setenforce 0 $ sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config   Export proxy and install kubeadm, kubelet, and kubectl:\n### Get the nslookup IP address of the master node to use with apiserver-advertise-address during setting up Kubernetes master ### as the host may have different internal ip (hostname -i) and nslookup $HOSTNAME $ ip_addr=`nslookup $(hostname -f) | grep -m2 Address | tail -n1| awk -F: '{print $2}'| tr -d \u0026quot; \u0026quot;` $ echo $ip_addr ### Set the proxies $ export NO_PROXY=localhost,127.0.0.0/8,ADD-YOUR-INTERNAL-NO-PROXY-LIST,/var/run/docker.sock,$ip_addr $ export no_proxy=localhost,127.0.0.0/8,ADD-YOUR-INTERNAL-NO-PROXY-LIST,/var/run/docker.sock,$ip_addr $ export http_proxy=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT $ export https_proxy=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT $ export HTTPS_PROXY=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT $ export HTTP_PROXY=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT ### install kubernetes 1.18.4-1 $ VERSION=1.18.4-1 $ yum install -y kubelet-$VERSION kubeadm-$VERSION kubectl-$VERSION --disableexcludes=kubernetes ### enable kubelet service so that it auto-restart on reboot $ systemctl enable --now kubelet   Ensure net.bridge.bridge-nf-call-iptables is set to 1 in your sysctl to avoid traffic routing issues:\n$ cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF $ sysctl --system   Disable swap check:\n$ sed -i 's/KUBELET_EXTRA_ARGS=/KUBELET_EXTRA_ARGS=\u0026quot;--fail-swap-on=false\u0026quot;/' /etc/sysconfig/kubelet $ cat /etc/sysconfig/kubelet ### Reload and restart kubelet $ systemctl daemon-reload $ systemctl restart kubelet   1.4 Set up Helm   Install Helm v3.x.\na. Download Helm from https://github.com/helm/helm/releases. Example to download Helm v3.2.4:\n$ wget https://get.helm.sh/helm-v3.2.4-linux-amd64.tar.gz b. Unpack tar.gz:\n$ tar -zxvf helm-v3.2.4-linux-amd64.tar.gz c. Find the Helm binary in the unpacked directory, and move it to its desired destination:\n$ mv linux-amd64/helm /usr/bin/helm   Run helm version to verify its installation:\n$ helm version version.BuildInfo{Version:\u0026quot;v3.2.4\u0026quot;, GitCommit:\u0026quot;0ad800ef43d3b826f31a5ad8dfbb4fe05d143688\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;, GoVersion:\u0026quot;go1.13.12\u0026quot;}   2. Set up a single instance Kubernetes cluster  Notes:\n These steps must be run with the root user, unless specified otherwise! If you choose to use a different cidr block (that is, other than 10.244.0.0/16 for the --pod-network-cidr= in the kubeadm init command), then also update NO_PROXY and no_proxy with the appropriate value.  Also make sure to update kube-flannel.yaml with the new value before deploying.   Replace the following with appropriate values:  ADD-YOUR-INTERNAL-NO-PROXY-LIST REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT     2.1 Set up the master node   Create a shell script that sets up the necessary environment variables. You can append this to the user’s .bashrc so that it will run at login. You must also configure your proxy settings here if you are behind an HTTP proxy:\n## grab my IP address to pass into kubeadm init, and to add to no_proxy vars ip_addr=`nslookup $(hostname -f) | grep -m2 Address | tail -n1| awk -F: '{print $2}'| tr -d \u0026quot; \u0026quot;` export pod_network_cidr=\u0026quot;10.244.0.0/16\u0026quot; export service_cidr=\u0026quot;10.96.0.0/12\u0026quot; export PATH=$PATH:/sbin:/usr/sbin ### Set the proxies export NO_PROXY=localhost,127.0.0.0/8,ADD-YOUR-INTERNAL-NO-PROXY-LIST,/var/run/docker.sock,$ip_addr,$pod_network_cidr,$service_cidr export no_proxy=localhost,127.0.0.0/8,ADD-YOUR-INTERNAL-NO-PROXY-LIST,/var/run/docker.sock,$ip_addr,$pod_network_cidr,$service_cidr export http_proxy=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT export https_proxy=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT export HTTPS_PROXY=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT export HTTP_PROXY=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT   Source the script to set up your environment variables:\n$ . ~/.bashrc   To implement command completion, add the following to the script:\n$ [ -f /usr/share/bash-completion/bash_completion ] \u0026amp;\u0026amp; . /usr/share/bash-completion/bash_completion $ source \u0026lt;(kubectl completion bash)   Run kubeadm init to create the master node:\n$ kubeadm init \\ --pod-network-cidr=$pod_network_cidr \\ --apiserver-advertise-address=$ip_addr \\ --ignore-preflight-errors=Swap \u0026gt; /tmp/kubeadm-init.out 2\u0026gt;\u0026amp;1   Log in to the terminal with YOUR_USERID:YOUR_GROUP. Then set up the ~/.bashrc similar to steps 1 to 3 with YOUR_USERID:YOUR_GROUP.\n Note that from now on we will be using YOUR_USERID:YOUR_GROUP to execute any kubectl commands and not root.\n   Set up YOUR_USERID:YOUR_GROUP to access the Kubernetes cluster:\n$ mkdir -p $HOME/.kube $ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config $ sudo chown $(id -u):$(id -g) $HOME/.kube/config   Verify that YOUR_USERID:YOUR_GROUP is set up to access the Kubernetes cluster using the kubectl command:\n$ kubectl get nodes  Note: At this step, the node is not in ready state as we have not yet installed the pod network add-on. After the next step, the node will show status as Ready.\n   Install a pod network add-on (flannel) so that your pods can communicate with each other.\n Note: If you are using a different cidr block than 10.244.0.0/16, then download and update kube-flannel.yml with the correct cidr address before deploying into the cluster:\n $ kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.12.0/Documentation/kube-flannel.yml   Verify that the master node is in Ready status:\n$ kubectl get nodes For example:\nNAME STATUS ROLES AGE VERSION mymasternode Ready master 8m26s v1.18.4 or:\n$ kubectl get pods -n kube-system For example:\nNAME READY STATUS RESTARTS AGE pod/coredns-86c58d9df4-58p9f 1/1 Running 0 3m59s pod/coredns-86c58d9df4-mzrr5 1/1 Running 0 3m59s pod/etcd-mymasternode 1/1 Running 0 3m4s pod/kube-apiserver-node 1/1 Running 0 3m21s pod/kube-controller-manager-mymasternode 1/1 Running 0 3m25s pod/kube-flannel-ds-amd64-6npx4 1/1 Running 0 49s pod/kube-proxy-4vsgm 1/1 Running 0 3m59s pod/kube-scheduler-mymasternode 1/1 Running 0 2m58s   To schedule pods on the master node, taint the node:\n$ kubectl taint nodes --all node-role.kubernetes.io/master-   Congratulations! Your Kubernetes cluster environment is ready to deploy your Oracle SOA Suite domain.\nFor additional references on Kubernetes cluster setup, check the cheat sheet.\n3. Get scripts and images 3.1 Set up the code repository to deploy Oracle SOA Suite domains Follow these steps to set up the source code repository required to deploy Oracle SOA Suite domains.\n3.2 Get required Docker images and add them to your local registry   If you don\u0026rsquo;t already have one, obtain a Docker store account, log in to the Docker store, and accept the license agreement for the WebLogic Server image.\n  Log in to the Docker store from your Docker client:\n$ docker login   Pull the operator image:\n$ docker pull oracle/weblogic-kubernetes-operator:3.0.1   Obtain the Oracle Database image and Oracle SOA Suite Docker image from the Oracle Container Registry:\na. For first time users, to pull an image from the Oracle Container Registry, navigate to https://container-registry.oracle.com and log in using the Oracle Single Sign-On (SSO) authentication service. If you do not already have SSO credentials, click the Sign In link at the top of the page to create them.\nUse the web interface to accept the Oracle Standard Terms and Restrictions for the Oracle software images that you intend to deploy. Your acceptance of these terms are stored in a database that links the software images to your Oracle Single Sign-On login credentials.\nTo obtain the image, log in to the Oracle Container Registry:\n$ docker login container-registry.oracle.com b. Find and then pull the Oracle Database image for 12.2.0.1:\n$ docker pull container-registry.oracle.com/database/enterprise:12.2.0.1-slim c. Find and then pull the prebuilt Oracle SOA Suite image 12.2.1.4 install image:\n$ docker pull container-registry.oracle.com/middleware/soasuite:12.2.1.4  Note: This image does not contain any Oracle SOA Suite product patches and can only be used for test and development purposes.\n   4. Install the WebLogic Kubernetes operator 4.1 Prepare for the WebLogic Kubernetes operator.   Create a namespace opns for the operator:\n$ kubectl create namespace opns   Create a service account op-sa for the operator in the operator’s namespace:\n$ kubectl create serviceaccount -n opns op-sa   4.2 Install the WebLogic Kubernetes operator Use Helm to install and start the operator from the directory you just cloned:\n $ cd ${WORKDIR}/weblogic-kubernetes-operator $ helm install weblogic-kubernetes-operator kubernetes/charts/weblogic-operator \\ --namespace opns \\ --set image=oracle/weblogic-kubernetes-operator:3.0.1 \\ --set serviceAccount=op-sa \\ --set \u0026quot;domainNamespaces={}\u0026quot; \\ --wait 4.3 Verify the WebLogic Kubernetes operator   Verify that the operator’s pod is running by listing the pods in the operator’s namespace. You should see one for the operator:\n$ kubectl get pods -n opns   Verify that the operator is up and running by viewing the operator pod\u0026rsquo;s logs:\n$ kubectl logs -n opns -c weblogic-operator deployments/weblogic-operator   The WebLogic Kubernetes operator v3.0.1 has been installed. Continue with the load balancer and Oracle SOA Suite domain setup.\n5. Install the Traefik (ingress-based) load balancer The Oracle WebLogic Server Kubernetes operator supports three load balancers: Traefik, Voyager, and Apache. Samples are provided in the documentation.\nThis Quick Start demonstrates how to install the Traefik ingress controller to provide load balancing for an Oracle SOA Suite domain.\n  Create a namespace for Traefik:\n$ kubectl create namespace traefik   Set up Helm for 3rd party services:\n$ helm repo add traefik https://containous.github.io/traefik-helm-chart   Install the Traefik operator in the traefik namespace with the provided sample values:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator $ helm install traefik traefik/traefik \\ --namespace traefik \\ --values kubernetes/samples/scripts/charts/traefik/values.yaml \\ --set \u0026quot;kubernetes.namespaces={traefik}\u0026quot; \\ --set \u0026quot;service.type=NodePort\u0026quot; \\ --wait   6. Create and configure an Oracle SOA Suite domain 6.1 Prepare for an Oracle SOA Suite domain   Create a namespace that can host Oracle SOA Suite domains:\n$ kubectl create namespace soans   Use Helm to configure the operator to manage Oracle SOA Suite domains in this namespace:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator $ helm upgrade weblogic-kubernetes-operator kubernetes/charts/weblogic-operator \\ --reuse-values \\ --namespace opns \\ --set \u0026quot;domainNamespaces={soans}\u0026quot; \\ --wait   Create Kubernetes secrets.\na. Create a Kubernetes secret for the domain in the same Kubernetes namespace as the domain. In this example, the username is weblogic, the password in Welcome1, and the namespace is soans:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-weblogic-domain-credentials $ ./create-weblogic-credentials.sh \\ -u weblogic \\ -p Welcome1 \\ -n soans \\ -d soainfra \\ -s soainfra-domain-credentials b. Create a Kubernetes secret for the RCU in the same Kubernetes namespace as the domain:\n Schema user : SOA1 Schema password : Oradoc_db1 DB sys user password : Oradoc_db1 Domain name : soainfra Domain Namespace : soans Secret name : soainfra-rcu-credentials  $ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-rcu-credentials $ ./create-rcu-credentials.sh \\ -u SOA1 \\ -p Oradoc_db1 \\ -a sys \\ -q Oradoc_db1 \\ -d soainfra \\ -n soans \\ -s soainfra-rcu-credentials   Create the Kubernetes persistence volume and persistence volume claim.\na. Create the Oracle SOA Suite domain home directory. Determine if a user already exists on your host system with uid:gid of 1000:\n$ sudo getent passwd 1000 If this command returns a username (which is the first field), you can skip the following useradd command. If not, create the oracle user with useradd:\n$ sudo useradd -u 1000 -g 1000 oracle Create the directory that will be used for the Oracle SOA Suite domain home:\n$ sudo mkdir /scratch/k8s_dir $ sudo chown -R 1000:1000 /scratch/k8s_dir b. Update create-pv-pvc-inputs.yaml with the following values:\n baseName: domain domainUID: soainfra namespace: soans weblogicDomainStoragePath: /scratch/k8s_dir  $ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-weblogic-domain-pv-pvc $ cp create-pv-pvc-inputs.yaml create-pv-pvc-inputs.yaml.orig $ sed -i -e \u0026quot;s:baseName\\: weblogic-sample:baseName\\: domain:g\u0026quot; create-pv-pvc-inputs.yaml $ sed -i -e \u0026quot;s:domainUID\\::domainUID\\: soainfra:g\u0026quot; create-pv-pvc-inputs.yaml $ sed -i -e \u0026quot;s:namespace\\: default:namespace\\: soans:g\u0026quot; create-pv-pvc-inputs.yaml $ sed -i -e \u0026quot;s:#weblogicDomainStoragePath\\: /scratch/k8s_dir:weblogicDomainStoragePath\\: /scratch/k8s_dir:g\u0026quot; create-pv-pvc-inputs.yaml c. Run the create-pv-pvc.sh script to create the PV and PVC configuration files:\n$ ./create-pv-pvc.sh -i create-pv-pvc-inputs.yaml -o output d. Create the PV and PVC using the configuration files created in the previous step:\n$ kubectl create -f output/pv-pvcs/soainfra-domain-pv.yaml $ kubectl create -f output/pv-pvcs/soainfra-domain-pvc.yaml   Install and configure the database for the Oracle SOA Suite domain.\nThis step is required only when a standalone database is not already set up and you want to use the database in a container.\nThe Oracle Database Docker images are supported only for non-production use. For more details, see My Oracle Support note: Oracle Support for Database Running on Docker (Doc ID 2216342.1). For production, it is suggested to use a standalone database. This example provides steps to create the database in a container.\n a. Create a database in a container:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-oracle-db-service $ ./start-db-service.sh -i container-registry.oracle.com/database/enterprise:12.2.0.1-slim -p none Once the database is successfully created, you can use the database connection string oracle-db.default.svc.cluster.local:1521/devpdb.k8s as an rcuDatabaseURL parameter in the create-domain-inputs.yaml file.\nb. Create Oracle SOA Suite schemas for the domain type (for example, soaessosb).\nTo install the Oracle SOA Suite schemas, run the create-rcu-schema.sh script with the following inputs:\n -s \u0026lt;RCU PREFIX\u0026gt; Here: SOA1 -t \u0026lt;SOA domain type\u0026gt; Here: soaessosb -i \u0026lt;SOASuite image\u0026gt; Here: container-registry.oracle.com/middleware/soasuite:12.2.1.4  $ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-rcu-schema $ ./create-rcu-schema.sh \\ -s SOA1 \\ -t soaessosb \\ -i container-registry.oracle.com/middleware/soasuite:12.2.1.4 \\ -q Oradoc_db1 \\ -r Oradoc_db1   Now the environment is ready to start the Oracle SOA Suite domain creation.\n6.2 Create an Oracle SOA Suite domain   The sample scripts for Oracle SOA Suite domain deployment are available at \u0026lt;weblogic-kubernetes-operator-project\u0026gt;/kubernetes/samples/scripts/create-soa-domain. You must edit create-domain-inputs.yaml (or a copy of it) to provide the details for your domain.\nUpdate create-domain-inputs.yaml with the following values for domain creation:\n domainType: soaessosb initialManagedServerReplicas: 1  $ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-soa-domain/domain-home-on-pv/ $ cp create-domain-inputs.yaml create-domain-inputs.yaml.orig $ sed -i -e \u0026quot;s:domainType\\: soa:domainType\\: soaessosb:g\u0026quot; create-domain-inputs.yaml $ sed -i -e \u0026quot;s:initialManagedServerReplicas\\: 2:initialManagedServerReplicas\\: 1:g\u0026quot; create-domain-inputs.yaml $ sed -i -e \u0026quot;s:image\\: soasuite\\:12.2.1.4:image\\: container-registry.oracle.com/middleware/soasuite\\:12.2.1.4:g\u0026quot; create-domain-inputs.yaml   Run the create-domain.sh script to create a domain:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-soa-domain/domain-home-on-pv/ $ ./create-domain.sh -i create-domain-inputs.yaml -o output   Create a Kubernetes domain object:\nOnce the create-domain.sh is successful, it generates the output/weblogic-domains/soainfra/domain.yaml that you can use to create the Kubernetes resource domain, which starts the domain and servers:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-soa-domain/domain-home-on-pv $ kubectl create -f output/weblogic-domains/soainfra/domain.yaml   Verify that the Kubernetes domain object named soainfra is created:\n$ kubectl get domain -n soans NAME AGE soainfra 3m18s   Once you create the domain, introspect pod is created. This inspects the domain home and then starts the soainfra-adminserver pod. Once the soainfra-adminserver pod starts successfully, then the Managed Server pods are started in parallel. Watch the soans namespace for the status of domain creation:\n$ kubectl get pods -n soans -w   Verify that the Oracle SOA Suite domain server pods and services are created and in Ready state:\n$ kubectl get all -n soans   6.3 Configure Traefik to access in Oracle SOA Suite domain services   Configure Traefik to manage ingresses created in the Oracle SOA Suite domain namespace (soans):\n$ helm upgrade traefik traefik/traefik \\ --reuse-values \\ --namespace traefik \\ --set \u0026quot;kubernetes.namespaces={traefik,soans}\u0026quot; \\ --wait   Create an ingress for the domain in the domain namespace by using the sample Helm chart:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator $ helm install soa-traefik-ingress kubernetes/samples/charts/ingress-per-domain \\ --namespace soans \\ --values kubernetes/samples/charts/ingress-per-domain/values.yaml \\ --set \u0026quot;traefik.hostname=$(hostname -f)\u0026quot; \\ --set domainType=soaessosb   Verify the created ingress per domain details:\n$ kubectl describe ingress soainfra-traefik -n soans   6.4 Verify that you can access the Oracle SOA Suite domain URL   Get the LOADBALANCER_HOSTNAME for your environment:\nexport LOADBALANCER_HOSTNAME=$(hostname -f)   The following URLs are available for Oracle SOA Suite domains of domain type soaessosb:\nCredentials: username: weblogic password: Welcome1\nhttp://${LOADBALANCER_HOSTNAME}:30305/console http://${LOADBALANCER_HOSTNAME}:30305/em http://${LOADBALANCER_HOSTNAME}:30305/servicebus http://${LOADBALANCER_HOSTNAME}:30305/soa-infra http://${LOADBALANCER_HOSTNAME}:30305/soa/composer http://${LOADBALANCER_HOSTNAME}:30305/integration/worklistapp http://${LOADBALANCER_HOSTNAME}:30305/ess http://${LOADBALANCER_HOSTNAME}:30305/EssHealthCheck   "
},
{
	"uri": "/fmw-kubernetes/oud/prepare-your-environment/",
	"title": "Prepare Your Environment",
	"tags": [],
	"description": "Prepare your environment",
	"content": " Set up your Kubernetes Cluster Check the Kubernetes Cluster is Ready Install the Oracle Unified Directory Image Setup the Code Repository To Deploy Oracle Unified Directory  Set up your Kubernetes Cluster If you need help setting up a Kubernetes environment, check our cheat sheet.\nIt is recommended you have a master node and one or more worker nodes. The examples in this documentation assume one master and two worker nodes.\nAfter creating Kubernetes clusters, you can optionally:\n Configure an Ingress to direct traffic to backend instances.  Check the Kubernetes Cluster is Ready  Run the following command on the master node to check the cluster and worker nodes are running:  $ kubectl get nodes,pods -n kube-system The output will look similar to the following:\nNAME STATUS ROLES AGE VERSION node/10.89.73.203 Ready \u0026lt;none\u0026gt; 66d v1.18.4 node/10.89.73.204 Ready \u0026lt;none\u0026gt; 66d v1.18.4 node/10.89.73.42 Ready master 67d v1.18.4 NAME READY STATUS RESTARTS AGE pod/coredns-66bff467f8-slxdq 1/1 Running 1 67d pod/coredns-66bff467f8-v77qt 1/1 Running 1 67d pod/etcd-10.89.73.42 1/1 Running 1 67d pod/kube-apiserver-10.89.73.42 1/1 Running 1 67d pod/kube-controller-manager-10.89.73.42 1/1 Running 27 67d pod/kube-flannel-ds-amd64-r2m8r 1/1 Running 2 48d pod/kube-flannel-ds-amd64-rdhrf 1/1 Running 2 6d1h pod/kube-flannel-ds-amd64-vpcbj 1/1 Running 3 66d pod/kube-proxy-jtcxm 1/1 Running 1 67d pod/kube-proxy-swfmm 1/1 Running 1 66d pod/kube-proxy-w6x6t 1/1 Running 1 66d pod/kube-scheduler-10.89.73.42 1/1 Running 29 67d Install the Oracle Unified Directory Image You can deploy Oracle Unified Directory images in the following ways:\n Download a pre-built Oracle Unified Directory image from My Oracle Support. by referring to the document ID 2723908.1. This image is prebuilt by Oracle and includes Oracle Unified Directory 12.2.1.4.0 and the latest PSU. Build your own Oracle Unified Directory container image either by using the WebLogic Image Tool or by using the dockerfile, scripts and base image from Oracle Container Registry (OCR). You can also build your own image by using only the dockerfile and scripts. For more information about the various ways in which you can build your own container image, see Installing the Oracle Unified Directory Image.  Choose one of these options based on your requirements.\nThe Oracle Unified Directory image must be installed on the master node and each of the worker nodes in your Kubernetes cluster. Alternatively you can place the image in a docker registry that your cluster can access.\n After installing the Oracle Unified Directory image run the following command to make sure the image is installed correctly on the master and worker nodes:\n$ docker images The output will look similar to the following:\nREPOSITORY TAG IMAGE ID CREATED SIZE oracle/oud 12.2.1.4.0 8a937042bef3 3 weeks ago 992MB k8s.gcr.io/kube-proxy v1.18.4 718fa77019f2 3 months ago 117MB k8s.gcr.io/kube-scheduler v1.18.4 c663567f869e 3 months ago 95.3MB k8s.gcr.io/kube-controller-manager v1.18.4 e8f1690127c4 3 months ago 162MB k8s.gcr.io/kube-apiserver v1.18.4 408913fc18eb 3 months ago 173MB quay.io/coreos/flannel v0.12.0-amd64 4e9f801d2217 6 months ago 52.8MB k8s.gcr.io/pause 3.2 80d28bedfe5d 7 months ago 683kB k8s.gcr.io/coredns 1.6.7 67da37a9a360 8 months ago 43.8MB k8s.gcr.io/etcd 3.4.3-0 303ce5db0e90 11 months ago 288MB quay.io/prometheus/node-exporter v0.18.1 e5a616e4b9cf 16 months ago 22.9MB quay.io/coreos/kube-rbac-proxy v0.4.1 70eeaa7791f2 20 months ago 41.3MB ... Setup the Code Repository To Deploy Oracle Unified Directory Oracle Unified Directory deployment on Kubernetes leverages deployment scripts provided by Oracle for creating Oracle Unified Directory containers using samples or Helm charts provided. To deploy Oracle Unified Directory on Kubernetes you should set up the deployment scripts on the master node as below:\nCreate a working directory to setup the source code.\n$ mkdir \u0026lt;work directory\u0026gt; For example:\n$ mkdir /scratch/OUDContainer From the directory you created, download the Oracle Unified Directory deployment scripts from the Oracle Unified Directory repository.\n$ git clone https://github.com/oracle/fmw-kubernetes.git You can now use the deployment scripts from \u0026lt;work directory\u0026gt;/fmw-kubernetes/OracleUnifiedDirectory/kubernetes/samples/ to set up the Oracle Unified Directory environments as further described in this document.\n"
},
{
	"uri": "/fmw-kubernetes/oudsm/prepare-your-environment/",
	"title": "Prepare Your Environment",
	"tags": [],
	"description": "Prepare your environment",
	"content": " Set up your Kubernetes Cluster Check the Kubernetes Cluster is Ready Install the Oracle Unified Directory Services Manager Image Setup the Code Repository To Deploy Oracle Unified Directory Services Manager  Set up your Kubernetes Cluster If you need help setting up a Kubernetes environment, check our cheat sheet.\nIt is recommended you have a master node and one or more worker nodes. The examples in this documentation assume one master and two worker nodes.\nAfter creating Kubernetes clusters, you can optionally:\n Configure an Ingress to direct traffic to backend instances.  Check the Kubernetes Cluster is Ready  Run the following command on the master node to check the cluster and worker nodes are running:  $ kubectl get nodes,pods -n kube-system The output will look similar to the following:\nNAME STATUS ROLES AGE VERSION node/10.89.73.203 Ready \u0026lt;none\u0026gt; 66d v1.18.4 node/10.89.73.204 Ready \u0026lt;none\u0026gt; 66d v1.18.4 node/10.89.73.42 Ready master 67d v1.18.4 NAME READY STATUS RESTARTS AGE pod/coredns-66bff467f8-slxdq 1/1 Running 1 67d pod/coredns-66bff467f8-v77qt 1/1 Running 1 67d pod/etcd-10.89.73.42 1/1 Running 1 67d pod/kube-apiserver-10.89.73.42 1/1 Running 1 67d pod/kube-controller-manager-10.89.73.42 1/1 Running 27 67d pod/kube-flannel-ds-amd64-r2m8r 1/1 Running 2 48d pod/kube-flannel-ds-amd64-rdhrf 1/1 Running 2 6d1h pod/kube-flannel-ds-amd64-vpcbj 1/1 Running 3 66d pod/kube-proxy-jtcxm 1/1 Running 1 67d pod/kube-proxy-swfmm 1/1 Running 1 66d pod/kube-proxy-w6x6t 1/1 Running 1 66d pod/kube-scheduler-10.89.73.42 1/1 Running 29 67d Install the Oracle Unified Directory Services Manager Image You can deploy Oracle Unified Directory Services Manager images in the following ways:\n Download a pre-built Oracle Unified Directory Services Manager image from My Oracle Support. by referring to the document ID 2723908.1. This image is prebuilt by Oracle and includes Oracle Unified Directory 12.2.1.4.0 and the latest PSU. Build your own Oracle Unified Directory Services Manager container image either by using the WebLogic Image Tool or by using the dockerfile, scripts and base image from Oracle Container Registry (OCR). You can also build your own image by using only the dockerfile and scripts. For more information about the various ways in which you can build your own container image, see Installing the Oracle Unified Directory Services Manager Image.  Choose one of these options based on your requirements.\nThe Oracle Unified Directory Services Manager image must be installed on the master node and each of the worker nodes in your Kubernetes cluster. Alternatively you can place the image in a docker registry that your cluster can access.\n After installing the Oracle Unified Directory Services Manager image run the following command to make sure the image is installed correctly on the master and worker nodes:\n$ docker images The output will look similar to the following:\nREPOSITORY TAG IMAGE ID CREATED SIZE oracle/oudsm 12.2.1.4.0 7157885054a2 2 weeks ago 2.74GB k8s.gcr.io/kube-proxy v1.18.4 718fa77019f2 3 months ago 117MB k8s.gcr.io/kube-scheduler v1.18.4 c663567f869e 3 months ago 95.3MB k8s.gcr.io/kube-controller-manager v1.18.4 e8f1690127c4 3 months ago 162MB k8s.gcr.io/kube-apiserver v1.18.4 408913fc18eb 3 months ago 173MB quay.io/coreos/flannel v0.12.0-amd64 4e9f801d2217 6 months ago 52.8MB k8s.gcr.io/pause 3.2 80d28bedfe5d 7 months ago 683kB k8s.gcr.io/coredns 1.6.7 67da37a9a360 8 months ago 43.8MB k8s.gcr.io/etcd 3.4.3-0 303ce5db0e90 11 months ago 288MB quay.io/prometheus/node-exporter v0.18.1 e5a616e4b9cf 16 months ago 22.9MB quay.io/coreos/kube-rbac-proxy v0.4.1 70eeaa7791f2 20 months ago 41.3MB ... Setup the Code Repository To Deploy Oracle Unified Directory Services Manager Oracle Unified Directory Services Manager deployment on Kubernetes leverages deployment scripts provided by Oracle for creating Oracle Unified Directory Services Manager containers using samples or Helm charts provided. To deploy Oracle Unified Directory Services Manager on Kubernetes you should set up the deployment scripts on the master node as below:\nCreate a working directory to setup the source code.\n$ mkdir \u0026lt;work directory\u0026gt; For example:\n$ mkdir /scratch/OUDSMContainer From the directory you created, download the Oracle Unified Directory Services Manager deployment scripts from the Oracle Unified Directory Services Manager repository.\n$ git clone https://github.com/oracle/fmw-kubernetes.git You can now use the deployment scripts from \u0026lt;work directory\u0026gt;/fmw-kubernetes/OracleUnifiedDirectorySM/kubernetes/samples/scripts/ to set up the Oracle Unified Directory Services Manager environments as further described in this document.\n"
},
{
	"uri": "/fmw-kubernetes/soa-domains/installguide/prepare-your-environment/",
	"title": "Prepare your environment",
	"tags": [],
	"description": "Prepare for creating Oracle SOA Suite domains, including required secrets creation, persistent volume and volume claim creation, database creation, and database schema creation.",
	"content": "To prepare your Oracle SOA Suite in Kubernetes environment, complete the following steps:\n  Set up your Kubernetes cluster\n  Install Helm\n  Get dependent images\n  Set up the code repository to deploy Oracle SOA Suite domains\n  Obtain the Oracle SOA Suite Docker image\n  Install the WebLogic Kubernetes operator\n  Prepare the environment for Oracle SOA Suite domains\na. Create a namespace for an Oracle SOA Suite domain\nb. Create a persistent storage for an Oracle SOA Suite domain\nc. Create a Kubernetes secret with domain credentials\nd. Create a Kubernetes secret with the RCU credentials\ne. Configure access to your database\nf. Run the Repository Creation Utility to set up your database schemas\n  Create an Oracle SOA Suite domain\n  Set up your Kubernetes cluster If you need help setting up a Kubernetes environment, check the cheat sheet.\nInstall Helm The operator uses Helm to create and deploy the necessary resources and then run the operator in a Kubernetes cluster. For Helm installation and usage information, see here.\nGet dependent images Obtain dependent images and add them to your local registry.\n  If you don\u0026rsquo;t already have one, obtain a Docker store account, log in to the Docker store and accept the license agreement for the WebLogic Server image.\n  Log in to the Docker store from your Docker client:\n$ docker login   Pull the operator image:\n$ docker pull oracle/weblogic-kubernetes-operator:3.0.1   Set up the code repository to deploy Oracle SOA Suite domains Oracle SOA Suite domain deployment on Kubernetes leverages the WebLogic Kubernetes operator infrastructure. To deploy an Oracle SOA Suite domain, you must set up the deployment scripts.\n  Create a working directory to set up the source code:\n$ export WORKDIR=$HOME/soa_20.4.2 $ mkdir ${WORKDIR}   Download the supported version of the WebLogic Kubernetes operator source code from the operator github project. Currently the supported operator version is 3.0.1:\n$ git clone https://github.com/oracle/weblogic-kubernetes-operator.git --branch release/3.0.1   Download the Oracle SOA Suite Kubernetes deployment scripts from the SOA repository and copy them to the WebLogic Kubernetes operator samples location:\n$ git clone https://github.com/oracle/fmw-kubernetes.git --branch release/20.4.2 $ mv -f ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-soa-domain ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-soa-domain_backup $ cp -rf ${WORKDIR}/fmw-kubernetes/OracleSOASuite/kubernetes/create-soa-domain ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/ $ mv -f ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/common ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/common_backup $ cp -rf ${WORKDIR}/fmw-kubernetes/OracleSOASuite/kubernetes/common ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/ $ mv -f ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-rcu-schema ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-rcu-schema_backup $ cp -rf ${WORKDIR}/fmw-kubernetes/OracleSOASuite/kubernetes/create-rcu-schema ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/ $ mv -f ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain_backup $ cp -rf ${WORKDIR}/fmw-kubernetes/OracleSOASuite/kubernetes/ingress-per-domain ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/charts/ $ cp -rf ${WORKDIR}/fmw-kubernetes/OracleSOASuite/kubernetes/imagetool-scripts ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/ $ cp -rf ${WORKDIR}/fmw-kubernetes/OracleSOASuite/kubernetes/charts ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/   You can now use the deployment scripts from ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/ to set up the Oracle SOA Suite domains.\nObtain the Oracle SOA Suite Docker image The Oracle SOA Suite image with latest bundle patch and required interim patches can be obtained from My Oracle Support (MOS). This is the only image supported for production deployments. Follow the below steps to download the Oracle SOA Suite image from My Oracle Support.\n  Download the patch 32215749 from My Oracle Support (MOS).\n  Unzip the downloaded patch zip file.\n  Load the image archive using docker load command:\nFor example:\n$ docker load \u0026lt; soasuite.12214.2042.tar Loaded image: soasuite:12.2.1.4 $   If you want to build and use an Oracle SOA Suite Docker image with any additional bundle patch or interim patches those are not part of the image obtained from My Oracle Support, then follow these steps to create the image.\n Note: The default Oracle SOA Suite image name used for Oracle SOA Suite domains deployment is soasuite:12.2.1.4. The image obtained must be tagged as soasuite:12.2.1.4 using the docker tag command. If you want to use a different name for the image, make sure to update the new image tag name in the create-domain-inputs.yaml file and also in other instances where the soasuite:12.2.1.4 image name is used.\n Install the WebLogic Kubernetes operator The WebLogic Kubernetes operator supports the deployment of Oracle SOA Suite domains in the Kubernetes environment. Follow the steps in this document to install the operator.\n Note: Optionally, you can execute these steps to send the contents of the operator’s logs to Elasticsearch.\n In the following example commands to install the WebLogic Kubernetes operator, opns is the namespace and op-sa is the service account created for the operator:\n$ kubectl create namespace opns $ kubectl create serviceaccount -n opns op-sa $ cd ${WORKDIR}/weblogic-kubernetes-operator $ helm install weblogic-kubernetes-operator kubernetes/charts/weblogic-operator --namespace opns --set image=oracle/weblogic-kubernetes-operator:3.0.1 --set serviceAccount=op-sa --set \u0026quot;domainNamespaces={}\u0026quot; --set \u0026quot;javaLoggingLevel=FINE\u0026quot; --wait Prepare the environment for Oracle SOA Suite domains Create a namespace for an Oracle SOA Suite domain Create a Kubernetes namespace (for example, soans) for the domain unless you intend to use the default namespace. Use the new namespace in the remaining steps in this section. For details, see Prepare to run a domain.\n $ kubectl create namespace soans $ helm upgrade --reuse-values --namespace opns --set \u0026quot;domainNamespaces={soans}\u0026quot; --wait weblogic-kubernetes-operator kubernetes/charts/weblogic-operator Create a persistent storage for an Oracle SOA Suite domain In the Kubernetes namespace you created, create the PV and PVC for the domain by running the create-pv-pvc.sh script. Follow the instructions for using the script to create a dedicated PV and PVC for the Oracle SOA Suite domain.\n  Review the configuration parameters for PV creation here. Based on your requirements, update the values in the create-pv-pvc-inputs.yaml file located at ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-weblogic-domain-pv-pvc/. Sample configuration parameter values for an Oracle SOA Suite domain are:\n baseName: domain domainUID: soainfra namespace: soans weblogicDomainStorageType: HOST_PATH weblogicDomainStoragePath: /scratch/k8s_dir/SOA    Ensure that the path for the weblogicDomainStoragePath property exists (if not, you need to create it), has full access permissions, and that the folder is empty.\n  Run the create-pv-pvc.sh script:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-weblogic-domain-pv-pvc $ ./create-pv-pvc.sh -i create-pv-pvc-inputs.yaml -o output_soainfra   The create-pv-pvc.sh script will create a subdirectory pv-pvcs under the given /path/to/output-directory directory and creates two YAML configuration files for PV and PVC. Apply these two YAML files to create the PV and PVC Kubernetes resources using the kubectl create -f command:\n$ kubectl create -f output_soainfra/pv-pvcs/soainfra-domain-pv.yaml $ kubectl create -f output_soainfra/pv-pvcs/soainfra-domain-pvc.yaml   Create a Kubernetes secret with domain credentials Create the Kubernetes secrets username and password of the administrative account in the same Kubernetes namespace as the domain:\n $ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-weblogic-domain-credentials $ ./create-weblogic-credentials.sh -u weblogic -p Welcome1 -n soans -d soainfra -s soainfra-domain-credentials For more details, see this document.\nYou can check the secret with the kubectl get secret command.\nFor example:\n  Click here to see the sample secret description.   $ kubectl get secret soainfra-domain-credentials -o yaml -n soans apiVersion: v1 data: password: T3JhZG9jX2RiMQ== sys_password: T3JhZG9jX2RiMQ== sys_username: c3lz username: U09BMQ== kind: Secret metadata: creationTimestamp: \u0026quot;2020-06-25T14:08:16Z\u0026quot; labels: weblogic.domainName: soainfra weblogic.domainUID: soainfra managedFields: - apiVersion: v1 fieldsType: FieldsV1 fieldsV1: f:data: .: {} f:password: {} f:sys_password: {} f:sys_username: {} f:username: {} f:metadata: f:labels: .: {} f:weblogic.domainName: {} f:weblogic.domainUID: {} f:type: {} manager: kubectl operation: Update time: \u0026quot;2020-06-25T14:08:16Z\u0026quot; name: soainfra-rcu-credentials namespace: soans resourceVersion: \u0026quot;265386\u0026quot; selfLink: /api/v1/namespaces/soans/secrets/soainfra-rcu-credentials uid: 2d93941c-656b-43a4-8af2-78ca8be0f293 type: Opaque    Create a Kubernetes secret with the RCU credentials You also need to create a Kubernetes secret containing the credentials for the database schemas. When you create your domain, it will obtain the RCU credentials from this secret.\nUse the provided sample script to create the secret:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-rcu-credentials $ ./create-rcu-credentials.sh \\  -u SOA1 \\  -p Oradoc_db1 \\  -a sys \\  -q Oradoc_db1 \\  -d soainfra \\  -n soans \\  -s soainfra-rcu-credentials The parameter values are:\n-u username for schema owner (regular user), required.\n-p password for schema owner (regular user), required.\n-a username for SYSDBA user, required.\n-q password for SYSDBA user, required.\n-d domainUID. Example: soainfra\n-n namespace. Example: soans\n-s secretName. Example: soainfra-rcu-credentials\nYou can confirm the secret was created as expected with the kubectl get secret command.\nFor example:\n  Click here to see the sample secret description.   $ kubectl get secret soainfra-rcu-credentials -o yaml -n soans apiVersion: v1 data: password: T3JhZG9jX2RiMQ== sys_password: T3JhZG9jX2RiMQ== sys_username: c3lz username: U09BMQ== kind: Secret metadata: creationTimestamp: \u0026#34;2020-06-25T14:08:16Z\u0026#34; labels: weblogic.domainName: soainfra weblogic.domainUID: soainfra managedFields: - apiVersion: v1 fieldsType: FieldsV1 fieldsV1: f:data: .: {} f:password: {} f:sys_password: {} f:sys_username: {} f:username: {} f:metadata: f:labels: .: {} f:weblogic.domainName: {} f:weblogic.domainUID: {} f:type: {} manager: kubectl operation: Update time: \u0026#34;2020-06-25T14:08:16Z\u0026#34; name: soainfra-rcu-credentials namespace: soans resourceVersion: \u0026#34;265386\u0026#34; selfLink: /api/v1/namespaces/soans/secrets/soainfra-rcu-credentials uid: 2d93941c-656b-43a4-8af2-78ca8be0f293 type: Opaque    Configure access to your database Oracle SOA Suite domains require a database with the necessary schemas installed in them. The Repository Creation Utility (RCU) allows you to create those schemas. You must set up the database before you create your domain. There are no additional requirements added by running Oracle SOA Suite in Kubernetes; the same existing requirements apply.\nFor production deployments, you must set up and use the standalone (non-container) based database running outside of Kubernetes.\nBefore creating a domain, you will need to set up the necessary schemas in your database.\nRun the Repository Creation Utility to set up your database schemas Create schemas To create the database schemas for Oracle SOA Suite, run the create-rcu-schema.sh script.\nFor example:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-rcu-schema $ ./create-rcu-schema.sh -h usage: ./create-rcu-schema.sh -s \u0026lt;schemaPrefix\u0026gt; -t \u0026lt;schemaType\u0026gt; -d \u0026lt;dburl\u0026gt; -i \u0026lt;image\u0026gt; -u \u0026lt;imagePullPolicy\u0026gt; -p \u0026lt;docker-store\u0026gt; -n \u0026lt;namespace\u0026gt; -q \u0026lt;sysPassword\u0026gt; -r \u0026lt;schemaPassword\u0026gt; -o \u0026lt;rcuOutputDir\u0026gt; -l \u0026lt;schemaProfileType\u0026gt; [-h] -s RCU Schema Prefix (required) -t RCU Schema Type (optional) (supported values: fmw(default), soa, osb, soaosb, soaess, soaessosb) -d RCU Oracle Database URL (optional) (default: oracle-db.default.svc.cluster.local:1521/devpdb.k8s) -p FMW Infrastructure ImagePullSecret (optional) (default: none) -i FMW Infrastructure Image (optional) (default: container-registry.oracle.com/middleware/fmw-infrastructure:12.2.1.4) -u FMW Infrastructure ImagePullPolicy (optional) (default: IfNotPresent) -n Namespace for RCU pod (optional) (default: default) -q password for database SYSDBA user. (optional) (default: Oradoc_db1) -r password for all schema owner (regular user). (optional) (default: Oradoc_db1) -o Output directory for the generated YAML file. (optional) (default: rcuoutput) -l Profile type for SOA RCU schema creation (optional). (supported values: SMALL(default), MED, LARGE) -h Help $ ./create-rcu-schema.sh \\  -s SOA1 \\  -t soaessosb \\  -d oracle-db.default.svc.cluster.local:1521/devpdb.k8s \\  -i soasuite:12.2.1.4 \\  -n default \\  -q Oradoc_db1 \\  -r Oradoc_db1 For Oracle SOA Suite domains, the create-rcu-schema.sh script supports:\n domain types: soa, osb, soaosb, soaess, and soaessosb. You must specify one of these using the -t flag. Optionally, you can specify the Oracle SOA schema profile type using the -l flag. Supported values are SMALL, MED, and LARGE.   Note: To use the LARGE schema profile type, make sure that the partitioning feature is enabled in the Oracle Database.\n Make sure that you maintain the association between the database schemas and the matching domain just like you did in a non-Kubernetes environment. There is no specific functionality provided to help with this.\nDrop schemas If you want to drop a schema, you can use the drop-rcu-schema.sh script.\nFor example:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-rcu-schema $ ./drop-rcu-schema.sh -h usage: ./drop-rcu-schema.sh -s \u0026lt;schemaPrefix\u0026gt; -d \u0026lt;dburl\u0026gt; -n \u0026lt;namespace\u0026gt; -q \u0026lt;sysPassword\u0026gt; -r \u0026lt;schemaPassword\u0026gt; -l \u0026lt;schemaProfileType\u0026gt; [-h] -s RCU Schema Prefix (required) -t RCU Schema Type (optional) (supported values: fmw(default), soa, osb, soaosb, soaess, soaessosb) -d Oracle Database URL (optional) (default: oracle-db.default.svc.cluster.local:1521/devpdb.k8s) -n Namespace where RCU pod is deployed (optional) (default: default) -q password for database SYSDBA user. (optional) (default: Oradoc_db1) -r password for all schema owner (regular user). (optional) (default: Oradoc_db1) -l Profile type for SOA RCU schema deletion (optional). (supported values: SMALL(default), MED, LARGE) -h Help $ $ ./drop-rcu-schema.sh \\  -s SOA1 \\  -t soaessosb \\  -d soasuite:12.2.1.4 \\  -n default \\  -q Oradoc_db1 \\  -r Oradoc_db1 For Oracle SOA Suite domains, the drop-rcu-schema.sh script supports:\n domain types: soa, osb, soaosb, soaess, and soaessosb. You must specify one of these using the -t flag. Optionally, you can specify the Oracle SOA schema profile type using the -l flag. Supported values are SMALL, MED, and LARGE.  Create an Oracle SOA Suite domain Now that you have your Docker images and you have created your RCU schemas, you are ready to create your domain. To continue, follow the instructions in Create Oracle SOA Suite domains.\n"
},
{
	"uri": "/fmw-kubernetes/soa-domains/adminguide/configuring-custom-ssl-certificates/",
	"title": "Configure SSL certificates",
	"tags": [],
	"description": "Create and configure custom SSL certificates for Oracle SOA Suite domains.",
	"content": "Secure Socket Layer (SSL) provides a secured communication for data sent over unsecured networks. In an SSL termination scenario, you can configure SSL between the client browser and the load balancer in your Oracle SOA Suite instance to ensure that applications are accessed securely. In an SSL end-to-end scenario, an Oracle SOA Suite domain is configured to use a self-signed SSL certificate that was generated during domain creation. Clients will typically receive a message indicating that the signing CA for the certificate is unknown and not trusted.\nThis section provides details on how to create and configure custom (CA-issued) SSL certificates for Oracle SOA Suite domains in both SSL end-to-end and SSL termination scenarios.\n Create custom SSL certificates in an SSL end-to-end scenario Create custom SSL certificates in an SSL termination at a load balancer  Create custom SSL certificates in an SSL end-to-end scenario These steps describe how to replace the identity and trust keystore of an Oracle SOA Suite domain with a custom identity and custom trust keystore and register with digital certificates procured from any third party authority.\nIn this documentation, the registered domain is mydomain.com and the CA signed certificates are taken from mydomain.\nCreate a custom identity and custom trust keystore and generate a certificate signing request (CSR) To create a custom identity and custom trust keystore and generate a CSR:\n  Log in to the Enterprise Manager (EM) Console and access the Keystores page by opening WebLogic Domain \u0026gt; Security \u0026gt; Keystore.\n  Under the system stripe, click Create Keystore to create a new keystore.\n  Provide the following details for custom identity:\nKeystore Name: custIdentity Protection: Select the Password option. Keystore Password: Enter the password. Confirm Password: Confirm the password.\n  Click Create Keystore to create another new keystore.\n  Provide the following details for custom trust:\n Keystore Name: custTrust Protection: Select the Password option. Keystore Password: Enter the password. Confirm Password: Confirm the password.    Click Manage on the custIdentity keystore name and provide the password that you specified previously.\n  Click Generate Keypair to create a new key pair, and provide the following details for custIdentity with alias as custIdentity and password:\n Alias Name: custIdentity Common Name: Common name, for example, soak8s.mydomain.com (Registered domain name) Organizational Unit: Name of the organizational unit Organization: Organization name Enter City, State, and Country names Key Type: RSA Key Size: 2048 Password: Enter the password    Click OK to generate the keypair.\n  Select the newly created keypair and click Generate CSR.\n  Export the created CSR, share it with Certificate Authority, such as digicert CA, and get root, intermediate, and signed certificates. The certificate is generated for the domain name you used in the Common Name field.\n  It is not mandatory to create identity and trust keystore under the system stripe that comes with default provisioning. You can create a new custom stripe and create identity and trust keystores under it.\nShare the CSR with CA to get CA-signed certificates   Select the new keypair under the custIdentity and click Generate CSR.\n  Export the created CSR and share it with the Certificate Authority and get root, intermediate, and signed certificates. The certificate is generated for the domain name you used in the Common Name field.\n  Download the certificates shared in the zip file from the CA. The zip file contains one of the following:\n the three certificates individually - root, intermediate, and signed certificates root and intermediate certificates in one chain and signed certificate separately    Double-click the certificate chain for root and intermediate certificates. You can see the full chain when you click on the certification path.\n  Extract the root and intermediate certificates individually by going to the certification path, select the certificate to be extracted (root or intermediate) and click View Certificate.\n  On the View Certificates pop-up, select the Details tab and click Copy to File.\n  In the Certificate Export wizard, click Next, then select Base 64 encoded X.509 (CER), and then click Next. Export the certificate.\n  Name the exported certificate as root and intermediate certificates respectively.\n  Import CA certificates Certificate Authority (CA) certificates must be imported in the following order: first the signed server certificate, then the intermediate certificate, and then the root certificate.\nTo import CA certificates:\n  Use WLST commands to import the certificate chain in the identity keystore (custIdentity):\na. Combine the three certificates into a single text file called chain.pem in the following order: signed server certificate, followed by intermediate certificate, followed by root certificate:\n-----BEGIN CERTIFICATE----- \u0026lt;signed server certificate\u0026gt; -----END CERTIFICATE----- -----BEGIN CERTIFICATE----- \u0026lt;intermediate certificate\u0026gt; -----END CERTIFICATE----- -----BEGIN CERTIFICATE----- \u0026lt;root certificate\u0026gt; -----END CERTIFICATE----- b. Place the chain.pem in /tmp from where you will be executing the kubectl commands (for example, on the master node).\nc. Enter the following command to change the file ownership to 1000:1000 user/group:\n$ sudo chown 1000:1000 /tmp/chain.pem d. Copy /tmp/chain.pem into the Administration Server pod (for example, soainfra-adminserver):\n$ kubectl cp /tmp/chain.pem soans/soainfra-adminserver:/tmp/chain.pem e. Exec into the Administration Server pod to perform all operations:\n$ kubectl exec -it soainfra-adminserver -n soans -- bash f. Start WLST and access the Oracle Platform Security Services (OPSS) key store service:\n$ cd /u01/oracle/oracle_common/common/bin/ $ ./wlst.sh : : wls:/offline\u0026gt; connect(\u0026quot;weblogic\u0026quot;,\u0026quot;Welcome1\u0026quot;,\u0026quot;t3://soainfra-adminserver:7001\u0026quot;) : : wls:/soainfra/serverConfig/\u0026gt; svc = getOpssService(name='KeyStoreService') g. Use the WLST importKeyStoreCertificate command to import chain.pem:\nsvc.importKeyStoreCertificate(appStripe='stripe', name='keystore', password='password', alias='alias', keypassword='keypassword', type='entrytype',filepath='absolute_file_path') For example:\nwls:/soainfra/serverConfig/\u0026gt; svc.importKeyStoreCertificate(appStripe='system', name='custIdentity', password=welcome1, alias='custIdentity', keypassword='welcome1', type='CertificateChain', filepath='/tmp/chain.pem') e. Exit WLST:\nexit()   Use Oracle Enterprise Manager to import the certificate chain into the trust keystore (custTrust):\na. Log in to the Enterprise Manager Console and access the Keystores page by opening WebLogic domain \u0026gt; Security \u0026gt; Keystore.\nb. Select the trust keystore (custTrust) and click Manage.\nc. Click Import Certificate and import the certificates in this order:\n  the signed server certificate as a trusted certificate (alias mySignedCert)\n  the intermediate certificate from CA as a trusted certificate (alias myInterCA)\n  the root certificate from CA as a trusted certificate (alias myRootCA)\n    Synchronize the local keystore with the security store Synchronize keystores to synchronize information between the domain home and the Oracle Platform Security Services (OPSS) store in the database.\nTo synchronize keystores:\n Exec into the Administration server pod (for example, soainfra-adminserver): $ kubectl exec -it soainfra-adminserver -n soans -- bash  Start WLST and access the Oracle Platform Security Services (OPSS) keystore service: $ cd /u01/oracle/oracle_common/common/bin/ $ ./wlst.sh : : wls:/offline\u0026gt; connect(\u0026quot;weblogic\u0026quot;,\u0026quot;Welcome1\u0026quot;,\u0026quot;t3://soainfra-adminserver:7001\u0026quot;) : : wls:/soainfra/serverConfig/\u0026gt; svc = getOpssService(name='KeyStoreService')  Enter the following commands to synchronize the custom identity and custom trust keystores:  Note: This step is necessary only if you are using the system stripe. You do not need to synchronize the keystores if you are using a custom stripe.\n wls:/soainfra/serverConfig/\u0026gt; svc.listKeyStoreAliases(appStripe=\u0026quot;system\u0026quot;, name=\u0026quot;custIdentity\u0026quot;, password=\u0026quot; ****\u0026quot;, type=\u0026quot;*\u0026quot;) wls:/soainfra/serverConfig/\u0026gt; syncKeyStores(appStripe='system',keystoreFormat='KSS') wls:/soainfra/serverConfig/\u0026gt; svc.listKeyStoreAliases (appStripe=\u0026quot;system\u0026quot;, name=\u0026quot;myKSSTrust\u0026quot;, password=\u0026quot;****\u0026quot;, type=\u0026quot;*\u0026quot;) wls:/soainfra/serverConfig/\u0026gt; syncKeyStores(appStripe='system',keystoreFormat='KSS')   Update the WebLogic keystores with custom identity and trust To update the WebLogic keystores with custom identity and custom trust:\n  In the WebLogic Server Administration Console, open Servers \u0026gt; AdminServer \u0026gt; Configurations \u0026gt; Keystores tab.\n  Change the Keystores to Custom Identity and Custom Trust and Save.\n  Provide the values for Custom Identity:\n Custom Identity Keystore: kss://system/custidentity Custom Identity KeyStore Type: KSS Custom Identity PassPhrase: enter password given while creating the custIdentity keystore. Confirm Custom Identity PassPhrase: reenter the password.    Provide the values for Custom Trust:\n Custom Trust Keystore: kss://system/custTrust Custom Trust KeyStore Type: KSS Custom Trust PassPhrase: enter password given while creating the custTrust keystore. Confirm Custom Trust PassPhrase: reenter the password.    Click Save and then Activate changes.\n  Open the SSL tab and provide the following details:\n Private Key Alias: custIdentity (this is the alias given while creating the key pair in the custIdentity keystore.) Private Key PassPhrase: enter password given while creating the key pair under the custIdentity keystore. Confirm Private Key PassPhrase: reenter the password.    In the Advanced section, change Hostname Verification to None. Click Save and Activate changes.\n  Repeat steps 1 to 7 for all Managed Servers.\n  Restart the domain.\n  Once the servers are up and running, you can check if the SSL URLs show the updated certificates.\n  For more details, refer to:\n Administering Oracle SOA Cloud Service Administering Oracle Fusion Middleware  Create custom SSL certificates in an SSL termination at a load balancer This section provides references to configure a custom SSL certificate at a load balancer.\nThere are multiple CA vendors in the marketplace today, each offering different levels of service at varying price points. Research and choose a CA vendor that meets your service-level and budget requirements.\nFor a CA vendor to issue you a CA-issued SSL certificate, you must provide the following information:\n Your custom domain name. Public information associated with the domain confirming you as the owner. Email address associated with the custom domain for verification.  Create a Certificate Signing Request (CSR) for your load balancer and submit the CSR to the CA vendor. After receiving the CA-issued certificate, refer to Administering Oracle SOA Cloud Service to import the CA-issued SSL certificate to the load balancer. If you are using openssl to create the certificates, you can refer to Manually Generate a Certificate Signing Request (CSR) Using OpenSSL to submit the CSR to the CA vendor.\n"
},
{
	"uri": "/fmw-kubernetes/soa-domains/adminguide/deploying-composites/deploy-using-maven-ant/",
	"title": "Deploy using Maven and Ant",
	"tags": [],
	"description": "Deploy Oracle SOA Suite and Oracle Service Bus composite applications using the Maven and Ant based approach in an Oracle SOA Suite deployment.",
	"content": "Learn how to deploy Oracle SOA Suite and Oracle Service Bus composite applications using the Maven and Ant based approach in an Oracle SOA Suite in WebLogic Kubernetes operator environment.\nBefore deploying composite applications, we need to create a Kubernetes pod in the same cluster where the Oracle SOA Suite domain is running, so that composite applications can be deployed using the internal Kubernetes Service for the Administration Server URL.\nPlace the SOA/OSB composite project at a share location (for example at /share/soa-deploy) mounted at /soacomposites inside container. Make sure to provide oracle user ( uid: 1000 and gid: 1000) permission to directory /share/soa-deploy, so that it is accessible and writable inside the container.\n$ sudo chown -R 1000:1000 /share/soa-deploy Follow the steps in this section to create a container and then use it to deploy Oracle SOA Suite and Oracle Service Bus composite applications using Maven or Ant.\nCreate a composite deployment container Before creating a Kubernetes pod, make sure that the Oracle SOA Suite Docker image is available on a node, or you can create an image pull secret so that the pod can pull the Docker image on the host where it gets created.\n  Create an image pull secret to pull image soasuite:12.2.1.4 by the Kubernetes pod:\n$ kubectl create secret docker-registry image-secret -n soans --docker-server=your-registry.com --docker-username=xxxxxx --docker-password=xxxxxxx --docker-email=my@company.com   Create a PersistentVolume and PersistentVolumeClaim (soadeploy-pv.yaml and soadeploy-pvc.yaml) with sample composites for build and deploy placed at /share/soa-deploy.\na) Create a PersistentVolume with the sample provided (soadeploy-pv.yaml), which uses NFS (you can use hostPath or any other supported PV type):\napiVersion: v1 kind: PersistentVolume metadata: name: soadeploy-pv spec: storageClassName: soadeploy-storage-class capacity: storage: 10Gi accessModes: - ReadWriteMany # Valid values are Retain, Delete or Recycle persistentVolumeReclaimPolicy: Retain # hostPath: nfs: server: X.X.X.X path: \u0026quot;/share/soa-deploy\u0026quot; b) Apply the YAML:\n$ kubectl apply -f soadeploy-pv.yaml c) Create a PersistentVolumeClaim (soadeploy-pvc.yaml):\nkind: PersistentVolumeClaim apiVersion: v1 metadata: name: soadeploy-pvc namespace: soans spec: storageClassName: soadeploy-storage-class accessModes: - ReadWriteMany resources: requests: storage: 10Gi d) Apply the YAML:\n$ kubectl apply apply -f soadeploy-pvc.yaml   Create a composite deploy pod using soadeploy.yaml to mount the composites inside pod at /composites:\napiVersion: v1 kind: Pod metadata: labels: run: soadeploy name: soadeploy namespace: soans spec: imagePullSecrets: - name: image-secret containers: - image: soasuite:12.2.1.4 name: soadeploy env: - name: M2_HOME value: /u01/oracle/oracle_common/modules/org.apache.maven_3.2.5 command: [\u0026quot;/bin/bash\u0026quot;, \u0026quot;-c\u0026quot;, \u0026quot;echo 'export PATH=$PATH:$M2_HOME/bin' \u0026gt;\u0026gt; $HOME/.bashrc; sleep infinity\u0026quot;] imagePullPolicy: Always volumeMounts: - name: mycomposite mountPath: /composites volumes: - name: mycomposite persistentVolumeClaim: claimName: soadeploy-pvc   Create the pod:\n$ kubectl apply -f soadeploy.yaml   Once the Kubernetes pod is deployed, exec into the pod to perform Maven/Ant based build and deploy:\n$ kubectl exec -it -n soans soadeploy -- bash   Maven based build and deploy  Note: Make sure to execute these commands inside the soadeploy pod.\n Set up proxy details for Maven to pull dependencies from the internet.\nIf your environment is not running behind a proxy, then skip this step. Otherwise, replace REPLACE-WITH-PROXY-HOST, REPLACE-WITH-PROXY-PORT and the value for nonProxyHosts attribute per your environment and create the settings.xml:\n $ mkdir $HOME/.m2 $ cat \u0026lt;\u0026lt;EOF \u0026gt; $HOME/.m2/settings.xml \u0026lt;settings\u0026gt; \u0026lt;proxies\u0026gt; \u0026lt;proxy\u0026gt; \u0026lt;active\u0026gt;true\u0026lt;/active\u0026gt; \u0026lt;protocol\u0026gt;http\u0026lt;/protocol\u0026gt; \u0026lt;host\u0026gt;REPLACE-WITH-PROXY-HOST\u0026lt;/host\u0026gt; \u0026lt;port\u0026gt;REPLACE-WITH-PROXY-PORT\u0026lt;/port\u0026gt; \u0026lt;nonProxyHosts\u0026gt;soainfra-cluster-soa-cluster|soainfra-adminserver\u0026lt;/nonProxyHosts\u0026gt; \u0026lt;/proxy\u0026gt; \u0026lt;/proxies\u0026gt; \u0026lt;/settings\u0026gt; EOF For Oracle SOA Suite composite applications   Set up the environment for Maven:\n#Perform Maven Sync $ cd /u01/oracle/oracle_common/plugins/maven/com/oracle/maven/oracle-maven-sync/12.2.1/ $ mvn install:install-file \\ -DpomFile=oracle-maven-sync-12.2.1.pom \\ -Dfile=oracle-maven-sync-12.2.1.jar #install Maven plugin $ mvn help:describe \\ -Dplugin=com.oracle.maven:oracle-maven-sync \\ -Ddetail #push libraries into internal repository $ mvn com.oracle.maven:oracle-maven-sync:push \\ -DoracleHome=/u01/oracle \\ -DtestingOnly=false $ mvn archetype:crawl \\ -Dcatalog=$HOME/.m2/archetype-catalog.xml \\ -DarchetypeArtifactId=oracle-soa-application \\ -DarchetypeVersion=12.2.1-4-0   Build the SOA Archive (SAR) for your sample deployment available at /composites/mavenproject/my-soa-app:\n$ cd /composites/mavenproject/my-soa-app $ mvn package The SAR will be generated at /composites/mavenproject/my-soa-app/my-project/target/sca_my-project.jar.\n  Deploy into the Oracle SOA Suite instance. For example, if the instance URL is http://soainfra-cluster-soa-cluster:8001 with credentials username: weblogic and password: Welcome1, enter the following commands:\n$ cd /composites/mavenproject/my-soa-app $ mvn pre-integration-test \\ -DoracleServerUrl=http://soainfra-cluster-soa-cluster:8001 \\ -DsarLocation=/composites/mavenproject/my-soa-app/my-project/target/sca_my-project.jar \\ -Doverwrite=true \\ -DforceDefault=true \\ -Dcomposite.partition=default \\ -Duser=weblogic -Dpassword=Welcome1   For Oracle Service Bus composite applications   Set up the environment for Maven:\n#Perform Maven Sync $ cd /u01/oracle/oracle_common/plugins/maven/com/oracle/maven/oracle-maven-sync/12.2.1/ $ mvn install:install-file \\ -DpomFile=oracle-maven-sync-12.2.1.pom \\ -Dfile=oracle-maven-sync-12.2.1.jar #push libraries into internal repository $ mvn com.oracle.maven:oracle-maven-sync:push \\ -DoracleHome=$ORACLE_HOME $ mvn archetype:crawl \\ -Dcatalog=$HOME/.m2/archetype-catalog.xml #Verify the mvn setup $ mvn help:describe \\ -DgroupId=com.oracle.servicebus.plugin \\ -DartifactId=oracle-servicebus-plugin \\ -Dversion=12.2.1-4-0   Build the Oracle Service Bus Archive (sbconfig.sbar)\nBuild sbconfig.sbar for your sample deployment available at /composites/mavenproject/HelloWorldSB:\n$ cd /composites/mavenproject/HelloWorldSB $ mvn com.oracle.servicebus.plugin:oracle-servicebus-plugin:package The Service Bus Archive (SBAR) will be generated at /composites/mavenproject/HelloWorldSB/.data/maven/sbconfig.sbar.\n  Deploy the generated sbconfig.sbar into the Oracle Service Bus instance. For example, if the Administration URL is http://soainfra-adminserver:7001 with credentials username: weblogic and password: Welcome1, enter the following commands: :\n$ cd /composites/mavenproject/HelloWorldSB $ mvn pre-integration-test \\ -DoracleServerUrl=t3://soainfra-adminserver:7001 \\ -DoracleUsername=weblogic -DoraclePassword=Welcome1   Ant based build and deploy  Note: Make sure to execute these commands inside the soadeploy pod.\n For Oracle SOA Suite composite applications   Build an Oracle SOA Suite composite application using Ant. For example, if the composite application to be deployed is available at /composites/antproject/Project, enter the following commands:\n$ cd /u01/oracle/soa/bin $ ant -f ant-sca-package.xml \\ -DcompositeDir=/composites/antproject/Project \\ -DcompositeName=Project \\ -Drevision=0.1 The SOA Archive is generated at /composites/antproject/Project/deploy/sca_Project_rev0.1.jar, which will be used for deploying.\n  Deploy into the Oracle SOA Suite instance using Ant:\n$ cd /u01/oracle/soa/bin $ ant -f ant-sca-deploy.xml \\ -DserverURL=http://soainfra-cluster-soa-cluster:8001 \\ -DsarLocation=/composites/antproject/Project/deploy/sca_Project_rev0.1.jar \\ -Doverwrite=true \\ -Duser=weblogic -Dpassword=Welcome1   For Oracle Service Bus composite applications See Developing Services Using Oracle Service Bus to deploy Oracle Service Bus composite applications using Ant.\n"
},
{
	"uri": "/fmw-kubernetes/soa-domains/adminguide/configure-load-balancer/nginx/",
	"title": "NGINX",
	"tags": [],
	"description": "Configure the ingress-based NGINX load balancer for Oracle SOA Suite domains.",
	"content": "This section provides information about how to install and configure the ingress-based NGINX load balancer to load balance Oracle SOA Suite domain clusters. You can configure NGINX for non-SSL, SSL termination, and end-to-end SSL access of the application URL.\nFollow these steps to set up NGINX as a load balancer for an Oracle SOA Suite domain in a Kubernetes cluster:\nSee the official installation document for prerequisites.\n  Non-SSL and SSL termination\n Install the NGINX load balancer Configure NGINX to manage ingresses Verify non-SSL and SSL termination access    End-to-end SSL configuration\n Install the NGINX load balancer for End-to-end SSL Deploy tls to access the services Verify End-to-end SSL access    To get repository information, enter the following Helm commands:\n$ helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx $ helm repo add stable https://kubernetes-charts.storage.googleapis.com/ $ helm repo update Non-SSL and SSL termination Install the NGINX load balancer   Deploy the ingress-nginx controller by using Helm on the domain namespace:\n$ helm install nginx-ingress -n soans \\  --set controller.service.type=NodePort \\  --set controller.admissionWebhooks.enabled=false \\  ingress-nginx/ingress-nginx    Click here to see the sample output.    NAME: nginx-ingress LAST DEPLOYED: Tue Sep 15 08:40:47 2020 NAMESPACE: soans STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: The ingress-nginx controller has been installed. Get the application URL by running these commands: export HTTP_NODE_PORT=$(kubectl --namespace soans get services -o jsonpath=\u0026quot;{.spec.ports[0].nodePort}\u0026quot; nginx-ingress-ingress-nginx-controller) export HTTPS_NODE_PORT=$(kubectl --namespace soans get services -o jsonpath=\u0026quot;{.spec.ports[1].nodePort}\u0026quot; nginx-ingress-ingress-nginx-controller) export NODE_IP=$(kubectl --namespace soans get nodes -o jsonpath=\u0026quot;{.items[0].status.addresses[1].address}\u0026quot;) echo \u0026quot;Visit http://$NODE_IP:$HTTP_NODE_PORT to access your application via HTTP.\u0026quot; echo \u0026quot;Visit https://$NODE_IP:$HTTPS_NODE_PORT to access your application via HTTPS.\u0026quot; An example ingress that makes use of the controller: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx name: example namespace: foo spec: rules: - host: www.example.com http: paths: - backend: serviceName: exampleService servicePort: 80 path: / # This section is only required if TLS is to be enabled for the ingress tls: - hosts: - www.example.com secretName: example-tls If TLS is enabled for the ingress, a secret containing the certificate and key must also be provided: apiVersion: v1 kind: Secret metadata: name: example-tls namespace: foo data: tls.crt: \u0026lt;base64 encoded cert\u0026gt; tls.key: \u0026lt;base64 encoded key\u0026gt; type: kubernetes.io/tls      Check the status of the deployed ingress controller:\n$ kubectl --namespace soans get services | grep ingress-nginx-controller Sample output:\nnginx-ingress-ingress-nginx-controller NodePort 10.106.186.235 \u0026lt;none\u0026gt; 80:32125/TCP,443:31376/TCP 19m   Configure NGINX to manage ingresses   Create an ingress for the domain in the domain namespace by using the sample Helm chart. Here path-based routing is used for ingress. Sample values for default configuration are shown in the file ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain/values.yaml. By default, type is TRAEFIK, tls is Non-SSL, and domainType is soa. These values can be overridden by passing values through the command line or can be edited in the sample file values.yaml. If needed, you can update the ingress YAML file to define more path rules (in section spec.rules.host.http.paths) based on the domain application URLs that need to be accessed. Update the template YAML file for the NGINX load balancer located at ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain/templates/nginx-ingress.yaml\n$ cd ${WORKDIR}/weblogic-kubernetes-operator $ helm install soa-nginx-ingress kubernetes/samples/charts/ingress-per-domain \\  --namespace soans \\  --values kubernetes/samples/charts/ingress-per-domain/values.yaml \\  --set \u0026#34;nginx.hostname=$(hostname -f)\u0026#34; \\  --set type=NGINX Sample output:\nNAME: soa-nginx-ingress LAST DEPLOYED: Fri Jul 24 09:34:03 2020 NAMESPACE: soans STATUS: deployed REVISION: 1 TEST SUITE: None   For secured access (SSL) to the Oracle SOA Suite application, create a certificate and generate a Kubernetes secret:\n$ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /tmp/tls1.key -out /tmp/tls1.crt -subj \u0026#34;/CN=domain1.org\u0026#34; $ kubectl -n soans create secret tls domain1-tls-cert --key /tmp/tls1.key --cert /tmp/tls1.crt  Note: Value of CN is the hostname on which this ingress is to be deployed.\n   Install ingress-per-domain using Helm for SSL configuration:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator $ helm install soa-nginx-ingress kubernetes/samples/charts/ingress-per-domain \\  --namespace soans \\  --values kubernetes/samples/charts/ingress-per-domain/values.yaml \\  --set \u0026#34;nginx.hostname=$(hostname -f)\u0026#34; \\  --set type=NGINX --set tls=SSL Sample output:\nNAME: soa-nginx-ingress LAST DEPLOYED: Fri Jul 24 09:34:03 2020 NAMESPACE: soans STATUS: deployed REVISION: 1 TEST SUITE: None   For non-SSL access to the Oracle SOA Suite application, get the details of the services by the ingress:\n$ kubectl describe ingress soainfra-nginx -n soans    Click here to see the sample output of the services supported by the above deployed ingress.   Name: soainfra-nginx Namespace: soans Address: 100.111.150.225 Default backend: default-http-backend:80 (\u0026lt;error: endpoints \u0026quot;default-http-backend\u0026quot; not found\u0026gt;) Rules: Host Path Backends ---- ---- -------- domain1.org /console soainfra-adminserver:7001 (10.244.0.45:7001) /em soainfra-adminserver:7001 (10.244.0.45:7001) /weblogic/ready soainfra-adminserver:7001 (10.244.0.45:7001) / soainfra-cluster-soa-cluster:8001 (10.244.0.46:8001,10.244.0.47:8001) /soa-infra soainfra-cluster-soa-cluster:8001 (10.244.0.46:8001,10.244.0.47:8001) /soa/composer soainfra-cluster-soa-cluster:8001 (10.244.0.46:8001,10.244.0.47:8001) /integration/worklistapp soainfra-cluster-soa-cluster:8001 (10.244.0.46:8001,10.244.0.47:8001) Annotations: \u0026lt;none\u0026gt; Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal CREATE 2m32s nginx-ingress-controller Ingress soans/soainfra-nginx Normal UPDATE 94s nginx-ingress-controller Ingress soans/soainfra-nginx      For SSL access to the Oracle SOA Suite application, get the details of the services by the above deployed ingress:\n$ kubectl describe ingress soainfra-nginx -n soans    Click here to see the sample output of the services supported by the above deployed ingress.    Name: soainfra-nginx Namespace: soans Address: 100.111.150.225 Default backend: default-http-backend:80 (\u0026lt;error: endpoints \u0026quot;default-http-backend\u0026quot; not found\u0026gt;) TLS: domain1-tls-cert terminates domain1.org Rules: Host Path Backends ---- ---- -------- domain1.org /console soainfra-adminserver:7001 (10.244.0.45:7001) /em soainfra-adminserver:7001 (10.244.0.45:7001) /weblogic/ready soainfra-adminserver:7001 (10.244.0.45:7001) / soainfra-cluster-soa-cluster:8001 (10.244.0.46:8001,10.244.0.47:8001) /soa-infra soainfra-cluster-soa-cluster:8001 (10.244.0.46:8001,10.244.0.47:8001) /soa/composer soainfra-cluster-soa-cluster:8001 (10.244.0.46:8001,10.244.0.47:8001) /integration/worklistapp soainfra-cluster-soa-cluster:8001 (10.244.0.46:8001,10.244.0.47:8001) Annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/configuration-snippet: more_set_input_headers \u0026quot;X-Forwarded-Proto: https\u0026quot;; more_set_input_headers \u0026quot;WL-Proxy-SSL: true\u0026quot;; nginx.ingress.kubernetes.io/ingress.allow-http: false Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal CREATE 3m47s nginx-ingress-controller Ingress soans/soainfra-nginx Normal UPDATE 3m25s nginx-ingress-controller Ingress soans/soainfra-nginx      Verify non-SSL and SSL termination access Non-SSL configuration Verify that the Oracle SOA Suite domain application URLs are accessible through the LOADBALANCER-Non-SSLPORT 30017:\nhttp://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/weblogic/ready http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/console http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/em http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/soa-infra http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/soa/composer http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/integration/worklistapp SSL configuration Verify that the Oracle SOA Suite domain application URLs are accessible through the LOADBALANCER-SSLPORT 30233:\nhttps://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/weblogic/ready https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/console https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/em https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/soa-infra https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/soa/composer https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/integration/worklistapp Uninstall the ingress Uninstall and delete the ingress-nginx deployment:\n$ helm delete soa-nginx-ingress -n soans End-to-end SSL configuration Install the NGINX load balancer for End-to-end SSL   For secured access (SSL) to the Oracle SOA Suite application, create a certificate and generate secrets:\n$ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /tmp/tls1.key -out /tmp/tls1.crt -subj \u0026#34;/CN=domain1.org\u0026#34; $ kubectl -n soans create secret tls domain1-tls-cert --key /tmp/tls1.key --cert /tmp/tls1.crt  Note: The value of CN is the host on which this ingress is to be deployed.\n   Deploy the ingress-nginx controller by using Helm on the domain namespace:\n$ helm install nginx-ingress -n soans \\  --set controller.extraArgs.default-ssl-certificate=soans/domain1-tls-cert \\  --set controller.service.type=NodePort \\  --set controller.admissionWebhooks.enabled=false \\  --set controller.extraArgs.enable-ssl-passthrough=true \\  ingress-nginx/ingress-nginx    Click here to see the sample output.   NAME: nginx-ingress LAST DEPLOYED: Tue Sep 15 08:40:47 2020 NAMESPACE: soans STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: The ingress-nginx controller has been installed. Get the application URL by running these commands: export HTTP_NODE_PORT=$(kubectl --namespace soans get services -o jsonpath=\u0026#34;{.spec.ports[0].nodePort}\u0026#34; nginx-ingress-ingress-nginx-controller) export HTTPS_NODE_PORT=$(kubectl --namespace soans get services -o jsonpath=\u0026#34;{.spec.ports[1].nodePort}\u0026#34; nginx-ingress-ingress-nginx-controller) export NODE_IP=$(kubectl --namespace soans get nodes -o jsonpath=\u0026#34;{.items[0].status.addresses[1].address}\u0026#34;) echo \u0026#34;Visit http://$NODE_IP:$HTTP_NODE_PORTto access your application via HTTP.\u0026#34; echo \u0026#34;Visit https://$NODE_IP:$HTTPS_NODE_PORTto access your application via HTTPS.\u0026#34; An example Ingress that makes use of the controller: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx name: example namespace: foo spec: rules: - host: www.example.com http: paths: - backend: serviceName: exampleService servicePort: 80 path: / # This section is only required if TLS is to be enabled for the Ingress tls: - hosts: - www.example.com secretName: example-tls If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided: apiVersion: v1 kind: Secret metadata: name: example-tls namespace: foo data: tls.crt: \u0026lt;base64 encoded cert\u0026gt; tls.key: \u0026lt;base64 encoded key\u0026gt; type: kubernetes.io/tls      Check the status of the deployed ingress controller:\n$ kubectl --namespace soans get services | grep ingress-nginx-controller Sample output:\nnginx-ingress-ingress-nginx-controller NodePort 10.96.177.215 \u0026lt;none\u0026gt; 80:32748/TCP,443:31940/TCP 23s   Deploy tls to access services   Deploy tls to securely access the services. Only one application can be configured with ssl-passthrough. A sample tls file for NGINX is shown below for the service soainfra-cluster-soa-cluster and port 8002. All the applications running on port 8002 can be securely accessed through this ingress. For each backend service, create different ingresses as NGINX does not support multiple path/rules with annotation ssl-passthrough That is, for soainfra-cluster-soa-cluster and soainfra-cluster-osb-cluster, different ingresses must be created.\n  As ssl-passthrough in NGINX works on the clusterIP of the backing service instead of individual endpoints, you must expose adminserver service as adminserver service created by the operator with clusterIP.\n  Click here to see the commands to expose adminserver service   Check the status of the admin service\n$ kubectl get svc -n soans | grep soainfra-adminserver Sample output:\nsoainfra-adminserver ClusterIP None \u0026lt;none\u0026gt; 7001/TCP,7002/TCP 1s Expose the Administration Server service soainfra-adminserver and use the new service name soainfra-adminserver-nginx-ssl:\n$ kubectl expose svc soainfra-adminserver -n soans --name=soainfra-adminserver-nginx-ssl --port=7002\t     Click here to see the sample backend services of domainUID soainfra   Backend for Oracle SOA Suite Service with domainUID \u0026quot;soainfra\u0026quot; backend: serviceName: soainfra-cluster-soa-cluster servicePort: 8002 Backend for Oracle Service Bus with domainUID \u0026quot;soainfra\u0026quot; backend: serviceName: soainfra-cluster-osb-cluster servicePort: 9002 Backend for Service admin service with domainUID \u0026quot;soainfra\u0026quot; backend: serviceName: soainfra-adminserver-nginx-ssl servicePort: 7002      Deploy the secured ingress:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain/tls $ kubectl create -f nginx-tls.yaml    Click here to check the content of the file nginx-tls.yaml    apiVersion: extensions/v1beta1 kind: Ingress metadata: name: soang-ingress namespace: soans annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/ssl-passthrough: \u0026quot;true\u0026quot; spec: tls: - hosts: - domain1.org secretName: domain1-tls-cert rules: - host: domain1.org http: paths: - path: backend: serviceName: soainfra-cluster-soa-cluster servicePort: 8002     Note: host is the server on which this ingress is deployed.\n   Check the services supported by the ingress:\n$ kubectl describe ingress soang-ingress -n soans    Click here check the services supported by the ingress.    Name: soang-ingress Namespace: soans Address: 100.111.150.225 Default backend: default-http-backend:80 (\u0026lt;error: endpoints \u0026quot;default-http-backend\u0026quot; not found\u0026gt;) TLS: domain1-tls-cert terminates domain1.org Rules: Host Path Backends ---- ---- -------- domain1.org soainfra-cluster-soa-cluster:8002 (10.244.0.105:8002,10.244.0.106:8002) Annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/ssl-passthrough: true Events: \u0026lt;none\u0026gt;      Verify end-to-end SSL access Verify that the Oracle SOA Suite domain application URLs are accessible through the LOADBALANCER-SSLPORT 30233:\nhttps://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/soa-infra https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/soa/composer https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/integration/worklistapp Uninstall ingress-nginx tls $ cd weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain/tls $ kubectl delete -f nginx-tls.yaml "
},
{
	"uri": "/fmw-kubernetes/soa-domains/patch_and_upgrade/upgrade-operator-release/",
	"title": "Upgrade an operator release",
	"tags": [],
	"description": "Upgrade the WebLogic Kubernetes operator release to a newer version.",
	"content": "These instructions apply to upgrading operators within the 3.x release family as additional versions are released.\nTo upgrade the Kubernetes operator, use the helm upgrade command. Make sure that the weblogic-kubernetes-operator repository on your local machine is at the operator release to which you are upgrading. When upgrading the operator, the helm upgrade command requires that you supply a new Helm chart and image. For example:\n$ helm upgrade \\ --reuse-values \\ --set image=oracle/weblogic-kubernetes-operator:3.0.1 \\ --namespace weblogic-operator-namespace \\ --wait \\ weblogic-operator \\ kubernetes/charts/weblogic-operator "
},
{
	"uri": "/fmw-kubernetes/oig/",
	"title": "Oracle Identity Governance",
	"tags": [],
	"description": "The Oracle WebLogic Kubernetes Operator supports deployment of Oracle Identity Governance. Follow the instructions in this guide to set up Oracle Identity Governance domains on Kubernetes.",
	"content": "The Oracle WebLogic Kubernetes Operator supports deployment of Oracle Identity Governance (OIG).\nIn this release, OIG domains are supported using the “domain on a persistent volume” model only, where the domain home is located in a persistent volume (PV).\nThe operator has several key features to assist you with deploying and managing OIG domains in a Kubernetes environment. You can:\n Create OIG instances in a Kubernetes persistent volume. This persistent volume can reside in an NFS file system or other Kubernetes volume types. Start servers based on declarative startup parameters and desired states. Expose the OIG Services for external access. Scale OIG domains by starting and stopping Managed Servers on demand. Publish operator and WebLogic Server logs into Elasticsearch and interact with them in Kibana. Monitor the OIG instance using Prometheus and Grafana.  Limitations See here for limitations in this release.\nGetting started For detailed information about deploying Oracle Identity Governance domains, start at Prerequisites and follow this documentation sequentially.\nCurrent release The current supported release of the Oracle WebLogic Kubernetes Operator, for Oracle Identity Governance domains deployment is 3.0.1.\n"
},
{
	"uri": "/fmw-kubernetes/oam/prepare-your-environment/",
	"title": "Prepare your environment",
	"tags": [],
	"description": "Sample for creating an OAM domain home on an existing PV or PVC, and the domain resource YAML file for deploying the generated OAM domain.",
	"content": " Set up your Kubernetes cluster Install Helm Check the Kubernetes cluster is ready Install the OAM Docker image Install the Oracle WebLogic Server Kubernetes Operator docker image Set up the code repository to deploy OAM domains Install the Oracle WebLogic Server Kubernetes Operator RCU schema creation Preparing the environment for domain creation  Configure the operator for the domain namespace Creating Kubernetes secrets for the domain and RCU Create a Kubernetes persistent volume and persistent volume claim    Set up your Kubernetes cluster If you need help setting up a Kubernetes environment, check our cheat sheet.\nIt is recommended you have a master node and one or more worker nodes. The examples in this documentation assume one master and two worker nodes.\nAfter creating Kubernetes clusters, you can optionally:\n Configure an Ingress to direct traffic to backend domains. Configure Kibana and Elasticsearch for your operator logs.  Install Helm As per the prerequisites an installation of Helm is required to create and deploy the necessary resources and then run the operator in a Kubernetes cluster. For Helm installation and usage information, refer to the README.\nCheck the Kubernetes cluster is ready   Run the following command on the master node to check the cluster and worker nodes are running:\n$ kubectl get nodes,pods -n kube-system The output will look similar to the following:\nNAME STATUS ROLES AGE VERSION node/worker-node1 Ready \u0026lt;none\u0026gt; 17h v1.18.4 node/worker-node2 Ready \u0026lt;none\u0026gt; 17h v1.18.4 node/master-node Ready master 23h v1.18.4 NAME READY STATUS RESTARTS AGE pod/coredns-66bff467f8-fnhbq 1/1 Running 0 23h pod/coredns-66bff467f8-xtc8k 1/1 Running 0 23h pod/etcd-master 1/1 Running 0 21h pod/kube-apiserver-master-node 1/1 Running 0 21h pod/kube-controller-manager-master-node 1/1 Running 0 21h pod/kube-flannel-ds-amd64-lxsfw 1/1 Running 0 17h pod/kube-flannel-ds-amd64-pqrqr 1/1 Running 0 17h pod/kube-flannel-ds-amd64-wj5nh 1/1 Running 0 17h pod/kube-proxy-2kxv2 1/1 Running 0 17h pod/kube-proxy-82vvj 1/1 Running 0 17h pod/kube-proxy-nrgw9 1/1 Running 0 23h pod/kube-scheduler-master 1/1 Running 0 21   Install the OAM Docker image You can deploy OAM Docker images in the following ways:\n  Download a prebuilt OAM Docker image from My Oracle Support by referring to the document ID 2723908.1. This image is prebuilt by Oracle and includes Oracle Access Management 12.2.1.4.0 and the latest PSU.\n  Build your own OAM image using the WebLogic Image Tool or by using the dockerfile, scripts and base images from Oracle Container Registry (OCR). You can also build your own image by using only the dockerfile and scripts. For more information about the various ways in which you can build your own container image, see Building the OAM Image.\n  Choose one of these options based on your requirements.\nIf building your own image for OAM, you must include the mandatory patch 30571576.\n The OAM Docker image must be installed on the master node and each of the worker nodes in your Kubernetes cluster. Alternatively you can place the image in a Docker registry that your cluster can access.\n After installing the OAM Docker image run the following command to make sure the image is installed correctly on the master and worker nodes:\n$ docker images The output will look similar to the following:\nREPOSITORY TAG IMAGE ID CREATED SIZE quay.io/coreos/flannel v0.13.0-rc2 79dd6d6368e2 7 days ago 57.2MB oracle/oam 12.2.1.4.0 720a172374e6 2 weeks ago 3.38GB k8s.gcr.io/kube-proxy v1.18.4 718fa77019f2 3 weeks ago 117MB k8s.gcr.io/kube-controller-manager v1.18.4 e8f1690127c4 3 weeks ago 162MB k8s.gcr.io/kube-apiserver v1.18.4 408913fc18eb 3 weeks ago 173MB k8s.gcr.io/kube-scheduler v1.18.4 c663567f869e 3 weeks ago 95.3MB k8s.gcr.io/pause 3.2 80d28bedfe5d 5 months ago 683kB k8s.gcr.io/coredns 1.6.7 67da37a9a360 5 months ago 43.8MB k8s.gcr.io/etcd 3.4.3-0 303ce5db0e90 8 months ago 288MB Install the Oracle WebLogic Server Kubernetes Operator Docker image In this release only Oracle WebLogic Server Kubernetes Operator 3.0.1 is supported.\nThe Oracle WebLogic Server Kubernetes Operator Docker image must be installed on the master node and each of the worker nodes in your Kubernetes cluster. Alternatively you can place the image in a Docker registry that your cluster can access.\n   Pull the Oracle WebLogic Server Kubernetes Operator 3.0.1 image by running the following command on the master node:\n$ docker pull oracle/weblogic-kubernetes-operator:3.0.1 The output will look similar to the following:\nTrying to pull repository docker.io/oracle/weblogic-kubernetes-operator ... 3.0.1: Pulling from docker.io/oracle/weblogic-kubernetes-operator bce8f778fef0: Already exists de14ddc50a70: Pull complete 77401a861078: Pull complete 9c5ac1423af4: Pull complete 2b6f244f998f: Pull complete 625e05083092: Pull complete Digest: sha256:27047d032ac5a9077b39bec512b99d8ca54bf9bf71227f5fd1b7b26ac80c20d3 Status: Downloaded newer image for oracle/weblogic-kubernetes-operator:3.0.1 oracle/weblogic-kubernetes-operator:3.0.1   Run the docker tag command as follows:\n$ docker tag oracle/weblogic-kubernetes-operator:3.0.1 weblogic-kubernetes-operator:3.0.1 After installing the Oracle WebLogic Server Kubernetes Operator 3.0.1 Docker image, repeat the above on the worker nodes.\n  Set up the code repository to deploy OAM domains OAM domain deployment on Kubernetes leverages the Oracle WebLogic Server Kubernetes Operator infrastructure. For deploying the OAM domains, you need to set up the deployment scripts on the master node as below:\n  Create a working directory to setup the source code.\n$ mkdir \u0026lt;work directory\u0026gt; For example:\n$ mkdir /scratch/OAMDockerK8S   Download the supported version of the WebLogic Kubernetes operator source code from the operator github project. Currently the supported operator version is 3.0.1:\n$ cd \u0026lt;work directory\u0026gt; $ git clone https://github.com/oracle/weblogic-kubernetes-operator.git --branch release/3.0.1 For example:\n$ cd /scratch/OAMDockerK8S $ git clone https://github.com/oracle/weblogic-kubernetes-operator.git --branch release/3.0.1 This will create the directory \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator\n  Download the OAM deployment scripts from the OAM repository and copy them in to the Oracle WebLogic Server Kubernetes Operator samples location.\n$ git clone https://github.com/oracle/fmw-kubernetes.git $ cp -rf \u0026lt;work directory\u0026gt;/fmw-kubernetes/OracleAccessManagement/kubernetes/3.0.1/create-access-domain \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/scripts/ $ mv -f \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain_backup $ cp -rf \u0026lt;work directory\u0026gt;/fmw-kubernetes/OracleAccessManagement/kubernetes/3.0.1/ingress-per-domain \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain For example:\n$ git clone https://github.com/oracle/fmw-kubernetes.git $ cp -rf /scratch/OAMDockerK8S/fmw-kubernetes/OracleAccessManagement/kubernetes/3.0.1/create-access-domain /scratch/OAMDockerK8S/weblogic-kubernetes-operator/kubernetes/samples/scripts/ $ mv -f /scratch/OAMDockerK8S/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain /scratch/OAMDockerK8S/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain_backup $ cp -rf /scratch/OAMDockerK8S/fmw-kubernetes/OracleAccessManagement/kubernetes/3.0.1/ingress-per-domain /scratch/OAMDockerK8S/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain You can now use the deployment scripts from \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/scripts/ to set up the OAM domains as further described in this document.\n  Run the following command and see if the WebLogic custom resource definition name already exists:\n$ kubectl get crd In the output you should see:\nNo resources found in default namespace. If you see the following:\nNAME AGE domains.weblogic.oracle 5d then run the following command to delete the existing crd:\n$ kubectl delete crd domains.weblogic.oracle customresourcedefinition.apiextensions.k8s.io \u0026#34;domains.weblogic.oracle\u0026#34; deleted   Install the Oracle WebLogic Server Kubernetes Operator   On the master node run the following command to create a namespace for the operator:\n$ kubectl create namespace \u0026lt;sample-kubernetes-operator-ns\u0026gt; For example:\n$ kubectl create namespace opns The output will look similar to the following:\nnamespace/opns created   Create a service account for the operator in the operator\u0026rsquo;s namespace by running the following command:\n$ kubectl create serviceaccount -n \u0026lt;sample-kubernetes-operator-ns\u0026gt; \u0026lt;sample-kubernetes-operator-sa\u0026gt; For example:\n$ kubectl create serviceaccount -n opns op-sa The output will look similar to the following:\nserviceaccount/op-sa created   If you want to to setup logging and visualisation with Elasticsearch and Kibana (post domain creation) edit the \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/charts/weblogic-operator/values.yaml and set the parameter elkIntegrationEnabled to true and make sure the following parameters are set:\n# elkIntegrationEnabled specifies whether or not ELK integration is enabled. elkIntegrationEnabled: true # logStashImage specifies the docker image containing logstash. # This parameter is ignored if \u0026#39;elkIntegrationEnabled\u0026#39; is false. logStashImage: \u0026#34;logstash:6.6.0\u0026#34; # elasticSearchHost specifies the hostname of where elasticsearch is running. # This parameter is ignored if \u0026#39;elkIntegrationEnabled\u0026#39; is false. elasticSearchHost: \u0026#34;elasticsearch.default.svc.cluster.local\u0026#34; # elasticSearchPort specifies the port number of where elasticsearch is running. # This parameter is ignored if \u0026#39;elkIntegrationEnabled\u0026#39; is false. elasticSearchPort: 9200 After the domain creation see Logging and Visualization in order to complete the setup of Elasticsearch and Kibana.\n  Run the following helm command to install and start the operator:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator $ helm install kubernetes/charts/weblogic-operator \\ --namespace \u0026lt;sample-kubernetes-operator-ns\u0026gt; \\ --set image=weblogic-kubernetes-operator:3.0.1 \\ --set serviceAccount=\u0026lt;sample-kubernetes-operator-sa\u0026gt; --set \u0026#34;domainNamespaces={}\u0026#34; --set \u0026#34;javaLoggingLevel=FINE\u0026#34; --wait For example:\n$ cd /scratch/OAMDockerK8S/weblogic-kubernetes-operator $ helm install weblogic-kubernetes-operator kubernetes/charts/weblogic-operator \\ --namespace opns --set image=weblogic-kubernetes-operator:3.0.1 \\ --set serviceAccount=op-sa --set \u0026#34;domainNamespaces={}\u0026#34; --set \u0026#34;javaLoggingLevel=FINE\u0026#34; --wait The output will look similar to the following:\nNAME: weblogic-kubernetes-operator LAST DEPLOYED: Wed Sep 23 08:04:20 2020 NAMESPACE: opns STATUS: deployed REVISION: 1 TEST SUITE: None   Verify that the operator\u0026rsquo;s pod and services are running by executing the following command:\n$ kubectl get all -n \u0026lt;sample-kubernetes-operator-ns\u0026gt; For example:\n$ kubectl get all -n opns The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE pod/weblogic-operator-759b7c657-8gd7g 2/2 Running 0 107s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/internal-weblogic-operator-svc ClusterIP 10.102.11.143 \u0026lt;none\u0026gt; 8082/TCP 107s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/weblogic-operator 1/1 1 1 107s NAME DESIRED CURRENT READY AGE replicaset.apps/weblogic-operator-759b7c657 1 1 1 107s   Verify the operator pod\u0026rsquo;s log:\n$ kubectl logs -n \u0026lt;sample-kubernetes-operator-ns\u0026gt; -c weblogic-operator deployments/weblogic-operator For example:\n$ kubectl logs -n opns -c weblogic-operator deployments/weblogic-operator The output will look similar to the following:\n... {\u0026#34;timestamp\u0026#34;:\u0026#34;09-23-2020T15:04:30.485+0000\u0026#34;,\u0026#34;thread\u0026#34;:28,\u0026#34;fiber\u0026#34;:\u0026#34;fiber-1\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;opns\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.rest.RestServer\u0026#34;,ethod\u0026#34;:\u0026#34;start\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1600873470485,\u0026#34;message\u0026#34;:\u0026#34;Started the internal ssl REST server on https://0.0.0.0:8082/operator\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;} {\u0026#34;timestamp\u0026#34;:\u0026#34;09-23-2020T15:04:30.487+0000\u0026#34;,\u0026#34;thread\u0026#34;:28,\u0026#34;fiber\u0026#34;:\u0026#34;fiber-1\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;opns\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.Main\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;mkReadyAndStartLivenessThread\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1600873470487,\u0026#34;message\u0026#34;:\u0026#34;Starting Operator Liveness Thread\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;} {\u0026#34;timestamp\u0026#34;:\u0026#34;09-23-2020T15:06:27.528+0000\u0026#34;,\u0026#34;thread\u0026#34;:22,\u0026#34;fiber\u0026#34;:\u0026#34;engine-operator-thread-5-fiber-2\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;opns\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;FINE\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.orator.helpers.ConfigMapHelper$ScriptConfigMapContext\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;loadScriptsFromClasspath\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1600873587528,\u0026#34;message\u0026#34;:\u0026#34;Loading scripts into domain control config mapor namespace: opns\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;} {\u0026#34;timestamp\u0026#34;:\u0026#34;09-23-2020T15:06:27.529+0000\u0026#34;,\u0026#34;thread\u0026#34;:22,\u0026#34;fiber\u0026#34;:\u0026#34;engine-operator-thread-5-fiber-2\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;opns\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;FINE\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.orator.Main\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;readExistingDomains\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1600873587529,\u0026#34;message\u0026#34;:\u0026#34;Listing WebLogic Domains\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;} {\u0026#34;timestamp\u0026#34;:\u0026#34;09-23-2020T15:06:27.576+0000\u0026#34;,\u0026#34;thread\u0026#34;:20,\u0026#34;fiber\u0026#34;:\u0026#34;fiber-2-child-1\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;opns\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;FINE\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.helpers.CfigMapHelper$ConfigMapContext$ReadResponseStep\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;logConfigMapExists\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1600873587576,\u0026#34;message\u0026#34;:\u0026#34;Existing config map, ConfigMapHelper$ConfigMapContext$Readsponse, is correct for namespace: opns.\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;}   RCU schema creation In this section you create the RCU schemas in the Oracle Database.\nBefore following the steps in this section, make sure that the database and listener are up and running and you can connect to the database via SQL*Plus or other client tool.\n  Run the following command to create a namespace for the domain:\n$ kubectl create namespace \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl create namespace accessns The output will look similar to the following:\nnamespace/accessns created   Run the following command to create a helper pod to run RCU:\n$ kubectl run helper --image \u0026lt;image_name\u0026gt; -n \u0026lt;domain_namespace\u0026gt; -- sleep infinity For example:\n$ kubectl run helper --image oracle/oam:12.2.1.4.0 -n accessns -- sleep infinity The output will look similar to the following:\npod/helper created   Run the following command to check the pod is running:\n$ kubectl get pods -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get pods -n accessns The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE helper 1/1 Running 0 8s   Run the following command to start a bash shell in the helper pod:\n$ kubectl exec -it helper -n \u0026lt;domain_namespace\u0026gt; -- /bin/bash For example:\n$ kubectl exec -it helper -n accessns -- /bin/bash This will take you into a bash shell in the running helper pod:\n[oracle@helper ~]$   In the helper bash shell run the following commands to set the environment:\n[oracle@helper ~]$ export CONNECTION_STRING=\u0026lt;db_host.domain\u0026gt;:\u0026lt;db_port\u0026gt;/\u0026lt;service_name\u0026gt; [oracle@helper ~]$ export RCUPREFIX=\u0026lt;rcu_schema_prefix\u0026gt; [oracle@helper ~]$ echo -e \u0026lt;db_pwd\u0026gt;\u0026#34;\\n\u0026#34;\u0026lt;rcu_schema_pwd\u0026gt; \u0026gt; /tmp/pwd.txt [oracle@helper ~]$ cat /tmp/pwd.txt where:\n\u0026lt;db_host.domain\u0026gt;:\u0026lt;db_port\u0026gt;/\u0026lt;service_name\u0026gt;\tis your database connect string\n\u0026lt;rcu_schema_prefix\u0026gt; is the RCU schema prefix you want to set\n\u0026lt;db_pwd\u0026gt; is the SYS password for the database\n\u0026lt;rcu_schema_pwd\u0026gt; is the password you want to set for the \u0026lt;rcu_schema_prefix\u0026gt;\nFor example:\n[oracle@helper ~]$ export CONNECTION_STRING=mydatabasehost.example.com:1521/orcl.example.com [oracle@helper ~]$ export RCUPREFIX=OAMK8S [oracle@helper ~]$ echo -e \u0026lt;password\u0026gt;\u0026#34;\\n\u0026#34;\u0026lt;password\u0026gt; \u0026gt; /tmp/pwd.txt [oracle@helper ~]$ cat /tmp/pwd.txt \u0026lt;password\u0026gt; \u0026lt;password\u0026gt;   In the helper bash shell run the following command to create the RCU schemas in the database:\n$ [oracle@helper ~]$ /u01/oracle/oracle_common/bin/rcu -silent -createRepository -databaseType ORACLE -connectString \\ $CONNECTION_STRING -dbUser sys -dbRole sysdba -useSamePasswordForAllSchemaUsers true \\ -selectDependentsForComponents true -schemaPrefix $RCUPREFIX -component MDS -component IAU \\ -component IAU_APPEND -component IAU_VIEWER -component OPSS -component WLS -component STB -component OAM -f \u0026lt; /tmp/pwd.txt The output will look similar to the following:\nRCU Logfile: /tmp/RCU2020-09-23_15-36_1649016162/logs/rcu.log Processing command line .... Repository Creation Utility - Checking Prerequisites Checking Global Prerequisites Repository Creation Utility - Checking Prerequisites Checking Component Prerequisites Repository Creation Utility - Creating Tablespaces Validating and Creating Tablespaces Create tablespaces in the repository database Repository Creation Utility - Create Repository Create in progress. Executing pre create operations Percent Complete: 18 Percent Complete: 18 Percent Complete: 19 Percent Complete: 20 Percent Complete: 21 Percent Complete: 21 Percent Complete: 22 Percent Complete: 22 Creating Common Infrastructure Services(STB) Percent Complete: 30 Percent Complete: 30 Percent Complete: 39 Percent Complete: 39 Percent Complete: 39 Creating Audit Services Append(IAU_APPEND) Percent Complete: 46 Percent Complete: 46 Percent Complete: 55 Percent Complete: 55 Percent Complete: 55 Creating Audit Services Viewer(IAU_VIEWER) Percent Complete: 62 Percent Complete: 62 Percent Complete: 63 Percent Complete: 63 Percent Complete: 64 Percent Complete: 64 Creating Metadata Services(MDS) Percent Complete: 73 Percent Complete: 73 Percent Complete: 73 Percent Complete: 74 Percent Complete: 74 Percent Complete: 75 Percent Complete: 75 Percent Complete: 75 Creating Weblogic Services(WLS) Percent Complete: 80 Percent Complete: 80 Percent Complete: 83 Percent Complete: 83 Percent Complete: 91 Percent Complete: 98 Percent Complete: 98 Creating Audit Services(IAU) Percent Complete: 100 Creating Oracle Platform Security Services(OPSS) Creating Oracle Access Manager(OAM) Executing post create operations Repository Creation Utility: Create - Completion Summary Database details: ----------------------------- Host Name : mydatabasehost.example.com Port : 1521 Service Name : ORCL.EXAMPLE.COM Connected As : sys Prefix for (prefixable) Schema Owners : OAMK8S RCU Logfile : /tmp/RCU2020-09-23_15-36_1649016162/logs/rcu.log Component schemas created: ----------------------------- Component Status Logfile Common Infrastructure Services Success /tmp/RCU2020-09-23_15-36_1649016162/logs/stb.log Oracle Platform Security Services Success /tmp/RCU2020-09-23_15-36_1649016162/logs/opss.log Oracle Access Manager Success /tmp/RCU2020-09-23_15-36_1649016162/logs/oam.log Audit Services Success /tmp/RCU2020-09-23_15-36_1649016162/logs/iau.log Audit Services Append Success /tmp/RCU2020-09-23_15-36_1649016162/logs/iau_append.log Audit Services Viewer Success /tmp/RCU2020-09-23_15-36_1649016162/logs/iau_viewer.log Metadata Services Success /tmp/RCU2020-09-23_15-36_1649016162/logs/mds.log WebLogic Services Success /tmp/RCU2020-09-23_15-36_1649016162/logs/wls.log Repository Creation Utility - Create : Operation Completed [oracle@helper ~]$   Exit the helper bash shell by issuing the command exit.\n  Preparing the environment for domain creation In this section you prepare the environment for the OAM domain creation. This involves the following steps:\n Configure the operator for the domain namespace Create Kubernetes secrets for the domain and RCU Create a Kubernetes PV and PVC (Persistent Volume and Persistent Volume Claim)  Configure the operator for the domain namespace   Configure the Oracle WebLogic Server Kubernetes Operator to manage the domain in the domain namespace by running the following command:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator $ helm upgrade --reuse-values --namespace \u0026lt;operator_namespace\u0026gt; --set \u0026#34;domainNamespaces={\u0026lt;domain_namespace\u0026gt;}\u0026#34; --wait weblogic-kubernetes-operator kubernetes/charts/weblogic-operator For example:\n$ cd /scratch/OAMDockerK8S/weblogic-kubernetes-operator $ helm upgrade --reuse-values --namespace opns --set \u0026#34;domainNamespaces={accessns}\u0026#34; --wait weblogic-kubernetes-operator kubernetes/charts/weblogic-operator The output will look similar to the following:\nRelease \u0026#34;weblogic-kubernetes-operator\u0026#34; has been upgraded. Happy Helming! NAME: weblogic-kubernetes-operator LAST DEPLOYED: Wed Sep 23 08:44:48 2020 NAMESPACE: opns STATUS: deployed REVISION: 2 TEST SUITE: None   Creating Kubernetes secrets for the domain and RCU   Create a Kubernetes secret for the domain using the create-weblogic-credentials script in the same Kubernetes namespace as the domain:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-weblogic-domain-credentials $ ./create-weblogic-credentials.sh -u weblogic -p \u0026lt;pwd\u0026gt; -n \u0026lt;domain_namespace\u0026gt; -d \u0026lt;domain_uid\u0026gt; -s \u0026lt;kubernetes_domain_secret\u0026gt; where:\n-u weblogic is the WebLogic username\n-p \u0026lt;pwd\u0026gt; is the password for the weblogic user\n-n \u0026lt;domain_namespace\u0026gt; is the domain namespace\n-d \u0026lt;domain_uid\u0026gt; is the domain UID to be created. The default is domain1 if not specified\n-s \u0026lt;kubernetes_domain_secret\u0026gt; is the name you want to create for the secret for this namespace. The default is to use the domainUID if not specified\nFor example:\n$ cd /scratch/OAMDockerK8S/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-weblogic-domain-credentials $ ./create-weblogic-credentials.sh -u weblogic -p \u0026lt;password\u0026gt; -n accessns -d accessinfra -s accessinfra-domain-credentials The output will look similar to the following:\nsecret/accessinfra-domain-credentials created secret/accessinfra-domain-credentials labeled The secret accessinfra-domain-credentials has been successfully created in the accessns namespace.   Verify the secret is created using the following command:\n$ kubectl get secret \u0026lt;kubernetes_domain_secret\u0026gt; -o yaml -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get secret accessinfra-domain-credentials -o yaml -n accessns The output will look similar to the following:\napiVersion: v1 data: password: V2VsY29tZTE= username: d2VibG9naWM= kind: Secret metadata: creationTimestamp: \u0026#34;2020-09-23T15:46:25Z\u0026#34; labels: weblogic.domainName: accessinfra weblogic.domainUID: accessinfra managedFields: - apiVersion: v1 fieldsType: FieldsV1 fieldsV1: f:data: .: {} f:password: {} f:username: {} f:metadata: f:labels: .: {} f:weblogic.domainName: {} f:weblogic.domainUID: {} f:type: {} manager: kubectl operation: Update time: \u0026#34;2020-09-23T15:46:25Z\u0026#34; name: accessinfra-domain-credentials namespace: accessns resourceVersion: \u0026#34;50606\u0026#34; selfLink: /api/v1/namespaces/accessns/secrets/accessinfra-domain-credentials uid: 29f638f5-11d9-4b62-9cbb-03ff13ae3a90 type: Opaque   Create a Kubernetes secret for RCU using the create-weblogic-credentials script in the same Kubernetes namespace as the domain:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-rcu-credentials $ ./create-rcu-credentials.sh -u \u0026lt;rcu_prefix\u0026gt; -p \u0026lt;rcu_schema_pwd\u0026gt; -a sys -q \u0026lt;sys_db_pwd\u0026gt; -d \u0026lt;domain_uid\u0026gt; -n \u0026lt;domain_namespace\u0026gt; -s \u0026lt;kubernetes_rcu_secret\u0026gt; where:\n-u \u0026lt;rcu_prefix\u0026gt; is the name of the RCU schema prefix created previously\n-p \u0026lt;rcu_schema_pwd\u0026gt; is the password for the RCU schema prefix\n-q \u0026lt;sys_db_pwd\u0026gt; is the sys database password\n-d \u0026lt;domain_uid\u0026gt; is the domain_uid that you created earlier\n-n \u0026lt;domain_namespace\u0026gt; is the domain namespace\n-s \u0026lt;kubernetes_rcu_secret\u0026gt; is the name of the rcu secret to create\nFor example:\n$ cd /scratch/OAMDockerK8S/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-rcu-credentials $ ./create-rcu-credentials.sh -u OAMK8S -p \u0026lt;password\u0026gt; -a sys -q \u0026lt;password\u0026gt; -d accessinfra -n accessns -s accessinfra-rcu-credentials The output will look similar to the following:\nsecret/accessinfra-rcu-credentials created secret/accessinfra-rcu-credentials labeled The secret accessinfra-rcu-credentials has been successfully created in the accessns namespace.   Verify the secret is created using the following command:\n$ kubectl get secret \u0026lt;kubernetes_rcu_secret\u0026gt; -o yaml -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get secret accessinfra-rcu-credentials -o yaml -n accessns The output will look similar to the following:\napiVersion: v1 data: password: V2VsY29tZTE= sys_password: V2VsY29tZTE= sys_username: c3lz username: T0FNSzhT kind: Secret metadata: creationTimestamp: \u0026#34;2020-09-23T15:50:04Z\u0026#34; labels: weblogic.domainName: accessinfra weblogic.domainUID: accessinfra managedFields: - apiVersion: v1 fieldsType: FieldsV1 fieldsV1: f:data: .: {} f:password: {} f:sys_password: {} f:sys_username: {} f:username: {} f:metadata: f:labels: .: {} f:weblogic.domainName: {} f:weblogic.domainUID: {} f:type: {} manager: kubectl operation: Update time: \u0026#34;2020-09-23T15:50:04Z\u0026#34; name: accessinfra-rcu-credentials namespace: accessns resourceVersion: \u0026#34;51134\u0026#34; selfLink: /api/v1/namespaces/accessns/secrets/accessinfra-rcu-credentials uid: fce2499c-d8c8-4e9c-93e0-b15722bfc4d7 type: Opaque   Create a Kubernetes persistent volume and persistent volume claim In the Kubernetes namespace created above, create the persistent volume (PV) and persistent volume claim (PVC) by running the create-pv-pvc.sh script.\n  Make a backup copy of the create-pv-pvc-inputs.yaml file and create required directories:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-weblogic-domain-pv-pvc $ cp create-pv-pvc-inputs.yaml create-pv-pvc-inputs.yaml.orig $ mkdir output_access $ mkdir -p /\u0026lt;work directory\u0026gt;/accessdomainpv $ chmod -R 777 /\u0026lt;work directory\u0026gt;/accessdomainpv For example:\n$ cd /scratch/OAMDockerK8S/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-weblogic-domain-pv-pvc $ cp create-pv-pvc-inputs.yaml create-pv-pvc-inputs.yaml.orig $ mkdir output_access $ mkdir -p /scratch/OAMDockerK8S/accessdomainpv $ chmod -R 777 /scratch/OAMDockerK8S/accessdomainpv Note: The persistent volume directory needs to be accessible to both the master and worker node(s) via NFS. Make sure this path has full access permissions, and that the folder is empty. In this example /scratch/OAMDockerK8S/accessdomainpv is accessible from all nodes via NFS.\n  Edit the create-pv-pvc-inputs.yaml file and update the following parameters to reflect your settings. Save the file when complete:\nbaseName: \u0026lt;domain\u0026gt; domainUID: \u0026lt;domain_uid\u0026gt; namespace: \u0026lt;domain_namespace\u0026gt; weblogicDomainStorageType: NFS weblogicDomainStorageNFSServer: \u0026lt;nfs_server\u0026gt; weblogicDomainStoragePath: \u0026lt;physical_path_of_persistent_storage\u0026gt; For example:\n# The base name of the pv and pvc baseName: domain # Unique ID identifying a domain. # If left empty, the generated pv can be shared by multiple domains # This ID must not contain an underscope (\u0026#34;_\u0026#34;), and must be lowercase and unique across all domains in a Kubernetes cluster. domainUID: accessinfra # Name of the namespace for the persistent volume claim namespace: accessns ... # Persistent volume type for the persistent storage. # The value must be \u0026#39;HOST_PATH\u0026#39; or \u0026#39;NFS\u0026#39;. # If using \u0026#39;NFS\u0026#39;, weblogicDomainStorageNFSServer must be specified. weblogicDomainStorageType: NFS # The server name or ip address of the NFS server to use for the persistent storage. # The following line must be uncomment and customized if weblogicDomainStorateType is NFS: weblogicDomainStorageNFSServer: mynfsserver # Physical path of the persistent storage. # When weblogicDomainStorageType is set to HOST_PATH, this value should be set the to path to the # domain storage on the Kubernetes host. # When weblogicDomainStorageType is set to NFS, then weblogicDomainStorageNFSServer should be set # to the IP address or name of the DNS server, and this value should be set to the exported path # on that server. # Note that the path where the domain is mounted in the WebLogic containers is not affected by this # setting, that is determined when you create your domain. # The following line must be uncomment and customized: weblogicDomainStoragePath: /scratch/OAMDockerK8S/accessdomainpv   Execute the create-pv-pvc.sh script to create the PV and PVC configuration files:\n$ ./create-pv-pvc.sh -i create-pv-pvc-inputs.yaml -o output_access The output will be similar to the following:\nInput parameters being used export version=\u0026#34;create-weblogic-sample-domain-pv-pvc-inputs-v1\u0026#34; export baseName=\u0026#34;domain\u0026#34; export domainUID=\u0026#34;accessinfra\u0026#34; export namespace=\u0026#34;accessns\u0026#34; export weblogicDomainStorageType=\u0026#34;NFS\u0026#34; export weblogicDomainStorageNFSServer=\u0026#34;mynfsserver\u0026#34; export weblogicDomainStoragePath=\u0026#34;/scratch/OAMDockerK8S/accessdomainpv\u0026#34; export weblogicDomainStorageReclaimPolicy=\u0026#34;Retain\u0026#34; export weblogicDomainStorageSize=\u0026#34;10Gi\u0026#34; Generating output_access/pv-pvcs/accessinfra-weblogic-sample-pv.yaml Generating output_access/pv-pvcs/accessinfra-weblogic-sample-pvc.yaml The following files were generated: output_access/pv-pvcs/accessinfra-weblogic-sample-pv.yaml output_access/pv-pvcs/accessinfra-weblogic-sample-pvc.yaml   Run the following to show the files are created:\n$ ls output_access/pv-pvcs create-pv-pvc-inputs.yaml accessinfra-weblogic-sample-pv.yaml accessinfra-weblogic-sample-pvc.yaml   Run the following kubectl command to create the PV and PVC in the domain namespace:\n$ kubectl create -f output_access/pv-pvcs/accessinfra-domain-pv.yaml -n \u0026lt;domain_namespace\u0026gt; $ kubectl create -f output_access/pv-pvcs/accessinfra-domain-pvc.yaml -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl create -f output_access/pv-pvcs/accessinfra-domain-pv.yaml -n accessns $ kubectl create -f output_access/pv-pvcs/accessinfra-domain-pvc.yaml -n accessns The output will look similar to the following:\npersistentvolume/accessinfra-domain-pv created persistentvolumeclaim/accessinfra-domain-pvc created   Run the following commands to verify the PV and PVC were created successfully:\n$ kubectl describe pv \u0026lt;pv_name\u0026gt; $ kubectl describe pvc \u0026lt;pvc_name\u0026gt; -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl describe pv accessinfra-domain-pv $ kubectl describe pvc accessinfra-domain-pvc -n accessns The output will look similar to the following:\n$ kubectl describe pv accessinfra-domain-pv Name: accessinfra-domain-pv Labels: weblogic.domainUID=accessinfra Annotations: pv.kubernetes.io/bound-by-controller: yes Finalizers: [kubernetes.io/pv-protection] StorageClass: accessinfra-domain-storage-class Status: Bound Claim: accessns/accessinfra-domain-pvc Reclaim Policy: Retain Access Modes: RWX VolumeMode: Filesystem Capacity: 10Gi Node Affinity: \u0026lt;none\u0026gt; Message: Source: Type: NFS (an NFS mount that lasts the lifetime of a pod) Server: mynfsserver Path: /scratch/OAMDockerK8S/accessdomainpv ReadOnly: false Events: \u0026lt;none\u0026gt; $ kubectl describe pvc accessinfra-domain-pvc -n accessns Name: accessinfra-domain-pvc Namespace: accessns StorageClass: accessinfra-domain-storage-class Status: Bound Volume: accessinfra-domain-pv Labels: weblogic.domainUID=accessinfra Annotations: pv.kubernetes.io/bind-completed: yes pv.kubernetes.io/bound-by-controller: yes Finalizers: [kubernetes.io/pvc-protection] Capacity: 10Gi Access Modes: RWX VolumeMode: Filesystem Events: \u0026lt;none\u0026gt; Mounted By: \u0026lt;none\u0026gt; You are now ready to create the OAM domain as per Create OAM Domains\n  "
},
{
	"uri": "/fmw-kubernetes/oig/prepare-your-environment/",
	"title": "Prepare your environment",
	"tags": [],
	"description": "Preparation to deploy OIG on Kubernetes",
	"content": " Set up your Kubernetes cluster Install Helm Check the Kubernetes cluster is ready Install the OIG Docker image Install the Oracle WebLogic Server Kubernetes Operator Docker Image Setup the Code Repository to Deploy Oracle Identity Governance Domains Install the Oracle WebLogic Kubernetes Operator RCU schema creation Preparing the environment for domain creation  Configure the operator for the domain namespace Creating Kubernetes secrets for the domain and RCU Create a Kubernetes persistent volume and persistent volume claim    Set up your Kubernetes cluster If you need help setting up a Kubernetes environment, check our cheat sheet.\nIt is recommended you have a master node and one or more worker nodes. The examples in this documentation assume one master and two worker nodes.\nAfter creating Kubernetes clusters, you can optionally:\n Configure an Ingress to direct traffic to backend domains. Configure Kibana and Elasticsearch for your operator logs.  Install Helm As per the prerequisites an installation of Helm is required to create and deploy the necessary resources and then run the operator in a Kubernetes cluster. For Helm installation and usage information, refer to the README.\nCheck the Kubernetes cluster is ready Run the following command on the master node to check the cluster and worker nodes are running:\n$ kubectl get nodes,pods -n kube-system The output will look similar to the following:\n$ kubectl get nodes,pods -n kube-system NAME STATUS ROLES AGE VERSION node/worker-node1 Ready \u0026lt;none\u0026gt; 10d v1.18.4 node/worker-node2 Ready \u0026lt;none\u0026gt; 10d v1.18.4 node/master-node Ready master 11d v1.18.4 NAME READY STATUS RESTARTS AGE pod/coredns-66bff467f8-slxdq 1/1 Running 0 11d pod/coredns-66bff467f8-v77qt 1/1 Running 0 11d pod/etcd-master-node 1/1 Running 0 11d pod/kube-apiserver-master-node 1/1 Running 0 11d pod/kube-controller-manager-master-node 1/1 Running 0 11d pod/kube-flannel-ds-amd64-dcqjn 1/1 Running 0 10d pod/kube-flannel-ds-amd64-g4ztq 1/1 Running 0 11d pod/kube-flannel-ds-amd64-vpcbj 1/1 Running 1 10d pod/kube-proxy-jtcxm 1/1 Running 0 11d pod/kube-proxy-swfmm 1/1 Running 0 10d pod/kube-proxy-w6x6t 1/1 Running 0 10d pod/kube-scheduler-master-node 1/1 Running 0 11d $ Install the OIG Docker Image You can deploy OIG Docker images in the following ways:\n  Download a prebuilt OIG Docker image from My Oracle Support by referring to the document ID 2723908.1. This image is prebuilt by Oracle and includes Oracle Identity Governance 12.2.1.4.0 and the latest PSU.\n  Build your own OIG image using the WebLogic Image Tool or by using the dockerfile, scripts and base images from Oracle Container Registry (OCR). You can also build your own image by using only the dockerfile and scripts. For more information about the various ways in which you can build your own container image, see Building the OIG Docker Image.\n  Choose one of these options based on your requirements.\nThe OIG Docker image must be installed on the master node AND each of the worker nodes in your Kubernetes cluster. Alternatively you can place the image in a Docker registry that your cluster can access.\n After installing the OIG Docker image run the following command to make sure the image is installed correctly on the master and worker nodes:\n$ docker images The output will look similar to the following:\nREPOSITORY TAG IMAGE ID CREATED SIZE oracle/oig 12.2.1.4.0 59ffc14dddbb 3 days ago 4.96GB k8s.gcr.io/kube-proxy v1.18.4 718fa77019f2 6 weeks ago 117MB k8s.gcr.io/kube-scheduler v1.18.4 c663567f869e 6 weeks ago 95.3MB k8s.gcr.io/kube-controller-manager v1.18.4 e8f1690127c4 6 weeks ago 162MB k8s.gcr.io/kube-apiserver v1.18.4 408913fc18eb 6 weeks ago 173MB quay.io/coreos/flannel v0.12.0-amd64 4e9f801d2217 4 months ago 52.8MB k8s.gcr.io/pause 3.2 80d28bedfe5d 5 months ago 683kB k8s.gcr.io/coredns 1.6.7 67da37a9a360 6 months ago 43.8MB k8s.gcr.io/etcd 3.4.3-0 303ce5db0e90 9 months ago 288MB Install the Oracle WebLogic Server Kubernetes Operator Docker Image In this release only Oracle WebLogic Server Kubernetes Operator 3.0.1 is supported.\nThe Oracle WebLogic Server Kubernetes Operator Docker image must be installed on the master node and each of the worker nodes in your Kubernetes cluster. Alternatively you can place the image in a Docker registry that your cluster can access.\n   Pull the Oracle WebLogic Server Kubernetes Operator 3.0.1 image by running the following command on the master node:\n$ docker pull oracle/weblogic-kubernetes-operator:3.0.1 The output will look similar to the following:\nTrying to pull repository docker.io/oracle/weblogic-kubernetes-operator ... 3.0.1: Pulling from docker.io/oracle/weblogic-kubernetes-operator bce8f778fef0: Already exists de14ddc50a70: Pull complete 77401a861078: Pull complete 9c5ac1423af4: Pull complete 2b6f244f998f: Pull complete 625e05083092: Pull complete Digest: sha256:27047d032ac5a9077b39bec512b99d8ca54bf9bf71227f5fd1b7b26ac80c20d3 Status: Downloaded newer image for oracle/weblogic-kubernetes-operator:3.0.1 oracle/weblogic-kubernetes-operator:3.0.1   Run the docker tag command as follows:\n$ docker tag oracle/weblogic-kubernetes-operator:3.0.1 weblogic-kubernetes-operator:3.0.1 After installing the Oracle WebLogic Server Kubernetes Operator 3.0.1 Docker image, repeat the above on the worker nodes.\n  Setup the Code Repository to Deploy Oracle Identity Governance Domains Oracle Identity Governance domain deployment on Kubernetes leverages the Oracle WebLogic Kubernetes Operator infrastructure. For deploying the Oracle Identity Governance domains, you need to set up the deployment scripts on the master node as below:\n  Create a working directory to setup the source code.\n$ mkdir \u0026lt;work directory\u0026gt; For example:\n$ mkdir /scratch/OIGDockerK8S   Download the supported version of the Oracle WebLogic Kubernetes Operator source code from the operator github project. Currently the supported operator version is 3.0.1:\n$ cd \u0026lt;work directory\u0026gt; $ git clone https://github.com/oracle/weblogic-kubernetes-operator.git --branch release/3.0.1 For example:\n$ cd /scratch/OIGDockerK8S $ git clone https://github.com/oracle/weblogic-kubernetes-operator.git --branch release/3.0.1 This will create the directory \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator\n  Clone the Oracle Identity Governance deployment scripts from the OIG repository and copy them into the WebLogic operator samples location.\n$ git clone https://github.com/oracle/fmw-kubernetes.git $ cp -rf \u0026lt;work directory\u0026gt;/fmw-kubernetes/OracleIdentityGovernance/kubernetes/3.0.1/create-oim-domain \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/scripts/ $ mv -f \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain_backup $ cp -rf \u0026lt;work directory\u0026gt;/fmw-kubernetes/OracleIdentityGovernance/kubernetes/3.0.1/ingress-per-domain \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain $ cp -rf \u0026lt;work directory\u0026gt;/fmw-kubernetes/OracleIdentityGovernance/kubernetes/3.0.1/design-console-ingress \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/charts/design-console-ingress For example:\n$ git clone https://github.com/oracle/fmw-kubernetes.git $ cp -rf /scratch/OIGDockerK8S/fmw-kubernetes/OracleIdentityGovernance/kubernetes/3.0.1/create-oim-domain /scratch/OIGDockerK8S/weblogic-kubernetes-operator/kubernetes/samples/scripts/ $ mv -f /scratch/OIGDockerK8S/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain /scratch/OIGDockerK8S/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain_backup $ cp -rf /scratch/OIGDockerK8S/fmw-kubernetes/OracleIdentityGovernance/kubernetes/3.0.1/ingress-per-domain /scratch/OIGDockerK8S/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain cp -rf /scratch/OIGDockerK8S/fmw-kubernetes/OracleIdentityGovernance/kubernetes/3.0.1/design-console-ingress /scratch/OIGDockerK8S/weblogic-kubernetes-operator/kubernetes/samples/charts/design-console-ingress You can now use the deployment scripts from \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/scripts/ to set up the OIG domains as further described in this document.\n  Run the following command and see if the WebLogic custom resource definition name already exists:\n$ kubectl get crd In the output you should see:\nNo resources found in default namespace. If you see the following:\nNAME AGE domains.weblogic.oracle 5d then run the following command to delete the existing crd:\n$ kubectl delete crd domains.weblogic.oracle customresourcedefinition.apiextensions.k8s.io \u0026#34;domains.weblogic.oracle\u0026#34; deleted   Install the Oracle WebLogic Kubernetes Operator   On the master node run the following command to create a namespace for the operator:\n$ kubectl create namespace \u0026lt;sample-kubernetes-operator-ns\u0026gt; For example:\n$ kubectl create namespace operator The output will look similar to the following:\nnamespace/operator created   Create a service account for the operator in the operator\u0026rsquo;s namespace by running the following command:\n$ kubectl create serviceaccount -n \u0026lt;sample-kubernetes-operator-ns\u0026gt; \u0026lt;sample-kubernetes-operator-sa\u0026gt; For example:\n$ kubectl create serviceaccount -n operator operator-serviceaccount The output will look similar to the following:\nserviceaccount/operator-serviceaccount created   If you want to setup logging and visualisation with Elasticsearch and Kibana (post domain creation) edit the \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/charts/weblogic-operator/values.yaml and set the parameter elkIntegrationEnabled to true and make sure the following parameters are set:\n# elkIntegrationEnabled specifies whether or not ELK integration is enabled. elkIntegrationEnabled: true # logStashImage specifies the docker image containing logstash. # This parameter is ignored if \u0026#39;elkIntegrationEnabled\u0026#39; is false. logStashImage: \u0026#34;logstash:6.6.0\u0026#34; # elasticSearchHost specifies the hostname of where elasticsearch is running. # This parameter is ignored if \u0026#39;elkIntegrationEnabled\u0026#39; is false. elasticSearchHost: \u0026#34;elasticsearch.default.svc.cluster.local\u0026#34; # elasticSearchPort specifies the port number of where elasticsearch is running. # This parameter is ignored if \u0026#39;elkIntegrationEnabled\u0026#39; is false. elasticSearchPort: 9200 After the domain creation see Logging and Visualization in order to complete the setup of Elasticsearch and Kibana.\n  Run the following helm command to install and start the operator:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator $ helm install kubernetes/charts/weblogic-operator \\ --namespace \u0026lt;sample-kubernetes-operator-ns\u0026gt; \\ --set image=weblogic-kubernetes-operator:3.0.1 \\ --set serviceAccount=\u0026lt;sample-kubernetes-operator-sa\u0026gt; \\ --set \u0026#34;domainNamespaces={}\u0026#34; For example:\n$ cd /scratch/OIGDockerK8S/weblogic-kubernetes-operator $ helm install weblogic-kubernetes-operator kubernetes/charts/weblogic-operator \\ --namespace operator \\ --set image=weblogic-kubernetes-operator:3.0.1 \\ --set serviceAccount=operator-serviceaccount \\ --set \u0026#34;domainNamespaces={}\u0026#34; The output will look similar to the following:\nNAME: weblogic-kubernetes-operator LAST DEPLOYED: Tue Sep 29 02:33:06 2020 NAMESPACE: operator STATUS: deployed REVISION: 1 TEST SUITE: None   Verify that the operator\u0026rsquo;s pod is running by executing the following command to list the pods in the operator\u0026rsquo;s namespace:\n$ kubectl get all -n \u0026lt;sample-kubernetes-operator-ns\u0026gt; For example:\n$ kubectl get all -n operator The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE pod/weblogic-operator-5d5dfb74ff-t7ct5 2/2 Running 0 17m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/internal-weblogic-operator-svc ClusterIP 10.101.11.127 \u0026lt;none\u0026gt; 8082/TCP 17m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/weblogic-operator 1/1 1 1 17m NAME DESIRED CURRENT READY AGE replicaset.apps/weblogic-operator-5d5dfb74ff 1 1 1 17m   Verify that the operator is up and running by viewing the operator pod\u0026rsquo;s log:\n$ kubectl logs -n \u0026lt;sample-kubernetes-operator-ns\u0026gt; -c weblogic-operator deployments/weblogic-operator For example:\n$ kubectl logs -n operator -c weblogic-operator deployments/weblogic-operator The output will look similar to the following:\n{\u0026#34;timestamp\u0026#34;:\u0026#34;09-29-2020T09:33:26.284+0000\u0026#34;,\u0026#34;thread\u0026#34;:27,\u0026#34;fiber\u0026#34;:\u0026#34;fiber-1\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;operator\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;WARNING\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.utils.Certificates\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;getCertificate\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1601372006284,\u0026#34;message\u0026#34;:\u0026#34;No external certificate configured for REST endpoint. Endpoint will be disabled.\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;} {\u0026#34;timestamp\u0026#34;:\u0026#34;09-29-2020T09:33:28.611+0000\u0026#34;,\u0026#34;thread\u0026#34;:27,\u0026#34;fiber\u0026#34;:\u0026#34;fiber-1\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;operator\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.rest.RestServer\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;start\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1601372008611,\u0026#34;message\u0026#34;:\u0026#34;Started the internal ssl REST server on https://0.0.0.0:8082/operator\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;} {\u0026#34;timestamp\u0026#34;:\u0026#34;09-29-2020T09:33:28.613+0000\u0026#34;,\u0026#34;thread\u0026#34;:27,\u0026#34;fiber\u0026#34;:\u0026#34;fiber-1\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;operator\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.Main\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;markReadyAndStartLivenessThread\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1601372008613,\u0026#34;message\u0026#34;:\u0026#34;Starting Operator Liveness Thread\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;}   RCU schema creation In this section you create the RCU schemas in the Oracle Database.\nBefore following the steps in this section, make sure that the database and listener are up and running and you can connect to the database via SQL*Plus or other client tool.\n  Run the following command to create a namespace for the domain:\n$ kubectl create namespace \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl create namespace oimcluster The output will look similar to the following:\nnamespace/oimcluster created Run the following command to create a helper pod:\n$ kubectl run helper --image \u0026lt;image_name\u0026gt; -n \u0026lt;domain_namespace\u0026gt; -- sleep infinity For example:\n$ kubectl run helper --image oracle/oig:12.2.1.4.0 -n oimcluster -- sleep infinity The output will look similar to the following:\npod/helper created   Run the following command to start a bash shell in the helper pod:\n$ kubectl exec -it helper -n \u0026lt;domain_namespace\u0026gt; -- /bin/bash For example:\n$ kubectl exec -it helper -n oimcluster -- /bin/bash This will take you into a bash shell in the running rcu pod:\n[oracle@helper oracle]$   In the helper bash shell run the following commands to set the environment:\n[oracle@helper oracle]$ export DB_HOST=\u0026lt;db_host.domain\u0026gt; [oracle@helper oracle]$ export DB_PORT=\u0026lt;db_port\u0026gt; [oracle@helper oracle]$ export DB_SERVICE=\u0026lt;service_name\u0026gt; [oracle@helper oracle]$ export RCUPREFIX=\u0026lt;rcu_schema_prefix\u0026gt; [oracle@helper oracle]$ export RCU_SCHEMA_PWD=\u0026lt;rcu_schema_pwd\u0026gt; [oracle@helper oracle]$ echo -e \u0026lt;db_pwd\u0026gt;\u0026#34;\\n\u0026#34;\u0026lt;rcu_schema_pwd\u0026gt; \u0026gt; /tmp/pwd.txt [oracle@helper oracle]$ cat /tmp/pwd.txt where:\n\u0026lt;db_host.domain\u0026gt; is the database server hostname\n\u0026lt;db_port\u0026gt; is the database listener port\n\u0026lt;service_name\u0026gt; is the database service name\n\u0026lt;rcu_schema_prefix\u0026gt; is the RCU schema prefix you want to set\n\u0026lt;rcu_schema_pwd\u0026gt; is the password you want to set for the \u0026lt;rcu_schema_prefix\u0026gt;\n\u0026lt;db_pwd\u0026gt; is the SYS password for the database\nFor example:\n[oracle@helper oracle]$ export DB_HOST=mydatabasehost.example.com [oracle@helper oracle]$ export DB_PORT=1521 [oracle@helper oracle]$ export DB_SERVICE=orcl.example.com [oracle@helper oracle]$ export RCUPREFIX=OIGK8S [oracle@helper oracle]$ export RCU_SCHEMA_PWD=\u0026lt;password\u0026gt; [oracle@helper oracle]$ echo -e \u0026lt;password\u0026gt;\u0026#34;\\n\u0026#34;\u0026lt;password\u0026gt; \u0026gt; /tmp/pwd.txt [oracle@helper oracle]$ cat /tmp/pwd.txt \u0026lt;password\u0026gt; \u0026lt;password\u0026gt;   In the helper bash shell run the following commands to create the RCU schemas in the database:\n[oracle@helper oracle]$ /u01/oracle/oracle_common/bin/rcu -silent -createRepository -databaseType ORACLE -connectString \\ $DB_HOST:$DB_PORT/$DB_SERVICE -dbUser sys -dbRole sysdba -useSamePasswordForAllSchemaUsers true \\ -selectDependentsForComponents true -schemaPrefix $RCUPREFIX -component OIM -component MDS -component SOAINFRA -component OPSS \\ -f \u0026lt; /tmp/pwd.txt The output will look similar to the following:\nRCU Logfile: /tmp/RCU2020-09-29_10-51_508080961/logs/rcu.log Processing command line .... Repository Creation Utility - Checking Prerequisites Checking Global Prerequisites Repository Creation Utility - Checking Prerequisites Checking Component Prerequisites Repository Creation Utility - Creating Tablespaces Validating and Creating Tablespaces Create tablespaces in the repository database Repository Creation Utility - Create Repository Create in progress. Percent Complete: 10 Executing pre create operations Percent Complete: 25 Percent Complete: 25 Percent Complete: 26 Percent Complete: 27 Percent Complete: 28 Percent Complete: 28 Percent Complete: 29 Percent Complete: 29 Creating Common Infrastructure Services(STB) Percent Complete: 36 Percent Complete: 36 Percent Complete: 44 Percent Complete: 44 Percent Complete: 44 Creating Audit Services Append(IAU_APPEND) Percent Complete: 51 Percent Complete: 51 Percent Complete: 59 Percent Complete: 59 Percent Complete: 59 Creating Audit Services Viewer(IAU_VIEWER) Percent Complete: 66 Percent Complete: 66 Percent Complete: 67 Percent Complete: 67 Percent Complete: 68 Percent Complete: 68 Creating Metadata Services(MDS) Percent Complete: 76 Percent Complete: 76 Percent Complete: 76 Percent Complete: 77 Percent Complete: 77 Percent Complete: 78 Percent Complete: 78 Percent Complete: 78 Creating Weblogic Services(WLS) Percent Complete: 82 Percent Complete: 82 Percent Complete: 83 Percent Complete: 84 Percent Complete: 86 Percent Complete: 88 Percent Complete: 88 Percent Complete: 88 Creating User Messaging Service(UCSUMS) Percent Complete: 92 Percent Complete: 92 Percent Complete: 95 Percent Complete: 95 Percent Complete: 100 Creating Audit Services(IAU) Creating Oracle Platform Security Services(OPSS) Creating SOA Infrastructure(SOAINFRA) Creating Oracle Identity Manager(OIM) Executing post create operations Repository Creation Utility: Create - Completion Summary Database details: ----------------------------- Host Name : mydatabasehost.example.com Port : 1521 Service Name : ORCL.EXAMPLE.COM Connected As : sys Prefix for (prefixable) Schema Owners : OIGK8S RCU Logfile : /tmp/RCU2020-09-29_10-51_508080961/logs/rcu.log Component schemas created: ----------------------------- Component Status Logfile Common Infrastructure Services Success /tmp/RCU2020-09-29_10-51_508080961/logs/stb.log Oracle Platform Security Services Success /tmp/RCU2020-09-29_10-51_508080961/logs/opss.log SOA Infrastructure Success /tmp/RCU2020-09-29_10-51_508080961/logs/soainfra.log Oracle Identity Manager Success /tmp/RCU2020-09-29_10-51_508080961/logs/oim.log User Messaging Service Success /tmp/RCU2020-09-29_10-51_508080961/logs/ucsums.log Audit Services Success /tmp/RCU2020-09-29_10-51_508080961/logs/iau.log Audit Services Append Success /tmp/RCU2020-09-29_10-51_508080961/logs/iau_append.log Audit Services Viewer Success /tmp/RCU2020-09-29_10-51_508080961/logs/iau_viewer.log Metadata Services Success /tmp/RCU2020-09-29_10-51_508080961/logs/mds.log WebLogic Services Success /tmp/RCU2020-09-29_10-51_508080961/logs/wls.log Repository Creation Utility - Create : Operation Completed [oracle@helper oracle]$   Run the following command to patch schemas in the database:\n[oracle@helper oracle]$ /u01/oracle/oracle_common/modules/thirdparty/org.apache.ant/1.10.5.0.0/apache-ant-1.10.5/bin/ant \\ -f /u01/oracle/idm/server/setup/deploy-files/automation.xml \\ run-patched-sql-files \\ -logger org.apache.tools.ant.NoBannerLogger \\ -logfile /u01/oracle/idm/server/bin/patch_oim_wls.log \\ -DoperationsDB.host=$DB_HOST \\ -DoperationsDB.port=$DB_PORT \\ -DoperationsDB.serviceName=$DB_SERVICE \\ -DoperationsDB.user=${RCUPREFIX}_OIM \\ -DOIM.DBPassword=$RCU_SCHEMA_PWD \\ -Dojdbc=/u01/oracle/oracle_common/modules/oracle.jdbc/ojdbc8.jar The output will look similar to the following:\nBuildfile: /u01/oracle/idm/server/setup/deploy-files/automation.xml   Verify the database was patched successfully by viewing the patch_oim_wls.log:\n[oracle@helper oracle]$ cat /u01/oracle/idm/server/bin/patch_oim_wls.log The output should look similar to below:\nrun-patched-sql-files: [sql] Executing resource: /u01/oracle/idm/server/db/oim/oracle/StoredProcedures/API/oim_role_mgmt_pkg_body.sql [sql] Executing resource: /u01/oracle/idm/server/db/oim/oracle/Upgrade/oim12cps4/list/oim12cps4_dml_pty_insert_sysprop_ssointg_grprecon_matching_rolename.sql [sql] Executing resource: /u01/oracle/idm/server/db/oim/oracle/Upgrade/oim12cps4/list/oim12cps4_dml_pty_insert_sysprop_oimadpswdpolicy.sql [sql] 3 of 3 SQL statements executed successfully BUILD SUCCESSFUL Total time: 1 second   Exit the helper bash shell by issuing the command exit.\n  Preparing the environment for domain creation In this section you prepare the environment for the OIG domain creation. This involves the following steps:\n Configure the operator for the domain namespace Create Kubernetes secrets for the domain and RCU Create a Kubernetes PV and PVC (Persistent Volume and Persistent Volume Claim)  Configure the operator for the domain namespace   Configure the Oracle WebLogic Kubernetes Operator to manage the domain in the domain namespace by running the following command:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator $ helm upgrade --reuse-values --namespace \u0026lt;operator_namespace\u0026gt; --set \u0026#34;domainNamespaces={oimcluster}\u0026#34; --wait weblogic-kubernetes-operator kubernetes/charts/weblogic-operator For example:\n$ cd /scratch/OIGDockerK8S/weblogic-kubernetes-operator $ helm upgrade --reuse-values --namespace operator --set \u0026#34;domainNamespaces={oimcluster}\u0026#34; --wait weblogic-kubernetes-operator kubernetes/charts/weblogic-operator The output will look similar to the following:\nRelease \u0026#34;weblogic-kubernetes-operator\u0026#34; has been upgraded. Happy Helming! NAME: weblogic-kubernetes-operator LAST DEPLOYED: Tue Sep 29 04:01:43 2020 NAMESPACE: operator STATUS: deployed REVISION: 2 TEST SUITE: None   Creating Kubernetes secrets for the domain and RCU   Create a Kubernetes secret for the domain using the create-weblogic-credentials script in the same Kubernetes namespace as the domain:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-weblogic-domain-credentials $ ./create-weblogic-credentials.sh -u weblogic -p \u0026lt;pwd\u0026gt; -n \u0026lt;domain_namespace\u0026gt; -d \u0026lt;domain_uid\u0026gt; -s \u0026lt;kubernetes_domain_secret\u0026gt; where:\n-u weblogic is the WebLogic username\n-p \u0026lt;pwd\u0026gt; is the password for the WebLogic user\n-n \u0026lt;domain_namespace\u0026gt; is the domain namespace\n-d \u0026lt;domain_uid\u0026gt; is the domain UID to be created. The default is domain1 if not specified\n-s \u0026lt;kubernetes_domain_secret\u0026gt; is the name you want to create for the secret for this namespace. The default is to use the domainUID if not specified\nFor example:\n$ cd /scratch/OIGDockerK8S/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-weblogic-domain-credentials $ ./create-weblogic-credentials.sh -u weblogic -p \u0026lt;password\u0026gt; -n oimcluster -d oimcluster -s oimcluster-domain-credentials The output will look similar to the following:\nsecret/oimcluster-domain-credentials created secret/oimcluster-domain-credentials labeled The secret oimcluster-domain-credentials has been successfully created in the oimcluster namespace.   Verify the secret is created using the following command:\n$ kubectl get secret \u0026lt;kubernetes_domain_secret\u0026gt; -o yaml -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get secret oimcluster-domain-credentials -o yaml -n oimcluster The output will look similar to the following:\n$ kubectl get secret oimcluster-domain-credentials -o yaml -n oimcluster apiVersion: v1 data: password: V2VsY29tZTE= username: d2VibG9naWM= kind: Secret metadata: creationTimestamp: \u0026#34;2020-09-29T11:04:44Z\u0026#34; labels: weblogic.domainName: oimcluster weblogic.domainUID: oimcluster managedFields: - apiVersion: v1 fieldsType: FieldsV1 fieldsV1: f:data: .: {} f:password: {} f:username: {} f:metadata: f:labels: .: {} f:weblogic.domainName: {} f:weblogic.domainUID: {} f:type: {} manager: kubectl operation: Update time: \u0026#34;2020-09-29T11:04:44Z\u0026#34; name: oimcluster-domain-credentials namespace: oimcluster resourceVersion: \u0026#34;1249007\u0026#34; selfLink: /api/v1/namespaces/oimcluster/secrets/oimcluster-domain-credentials uid: 4ade08f3-7b11-4bb0-9340-7304a2ef9b64 type: Opaque   Create a Kubernetes secret for RCU in the same Kubernetes namespace as the domain, using the create-weblogic-credentials.sh script:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-rcu-credentials $ ./create-rcu-credentials.sh -u \u0026lt;rcu_prefix\u0026gt; -p \u0026lt;rcu_schema_pwd\u0026gt; -a sys -q \u0026lt;sys_db_pwd\u0026gt; -d \u0026lt;domain_uid\u0026gt; -n \u0026lt;domain_namespace\u0026gt; -s \u0026lt;kubernetes_rcu_secret\u0026gt; where:\n-u \u0026lt;rcu_prefix\u0026gt; is the name of the RCU schema prefix created previously\n-p \u0026lt;rcu_schema_pwd\u0026gt; is the password for the RCU schema prefix\n-q \u0026lt;sys_db_pwd\u0026gt; is the sys database password\n-d \u0026lt;domain_uid\u0026gt; is the domain_uid that you created earlier\n-n \u0026lt;domain_namespace\u0026gt; is the domain namespace\n-s \u0026lt;kubernetes_rcu_secret\u0026gt; is the name of the rcu secret to create\nFor example:\n$ cd /scratch/OIGDockerK8S/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-rcu-credentials $ ./create-rcu-credentials.sh -u OIGK8S -p \u0026lt;password\u0026gt; -a sys -q \u0026lt;password\u0026gt; -d oimcluster -n oimcluster -s oimcluster-rcu-credentials The output will look similar to the following:\nsecret/oimcluster-rcu-credentials created secret/oimcluster-rcu-credentials labeled The secret oimcluster-rcu-credentials has been successfully created in the oimcluster namespace.   Verify the secret is created using the following command:\n$ kubectl get secret \u0026lt;kubernetes_rcu_secret\u0026gt; -o yaml -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get secret oimcluster-rcu-credentials -o yaml -n oimcluster The output will look similar to the following:\napiVersion: v1 data: password: V2VsY29tZTE= sys_password: V2VsY29tZTE= sys_username: c3lz username: T0lHSzhT kind: Secret metadata: creationTimestamp: \u0026#34;2020-09-29T11:18:45Z\u0026#34; labels: weblogic.domainName: oimcluster weblogic.domainUID: oimcluster managedFields: - apiVersion: v1 fieldsType: FieldsV1 fieldsV1: f:data: .: {} f:password: {} f:sys_password: {} f:sys_username: {} f:username: {} f:metadata: f:labels: .: {} f:weblogic.domainName: {} f:weblogic.domainUID: {} f:type: {} manager: kubectl operation: Update time: \u0026#34;2020-09-29T11:18:45Z\u0026#34; name: oimcluster-rcu-credentials namespace: oimcluster resourceVersion: \u0026#34;1251020\u0026#34; selfLink: /api/v1/namespaces/oimcluster/secrets/oimcluster-rcu-credentials uid: aee4213e-ffe2-45a6-9b96-11c4e88d12f2 type: Opaque   Create a Kubernetes persistent volume and persistent volume claim In the Kubernetes domain namespace created above, create the persistent volume (PV) and persistent volume claim (PVC) by running the create-pv-pvc.sh script.\n  Make a backup copy of the create-pv-pvc-inputs.yaml file and create required directories:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-weblogic-domain-pv-pvc $ cp create-pv-pvc-inputs.yaml create-pv-pvc-inputs.yaml.orig $ mkdir output_oimcluster $ mkdir -p /\u0026lt;work directory\u0026gt;/oimclusterdomainpv $ chmod -R 777 /\u0026lt;work directory\u0026gt;/oimclusterdomainpv For example:\n$ cd /scratch/OIGDockerK8S/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-weblogic-domain-pv-pvc $ cp create-pv-pvc-inputs.yaml create-pv-pvc-inputs.yaml.orig $ mkdir output_oimcluster $ mkdir -p /scratch/OIGDockerK8S/oimclusterdomainpv $ chmod -R 777 /scratch/OIGDockerK8S/oimclusterdomainpv Note: The persistent volume directory needs to be accessible to both the master and worker node(s) via NFS. Make sure this path has full access permissions, and that the folder is empty. In this example /scratch/OIGDockerK8S/oimclusterdomainpv is accessible from all nodes via NFS.\n  Edit the create-pv-pvc-inputs.yaml file and update the following parameters to reflect your settings. Save the file when complete:\nbaseName: \u0026lt;domain\u0026gt; domainUID: \u0026lt;domain_uid\u0026gt; namespace: \u0026lt;domain_namespace\u0026gt; weblogicDomainStorageType: NFS weblogicDomainStorageNFSServer: \u0026lt;nfs_server\u0026gt; weblogicDomainStoragePath: \u0026lt;physical_path_of_persistent_storage\u0026gt; For example:\n# The base name of the pv and pvc baseName: oim # Unique ID identifying a domain. # If left empty, the generated pv can be shared by multiple domains # This ID must not contain an underscope (\u0026#34;_\u0026#34;), and must be lowercase and unique across all domains in a Kubernetes cluster. domainUID: oimcluster # Name of the namespace for the persistent volume claim namespace: oimcluster # Persistent volume type for the persistent storage. # The value must be \u0026#39;HOST_PATH\u0026#39; or \u0026#39;NFS\u0026#39;. # If using \u0026#39;NFS\u0026#39;, weblogicDomainStorageNFSServer must be specified. weblogicDomainStorageType: NFS # The server name or ip address of the NFS server to use for the persistent storage. # The following line must be uncomment and customized if weblogicDomainStorateType is NFS: weblogicDomainStorageNFSServer: mynfsserver # Physical path of the persistent storage. # When weblogicDomainStorageType is set to HOST_PATH, this value should be set the to path to the # domain storage on the Kubernetes host. # When weblogicDomainStorageType is set to NFS, then weblogicDomainStorageNFSServer should be set # to the IP address or name of the DNS server, and this value should be set to the exported path # on that server. # Note that the path where the domain is mounted in the WebLogic containers is not affected by this # setting, that is determined when you create your domain. # The following line must be uncomment and customized: weblogicDomainStoragePath: /scratch/OIGDockerK8S/oimclusterdomainpv   Execute the create-pv-pvc.sh script to create the PV and PVC configuration files:\n$ ./create-pv-pvc.sh -i create-pv-pvc-inputs.yaml -o output_oimcluster The output will be similar to the following:\nInput parameters being used export version=\u0026#34;create-weblogic-sample-domain-pv-pvc-inputs-v1\u0026#34; export baseName=\u0026#34;oim\u0026#34; export domainUID=\u0026#34;oimcluster\u0026#34; export namespace=\u0026#34;oimcluster\u0026#34; export weblogicDomainStorageType=\u0026#34;NFS\u0026#34; export weblogicDomainStorageNFSServer=\u0026#34;mynfsserver\u0026#34; export weblogicDomainStoragePath=\u0026#34;/scratch/OIGDockerK8S/oimclusterdomainpv\u0026#34; export weblogicDomainStorageReclaimPolicy=\u0026#34;Retain\u0026#34; export weblogicDomainStorageSize=\u0026#34;10Gi\u0026#34; Generating output_oimcluster/pv-pvcs/oimcluster-oim-pv.yaml Generating output_oimcluster/pv-pvcs/oimcluster-oim-pvc.yaml The following files were generated: output_oimcluster/pv-pvcs/oimcluster-oim-pv.yaml output_oimcluster/pv-pvcs/oimcluster-oim-pvc.yaml Completed   Run the following to show the files are created:\n$ ls output_oimcluster/pv-pvcs create-pv-pvc-inputs.yaml oimcluster-oim-pvc.yaml oimcluster-oim-pv.yaml   Run the following kubectl command to create the PV and PVC in the domain namespace:\n$ kubectl create -f output_oimcluster/pv-pvcs/oimcluster-oim-pv.yaml -n \u0026lt;domain_namespace\u0026gt; $ kubectl create -f output_oimcluster/pv-pvcs/oimcluster-oim-pvc.yaml -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl create -f output_oimcluster/pv-pvcs/oimcluster-oim-pv.yaml -n oimcluster $ kubectl create -f output_oimcluster/pv-pvcs/oimcluster-oim-pvc.yaml -n oimcluster The output will look similar to the following:\npersistentvolume/oimcluster-oim-pv created persistentvolumeclaim/oimcluster-oim-pvc created   Run the following commands to verify the PV and PVC were created successfully:\n$ kubectl describe pv \u0026lt;pv_name\u0026gt; $ kubectl describe pvc \u0026lt;pvc_name\u0026gt; -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl describe pv oimcluster-oim-pv $ kubectl describe pvc oimcluster-oim-pvc -n oimcluster The output will look similar to the following:\n$ kubectl describe pv oimcluster-oim-pv Name: oimcluster-oim-pv Labels: weblogic.domainUID=oimcluster Annotations: pv.kubernetes.io/bound-by-controller: yes Finalizers: [kubernetes.io/pv-protection] StorageClass: oimcluster-oim-storage-class Status: Bound Claim: oimcluster/oimcluster-oim-pvc Reclaim Policy: Retain Access Modes: RWX VolumeMode: Filesystem Capacity: 10Gi Node Affinity: \u0026lt;none\u0026gt; Message: Source: Type: NFS (an NFS mount that lasts the lifetime of a pod) Server: mynfsserver Path: /scratch/OIGDockerK8S/oimclusterdomainpv ReadOnly: false Events: \u0026lt;none\u0026gt; $ kubectl describe pvc oimcluster-oim-pvc -n oimcluster Name: oimcluster-oim-pvc Namespace: oimcluster StorageClass: oimcluster-oim-storage-class Status: Bound Volume: oimcluster-oim-pv Labels: weblogic.domainUID=oimcluster Annotations: pv.kubernetes.io/bind-completed: yes pv.kubernetes.io/bound-by-controller: yes Finalizers: [kubernetes.io/pvc-protection] Capacity: 10Gi Access Modes: RWX VolumeMode: Filesystem Mounted By: \u0026lt;none\u0026gt; Events: \u0026lt;none\u0026gt; You are now ready to create the OIG domain as per Create OIG Domains\n  "
},
{
	"uri": "/fmw-kubernetes/wcsites-domains/prepare-your-environment/",
	"title": "Prepare your environment",
	"tags": [],
	"description": "",
	"content": "Contents This document describes the steps to set up the environment that includes setting up of a Kubernetes cluster and setting up the Weblogic Operator including the database.\n Introduction Set Up your Kubernetes Cluster Build Oracle WebCenter Sites Image Pull Other Dependent Images Set Up the Code Repository to Deploy Oracle WebCenter Sites Domain Grant Roles and Clear Stale Resources Install the WebLogic Kubernetes Operator Configure NFS Server Prepare the Environment for the WebCenter Sites Domain Set Up the Database  Introduction Set Up your Kubernetes Cluster If you need help in setting up a Kubernetes environment, check our cheat sheet.\nAfter creating Kubernetes clusters, you can optionally:\n Create load balancers to direct traffic to backend domains. Configure Kibana and Elasticsearch for your operator logs.  Build Oracle WebCenter Sites Image Build Oracle WebCenter Sites 12.2.1.4.0 Image by following steps 4A, 4C, 4D and 5 from this document.\nPull Other Dependent Images Dependent images include WebLogic Kubernetes Operator, Database, and Traefik. Pull these images and add them to your local registry:\n Pull these docker images and re-tag them as shown:  To pull an image from the Oracle Container Registry, in a web browser, navigate to https://container-registry.oracle.com and log in using the Oracle Single Sign-On authentication service. If you do not already have SSO credentials, at the top of the page, click the Sign In link to create them.\nUse the web interface to accept the Oracle Standard Terms and Restrictions for the Oracle software images that you intend to deploy. Your acceptance of these terms are stored in a database that links the software images to your Oracle Single Sign-On login credentials.\nThen, pull these docker images and re-tag them:\ndocker login https://container-registry.oracle.com (enter your Oracle email Id and password) This step is required once at every node to get access to the Oracle Container Registry. WebLogic Kubernetes Operator image:\n$ docker pull oracle/weblogic-kubernetes-operator:2.4.0 Database image:\n$ docker pull container-registry.oracle.com/database/enterprise:12.2.0.1-slim $ docker tag container-registry.oracle.com/database/enterprise:12.2.0.1-slim oracle/database:12.2.0.1 Copy all the above built and pulled images to all the nodes in your cluster or add to a Docker registry that your cluster can access.  NOTE: If you\u0026rsquo;re not running Kubernetes on your development machine, you\u0026rsquo;ll need to make the Docker image available to a registry visible to your Kubernetes cluster. Upload your image to a machine running Docker and Kubernetes as follows:\n# on your build machine $ docker save Image_Name:Tag \u0026gt; Image_Name-Tag.tar $ scp Image_Name-Tag.tar YOUR_USER@YOUR_SERVER:/some/path/Image_Name-Tag.tar # on the Kubernetes server $ docker load \u0026lt; /some/path/Image_Name-Tag.tar Set Up the Code Repository to Deploy Oracle WebCenter Sites Domain Oracle WebCenter Sites domain deployment on Kubernetes leverages the Oracle WebLogic Kubernetes Operator infrastructure. For deploying the Oracle WebCenter Sites domain, you need to set up the deployment scripts as below:\n  Create a working directory to setup the source code.\n$ mkdir \u0026lt;work directory\u0026gt; $ cd \u0026lt;work directory\u0026gt;   Download the supported version of Oracle WebLogic Kubernetes Operator source code archieve file (.zip/.tar.gz) from the operator relases page. Currently the supported operator version can be downloaded from 2.4.0.\n  Extract the source code archive file (.zip/.tar.gz) in to the work directory.\n  Download the WebCenter Sites kubernetes deployment scripts from this repository and copy them in to WebLogic operator samples location.\n$ git clone https://github.com/oracle/fmw-kubernetes.git $ cp -rf \u0026lt;work directory\u0026gt;/fmw-kubernetes/OracleWebCenterSites/kubernetes/2.4.0/create-wcsites-domain \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator-2.4.0/kubernetes/samples/scripts/   You can now use the deployment scripts from \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator-2.4.0 to set up the WebCenter Sites domain as further described in this document.\nThis will be your home directory for runnning all the required scripts.\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator-2.4.0 Grant Roles and Clear Stale Resources   Grant the Helm service account the cluster-admin role:\n$ cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: helm-user-cluster-admin-role roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: default namespace: kube-system EOF output: clusterrolebinding.rbac.authorization.k8s.io/helm-user-cluster-admin-role configured   To confirm if there is already a WebLogic custom resource definition, execute the following command:\n$ kubectl get crd NAME CREATED AT domains.weblogic.oracle 2020-03-14T12:10:21Z   If you find any WebLogic custom resource definition, then delete it by executing the following command:\n$ kubectl delete crd domains.weblogic.oracle customresourcedefinition.apiextensions.k8s.io \u0026#34;domains.weblogic.oracle\u0026#34; deleted   Install the WebLogic Kubernetes Operator   Create a namespace for the WebLogic Kubernetes Operator:\n$ kubectl create namespace operator-ns namespace/operator-ns created NOTE: For this exercise we are creating a namespace called \u0026ldquo;operator-ns\u0026rdquo; (can be any name).\nYou can also use:\n domainUID/domainname as wcsitesinfra Domain namespace as wcsites-ns Operator namespace as operator-ns traefik namespace as traefik    Create a service account for the WebLogic Kubernetes Operator in the Operator\u0026rsquo;s namespace:\n$ kubectl create serviceaccount -n operator-ns operator-sa serviceaccount/operator-sa created   To be able to set up the log-stash and Elasticsearch after creating the domain, set the value of the field elkIntegrationEnabled to true in the file kubernetes/charts/weblogic-operator/values.yaml.\n  Use helm to install and start the WebLogic Kubernetes Operator from the downloaded repository:\n$ helm install kubernetes/charts/weblogic-operator --name weblogic-kubernetes-operator \\ --namespace operator-ns --set serviceAccount=operator-sa --set \u0026#34;domainNamespaces={}\u0026#34; --wait OUTPUT:\nNAME: weblogic-kubernetes-operator LAST DEPLOYED: Sat Mar 14 12:19:45 2020 NAMESPACE: operator-ns STATUS: DEPLOYED RESOURCES: ==\u0026gt; v1/ClusterRoleBinding NAME AGE operator-ns-weblogic-operator-clusterrolebinding-nonresource 56s operator-ns-weblogic-operator-clusterrolebinding-discovery 56s operator-ns-weblogic-operator-clusterrolebinding-auth-delegator 56s operator-ns-weblogic-operator-clusterrolebinding-general 56s ==\u0026gt; v1/RoleBinding NAME AGE weblogic-operator-rolebinding-namespace 56s weblogic-operator-rolebinding 56s ==\u0026gt; v1/Service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE internal-weblogic-operator-svc ClusterIP 10.105.252.222 \u0026lt;none\u0026gt; 8082/TCP 56s ==\u0026gt; v1beta1/Deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE weblogic-operator 1 1 1 1 56s ==\u0026gt; v1/Secret NAME TYPE DATA AGE weblogic-operator-secrets Opaque 1 56s ==\u0026gt; v1/ConfigMap NAME DATA AGE weblogic-operator-cm 3 56s ==\u0026gt; v1/ClusterRole NAME AGE operator-ns-weblogic-operator-clusterrole-namespace 56s operator-ns-weblogic-operator-clusterrole-general 56s operator-ns-weblogic-operator-clusterrole-nonresource 56s operator-ns-weblogic-operator-clusterrole-operator-admin 56s operator-ns-weblogic-operator-clusterrole-domain-admin 56s ==\u0026gt; v1/Role NAME AGE weblogic-operator-role 56s ==\u0026gt; v1/Pod(related) NAME READY STATUS RESTARTS AGE weblogic-operator-67df5fddc5-tlc4b 2/2 Running 0 56s ``\n  To verify that the Operator\u0026rsquo;s pod is running, list the pods in the Operator\u0026rsquo;s namespace. You should see one for the WebLogic Kubernetes Operator:\n$ kubectl get pods -n operator-ns NAME READY STATUS RESTARTS AGE weblogic-operator-67df5fddc5-tlc4b 2/2 Running 0 3m15s   Then, check by viewing the Operator pod\u0026rsquo;s log as shown in the following sample log snippet:\n$ kubectl logs -n operator-ns -c weblogic-operator deployments/weblogic-operator Launching Oracle WebLogic Server Kubernetes Operator... Importing keystore /operator/internal-identity/temp/weblogic-operator.jks to /operator/internal-identity/temp/weblogic-operator.p12... Entry for alias weblogic-operator-alias successfully imported. Import command completed: 1 entries successfully imported, 0 entries failed or cancelled Warning: The -srcstorepass option is specified multiple times. All except the last one will be ignored. MAC verified OK % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 4249 0 2394 100 1855 6884 5334 --:--:-- --:--:-- --:--:-- 6899 % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 5558 0 3028 100 2530 22704 18970 --:--:-- --:--:-- --:--:-- 22766 OpenJDK 64-Bit Server VM warning: Option MaxRAMFraction was deprecated in version 10.0 and will likely be removed in a future release. VM settings: Max. Heap Size (Estimated): 14.08G Using VM: OpenJDK 64-Bit Server VM {\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:53.438+0000\u0026#34;,\u0026#34;thread\u0026#34;:1,\u0026#34;fiber\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.TuningParametersImpl\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;update\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168593438,\u0026#34;message\u0026#34;:\u0026#34;Reloading tuning parameters from Operator\u0026#39;s config map\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;} {\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:53.944+0000\u0026#34;,\u0026#34;thread\u0026#34;:1,\u0026#34;fiber\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.Main\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;main\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168593944,\u0026#34;message\u0026#34;:\u0026#34;Oracle WebLogic Server Kubernetes Operator, version: 2.4.0, implementation: master.4d4fe0a, build time: 2019-11-15T21:19:56-0500\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;} {\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:53.972+0000\u0026#34;,\u0026#34;thread\u0026#34;:11,\u0026#34;fiber\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.Main\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;begin\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168593972,\u0026#34;message\u0026#34;:\u0026#34;Operator namespace is: operator-ns\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;} {\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:54.009+0000\u0026#34;,\u0026#34;thread\u0026#34;:11,\u0026#34;fiber\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.Main\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;begin\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168594009,\u0026#34;message\u0026#34;:\u0026#34;Operator target namespaces are: operator-ns\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;} {\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:54.013+0000\u0026#34;,\u0026#34;thread\u0026#34;:11,\u0026#34;fiber\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.Main\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;begin\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168594013,\u0026#34;message\u0026#34;:\u0026#34;Operator service account is: operator-sa\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;}\t{\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:54.031+0000\u0026#34;,\u0026#34;thread\u0026#34;:11,\u0026#34;fiber\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.helpers.HealthCheckHelper\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;performK8sVersionCheck\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168594031,\u0026#34;message\u0026#34;:\u0026#34;Verifying Kubernetes minimum version\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;}\t{\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:54.286+0000\u0026#34;,\u0026#34;thread\u0026#34;:11,\u0026#34;fiber\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.helpers.ClientPool\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;getApiClient\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168594286,\u0026#34;message\u0026#34;:\u0026#34;The Kuberenetes Master URL is set to https://10.96.0.1:443\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;}\t{\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:54.673+0000\u0026#34;,\u0026#34;thread\u0026#34;:11,\u0026#34;fiber\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.helpers.HealthCheckHelper\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;createAndValidateKubernetesVersion\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168594673,\u0026#34;message\u0026#34;:\u0026#34;Kubernetes version is: v1.13.7\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;}\t{\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:55.259+0000\u0026#34;,\u0026#34;thread\u0026#34;:12,\u0026#34;fiber\u0026#34;:\u0026#34;engine-operator-thread-2-fiber-1\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.helpers.CrdHelper$CrdContext$CreateResponseStep\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;onSuccess\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168595259,\u0026#34;message\u0026#34;:\u0026#34;Create Custom Resource Definition: oracle.kubernetes.operator.calls.CallResponse@470b40c\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;}\t{\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:55.356+0000\u0026#34;,\u0026#34;thread\u0026#34;:16,\u0026#34;fiber\u0026#34;:\u0026#34;fiber-1-child-2\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.helpers.HealthCheckHelper\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;performSecurityChecks\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168595356,\u0026#34;message\u0026#34;:\u0026#34;Verifying that operator service account can access required operations on required resources in namespace operator-ns\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;}\t{\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:55.598+0000\u0026#34;,\u0026#34;thread\u0026#34;:18,\u0026#34;fiber\u0026#34;:\u0026#34;fiber-1-child-2\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.helpers.ConfigMapHelper$ScriptConfigMapContext$CreateResponseStep\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;onSuccess\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168595598,\u0026#34;message\u0026#34;:\u0026#34;Creating domain config map, operator-ns, for namespace: {1}.\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;}\t{\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:55.937+0000\u0026#34;,\u0026#34;thread\u0026#34;:21,\u0026#34;fiber\u0026#34;:\u0026#34;fiber-1\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;WARNING\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.utils.Certificates\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;getCertificate\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168595937,\u0026#34;message\u0026#34;:\u0026#34;Can\u0026#39;t read certificate at /operator/external-identity/externalOperatorCert\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\\njava.nio.file.NoSuchFileException: /operator/external-identity/externalOperatorCert\\n\\tat java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)\\n\\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)\\n\\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)\\n\\tat java.base/sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:215)\\n\\tat java.base/java.nio.file.Files.newByteChannel(Files.java:370)\\n\\tat java.base/java.nio.file.Files.newByteChannel(Files.java:421)\\n\\tat java.base/java.nio.file.Files.readAllBytes(Files.java:3205)\\n\\tat oracle.kubernetes.operator.utils.Certificates.getCertificate(Certificates.java:48)\\n\\tat oracle.kubernetes.operator.utils.Certificates.getOperatorExternalCertificateData(Certificates.java:39)\\n\\tat oracle.kubernetes.operator.rest.RestConfigImpl.getOperatorExternalCertificateData(RestConfigImpl.java:52)\\n\\tat oracle.kubernetes.operator.rest.RestServer.isExternalSslConfigured(RestServer.java:383)\\n\\tat oracle.kubernetes.operator.rest.RestServer.start(RestServer.java:199)\\n\\tat oracle.kubernetes.operator.Main.startRestServer(Main.java:353)\\n\\tat oracle.kubernetes.operator.Main.completeBegin(Main.java:198)\\n\\tat oracle.kubernetes.operator.Main$NullCompletionCallback.onCompletion(Main.java:701)\\n\\tat oracle.kubernetes.operator.work.Fiber.completionCheck(Fiber.java:475)\\n\\tat oracle.kubernetes.operator.work.Fiber.run(Fiber.java:448)\\n\\tat oracle.kubernetes.operator.work.ThreadLocalContainerResolver.lambda$wrapExecutor$0(ThreadLocalContainerResolver.java:87)\\n\\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\\n\\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\\n\\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\\n\\tat java.base/java.lang.Thread.run(Thread.java:834)\\n\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;} {\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:55.967+0000\u0026#34;,\u0026#34;thread\u0026#34;:21,\u0026#34;fiber\u0026#34;:\u0026#34;fiber-1\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.rest.RestServer\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;start\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168595967,\u0026#34;message\u0026#34;:\u0026#34;Did not start the external ssl REST server because external ssl has not been configured.\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;} {\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:57.910+0000\u0026#34;,\u0026#34;thread\u0026#34;:21,\u0026#34;fiber\u0026#34;:\u0026#34;fiber-1\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.rest.RestServer\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;start\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168597910,\u0026#34;message\u0026#34;:\u0026#34;Started the internal ssl REST server on https://0.0.0.0:8082/operator\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;}\t{\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:57.913+0000\u0026#34;,\u0026#34;thread\u0026#34;:21,\u0026#34;fiber\u0026#34;:\u0026#34;fiber-1\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.Main\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;markReadyAndStartLivenessThread\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168597913,\u0026#34;message\u0026#34;:\u0026#34;Starting Operator Liveness Thread\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;}   Configure NFS (Network File System) Server To configure NFS server, install the nfs-utils package preferably on Master node:\n$ sudo yum install nfs-utils To start the nfs-server service, and configure the service to start following a system reboot:\n$ sudo systemctl start nfs-server $ sudo systemctl enable nfs-server Create the directory you want to export as the NFS share, for example /scratch/K8SVolume:\n$ sudo mkdir -p /scratch/K8SVolume $ sudo chown -R 1000:1000 /scratch/K8SVolume host name or IP address of the NFS Server\nNote: Host name or IP address of the NFS Server and NFS Share path which is used when you create PV/PVC in further sections.\nPrepare the Environment for the WebCenter Sites Domain   Unless you would like to use the default namespace, create a Kubernetes namespace that can host one or more domains:\n$ kubectl create namespace wcsites-ns namespace/wcsites-ns created   To manage domains in this namespace, configure the Operator using helm:\n$ helm upgrade --reuse-values --set \u0026#34;domainNamespaces={wcsites-ns}\u0026#34; \\  --wait weblogic-kubernetes-operator kubernetes/charts/weblogic-operator Release \u0026#34;weblogic-kubernetes-operator\u0026#34; has been upgraded. Happy Helming! LAST DEPLOYED: Sat Mar 14 12:25:36 2020 NAMESPACE: operator-ns STATUS: DEPLOYED RESOURCES: ==\u0026gt; v1/Pod(related) NAME READY STATUS RESTARTS AGE weblogic-operator-67df5fddc5-tlc4b 2/2 Running 0 5m53s ==\u0026gt; v1/ClusterRole NAME AGE operator-ns-weblogic-operator-clusterrole-domain-admin 5m53s operator-ns-weblogic-operator-clusterrole-operator-admin 5m53s operator-ns-weblogic-operator-clusterrole-nonresource 5m53s operator-ns-weblogic-operator-clusterrole-general 5m53s operator-ns-weblogic-operator-clusterrole-namespace 5m53s ==\u0026gt; v1/ClusterRoleBinding NAME AGE operator-ns-weblogic-operator-clusterrolebinding-general 5m53s operator-ns-weblogic-operator-clusterrolebinding-discovery 5m53s operator-ns-weblogic-operator-clusterrolebinding-nonresource 5m53s operator-ns-weblogic-operator-clusterrolebinding-auth-delegator 5m53s ==\u0026gt; v1/RoleBinding NAME AGE weblogic-operator-rolebinding-namespace 3s weblogic-operator-rolebinding 5m53s ==\u0026gt; v1/Service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE internal-weblogic-operator-svc ClusterIP 10.105.252.222 \u0026lt;none\u0026gt; 8082/TCP 5m53s ==\u0026gt; v1beta1/Deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE weblogic-operator 1 1 1 1 5m53s ==\u0026gt; v1/Secret NAME TYPE DATA AGE weblogic-operator-secrets Opaque 1 5m53s ==\u0026gt; v1/ConfigMap NAME DATA AGE weblogic-operator-cm 3 5m53s ==\u0026gt; v1/Role NAME AGE weblogic-operator-role 5m53s   Create Kubernetes secrets:\na. Using the create-weblogic-credentials script, create a Kubernetes secret that contains the user name and password for the domain in the same Kubernetes namespace as the domain:\nOutput:\n$ sh kubernetes/samples/scripts/create-weblogic-domain-credentials/create-weblogic-credentials.sh \\  -u weblogic -p Welcome1 -n wcsites-ns \\  -d wcsitesinfra -s wcsitesinfra-domain-credentials secret/wcsitesinfra-domain-credentials created secret/wcsitesinfra-domain-credentials labeled The secret wcsitesinfra-domain-credentials has been successfully created in the wcsites-ns namespace. Where:\n* weblogic is the weblogic username * Welcome1 is the weblogic password * wcsitesinfra is the domain name * wcsites-ns is the domain namespace * wcsitesinfra-domain-credentials is the secret name  Note: You can inspect the credentials as follows:\n$ kubectl get secret wcsitesinfra-domain-credentials -o yaml -n wcsites-ns b. Create a Kubernetes secret for the Repository Configuration Utility (user name and password) using the create-rcu-credentials.sh script in the same Kubernetes namespace as the domain:\nOutput:\n$ sh kubernetes/samples/scripts/create-rcu-credentials/create-rcu-credentials.sh \\  -u WCS1 -p Welcome1 -a sys -q Oradoc_db1 -n wcsites-ns \\  -d wcsitesinfra -s wcsitesinfra-rcu-credentials secret/wcsitesinfra-rcu-credentials created secret/wcsitesinfra-rcu-credentials labeled The secret wcsitesinfra-rcu-credentials has been successfully created in the wcsites-ns namespace. Where:\n* WCS1 is the schema user * Welcome1 is the schema password * Oradoc_db1 is the database SYS users password * wcsitesinfra is the domain name * wcsites-ns is the domain namespace * wcsitesinfra-rcu-credentials is the secret name  Note: You can inspect the credentials as follows:\n$ kubectl get secret wcsitesinfra-rcu-credentials -o yaml -n wcsites-ns   Create a Kubernetes PV and PVC (Persistent Volume and Persistent Volume Claim):\na. Update the kubernetes/samples/scripts/create-wcsites-domain/utils/create-wcsites-pv-pvc-inputs.yaml.\nReplace the token %NFS_SERVER% with the host name/IP of NFS Server created in Configure NFS Server section.\nIn the NFS Server, create a folder and grant permissions as given below:\n$ sudo rm -rf /scratch/K8SVolume/WCSites \u0026amp;\u0026amp; sudo mkdir -p /scratch/K8SVolume/WCSites \u0026amp;\u0026amp; sudo chown 1000:1000 /scratch/K8SVolume/WCSites Update the weblogicDomainStoragePath paramter with /scratch/K8SVolume/WCSites.\nb. Execute the create-pv-pvc.sh script to create the PV and PVC configuration files:\n$ sh kubernetes/samples/scripts/create-weblogic-domain-pv-pvc/create-pv-pvc.sh \\  -i kubernetes/samples/scripts/create-wcsites-domain/utils/create-wcsites-pv-pvc-inputs.yaml \\  -o kubernetes/samples/scripts/create-wcsites-domain/output Input parameters being used export version=\u0026#34;create-weblogic-sample-domain-pv-pvc-inputs-v1\u0026#34; export baseName=\u0026#34;domain\u0026#34; export domainUID=\u0026#34;wcsitesinfra\u0026#34; export namespace=\u0026#34;wcsites-ns\u0026#34; export weblogicDomainStorageType=\u0026#34;HOST_PATH\u0026#34; export weblogicDomainStoragePath=\u0026#34;/scratch/K8SVolume/WCSites\u0026#34; export weblogicDomainStorageReclaimPolicy=\u0026#34;Retain\u0026#34; export weblogicDomainStorageSize=\u0026#34;10Gi\u0026#34; Generating kubernetes/samples/scripts/create-wcsites-domain/output/pv-pvcs/wcsitesinfra-domain-pv.yaml Generating kubernetes/samples/scripts/create-wcsites-domain/output/pv-pvcs/wcsitesinfra-domain-pvc.yaml The following files were generated: kubernetes/samples/scripts/create-wcsites-domain/output/pv-pvcs/wcsitesinfra-domain-pv.yaml kubernetes/samples/scripts/create-wcsites-domain/output/pv-pvcs/wcsitesinfra-domain-pvc.yaml Completed c. To create the PV and PVC, use kubectl create with output configuration files:\nOutput:\n$ kubectl apply -f kubernetes/samples/scripts/create-wcsites-domain/output/pv-pvcs/wcsitesinfra-domain-pv.yaml \\  -f kubernetes/samples/scripts/create-wcsites-domain/output/pv-pvcs/wcsitesinfra-domain-pvc.yaml persistentvolume/wcsitesinfra-domain-pv created persistentvolumeclaim/wcsitesinfra-domain-pvc created Note: You can verify the PV and PV\u0026rsquo;s details as follows:\n$ kubectl describe pv wcsitesinfra-domain-pv -n wcsites-ns $ kubectl describe pvc wcsitesinfra-domain-pvc -n wcsites-ns   Label the nodes in the Kubernetes cluster for the targeted scheduling of the servers on particular nodes as needed:\nkubectl label node \u0026lt;node-name\u0026gt; name=abc Note: Here \u0026lt;node-name\u0026gt; is the node as displayed in the NAME field of kubectl get nodes command. abc is the label that we are defining. Label is a key, value pair and can be anything meaningful. The same should be used for nodeSelector.\nFor scheduling we can select these nodes based on the labels.\n  Set Up the Database You must set up the database before you create your domain. For testing and development, you may choose to run your database inside Kubernetes or outside of Kubernetes.\nThe Oracle Database Docker images are supported for non-production use only. For more details, see My Oracle Support note: Oracle Support for Database Running on Docker (Doc ID 2216342.1).   Database Creation with PV: (Recommended)\nFor testing and development of heavy usage, you may choose to run your database inside Kubernetes or outside of Kubernetes.\nReplace the token %NFS_SERVER% with the host name/IP of NFS Server created in Configure NFS Server section.\nIn the NFS Server, create a folder and grant permissions as given below:\n$ sudo rm -rf /scratch/K8SVolume/WCSitesDB \u0026amp;\u0026amp; sudo mkdir -p /scratch/K8SVolume/WCSitesDB \u0026amp;\u0026amp; sudo chown 54321 /scratch/K8SVolume/WCSitesDB Update the above Persistent Volume created value for the path parameter in kubernetes/samples/scripts/create-wcsites-domain/create-database/db-with-pv.yaml\nCreate a Kubernetes namespace for database.\n$ kubectl create namespace wcsitesdb-ns namespace/wcsitesdb-ns created -bash-4.2$ kubectl apply -f kubernetes/samples/scripts/create-wcsites-domain/create-database/db-with-pv.yaml persistentvolume/oracle-db-pv created persistentvolumeclaim/oracle-db-pvc created service/oracle-db created deployment.extensions/oracle-db created To get the pod details for inspecting the logs (if required).\n-bash-4.2$ kubectl get all -n wcsitesdb-ns NAME READY STATUS RESTARTS AGE pod/oracle-db-7bcd584846-6x5lq 0/1 Running 0 3s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/oracle-db LoadBalancer 10.97.14.205 \u0026lt;pending\u0026gt; 1521:30011/TCP 44s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/oracle-db 0/1 1 0 3s NAME DESIRED CURRENT READY AGE replicaset.apps/oracle-db-7bcd584846 1 1 0 3s For checking database logs:\n-bash-4.2$ kubectl logs -f -n wcsitesdb-ns oracle-db-7bcd584846-6x5lq Check for value Done ! The database is ready for use . to confirm if database is started successfully.\nNow, for creating a Fusion Middleware domain, you can use the database connection string, oracle-db.wcsitesdb-ns.svc.cluster.local:1521/devpdb.k8s, as an rcuDatabaseURL parameter in the domain.input.yaml file.\n  Database Creation without PV:\nFor quick testing and development of normal usage, you may choose to run your database inside Kubernetes or outside of Kubernetes.\nCreate a Kubernetes namespace for database:\n$ kubectl create namespace wcsitesdb-ns namespace/wcsitesdb-ns created Check the help command to see the input parameters:\n$ sh kubernetes/samples/scripts/create-oracle-db-service/start-db-service.sh -h usage: kubernetes/samples/scripts/create-oracle-db-service/start-db-service.sh -p \u0026lt;nodeport\u0026gt; -i \u0026lt;image\u0026gt; -s \u0026lt;pullsecret\u0026gt; -n \u0026lt;namespace\u0026gt; [-h] -i Oracle DB Image (optional) (default: container-registry.oracle.com/database/enterprise:12.2.0.1-slim ) -p DB Service NodePort (optional) (default: 30011) -s DB Image PullSecret (optional) (default: docker-store) -n Configurable Kubernetes NameSpace for Oracle DB Service (optional) (default: default) -h Help To create and start the database, run the below command and then monitor the status till the database is ready for use:\n-bash-4.2$ sh kubernetes/samples/scripts/create-oracle-db-service/start-db-service.sh -n wcsitesdb-ns Checking Status for NameSpace [wcsitesdb-ns] Error from server (NotFound): namespaces \u0026#34;wcsitesdb-ns\u0026#34; not found Adding NameSpace[wcsitesdb-ns] to Kubernetes Cluster namespace/wcsitesdb-ns created NodePort[30011] ImagePullSecret[docker-store] Image[container-registry.oracle.com/database/enterprise:12.2.0.1-slim] NameSpace[wcsitesdb-ns] service/oracle-db created deployment.extensions/oracle-db created service/oracle-db unchanged deployment.extensions/oracle-db unchanged [oracle-db-99df9b6c9-rpz5l] already initialized .. Checking Pod READY column for State [1/1] Pod [oracle-db-99df9b6c9-rpz5l] Status is Ready Iter [1/60] NAME READY STATUS RESTARTS AGE oracle-db-99df9b6c9-rpz5l 1/1 Running 0 6s NAME READY STATUS RESTARTS AGE oracle-db-99df9b6c9-rpz5l 1/1 Running 0 6s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE oracle-db LoadBalancer 10.109.254.7 \u0026lt;pending\u0026gt; 1521:30011/TCP 6s [1/20] Retrying for Oracle Database Availability... [2/20] Retrying for Oracle Database Availability... ... [9/20] Retrying for Oracle Database Availability... [10/20] Retrying for Oracle Database Availability... Done ! The database is ready for use . Oracle DB Service is RUNNING with NodePort [30011] Oracle DB Service URL [oracle-db.wcsitesdb-ns.svc.cluster.local:1521/devpdb.k8s] To get the pod details for inspecting the logs if required:\n-bash-4.2$ kubectl get all -n wcsitesdb-ns NAME READY STATUS RESTARTS AGE pod/oracle-db-99df9b6c9-rpz5l 1/1 Running 0 3m6s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/oracle-db LoadBalancer 10.109.254.7 \u0026lt;pending\u0026gt; 1521:30011/TCP 3m6s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/oracle-db 1/1 1 1 3m6s NAME DESIRED CURRENT READY AGE replicaset.apps/oracle-db-99df9b6c9 1 1 1 3m6s Now, for creating a Fusion Middleware domain, you can use the database connection string, oracle-db.wcsitesdb-ns.svc.cluster.local:1521/devpdb.k8s, as an rcuDatabaseURL parameter in the domain.input.yaml file.\n  "
},
{
	"uri": "/fmw-kubernetes/oam/manage-oam-domains/wlst-admin-operations/",
	"title": "WLST Administration Operations",
	"tags": [],
	"description": "Describes the steps for WLST administration using helper pod running in the same Kubernetes Cluster as OAM Domain.",
	"content": "To use WLST to administer the OAM domain, use the helper pod in the same Kubernetes cluster as the OAM Domain.\n  Run the following command to start a bash shell in the helper pod (if one is not already running):\n$ kubectl exec -it helper -n \u0026lt;domain_namespace\u0026gt; -- /bin/bash For example:\n$ kubectl exec -it helper -n accessns -- /bin/bash This will take you into a bash shell in the running helper pod:\n[oracle@helper ~]$   Connect to WLST using the following command:\ncd $ORACLE_HOME/oracle_common/common/bin ./wlst.sh The output will look similar to the following:\nInitializing WebLogic Scripting Tool (WLST) ... Jython scans all the jar files it can find at first startup. Depending on the system, this process may take a few minutes to complete, and WLST may not return a prompt right away. Welcome to WebLogic Server Administration Scripting Shell Type help() for help on available commands wls:/offline\u0026gt;   To access t3 for the Administration Server connect as follows:\nconnect(\u0026#39;weblogic\u0026#39;,\u0026#39;\u0026lt;password\u0026gt;\u0026#39;,\u0026#39;t3://accessinfra-adminserver:7001\u0026#39;) The output will look similar to the following:\nConnecting to t3://accessinfra-adminserver:7001 with userid weblogic ... Successfully connected to Admin Server \u0026#34;AdminServer\u0026#34; that belongs to domain \u0026#34;accessinfra\u0026#34;. Warning: An insecure protocol was used to connect to the server. To ensure on-the-wire security, the SSL port or Admin port should be used instead. wls:/accessinfra/serverConfig/\u0026gt; Or to access t3 for the OAM Cluster service, connect as follows:\nconnect(\u0026#39;weblogic\u0026#39;,\u0026#39;\u0026lt;password\u0026gt;\u0026#39;,\u0026#39;t3://accessinfra-cluster-oam-cluster:14100\u0026#39;) The output will look similar to the following:\nConnecting to t3://accessinfra-cluster-oam-cluster:14100 with userid weblogic ... Successfully connected to managed Server \u0026#34;oam_server1\u0026#34; that belongs to domain \u0026#34;accessinfra\u0026#34;. Warning: An insecure protocol was used to connect to the server. To ensure on-the-wire security, the SSL port or Admin port should be used instead. wls:/accessinfra/serverConfig/\u0026gt;   Sample operations For a full list of WLST operations refer to WebLogic Server WLST Online and Offline Command Reference.\nDisplay servers wls:/accessinfra/serverConfig/\u0026gt; cd(\u0026#39;/Servers\u0026#39;) wls:/accessinfra/serverConfig/Servers\u0026gt; ls() dr-- AdminServer dr-- oam_policy_mgr1 dr-- oam_policy_mgr2 dr-- oam_policy_mgr3 dr-- oam_policy_mgr4 dr-- oam_policy_mgr5 dr-- oam_server1 dr-- oam_server2 dr-- oam_server3 dr-- oam_server4 dr-- oam_server5 wls:/accessinfra/serverConfig/Servers\u0026gt; Configure logging for managed servers Connect to the Administration Server and run the following:\nwls:/accessinfra/serverConfig/\u0026gt; domainRuntime() Location changed to domainRuntime tree. This is a read-only tree with DomainMBean as the root MBean. For more help, use help(\u0026#39;domainRuntime\u0026#39;) wls:/accessinfra/domainRuntime/\u0026gt; wls:/accessinfra/domainRuntime/\u0026gt; listLoggers(pattern=\u0026#34;oracle.oam.*\u0026#34;,target=\u0026#34;oam_server1\u0026#34;) ------------------------------------------+----------------- Logger | Level ------------------------------------------+----------------- oracle.oam | \u0026lt;Inherited\u0026gt; oracle.oam.admin.foundation.configuration | \u0026lt;Inherited\u0026gt; oracle.oam.admin.service.config | \u0026lt;Inherited\u0026gt; oracle.oam.agent | \u0026lt;Inherited\u0026gt; oracle.oam.agent-default | \u0026lt;Inherited\u0026gt; oracle.oam.audit | \u0026lt;Inherited\u0026gt; oracle.oam.binding | \u0026lt;Inherited\u0026gt; oracle.oam.certvalidation | \u0026lt;Inherited\u0026gt; oracle.oam.certvalidation.mbeans | \u0026lt;Inherited\u0026gt; oracle.oam.common.healthcheck | \u0026lt;Inherited\u0026gt; oracle.oam.common.runtimeent | \u0026lt;Inherited\u0026gt; oracle.oam.commonutil | \u0026lt;Inherited\u0026gt; oracle.oam.config | \u0026lt;Inherited\u0026gt; oracle.oam.controller | \u0026lt;Inherited\u0026gt; oracle.oam.credcollector | \u0026lt;Inherited\u0026gt; oracle.oam.default | \u0026lt;Inherited\u0026gt; oracle.oam.diagnostic | \u0026lt;Inherited\u0026gt; oracle.oam.engine.authn | \u0026lt;Inherited\u0026gt; oracle.oam.engine.authz | \u0026lt;Inherited\u0026gt; oracle.oam.engine.policy | \u0026lt;Inherited\u0026gt; oracle.oam.engine.ptmetadata | \u0026lt;Inherited\u0026gt; oracle.oam.engine.session | \u0026lt;Inherited\u0026gt; oracle.oam.engine.sso | \u0026lt;Inherited\u0026gt; oracle.oam.engine.token | \u0026lt;Inherited\u0026gt; oracle.oam.esso | \u0026lt;Inherited\u0026gt; oracle.oam.extensibility.lifecycle | \u0026lt;Inherited\u0026gt; oracle.oam.foundation.access | \u0026lt;Inherited\u0026gt; oracle.oam.idm | \u0026lt;Inherited\u0026gt; oracle.oam.install | \u0026lt;Inherited\u0026gt; oracle.oam.install.bootstrap | \u0026lt;Inherited\u0026gt; oracle.oam.install.mbeans | \u0026lt;Inherited\u0026gt; oracle.oam.ipf.rest.api | \u0026lt;Inherited\u0026gt; oracle.oam.oauth | \u0026lt;Inherited\u0026gt; oracle.oam.plugin | \u0026lt;Inherited\u0026gt; oracle.oam.proxy.oam | \u0026lt;Inherited\u0026gt; oracle.oam.proxy.oam.workmanager | \u0026lt;Inherited\u0026gt; oracle.oam.proxy.opensso | \u0026lt;Inherited\u0026gt; oracle.oam.proxy.osso | \u0026lt;Inherited\u0026gt; oracle.oam.pswd.service.provider | \u0026lt;Inherited\u0026gt; oracle.oam.replication | \u0026lt;Inherited\u0026gt; oracle.oam.user.identity.provider | \u0026lt;Inherited\u0026gt; wls:/accessinfra/domainRuntime/\u0026gt; Set the log level to TRACE:32:\nwls:/accessinfra/domainRuntime/\u0026gt; setLogLevel(target=\u0026#39;oam_server1\u0026#39;,logger=\u0026#39;oracle.oam\u0026#39;,level=\u0026#39;TRACE:32\u0026#39;,persist=\u0026#34;1\u0026#34;,addLogger=1) wls:/accessinfra/domainRuntime/\u0026gt; wls:/accessinfra/domainRuntime/\u0026gt; listLoggers(pattern=\u0026#34;oracle.oam.*\u0026#34;,target=\u0026#34;oam_server1\u0026#34;) ------------------------------------------+----------------- Logger | Level ------------------------------------------+----------------- oracle.oam | TRACE:32 oracle.oam.admin.foundation.configuration | \u0026lt;Inherited\u0026gt; oracle.oam.admin.service.config | \u0026lt;Inherited\u0026gt; oracle.oam.agent | \u0026lt;Inherited\u0026gt; oracle.oam.agent-default | \u0026lt;Inherited\u0026gt; oracle.oam.audit | \u0026lt;Inherited\u0026gt; oracle.oam.binding | \u0026lt;Inherited\u0026gt; oracle.oam.certvalidation | \u0026lt;Inherited\u0026gt; oracle.oam.certvalidation.mbeans | \u0026lt;Inherited\u0026gt; oracle.oam.common.healthcheck | \u0026lt;Inherited\u0026gt; oracle.oam.common.runtimeent | \u0026lt;Inherited\u0026gt; oracle.oam.commonutil | \u0026lt;Inherited\u0026gt; oracle.oam.config | \u0026lt;Inherited\u0026gt; oracle.oam.controller | \u0026lt;Inherited\u0026gt; oracle.oam.credcollector | \u0026lt;Inherited\u0026gt; oracle.oam.default | \u0026lt;Inherited\u0026gt; oracle.oam.diagnostic | \u0026lt;Inherited\u0026gt; oracle.oam.engine.authn | \u0026lt;Inherited\u0026gt; oracle.oam.engine.authz | \u0026lt;Inherited\u0026gt; oracle.oam.engine.policy | \u0026lt;Inherited\u0026gt; oracle.oam.engine.ptmetadata | \u0026lt;Inherited\u0026gt; oracle.oam.engine.session | \u0026lt;Inherited\u0026gt; oracle.oam.engine.sso | \u0026lt;Inherited\u0026gt; oracle.oam.engine.token | \u0026lt;Inherited\u0026gt; oracle.oam.esso | \u0026lt;Inherited\u0026gt; oracle.oam.extensibility.lifecycle | \u0026lt;Inherited\u0026gt; oracle.oam.foundation.access | \u0026lt;Inherited\u0026gt; oracle.oam.idm | \u0026lt;Inherited\u0026gt; oracle.oam.install | \u0026lt;Inherited\u0026gt; oracle.oam.install.bootstrap | \u0026lt;Inherited\u0026gt; oracle.oam.install.mbeans | \u0026lt;Inherited\u0026gt; oracle.oam.ipf.rest.api | \u0026lt;Inherited\u0026gt; oracle.oam.oauth | \u0026lt;Inherited\u0026gt; oracle.oam.plugin | \u0026lt;Inherited\u0026gt; oracle.oam.proxy.oam | \u0026lt;Inherited\u0026gt; oracle.oam.proxy.oam.workmanager | \u0026lt;Inherited\u0026gt; oracle.oam.proxy.opensso | \u0026lt;Inherited\u0026gt; oracle.oam.proxy.osso | \u0026lt;Inherited\u0026gt; oracle.oam.pswd.service.provider | \u0026lt;Inherited\u0026gt; oracle.oam.replication | \u0026lt;Inherited\u0026gt; oracle.oam.user.identity.provider | \u0026lt;Inherited\u0026gt; wls:/accessinfra/domainRuntime/\u0026gt; Verify that TRACE:32 log level is set by connecting to the Administration Server and viewing the logs:\n$ kubectl exec -it accessinfra-adminserver -n accessns -- /bin/bash [oracle@accessinfra-adminserver oracle]$ [oracle@accessinfra-adminserver oracle]$ cd /u01/oracle/user_projects/domains/accessinfra/servers/oam_server1/logs [oracle@accessinfra-adminserver logs]$ tail oam_server1-diagnostic.log [2020-09-25T09:02:19.492+00:00] [oam_server1] [TRACE:32] [] [oracle.oam.config] [tid: Configuration Store Observer] [userId: \u0026lt;anonymous\u0026gt;] [ecid: 0dc53783-fada-4709-b7c1-8958bbbaac95-0000000b,0:1062] [APP: oam_server] [partition-name: DOMAIN] [tenant-name: GLOBAL] [SRC_CLASS: oracle.security.am.admin.config.util.store.DbStore] [SRC_METHOD: getSelectSQL] SELECT SQL:SELECT version from IDM_OBJECT_STORE where id = ? and version = (select max(version) from IDM_OBJECT_STORE where id = ?) [2020-09-25T09:02:19.494+00:00] [oam_server1] [TRACE] [] [oracle.oam.config] [tid: Configuration Store Observer] [userId: \u0026lt;anonymous\u0026gt;] [ecid: 0dc53783-fada-4709-b7c1-8958bbbaac95-0000000b,0:1062] [APP: oam_server] [partition-name: DOMAIN] [tenant-name: GLOBAL] [SRC_CLASS: oracle.security.am.admin.config.util.store.DbStore] [SRC_METHOD: load] Time (ms) to load key CONFIG:-1{FIELD_TYPES=INT, SELECT_FIELDS=SELECT version from IDM_OBJECT_STORE }:3 [2020-09-25T09:02:19.494+00:00] [oam_server1] [TRACE:16] [] [oracle.oam.config] [tid: Configuration Store Observer] [userId: \u0026lt;anonymous\u0026gt;] [ecid: 0dc53783-fada-4709-b7c1-8958bbbaac95-0000000b,0:1062] [APP: oam_server] [partition-name: DOMAIN] [tenant-name: GLOBAL] [SRC_CLASS: oracle.security.am.admin.config.util.store.DbStore] [SRC_METHOD: load] RETURN [2020-09-25T09:02:20.050+00:00] [oam_server1] [TRACE:16] [] [oracle.oam.engine.session] [tid: OAM SME Service - 2] [userId: \u0026lt;anonymous\u0026gt;] [ecid: 0dc53783-fada-4709-b7c1-8958bbbaac95-0000000b,0:1777] [APP: oam_server] [partition-name: DOMAIN] [tenant-name: GLOBAL] [SRC_CLASS: oracle.security.am.engines.sme.mgrdb.SessionManagerImpl$3] [SRC_METHOD: run] ENTRY [2020-09-25T09:02:20.057+00:00] [oam_server1] [TRACE] [] [oracle.oam.engine.session] [tid: OAM SME Service - 2] [userId: \u0026lt;anonymous\u0026gt;] [ecid: 0dc53783-fada-4709-b7c1-8958bbbaac95-0000000b,0:1777] [APP: oam_server] [partition-name: DOMAIN] [tenant-name: GLOBAL] [SRC_CLASS: oracle.security.am.engines.sme.mgrdb.SessionManagerImpl$3] [SRC_METHOD: run] Session Store Current status: UP, at time: Fri Sep 25 09:02:20 GMT 2020. Previous known status: UP. Polling Interval: 15000 milliseconds [2020-09-25T09:02:20.057+00:00] [oam_server1] [TRACE:16] [] [oracle.oam.engine.session] [tid: OAM SME Service - 2] [userId: \u0026lt;anonymous\u0026gt;] [ecid: 0dc53783-fada-4709-b7c1-8958bbbaac95-0000000b,0:1777] [APP: oam_server] [partition-name: DOMAIN] [tenant-name: GLOBAL] [SRC_CLASS: oracle.security.am.engines.sme.mgrdb.SessionManagerImpl$3] [SRC_METHOD: run] RETURN [2020-09-25T09:02:22.602+00:00] [oam_server1] [NOTIFICATION] [] [oracle.wsm.agent.handler.jaxrs.RESTJeeResourceFilter] [tid: [ACTIVE].ExecuteThread: \u0026#39;9\u0026#39; for queue: \u0026#39;weblogic.kernel.Default (self-tuning)\u0026#39;] [userId: weblogic] [ecid: 0dc53783-fada-4709-b7c1-8958bbbaac95-000000c8,0] [APP: wls-management-services] [partition-name: DOMAIN] [tenant-name: GLOBAL] Tenant: default, ProcessResponse is set to false [2020-09-25T09:02:27.608+00:00] [oam_server1] [NOTIFICATION] [] [oracle.wsm.agent.handler.jaxrs.RESTJeeResourceFilter] [tid: [ACTIVE].ExecuteThread: \u0026#39;43\u0026#39; for queue: \u0026#39;weblogic.kernel.Default (self-tuning)\u0026#39;] [userId: weblogic] [ecid: 0dc53783-fada-4709-b7c1-8958bbbaac95-000000c9,0] [APP: wls-management-services] [partition-name: DOMAIN] [tenant-name: GLOBAL] Tenant: default, ProcessResponse is set to false Performing WLST Administration via SSL   By default the SSL port is not enabled for the Administration Server or OAM Managed Servers. To configure the SSL port for the Administration Server and Managed Servers login to WebLogic Administration console https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/console and navigate to Lock \u0026amp; Edit -\u0026gt; Environment -\u0026gt;Servers -\u0026gt; server_name -\u0026gt;Configuration -\u0026gt; General -\u0026gt; SSL Listen Port Enabled -\u0026gt; Provide SSL Port ( For Administration Server: 7002 and for OAM Managed Server (oam_server1): 14101) - \u0026gt; Save -\u0026gt; Activate Changes.\nNote: If configuring the OAM Managed Servers for SSL you must enable SSL on the same port for all servers (oam_server1 through oam_server5)\n  Create a myscripts directory as follows:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/scripts $ mkdir myscripts $ cd myscripts   Create a sample yaml template file in the myscripts directory called \u0026lt;domain_uid\u0026gt;-adminserver-ssl.yaml to create a Kubernetes service for the Administration Server:\nNote: Update the domainName, domainUID and namespace based on your environment. For example:\napiVersion: v1 kind: Service metadata: labels: serviceType: SERVER weblogic.domainName: accessinfra weblogic.domainUID: accessinfra weblogic.resourceVersion: domain-v2 weblogic.serverName: AdminServer name: accessinfra-adminserverssl namespace: accessns spec: clusterIP: None ports: - name: default port: 7002 protocol: TCP targetPort: 7002 selector: weblogic.createdByOperator: \u0026#34;true\u0026#34; weblogic.domainUID: accessinfra weblogic.serverName: AdminServer type: ClusterIP and the following sample yaml template file \u0026lt;domain_uid\u0026gt;-oamcluster-ssl.yaml for the OAM Managed Server:\napiVersion: v1 kind: Service metadata: labels: serviceType: SERVER weblogic.domainName: accessinfra weblogic.domainUID: accessinfra weblogic.resourceVersion: domain-v2 name: accessinfra-oamcluster-ssl namespace: accessns spec: clusterIP: None ports: - name: default port: 14101 protocol: TCP targetPort: 14101 selector: weblogic.clusterName: oam_cluster weblogic.createdByOperator: \u0026#34;true\u0026#34; weblogic.domainUID: accessinfra type: ClusterIP   Apply the template using the following command for the AdminServer:\n$ kubectl apply -f \u0026lt;domain_uid\u0026gt;-adminserver-ssl.yaml For example:\n$ kubectl apply -f accessinfra-adminserver-ssl.yaml service/accessinfra-adminserverssl created and using the following command for the OAM Managed Server:\n$ kubectl apply -f \u0026lt;domain_uid\u0026gt;-oamcluster-ssl.yaml For example:\n$ kubectl apply -f accessinfra-oamcluster-ssl.yaml service/accessinfra-oamcluster-ssl created   Validate that the Kubernetes Services to access SSL ports are created successfully:\n$ kubectl get svc -n \u0026lt;domain_namespace\u0026gt; |grep ssl For example:\n$ kubectl get svc -n accessns |grep ssl The output will look similar to the following:\naccessinfra-adminserverssl ClusterIP None \u0026lt;none\u0026gt; 7002/TCP 102s accessinfra-oamcluster-ssl ClusterIP None \u0026lt;none\u0026gt; 14101/TCP 35s   Inside the bash shell of the running helper pod, run the following:\n[oracle@helper bin]$ export WLST_PROPERTIES=\u0026#34;-Dweblogic.security.SSL.ignoreHostnameVerification=true -Dweblogic.security.TrustKeyStore=DemoTrust\u0026#34; [oracle@helper bin]$ cd /u01/oracle/oracle_common/common/bin [oracle@helper bin]$ ./wlst.sh Initializing WebLogic Scripting Tool (WLST) ... Welcome to WebLogic Server Administration Scripting Shell Type help() for help on available commands wls:/offline\u0026gt; To connect to the Administration Server t3s service:\nwls:/offline\u0026gt; connect(\u0026#39;weblogic\u0026#39;,\u0026#39;\u0026lt;password\u0026gt;\u0026#39;,\u0026#39;t3s://accessinfra-adminserverssl:7002\u0026#39;) Connecting to t3s://accessinfra-adminserverssl:7002 with userid weblogic ... \u0026lt;Sep 25, 2020 9:11:24 AM GMT\u0026gt; \u0026lt;Info\u0026gt; \u0026lt;Security\u0026gt; \u0026lt;BEA-090905\u0026gt; \u0026lt;Disabling the CryptoJ JCE Provider self-integrity check for better startup performance. To enable this check, specify -Dweblogic.security.allowCryptoJDefaultJCEVerification=true.\u0026gt; \u0026lt;Sep 25, 2020 9:11:24 AM GMT\u0026gt; \u0026lt;Info\u0026gt; \u0026lt;Security\u0026gt; \u0026lt;BEA-090906\u0026gt; \u0026lt;Changing the default Random Number Generator in RSA CryptoJ from ECDRBG128 to HMACDRBG. To disable this change, specify -Dweblogic.security.allowCryptoJDefaultPRNG=true.\u0026gt; \u0026lt;Sep 25, 2020 9:11:24 AM GMT\u0026gt; \u0026lt;Info\u0026gt; \u0026lt;Security\u0026gt; \u0026lt;BEA-090909\u0026gt; \u0026lt;Using the configured custom SSL Hostname Verifier implementation: weblogic.security.utils.SSLWLSHostnameVerifier$NullHostnameVerifier.\u0026gt; Successfully connected to Admin Server \u0026#34;AdminServer\u0026#34; that belongs to domain \u0026#34;accessinfra\u0026#34;. wls:/accessinfra/serverConfig/\u0026gt; To connect to the OAM Managed Server t3s service:\nwls:/offline\u0026gt; connect(\u0026#39;weblogic\u0026#39;,\u0026#39;\u0026lt;password\u0026gt;\u0026#39;,\u0026#39;t3s://accessinfra-oamcluster-ssl:14101\u0026#39;) Connecting to t3s://accessinfra-oamcluster-ssl:14101 with userid weblogic ... Successfully connected to managed Server \u0026#34;oam_server1\u0026#34; that belongs to domain \u0026#34;accessinfra\u0026#34;.   "
},
{
	"uri": "/fmw-kubernetes/oig/manage-oig-domains/wlst-admin-operations/",
	"title": "WLST Administration Operations",
	"tags": [],
	"description": "Describes the steps for WLST administration using helper pod running in the same Kubernetes Cluster as OIG Domain.",
	"content": "Invoke WLST and Access Administration Server To use WLST to administer the OIG domain, use a helper pod in the same Kubernetes cluster as the OIG Domain.\n  Run the following command to create a helper pod if one doesn\u0026rsquo;t already exist:\n$ kubectl run helper --image \u0026lt;image_name\u0026gt; -n \u0026lt;domain_namespace\u0026gt; -- sleep infinity For example:\n$ kubectl run helper --image oracle/oig:12.2.1.4.0 -n oimcluster -- sleep infinity The output will look similar to the following:\n$ kubectl run helper --image oracle/oig:12.2.1.4.0 -n oimcluster -- sleep infinity pod/helper created   Run the following command to start a bash shell in the helper pod:\n$ kubectl exec -it helper -n \u0026lt;domain_namespace\u0026gt; -- /bin/bash For example:\n$ kubectl exec -it helper -n oimcluster -- /bin/bash This will take you into a bash shell in the running helper pod:\n[oracle@helper ~]$   Connect to WLST using the following commands:\n[oracle@helper ~]$ cd $ORACLE_HOME/oracle_common/common/bin [oracle@helper ~]$ ./wlst.sh The output will look similar to the following:\n[oracle@helper bin]$ ./wlst.sh Initializing WebLogic Scripting Tool (WLST) ... Jython scans all the jar files it can find at first startup. Depending on the system, this process may take a few minutes to complete, and WLST may not return a prompt right away. Welcome to WebLogic Server Administration Scripting Shell Type help() for help on available commands wls:/offline\u0026gt;   To access t3 for the Administration Server connect as follows:\nconnect('weblogic','\u0026lt;password\u0026gt;','t3://oimcluster-adminserver:7001') The output will look similar to the following:\nwls:/offline\u0026gt; connect('weblogic','\u0026lt;password\u0026gt;','t3://oimcluster-adminserver:7001') Connecting to t3://oimcluster-adminserver:7001 with userid weblogic ... Successfully connected to Admin Server \u0026quot;AdminServer\u0026quot; that belongs to domain \u0026quot;oimcluster\u0026quot;. Warning: An insecure protocol was used to connect to the server. To ensure on-the-wire security, the SSL port or Admin port should be used instead. wls:/oimcluster/serverConfig/\u0026gt; Or to access t3 for the OIG Cluster service, connect as follows:\nconnect('weblogic','\u0026lt;password\u0026gt;','t3://oimcluster-cluster-oim-cluster:14100') The output will look similar to the following:\nwls:/offline\u0026gt; connect('weblogic','\u0026lt;password\u0026gt;','t3://oimcluster-cluster-oim-cluster:14000') Connecting to t3://oimcluster-cluster-oim-cluster:14000 with userid weblogic ... Successfully connected to managed Server \u0026quot;oim_server1\u0026quot; that belongs to domain \u0026quot;oimcluster\u0026quot;. Warning: An insecure protocol was used to connect to the server. To ensure on-the-wire security, the SSL port or Admin port should be used instead. wls:/oimcluster/serverConfig/\u0026gt;   Sample operations For a full list of WLST operations refer to WebLogic Server WLST Online and Offline Command Reference.\nDisplay servers wls:/oimcluster/serverConfig/\u0026gt; cd('/Servers') wls:/oimcluster/serverConfig/Servers\u0026gt; ls () dr-- AdminServer dr-- oim_server1 dr-- oim_server2 dr-- oim_server3 dr-- oim_server4 dr-- oim_server5 dr-- soa_server1 dr-- soa_server2 dr-- soa_server3 dr-- soa_server4 dr-- soa_server5 wls:/oimcluster/serverConfig/Servers\u0026gt; Performing WLST Administration via SSL   By default the SSL port is not enabled for the Administration Server or OIG Managed Servers. To configure the SSL port for the Administration Server and Managed Servers login to WebLogic Administration console https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/console and navigate to Lock \u0026amp; Edit -\u0026gt; Environment -\u0026gt;Servers -\u0026gt; server_name -\u0026gt;Configuration -\u0026gt; General -\u0026gt; SSL Listen Port Enabled -\u0026gt; Provide SSL Port ( For Administration Server: 7002 and for OIG Managed Server (oim_server1): 14101) - \u0026gt; Save -\u0026gt; Activate Changes.\nNote: If configuring the OIG Managed Servers for SSL you must enable SSL on the same port for all servers (oim_server1 through oim_server4)\n  Create a myscripts directory as follows:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/scripts $ mkdir myscripts $ cd myscripts   Create a sample yaml template file in the myscripts directory called \u0026lt;domain_uid\u0026gt;-adminserver-ssl.yaml to create a Kubernetes service for the Administration Server:\nNote: Update the domainName, domainUID and namespace based on your environment.\napiVersion: v1 kind: Service metadata: labels: serviceType: SERVER weblogic.domainName: oimcluster weblogic.domainUID: oimcluster weblogic.resourceVersion: domain-v2 weblogic.serverName: AdminServer name: oimcluster-adminserver-ssl namespace: oimcluster spec: clusterIP: None ports: - name: default port: 7002 protocol: TCP targetPort: 7002 selector: weblogic.createdByOperator: \u0026quot;true\u0026quot; weblogic.domainUID: oimcluster weblogic.serverName: AdminServer type: ClusterIP and create the following sample yaml template file \u0026lt;domain_uid\u0026gt;-oim-cluster-ssl.yaml for the OIG Managed Server:\napiVersion: v1 kind: Service metadata: labels: serviceType: SERVER weblogic.domainName: oimcluster weblogic.domainUID: oimcluster weblogic.resourceVersion: domain-v2 name: oimcluster-cluster-oim-cluster-ssl namespace: oimcluster spec: clusterIP: None ports: - name: default port: 14101 protocol: TCP targetPort: 14101 selector: weblogic.clusterName: oim_cluster weblogic.createdByOperator: \u0026quot;true\u0026quot; weblogic.domainUID: oimcluster type: ClusterIP   Apply the template using the following command for the Administration Server:\n$ kubectl apply -f oimcluster-adminserver-ssl.yaml service/oimcluster-adminserver-ssl created or using the following command for the OIG Managed Server:\n$ kubectl apply -f oimcluster-oim-cluster-ssl.yaml service/oimcluster-cluster-oim-cluster-ssl created   Validate that the Kubernetes Services to access SSL ports are created successfully:\n$ kubectl get svc -n \u0026lt;domain_namespace\u0026gt; |grep ssl For example:\n$ kubectl get svc -n oimcluster |grep ssl The output will look similar to the following:\noimcluster-adminserver-ssl ClusterIP None \u0026lt;none\u0026gt; 7002/TCP 74s oimcluster-cluster-oim-cluster-ssl ClusterIP None \u0026lt;none\u0026gt; 14101/TCP 21s   Connect to a bash shell of the helper pod:\n$ kubectl exec -it helper -n oimcluster -- /bin/bash   In the bash shell run the following:\n[oracle@oimcluster-adminserver oracle]$ export WLST_PROPERTIES=\u0026quot;-Dweblogic.security.SSL.ignoreHostnameVerification=true -Dweblogic.security.TrustKeyStore=DemoTrust\u0026quot; [oracle@oimcluster-adminserver oracle]$ cd /u01/oracle/oracle_common/common/bin [oracle@oimcluster-adminserver oracle]$ ./wlst.sh Initializing WebLogic Scripting Tool (WLST) ... Welcome to WebLogic Server Administration Scripting Shell Type help() for help on available commands wls:/offline\u0026gt; Connect to the Administration Server t3s service:\nwls:/offline\u0026gt; connect('weblogic','\u0026lt;password\u0026gt;','t3s://oimcluster-adminserver-ssl:7002') \u0026lt;Sep 30, 2020 3:16:48 PM GMT\u0026gt; \u0026lt;Info\u0026gt; \u0026lt;Security\u0026gt; \u0026lt;BEA-090905\u0026gt; \u0026lt;Disabling the CryptoJ JCE Provider self-integrity check for better startup performance. To enable this check, specify -Dweblogic.security.allowCryptoJDefaultJCEVerification=true.\u0026gt; \u0026lt;Sep 30, 2020 3:16:48 PM GMT\u0026gt; \u0026lt;Info\u0026gt; \u0026lt;Security\u0026gt; \u0026lt;BEA-090906\u0026gt; \u0026lt;Changing the default Random Number Generator in RSA CryptoJ from ECDRBG128 to HMACDRBG. To disable this change, specify -Dweblogic.security.allowCryptoJDefaultPRNG=true.\u0026gt; \u0026lt;Sep 30, 2020 3:16:48 PM GMT\u0026gt; \u0026lt;Info\u0026gt; \u0026lt;Security\u0026gt; \u0026lt;BEA-090909\u0026gt; \u0026lt;Using the configured custom SSL Hostname Verifier implementation: weblogic.security.utils.SSLWLSHostnameVerifier$NullHostnameVerifier.\u0026gt; Successfully connected to Admin Server \u0026quot;AdminServer\u0026quot; that belongs to domain \u0026quot;oimcluster\u0026quot;. wls:/oimcluster/serverConfig/\u0026gt; To connect to the OIG Managed Server t3s service:\nwls:/offline\u0026gt; connect('weblogic','\u0026lt;password\u0026gt;','t3s://oimcluster-cluster-oim-cluster-ssl:14101') Connecting to t3s://oimcluster-cluster-oim-cluster-ssl:14101 with userid weblogic ... Successfully connected to managed Server \u0026quot;oim_server1\u0026quot; that belongs to domain \u0026quot;oimcluster\u0026quot;. wls:/oimcluster/serverConfig/\u0026gt;   "
},
{
	"uri": "/fmw-kubernetes/soa-domains/installguide/create-soa-domains/",
	"title": "Create Oracle SOA Suite domains",
	"tags": [],
	"description": "Create an Oracle SOA Suite domain home on an existing PV or PVC, and create the domain resource YAML file for deploying the generated Oracle SOA Suite domain.",
	"content": "The SOA deployment scripts demonstrate the creation of an Oracle SOA Suite domain home on an existing Kubernetes persistent volume (PV) and persistent volume claim (PVC). The scripts also generate the domain YAML file, which can then be used to start the Kubernetes artifacts of the corresponding domain.\nPrerequisites Before you begin, complete the following steps:\n Review the Domain resource documentation. Review the requirements and limitations. Ensure that you have executed all the preliminary steps in Prepare your environment. Ensure that the database and the WebLogic Kubernetes operator are running.  Prepare to use the create domain script The sample scripts for Oracle SOA Suite domain deployment are available at ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-soa-domain.\nYou must edit create-domain-inputs.yaml (or a copy of it) to provide the details for your domain. Refer to the configuration parameters below to understand the information that you must provide in this file.\nConfiguration parameters The following parameters can be provided in the inputs file.\n   Parameter Definition Default     sslEnabled Boolean indicating whether to enable SSL for each WebLogic Server instance. false   adminPort Port number for the Administration Server inside the Kubernetes cluster. 7001   adminServerSSLPort SSL port number of the Administration Server inside the Kubernetes cluster. 7002   adminNodePort Port number of the Administration Server outside the Kubernetes cluster. 30701   adminServerName Name of the Administration Server. AdminServer   clusterName Name of the WebLogic cluster instance to generate for the domain. By default the cluster name is soa_cluster for the SOA domain type. You can update this to osb_cluster for an OSB domain type or soa_cluster for SOAESS or SOAOSB or SOAESSOSB domain types. soa_cluster   configuredManagedServerCount Number of Managed Server instances to generate for the domain. 5   createDomainFilesDir Directory on the host machine to locate all the files to create a WebLogic domain, including the script that is specified in the createDomainScriptName property. By default, this directory is set to the relative path wlst, and the create script will use the built-in WLST offline scripts in the wlst directory to create the WebLogic domain. An absolute path is also supported to point to an arbitrary directory in the file system. The built-in scripts can be replaced by the user-provided scripts as long as those files are in the specified directory. Files in this directory are put into a Kubernetes config map, which in turn is mounted to the createDomainScriptsMountPath, so that the Kubernetes pod can use the scripts and supporting files to create a domain home. wlst   createDomainScriptsMountPath Mount path where the create domain scripts are located inside a pod. The create-domain.sh script creates a Kubernetes job to run the script (specified in the createDomainScriptName property) in a Kubernetes pod to create a domain home. Files in the createDomainFilesDir directory are mounted to this location in the pod, so that the Kubernetes pod can use the scripts and supporting files to create a domain home. /u01/weblogic   createDomainScriptName Script that the create domain script uses to create a WebLogic domain. The create-domain.sh script creates a Kubernetes job to run this script to create a domain home. The script is located in the in-pod directory that is specified in the createDomainScriptsMountPath property. If you need to provide your own scripts to create the domain home, instead of using the built-it scripts, you must use this property to set the name of the script that you want the create domain job to run. create-domain-job.sh   domainHome Home directory of the SOA domain. If not specified, the value is derived from the domainUID as /shared/domains/\u0026lt;domainUID\u0026gt;. /u01/oracle/user_projects/domains/soainfra   domainPVMountPath Mount path of the domain persistent volume. /u01/oracle/user_projects   domainUID Unique ID that will be used to identify this particular domain. Used as the name of the generated WebLogic domain as well as the name of the Kubernetes domain resource. This ID must be unique across all domains in a Kubernetes cluster. This ID cannot contain any character that is not valid in a Kubernetes service name. soainfra   domainType Type of the domain. Mandatory input for Oracle SOA Suite domains. You must provide one of the supported domain type values: soa (deploys a SOA domain),osb (deploys an OSB (Oracle Service Bus) domain),soaess (deploys a SOA domain with Enterprise Scheduler (ESS)),soaosb (deploys a domain with SOA and OSB), and soaessosb (deploys a domain with SOA, OSB, and ESS). soa   exposeAdminNodePort Boolean indicating if the Administration Server is exposed outside of the Kubernetes cluster. false   exposeAdminT3Channel Boolean indicating if the T3 administrative channel is exposed outside the Kubernetes cluster. false   httpAccessLogInLogHome Boolean indicating if server HTTP access log files should be written to the same directory as logHome. Otherwise, server HTTP access log files will be written to the directory specified in the WebLogic domain home configuration. true   image SOA Suite Docker image. The operator requires Oracle SOA Suite 12.2.1.4. Refer to Obtain the Oracle SOA Suite Docker image for details on how to obtain or create the image. soasuite:12.2.1.4   imagePullPolicy WebLogic Docker image pull policy. Legal values are IfNotPresent, Always, or Never. IfNotPresent   imagePullSecretName Name of the Kubernetes secret to access the Docker Store to pull the WebLogic Server Docker image. The presence of the secret will be validated when this parameter is specified.    includeServerOutInPodLog Boolean indicating whether to include the server .out to the pod\u0026rsquo;s stdout. true   initialManagedServerReplicas Number of Managed Servers to initially start for the domain. 2   javaOptions Java options for starting the Administration Server and Managed Servers. A Java option can have references to one or more of the following pre-defined variables to obtain WebLogic domain information: $(DOMAIN_NAME), $(DOMAIN_HOME), $(ADMIN_NAME), $(ADMIN_PORT), and $(SERVER_NAME). If sslEnabled is set to true and the WebLogic demo certificate is used, add -Dweblogic.security.SSL.ignoreHostnameVerification=true to allow the managed servers to connect to the Administration Server while booting up. The WebLogic generated demo certificate in this environment typically contains a host name that is different from the runtime container\u0026rsquo;s host name. -Dweblogic.StdoutDebugEnabled=false   logHome The in-pod location for the domain log, server logs, server out, and Node Manager log files. If not specified, the value is derived from the domainUID as /shared/logs/\u0026lt;domainUID\u0026gt;. /u01/oracle/user_projects/domains/logs/soainfra   managedServerNameBase Base string used to generate Managed Server names. By default the managedServerNameBase is soa_server for the SOA domain type. You can update this to osb_server for an OSB domain type or soa_server for SOAESS or SOAOSB or SOAESSOSB domain types. soa_server   managedServerPort Port number for each Managed Server. By default the managedServerPort is 8001 for the SOA domain type. You can update this to 9001 for an OSB domain type or 8001 for SOAESS or SOAOSB or SOAESSOSB domain types. 8001   managedServerSSLPort SSL port number for each Managed Server. By default the managedServerSSLPort is 8002 for the SOA domain type. You can update this to 9002 for an OSB domain type or 8002 for SOAESS or SOAOSB or SOAESSOSB domain types. 8002   namespace Kubernetes namespace in which to create the domain. soans   persistentVolumeClaimName Name of the persistent volume claim created to host the domain home. If not specified, the value is derived from the domainUID as \u0026lt;domainUID\u0026gt;-weblogic-sample-pvc. soainfra-domain-pvc   productionModeEnabled Boolean indicating if production mode is enabled for the domain. true   serverStartPolicy Determines which WebLogic Server instances will be started. Legal values are NEVER, IF_NEEDED, ADMIN_ONLY. IF_NEEDED   t3ChannelPort Port for the t3 channel of the NetworkAccessPoint. 30012   t3PublicAddress Public address for the T3 channel. This should be set to the public address of the Kubernetes cluster. This would typically be a load balancer address. For development environments only: In a single server (all-in-one) Kubernetes deployment, this may be set to the address of the master, or at the very least, it must be set to the address of one of the worker nodes. If not provided, the script will attempt to set it to the IP address of the Kubernetes cluster   weblogicCredentialsSecretName Name of the Kubernetes secret for the Administration Server\u0026rsquo;s user name and password. If not specified, then the value is derived from the domainUID as \u0026lt;domainUID\u0026gt;-weblogic-credentials. soainfra-domain-credentials   weblogicImagePullSecretName Name of the Kubernetes secret for the Docker Store, used to pull the WebLogic Server image.    serverPodCpuRequest, serverPodMemoryRequest, serverPodCpuCLimit, serverPodMemoryLimit The maximum amount of compute resources allowed, and minimum amount of compute resources required, for each server pod. Please refer to the Kubernetes documentation on Managing Compute Resources for Containers for details. Resource requests and resource limits are not specified.   rcuSchemaPrefix The schema prefix to use in the database, for example SOA1. You may wish to make this the same as the domainUID in order to simplify matching domains to their RCU schemas. SOA1   rcuDatabaseURL The database URL. oracle-db.default.svc.cluster.local:1521/devpdb.k8s   rcuCredentialsSecret The Kubernetes secret containing the database credentials. soainfra-rcu-credentials   persistentStore The persistent store for \u0026lsquo;JMS servers\u0026rsquo; and \u0026lsquo;Transaction log store\u0026rsquo; in the domain. Legal values are jdbc, file. jdbc    Note that the names of the Kubernetes resources in the generated YAML files may be formed with the value of some of the properties specified in the create-inputs.yaml file. Those properties include the adminServerName, clusterName and managedServerNameBase. If those values contain any characters that are invalid in a Kubernetes service name, those characters are converted to valid values in the generated YAML files. For example, an uppercase letter is converted to a lowercase letter and an underscore (\u0026quot;_\u0026quot;) is converted to a hyphen (\u0026quot;-\u0026quot;).\nThe sample demonstrates how to create an Oracle SOA Suite domain home and associated Kubernetes resources for a domain that has one cluster only. In addition, the sample provides the capability for users to supply their own scripts to create the domain home for other use cases. The generated domain YAML file could also be modified to cover more use cases.\nRun the create domain script Run the create domain script, specifying your inputs file and an output directory to store the generated artifacts:\n$ ./create-domain.sh \\ -i create-domain-inputs.yaml \\ -o \u0026lt;path to output-directory\u0026gt; The script will perform the following steps:\n  Create a directory for the generated Kubernetes YAML files for this domain if it does not already exist. The path name is \u0026lt;path to output-directory\u0026gt;/weblogic-domains/\u0026lt;domainUID\u0026gt;. If the directory already exists, its contents must be removed before using this script.\n  Create a Kubernetes job that will start up a utility Oracle SOA Suite container and run offline WLST scripts to create the domain on the shared storage.\n  Run and wait for the job to finish.\n  Create a Kubernetes domain YAML file, domain.yaml, in the \u0026ldquo;output\u0026rdquo; directory that was created above. This YAML file can be used to create the Kubernetes resource using the kubectl create -f or kubectl apply -f command:\n$ kubectl apply -f \u0026lt;path to output-directory\u0026gt;/weblogic-domains/\u0026lt;domainUID\u0026gt;/domain.yaml   Create a convenient utility script, delete-domain-job.yaml, to clean up the domain home created by the create script.\n  The default domain created by the script has the following characteristics:\n An Administration Server named AdminServer listening on port 7001. A configured cluster named soa_cluster of size 5. Two Managed Servers, named soa_server1 and soa_server2, listening on port 8001. Log files that are located in /shared/logs/\u0026lt;domainUID\u0026gt;. SOA Infra, SOA composer and WorklistApp applications deployed.  Verify the results The create domain script will verify that the domain was created, and will report failure if there was any error. However, it may be desirable to manually verify the domain, even if just to gain familiarity with the various Kubernetes objects that were created by the script.\nGenerated YAML files with the default inputs   Click here to see sample content of the generated `domain.yaml` for `soaessosb` domainType that creates SOA and OSB clusters.   $ cat output/weblogic-domains/soainfra/domain.yaml # Copyright (c) 2017, 2020, Oracle Corporation and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # # This is an example of how to define a Domain resource. # apiVersion: \u0026quot;weblogic.oracle/v8\u0026quot; kind: Domain metadata: name: soainfra namespace: soans labels: weblogic.domainUID: soainfra spec: # The WebLogic Domain Home domainHome: /u01/oracle/user_projects/domains/soainfra # The domain home source type # Set to PersistentVolume for domain-in-pv, Image for domain-in-image, or FromModel for model-in-image domainHomeSourceType: PersistentVolume # The WebLogic Server Docker image that the operator uses to start the domain image: \u0026quot;soasuite:12.2.1.4\u0026quot; # imagePullPolicy defaults to \u0026quot;Always\u0026quot; if image version is :latest imagePullPolicy: \u0026quot;IfNotPresent\u0026quot; # Identify which Secret contains the credentials for pulling an image #imagePullSecrets: #- name: # Identify which Secret contains the WebLogic Admin credentials (note that there is an example of # how to create that Secret at the end of this file) webLogicCredentialsSecret: name: soainfra-domain-credentials # Whether to include the server out file into the pod's stdout, default is true includeServerOutInPodLog: true # Whether to enable log home logHomeEnabled: true # Whether to write HTTP access log file to log home httpAccessLogInLogHome: true # The in-pod location for domain log, server logs, server out, and Node Manager log files logHome: /u01/oracle/user_projects/domains/logs/soainfra # An (optional) in-pod location for data storage of default and custom file stores. # If not specified or the value is either not set or empty (e.g. dataHome: \u0026quot;\u0026quot;) then the # data storage directories are determined from the WebLogic domain home configuration. dataHome: \u0026quot;\u0026quot; # serverStartPolicy legal values are \u0026quot;NEVER\u0026quot;, \u0026quot;IF_NEEDED\u0026quot;, or \u0026quot;ADMIN_ONLY\u0026quot; # This determines which WebLogic Servers the operator will start up when it discovers this Domain # - \u0026quot;NEVER\u0026quot; will not start any server in the domain # - \u0026quot;ADMIN_ONLY\u0026quot; will start up only the administration server (no managed servers will be started) # - \u0026quot;IF_NEEDED\u0026quot; will start all non-clustered servers, including the administration server and clustered servers up to the replica count serverStartPolicy: \u0026quot;IF_NEEDED\u0026quot; serverPod: # an (optional) list of environment variable to be set on the servers env: - name: JAVA_OPTIONS value: \u0026quot;-Dweblogic.StdoutDebugEnabled=false -Dweblogic.ssl.Enabled=true -Dweblogic.security.SSL.ignoreHostnameVerification=true\u0026quot; - name: USER_MEM_ARGS value: \u0026quot;-Djava.security.egd=file:/dev/./urandom -Xms256m -Xmx1024m \u0026quot; volumes: - name: weblogic-domain-storage-volume persistentVolumeClaim: claimName: soainfra-domain-pvc volumeMounts: - mountPath: /u01/oracle/user_projects name: weblogic-domain-storage-volume # adminServer is used to configure the desired behavior for starting the administration server. adminServer: # serverStartState legal values are \u0026quot;RUNNING\u0026quot; or \u0026quot;ADMIN\u0026quot; # \u0026quot;RUNNING\u0026quot; means the listed server will be started up to \u0026quot;RUNNING\u0026quot; mode # \u0026quot;ADMIN\u0026quot; means the listed server will be start up to \u0026quot;ADMIN\u0026quot; mode serverStartState: \u0026quot;RUNNING\u0026quot; # adminService: # channels: # The Admin Server's NodePort # - channelName: default # nodePort: 30701 # Uncomment to export the T3Channel as a service # - channelName: T3Channel serverPod: # an (optional) list of environment variable to be set on the admin servers env: - name: USER_MEM_ARGS value: \u0026quot;-Djava.security.egd=file:/dev/./urandom -Xms512m -Xmx1024m \u0026quot; # clusters is used to configure the desired behavior for starting member servers of a cluster. # If you use this entry, then the rules will be applied to ALL servers that are members of the named clusters. clusters: - clusterName: osb_cluster serverService: precreateService: true serverStartState: \u0026quot;RUNNING\u0026quot; serverPod: # Instructs Kubernetes scheduler to prefer nodes for new cluster members where there are not # already members of the same cluster. affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: \u0026quot;weblogic.clusterName\u0026quot; operator: In values: - $(CLUSTER_NAME) topologyKey: \u0026quot;kubernetes.io/hostname\u0026quot; replicas: 2 # The number of managed servers to start for unlisted clusters # replicas: 1 # Istio # configuration: # istio: # enabled: # readinessPort: - clusterName: soa_cluster serverService: precreateService: true serverStartState: \u0026quot;RUNNING\u0026quot; serverPod: # Instructs Kubernetes scheduler to prefer nodes for new cluster members where there are not # already members of the same cluster. affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: \u0026quot;weblogic.clusterName\u0026quot; operator: In values: - $(CLUSTER_NAME) topologyKey: \u0026quot;kubernetes.io/hostname\u0026quot; replicas: 2 # The number of managed servers to start for unlisted clusters # replicas: 1    Verify the domain To confirm that the domain was created, enter the following command:\n$ kubectl describe domain DOMAINUID -n NAMESPACE Replace DOMAINUID with the domainUID and NAMESPACE with the actual namespace.\n  Click here to see a sample domain description.   $ kubectl describe domain soainfra -n soans Name: soainfra Namespace: soans Labels: weblogic.domainUID=soainfra Annotations: \u0026lt;none\u0026gt; API Version: weblogic.oracle/v8 Kind: Domain Metadata: Creation Timestamp: 2020-08-04T09:24:15Z Generation: 1 Managed Fields: API Version: weblogic.oracle/v8 Fields Type: FieldsV1 fieldsV1: f:metadata: f:labels: .: f:weblogic.domainUID: Manager: kubectl Operation: Update Time: 2020-08-04T09:24:15Z API Version: weblogic.oracle/v8 Fields Type: FieldsV1 fieldsV1: f:status: .: f:clusters: f:conditions: f:servers: f:startTime: Manager: OpenAPI-Generator Operation: Update Time: 2020-08-04T10:15:54Z Resource Version: 8691832 Self Link: /apis/weblogic.oracle/v8/namespaces/soans/domains/soainfra UID: 15918057-d911-45b6-9690-95b9b479f38a Spec: Admin Server: Admin Service: Channels: Channel Name: T3Channel Server Pod: Env: Name: USER_MEM_ARGS Value: -Djava.security.egd=file:/dev/./urandom -Xms512m -Xmx1024m Server Start State: RUNNING Clusters: Cluster Name: osb_cluster Replicas: 2 Server Pod: Affinity: Pod Anti Affinity: Preferred During Scheduling Ignored During Execution: Pod Affinity Term: Label Selector: Match Expressions: Key: weblogic.clusterName Operator: In Values: $(CLUSTER_NAME) Topology Key: kubernetes.io/hostname Weight: 100 Server Service: Precreate Service: true Server Start State: RUNNING Cluster Name: soa_cluster Replicas: 2 Server Pod: Affinity: Pod Anti Affinity: Preferred During Scheduling Ignored During Execution: Pod Affinity Term: Label Selector: Match Expressions: Key: weblogic.clusterName Operator: In Values: $(CLUSTER_NAME) Topology Key: kubernetes.io/hostname Weight: 100 Server Service: Precreate Service: true Server Start State: RUNNING Data Home: Domain Home: /u01/oracle/user_projects/domains/soainfra Domain Home Source Type: PersistentVolume Http Access Log In Log Home: true Image: soasuite:12.2.1.4 Image Pull Policy: IfNotPresent Include Server Out In Pod Log: true Log Home: /u01/oracle/user_projects/domains/logs/soainfra Log Home Enabled: true Server Pod: Env: Name: JAVA_OPTIONS Value: -Dweblogic.StdoutDebugEnabled=false -Dweblogic.ssl.Enabled=true -Dweblogic.security.SSL.ignoreHostnameVerification=true Name: USER_MEM_ARGS Value: -Djava.security.egd=file:/dev/./urandom -Xms256m -Xmx1024m Volume Mounts: Mount Path: /u01/oracle/user_projects Name: weblogic-domain-storage-volume Volumes: Name: weblogic-domain-storage-volume Persistent Volume Claim: Claim Name: soainfra-domain-pvc Server Start Policy: IF_NEEDED Web Logic Credentials Secret: Name: soainfra-domain-credentials Status: Clusters: Cluster Name: osb_cluster Maximum Replicas: 5 Minimum Replicas: 0 Ready Replicas: 2 Replicas: 2 Replicas Goal: 2 Cluster Name: soa_cluster Maximum Replicas: 5 Minimum Replicas: 0 Ready Replicas: 2 Replicas: 2 Replicas Goal: 2 Conditions: Last Transition Time: 2020-08-04T10:15:54.705Z Reason: ManagedServersStarting Status: True Type: Progressing Servers: Desired State: RUNNING Node Name: k8sdev Server Name: AdminServer State: UNKNOWN Cluster Name: osb_cluster Desired State: RUNNING Node Name: k8sdev Server Name: osb_server1 State: UNKNOWN Cluster Name: osb_cluster Desired State: RUNNING Node Name: k8sdev Server Name: osb_server2 State: UNKNOWN Cluster Name: osb_cluster Desired State: SHUTDOWN Server Name: osb_server3 Cluster Name: osb_cluster Desired State: SHUTDOWN Server Name: osb_server4 Cluster Name: osb_cluster Desired State: SHUTDOWN Server Name: osb_server5 Cluster Name: soa_cluster Desired State: RUNNING Node Name: k8sdev Server Name: soa_server1 State: UNKNOWN Cluster Name: soa_cluster Desired State: RUNNING Node Name: k8sdev Server Name: soa_server2 State: UNKNOWN Cluster Name: soa_cluster Desired State: SHUTDOWN Server Name: soa_server3 Cluster Name: soa_cluster Desired State: SHUTDOWN Server Name: soa_server4 Cluster Name: soa_cluster Desired State: SHUTDOWN Server Name: soa_server5 Start Time: 2020-08-04T09:24:15.395Z Events: \u0026lt;none\u0026gt;    In the Status section of the output, the available servers and clusters are listed. Note that if this command is issued very soon after the script finishes, there may be no servers available yet, or perhaps only the Administration Server but no Managed Servers. The operator will start up the Administration Server first and wait for it to become ready before starting the Managed Servers.\nVerify the pods Enter the following command to see the pods running the servers:\n$ kubectl get pods -n NAMESPACE Here is an example of the output of this command. You can verify that an Administration Server and two Managed Servers for each cluster (SOA and OSB) are running for soaessosb domain type.\n$ kubectl get pods -n soans NAME READY STATUS RESTARTS AGE soainfra-adminserver 1/1 Running 0 53m soainfra-osb-server1 1/1 Running 0 50m soainfra-osb-server2 1/1 Running 0 50m soainfra-soa-server1 1/1 Running 0 50m soainfra-soa-server2 1/1 Running 0 50m Verify the services Enter the following command to see the services for the domain:\n$ kubectl get services -n NAMESPACE Here is an example of the output of this command. You can verify that services for Administration Server and Managed Servers (for SOA and OSB clusters) are created for soaessosb domain type.\n  Click here to see a sample list of services.   $ kubectl get services -n soans NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE soainfra-adminserver ClusterIP None \u0026lt;none\u0026gt; 30012/TCP,7001/TCP,7002/TCP 54m soainfra-cluster-osb-cluster ClusterIP 10.100.138.57 \u0026lt;none\u0026gt; 9001/TCP,9002/TCP 51m soainfra-cluster-soa-cluster ClusterIP 10.99.117.240 \u0026lt;none\u0026gt; 8001/TCP,8002/TCP 51m soainfra-osb-server1 ClusterIP None \u0026lt;none\u0026gt; 9001/TCP,9002/TCP 51m soainfra-osb-server2 ClusterIP None \u0026lt;none\u0026gt; 9001/TCP,9002/TCP 51m soainfra-osb-server3 ClusterIP 10.108.71.8 \u0026lt;none\u0026gt; 9001/TCP,9002/TCP 51m soainfra-osb-server4 ClusterIP 10.100.1.144 \u0026lt;none\u0026gt; 9001/TCP,9002/TCP 51m soainfra-osb-server5 ClusterIP 10.108.57.147 \u0026lt;none\u0026gt; 9001/TCP,9002/TCP 51m soainfra-soa-server1 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP,8002/TCP 51m soainfra-soa-server2 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP,8002/TCP 51m soainfra-soa-server3 ClusterIP 10.98.160.126 \u0026lt;none\u0026gt; 8001/TCP,8002/TCP 51m soainfra-soa-server4 ClusterIP 10.105.164.133 \u0026lt;none\u0026gt; 8001/TCP,8002/TCP 51m soainfra-soa-server5 ClusterIP 10.109.168.179 \u0026lt;none\u0026gt; 8001/TCP,8002/TCP 51m    "
},
{
	"uri": "/fmw-kubernetes/oud/create-oud-instances/",
	"title": "Create Oracle Unified Directory Instances Using Samples",
	"tags": [],
	"description": "Samples for deploying Oracle Unified Directory instances to a Kubernetes POD.",
	"content": " Introduction Preparing the Environment for Container Creation  Create Kubernetes Namespace Create Secrets for User IDs and Passwords Prepare a Host Directory to be used for Filesystem Based PersistentVolume Create PersistentVolume (PV) and PersistentVolumeClaim (PVC) for your Namespace   Directory Server (instanceType=Directory) Directory Server (instanceType=Directory) as a Kubernetes Service Proxy Server (instanceType=Proxy) as a Kubernetes Service Replication Server (instanceType=Replication) as a Kubernetes Service Directory Server/Service added to existing Replication Server/Service (instanceType=AddDS2RS) Appendix A : Reference  Introduction The Oracle Unified Directory deployment scripts provided in the samples directory of this project demonstrate the creation of different types of Oracle Unified Directory Instances (Directory Service, Proxy, Replication) in containers within a Kubernetes environment.\nNote: The sample files to assist you in creating and configuring your Oracle Unified Directory Kubernetes environment can be found in the project at the following location:\nhttps://github.com/oracle/fmw-kubernetes/tree/master/OracleUnifiedDirectory/kubernetes/samples\nPreparing the Environment for Container Creation In this section you prepare the environment for the Oracle Unified Directory container creation. This involves the following steps:\n Create Kubernetes Namespace Create Secrets for User IDs and Passwords Prepare a host directory to be used for Filesystem based PersistentVolume Create PersistentVolume (PV) and PersistentVolumeClaim (PVC) for your Namespace  Note: Sample files to assist you in creating and configuring your Oracle Unified Directory Kubernetes environment can be found in the project at the following location:\nhttps://github.com/oracle/fmw-kubernetes/tree/master/OracleUnifiedDirectory/kubernetes/samples\nCreate Kubernetes Namespace You should create a Kubernetes namespace to provide a scope for other objects such as pods and services that you create in the environment. To create your namespace you should refer to the oudns.yaml file.\nUpdate the oudns.yaml file and replace %NAMESPACE% with the value of the namespace you would like to create. In the example below the value \u0026lsquo;myoudns\u0026rsquo; is used.\nTo create the namespace apply the file using kubectl:\n$ kubectl apply -f oudns.yaml namespace/myoudns created Confirm that the namespace is created:\n$ kubectl get namespaces NAME STATUS AGE default Active 4d kube-public Active 4d kube-system Active 4d myoudns Active 53s Create Secrets for User IDs and Passwords To protect sensitive information, namely user IDs and passwords, you should create Kubernetes Secrets for the key-value pairs with following keys. The Secret with key-value pairs will be used to pass values to containers created through the Oracle Unified Directory image:\n rootUserDN rootUserPassword adminUID adminPassword bindDN1 bindPassword1 bindDN2 bindPassword2  There are two ways by which a Kubernetes secret object can be created with required key-value pairs.\nUsing samples/secrets.yaml file In this method you update the samples/secrets.yaml file with the value for %SECRET_NAME% and %NAMESPACE%, together with the Base64 value for each secret.\n %rootUserDN% - With Base64 encoded value for rootUserDN parameter. %rootUserPassword% - With Base64 encoded value for rootUserPassword parameter. %adminUID% - With Base64 encoded value for adminUID parameter. %adminPassword% - With Base64 encoded value for adminPassword parameter. %bindDN1% - With Base64 encoded value for bindDN1 parameter. %bindPassword1% - With Base64 encoded value for bindPassword1 parameter. %bindDN2% - With Base64 encoded value for bindDN2 parameter. %bindPassword2% - With Base64 encoded value for bindPassword2 parameter.  Obtain the base64 value for your secrets, for example:\n$ echo -n cn=Directory Manager | base64 Y249RGlyZWN0b3J5IE1hbmFnZXI= $ echo -n Oracle123 | base64 T3JhY2xlMTIz $ echo -n admin | base64 YWRtaW4= Note: Ensure that you use the -n parameter with the echo command. If the parameter is omitted Base64 values willbe generated with a new-line character included.\nUpdate the secrets.yaml file with your values. It should look similar to the file shown below:\napiVersion: v1 kind: Secret metadata: name: oudsecret namespace: myoudns type: Opaque data: rootUserDN: Y249RGlyZWN0b3J5IE1hbmFnZXI= rootUserPassword: T3JhY2xlMTIz adminUID: YWRtaW4= adminPassword: T3JhY2xlMTIz bindDN1: Y249RGlyZWN0b3J5IE1hbmFnZXI= bindPassword1: T3JhY2xlMTIz bindDN2: Y249RGlyZWN0b3J5IE1hbmFnZXI= bindPassword2: T3JhY2xlMTIz Apply the file:\n$ kubectl apply -f secrets.yaml secret/oudsecret created Verify that the secret has been created:\n$ kubectl --namespace myoudns get secret NAME TYPE DATA AGE default-token-fztcb kubernetes.io/service-account-token 3 15m oudsecret Opaque 8 99s Using kubectl create secret command The Kubernetes secret can be created using the command line with the following syntax:\n$ kubectl --namespace %NAMESPACE% create secret generic %SECRET_NAME% \\ --from-literal=rootUserDN=\u0026quot;%rootUserDN%\u0026quot; \\ --from-literal=rootUserPassword=\u0026quot;%rootUserPassword%\u0026quot; \\ --from-literal=adminUID=\u0026quot;%adminUID%\u0026quot; \\ --from-literal=adminPassword=\u0026quot;%adminPassword%\u0026quot; \\ --from-literal=bindDN1=\u0026quot;%bindDN1%\u0026quot; \\ --from-literal=bindPassword1=\u0026quot;%bindPassword1%\u0026quot; \\ --from-literal=bindDN2=\u0026quot;%bindDN2%\u0026quot; \\ --from-literal=bindPassword2=\u0026quot;%bindPassword2%\u0026quot; Update the following placeholders in the command with the relevant value:\n %NAMESPACE% - With name of namespace in which secret is required to be created %SECRET_NAME% - Name for the secret object %rootUserDN% - With Base64 encoded value for rootUserDN parameter. %rootUserPassword% - With Base64 encoded value for rootUserPassword parameter. %adminUID% - With Base64 encoded value for adminUID parameter. %adminPassword% - With Base64 encoded value for adminPassword parameter. %bindDN1% - With Base64 encoded value for bindDN1 parameter. %bindPassword1% - With Base64 encoded value for bindPassword1 parameter. %bindDN2% - With Base64 encoded value for bindDN2 parameter. %bindPassword2% - With Base64 encoded value for bindPassword2 parameter.  After executing the kubectl create secret command, verify that the secret has been created:\n$ kubectl --namespace myoudns get secret NAME TYPE DATA AGE default-token-fztcb kubernetes.io/service-account-token 3 15m oudsecret Opaque 8 99s Prepare a Host Directory to be used for Filesystem Based PersistentVolume It is required to prepare a directory on the Host filesystem to store Oracle Unified Directory Instances and other configuration outside the container filesystem. That directory from the Host filesystem will be associated with a PersistentVolume.\nIn the case of a multi-node Kubernetes cluster, the Host directory to be associated with the PersistentVolume should be accessible on all the nodes at the same path.\nTo prepare a Host directory (for example: /scratch/user_projects) for mounting as a file system based PersistentVolume inside your containers, execute the command below on your Host:\n The userid can be anything but it must have uid:guid as 1000:1000, which is the same as the \u0026lsquo;oracle\u0026rsquo; user running in the container. This ensures the \u0026lsquo;oracle\u0026rsquo; user has access to the shared volume/directory.\n $ sudo su - root $ mkdir -p /scratch/user_projects $ chown 1000:1000 /scratch/user_projects $ exit All container operations are performed as the oracle user.\nNote: If a user already exists with -u 1000 -g 1000 then use the same user. Else modify the existing user to have uid-gid as '-u 1000 -g 1000\u0026rsquo;\nCreate PersistentVolume (PV) and PersistentVolumeClaim (PVC) for your Namespace A PersistentVolume (PV) is a storage resource, while a PersistentVolumeClaim (PVC) is a request for that resource. To provide storage for your namespace, update the persistent-volume.yaml file.\nUpdate the following to values specific to your environment:\n   Param Value Example     %PV_NAME% PV name oudpv   %PV_HOST_PATH% Valid path on localhost /scratch/user_projects   %PVC_NAME% PVC name oudpvc   %NAMESPACE% Namespace myoudns    Apply the file:\n$ kubectl apply -f persistent-volume.yaml persistentvolume/oudpv created persistentvolumeclaim/oudpvc created Verify the PersistentVolume:\n$ kubectl --namespace myoudns describe persistentvolume oudpv Name: oudpv Labels: type=oud-pv Annotations: pv.kubernetes.io/bound-by-controller: yes Finalizers: [kubernetes.io/pv-protection] StorageClass: manual Status: Bound Claim: myoudns/oudpvc Reclaim Policy: Retain Access Modes: RWX VolumeMode: Filesystem Capacity: 10Gi Node Affinity: \u0026lt;none\u0026gt; Message: Source: Type: HostPath (bare host directory volume) Path: /scratch/user_projects HostPathType: Events: \u0026lt;none\u0026gt; Verify the PersistentVolumeClaim:\n$ kubectl --namespace myoudns describe pvc oudpvc Name: oudpvc Namespace: myoudns StorageClass: manual Status: Bound Volume: oudpv Labels: \u0026lt;none\u0026gt; Annotations: pv.kubernetes.io/bind-completed: yes pv.kubernetes.io/bound-by-controller: yes Finalizers: [kubernetes.io/pvc-protection] Capacity: 10Gi Access Modes: RWX VolumeMode: Filesystem Mounted By: \u0026lt;none\u0026gt; Events: \u0026lt;none\u0026gt; Directory Server (instanceType=Directory) In this example you create a POD (oudpod1) which comprises a single container based on an Oracle Unified Directory 12c PS4 (12.2.1.4.0) image.\nTo create the POD update the oud-dir-pod.yaml file.\nUpdate the following to values specific to your environment:\n   Param Value Example     %NAMESPACE% Namespace myoudns   %IMAGE% Oracle image tag oracle/oud:12.2.1.4.0   %SECRET_NAME% Secret name oudsecret   %PV_NAME% PV name oudpv   %PVC_NAME% PVC name oudpvc    Apply the file:\n$ kubectl apply -f oud-dir-pod.yaml pod/oudpod1 created To check the status of the created pod:\n$ kubectl get pods -n myoudns NAME READY STATUS RESTARTS AGE oudpod1 1/1 Running 0 14m If you see any errors then use the following commands to debug the pod/container.\nTo review issues with the pod e.g. CreateContainerConfigError:\n$ kubectl --namespace \u0026lt;namespace\u0026gt; describe pod \u0026lt;pod\u0026gt; For example:\n$ kubectl --namespace myoudns describe pod oudpod1 To tail the container logs while it is initialising use the following command:\n$ kubectl --namespace \u0026lt;namespace\u0026gt; logs -f -c \u0026lt;container\u0026gt; \u0026lt;pod\u0026gt; For example:\n$ kubectl --namespace myoudns logs -f -c oudds1 oudpod1 To view the full container logs:\n$ kubectl --namespace \u0026lt;namespace\u0026gt; logs -c \u0026lt;container\u0026gt; \u0026lt;pod\u0026gt; To validate that the Oracle Unified Directory directory server instance is running, connect to the container:\n$ kubectl --namespace myoudns exec -it -c oudds1 oudpod1 /bin/bash In the container, run ldapsearch to return entries from the directory server:\n$ cd /u01/oracle/user_projects/oudpod1/OUD/bin $ ./ldapsearch -h localhost -p 1389 -D \u0026quot;cn=Directory Manager\u0026quot; -w Oracle123 -b \u0026quot;\u0026quot; -s sub \u0026quot;(objectclass=*)\u0026quot; dn dn: dc=example1,dc=com dn: ou=People,dc=example1,dc=com dn: uid=user.0,ou=People,dc=example1,dc=com ... dn: uid=user.99,ou=People,dc=example1,dc=com Directory Server (instanceType=Directory) as a Kubernetes Service In this example you will create two pods and 2 associated containers, both running Oracle Unified Directory 12c Directory Server instances. This demonstrates how you can expose Oracle Unified Directory 12c as a network service. This provides a way of abstracting access to the backend service independent of the pod details.\nTo create the POD update the oud-dir-svc.yaml file.\nUpdate the following to values specific to your environment:\n   Param Value Example     %NAMESPACE% Namespace myoudns   %IMAGE% Oracle image tag oracle/oud:12.2.1.4.0   %SECRET_NAME% Secret name oudsecret   %PV_NAME% PV name oudpv   %PVC_NAME% PVC name oudpvc    Apply the file:\n$ kubectl apply -f oud-dir-svc.yaml service/oud-dir-svc-1 created pod/oud-dir1 created service/oud-dir-svc-2 created pod/oud-dir2 created To check the status of the created pods (oud-dir1 and oud-dir2) and services (oud-dir-svc-1 and oud-dir-svc-2):\n$ kubectl --namespace myoudns get all NAME READY STATUS RESTARTS AGE pod/oud-dir1 1/1 Running 0 28m pod/oud-dir2 1/1 Running 0 28m pod/oudpod1 1/1 Running 0 22h NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/oud-dir-svc-1 NodePort 10.107.171.235 \u0026lt;none\u0026gt; 1444:30616/TCP,1888:32605/TCP,1389:31405/TCP,1636:32544/TCP,1080:31509/TCP,1081:32395/TCP,1898:31116/TCP 28m service/oud-dir-svc-2 NodePort 10.106.206.229 \u0026lt;none\u0026gt; 1444:30882/TCP,1888:30427/TCP,1389:31299/TCP,1636:31529/TCP,1080:30056/TCP,1081:30458/TCP,1898:31796/TCP 28m From this example you can see that the following service port mappings are available to access the container:\n service/oud-dir-svc-1 : 10.107.171.235 : 1389:31405 service/oud-dir-svc-2 : 10.106.206.229 : 1389:31299  To access the Oracle Unified Directory directory server running in pod/oud-dir1 via the LDAP port 1389 you would use the service port : 31405.\nTo access the Oracle Unified Directory directory server running in pod/oud-dir2 via the LDAP port 1389 you would use the service port : 31299.\nFor example:\nNote: use the ldapsearch from the Oracle Unified Directory ORACLE_HOME when accessing the cluster externally.\n$ ldapsearch -h $HOSTNAME -p 31405 -D \u0026quot;cn=Directory Manager\u0026quot; -w Oracle123 -b \u0026quot;\u0026quot; -s sub \u0026quot;(objectclass=*)\u0026quot; dn dn: dc=example1,dc=com dn: ou=People,dc=example1,dc=com dn: uid=user.0,ou=People,dc=example1,dc=com ... dn: uid=user.98,ou=People,dc=example1,dc=com dn: uid=user.99,ou=People,dc=example1,dc=com $ ldapsearch -h $HOSTNAME -p 31299 -D \u0026quot;cn=Directory Manager\u0026quot; -w Oracle123 -b \u0026quot;\u0026quot; -s sub \u0026quot;(objectclass=*)\u0026quot; dn dn: dc=example2,dc=com dn: ou=People,dc=example2,dc=com dn: uid=user.0,ou=People,dc=example2,dc=com ... dn: uid=user.98,ou=People,dc=example2,dc=com dn: uid=user.99,ou=People,dc=example2,dc=com Validation It is possible to access the Oracle Unified Directory instances and the data within externally from the cluster, using commands like curl commands. In this way you can access interfaces exposed through NodePort. In the example below, two services (service/oud-dir-svc-1 and service/oud-dir-svc-2) expose a set of ports. The following curl commands can be executed against the ports exposed through each service.\nCurl command example for Oracle Unified Directory Admin REST: curl --noproxy \u0026quot;*\u0026quot; --insecure --location --request GET \\ 'https://\u0026lt;HOSTNAME\u0026gt;:\u0026lt;AdminHttps NodePort mapped to 1888\u0026gt;/rest/v1/admin/?scope=base\u0026amp;attributes=%2b' \\ --header 'Content-Type: application/json' \\ --header 'Authorization: Basic Y249RGlyZWN0b3J5IE1hbmFnZXI6T3JhY2xlMTIz' Curl command example for Oracle Unified Directory Data REST : curl --noproxy \u0026quot;*\u0026quot; --insecure --location --request GET \\ 'https://\u0026lt;HOSTNAME\u0026gt;:\u0026lt;Https NodePort mapped to 1081\u0026gt;/rest/v1/directory/?scope=base\u0026amp;attributes=%2b' \\ --header 'Authorization: Basic Y249RGlyZWN0b3J5IE1hbmFnZXI6T3JhY2xlMTIz' Curl command example for Oracle Unified Directory Data SCIM: curl --noproxy \u0026quot;*\u0026quot; --insecure --location --request GET \\ 'https://\u0026lt;HOSTNAME\u0026gt;:\u0026lt;Https NodePort mapped to 1081\u0026gt;/iam/directory/oud/scim/v1/Schemas/urn:ietf:params:scim:schemas:core:2.0:Schema' \\ --header 'Authorization: Basic Y249RGlyZWN0b3J5IE1hbmFnZXI6T3JhY2xlMTIz' Proxy Server (instanceType=Proxy) as a Kubernetes Service In this example you will create a service, pod and associated container, in which an Oracle Unified Directory 12c Proxy Server instance is deployed. This acts as a proxy to the 2 services you created in the previous example.\nTo create the POD update the oud-ds_proxy-svc.yaml file.\nUpdate the following to values specific to your environment:\n   Param Value Example     %NAMESPACE% Namespace myoudns   %IMAGE% Oracle image tag oracle/oud:12.2.1.4.0   %SECRET_NAME% Secret name oudsecret   %PV_NAME% PV name oudpv   %PVC_NAME% PVC name oudpvc    Apply the file:\n$ kubectl apply -f oud-ds_proxy-svc.yaml service/oud-ds-proxy-svc created pod/oudp1 created Check the status of the new pod/service:\n$ kubectl --namespace myoudns get all NAME READY STATUS RESTARTS AGE pod/oud-dir1 1/1 Running 0 166m pod/oud-dir2 1/1 Running 0 166m pod/oudp1 1/1 Running 0 20m pod/oudpod1 1/1 Running 0 25h NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/oud-dir-svc-1 NodePort 10.107.171.235 \u0026lt;none\u0026gt; 1444:30616/TCP,1888:32605/TCP,1389:31405/TCP,1636:32544/TCP,1080:31509/TCP,1081:32395/TCP,1898:31116/TCP 166m service/oud-dir-svc-2 NodePort 10.106.206.229 \u0026lt;none\u0026gt; 1444:30882/TCP,1888:30427/TCP,1389:31299/TCP,1636:31529/TCP,1080:30056/TCP,1081:30458/TCP,1898:31796/TCP 166m service/oud-ds-proxy-svc NodePort 10.103.41.171 \u0026lt;none\u0026gt; 1444:30878/TCP,1888:30847/TCP,1389:31810/TCP,1636:30873/TCP,1080:32076/TCP,1081:30762/TCP,1898:31269/TCP 20m Verify operation of the proxy server, accessing through the external service port:\n$ ldapsearch -h $HOSTNAME -p 31810 -D \u0026quot;cn=Directory Manager\u0026quot; -w Oracle123 -b \u0026quot;\u0026quot; -s sub \u0026quot;(objectclass=*)\u0026quot; dn dn: dc=example1,dc=com dn: ou=People,dc=example1,dc=com dn: uid=user.0,ou=People,dc=example1,dc=com ... dn: uid=user.99,ou=People,dc=example1,dc=com dn: dc=example2,dc=com dn: ou=People,dc=example2,dc=com dn: uid=user.0,ou=People,dc=example2,dc=com ... dn: uid=user.98,ou=People,dc=example2,dc=com dn: uid=user.99,ou=People,dc=example2,dc=com Note: Entries are returned from both backend directory servers (dc=example1,dc=com and dc=example2,dc=com) via the proxy server.\nReplication Server (instanceType=Replication) as a Kubernetes Service In this example you will create a service, pod and associated container, in which an Oracle Unified Directory 12c Replication Server instance is deployed. This creates a single Replication Server which has 2 Directory Servers as its replication group. This example extends the Oracle Unified Directory instances created as part of Directory Server (instanceType=Directory) as a Kubernetes Service.\nTo create the POD update the oud-ds_rs_ds-svc.yaml file.\nUpdate the following to values specific to your environment:\n   Param Value Example     %NAMESPACE% Namespace myoudns   %IMAGE% Oracle image tag oracle/oud:12.2.1.4.0   %SECRET_NAME% Secret name oudsecret   %PV_NAME% PV name oudpv   %PVC_NAME% PVC name oudpvc    Apply the file:\n$ kubectl apply -f oud-ds_rs_ds-svc.yaml service/oud-rs-svc-1 created pod/oudpodrs1 created service/oud-ds-svc-1a created pod/oudpodds1a created service/oud-ds-svc-1b created pod/oudpodds1b created Check the status of the new services:\n$ kubectl --namespace myoudns get all NAME READY STATUS RESTARTS AGE pod/oud-dir1 1/1 Running 0 2d20h pod/oud-dir2 1/1 Running 0 2d20h pod/oudp1 1/1 Running 0 2d18h pod/oudpod1 1/1 Running 0 3d18h pod/oudpodds1a 0/1 Running 0 2m44s pod/oudpodds1b 0/1 Running 0 2m44s pod/oudpodrs1 0/1 Running 0 2m45s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/oud-dir-svc-1 NodePort 10.107.171.235 \u0026lt;none\u0026gt; 1444:30616/TCP,1888:32605/TCP,1389:31405/TCP,1636:32544/TCP,1080:31509/TCP,1081:32395/TCP,1898:31116/TCP 2d20h service/oud-dir-svc-2 NodePort 10.106.206.229 \u0026lt;none\u0026gt; 1444:30882/TCP,1888:30427/TCP,1389:31299/TCP,1636:31529/TCP,1080:30056/TCP,1081:30458/TCP,1898:31796/TCP 2d20h service/oud-ds-proxy-svc NodePort 10.103.41.171 \u0026lt;none\u0026gt; 1444:30878/TCP,1888:30847/TCP,1389:31810/TCP,1636:30873/TCP,1080:32076/TCP,1081:30762/TCP,1898:31269/TCP 2d18h service/oud-ds-svc-1a NodePort 10.102.218.25 \u0026lt;none\u0026gt; 1444:30347/TCP,1888:30392/TCP,1389:32482/TCP,1636:31161/TCP,1080:31241/TCP,1081:32597/TCP 2m45s service/oud-ds-svc-1b NodePort 10.104.6.215 \u0026lt;none\u0026gt; 1444:32031/TCP,1888:31621/TCP,1389:32511/TCP,1636:31698/TCP,1080:30737/TCP,1081:30748/TCP 2m44s service/oud-rs-svc-1 NodePort 10.110.237.193 \u0026lt;none\u0026gt; 1444:32685/TCP,1888:30176/TCP,1898:30543/TCP 2m45s Validation To validate that the Oracle Unified Directory replication group is running, connect to the replication server container (oudrs1):\n$ kubectl --namespace myoudns exec -it -c oudrs1 oudpodrs1 /bin/bash $ cd /u01/oracle/user_projects/oudpodrs1/OUD/bin In the container, run dsreplication to return details of the replication group:\n$ ./dsreplication status --trustAll --hostname localhost --port 1444 --adminUID admin --dataToDisplay compat-view --dataToDisplay rs-connections \u0026gt;\u0026gt;\u0026gt;\u0026gt; Specify Oracle Unified Directory LDAP connection parameters Password for user 'admin': Establishing connections and reading configuration ..... Done. dc=example1,dc=com - Replication Enabled ======================================== Server : Entries : M.C. [1] : A.O.M.C. [2] : Port [3] : Encryption [4] : Trust [5] : U.C. [6] : Status [7] : ChangeLog [8] : Group ID [9] : Connected To [10] --------------------:----------:----------:--------------:----------:----------------:-----------:----------:------------:---------------:--------------:--------------------------- oud-rs-svc-1:1444 : -- [11] : 0 : -- : 1898 : Disabled : -- : -- : Up : -- : 1 : -- oud-ds-svc-1a:1444 : 1 : 0 : 0 : -- [12] : Disabled : Trusted : -- : Normal : Enabled : 1 : oud-rs-svc-1:1898 (GID=1) oud-ds-svc-1b:1444 : 1 : 0 : 0 : -- [12] : Disabled : Trusted : -- : Normal : Enabled : 1 : oud-rs-svc-1:1898 (GID=1) You can see that the Replication Server is running as the oud-rs-svc-1:1444, while you have Directory Server services running on oud-ds-svc-1a:1444 and oud-ds-svc-1b:1444.\nFrom outside the cluster, you can invoke curl commands, as shown in the following examples, to access interfaces exposed through NodePort. In this example, there are two Directory services (service/oud-ds-svc-1a and service/oud-ds-svc-1b) exposing a set of ports. The following curl commands can be executed against ports exposed through each service.\nCurl command example for Oracle Unified Directory Admin REST: curl --noproxy \u0026quot;*\u0026quot; --insecure --location --request GET \\ 'https://\u0026lt;HOSTNAME\u0026gt;:\u0026lt;AdminHttps NodePort mapped to 1888\u0026gt;/rest/v1/admin/?scope=base\u0026amp;attributes=%2b' \\ --header 'Content-Type: application/json' \\ --header 'Authorization: Basic Y249RGlyZWN0b3J5IE1hbmFnZXI6T3JhY2xlMTIz' Note: This can be executed against the replication service (oud-rs-svc-1) as well.\nCurl command example for Oracle Unified Directory Data REST : curl --noproxy \u0026quot;*\u0026quot; --insecure --location --request GET \\ 'https://\u0026lt;HOSTNAME\u0026gt;:\u0026lt;Https NodePort mapped to 1081\u0026gt;/rest/v1/directory/?scope=base\u0026amp;attributes=%2b' \\ --header 'Authorization: Basic Y249RGlyZWN0b3J5IE1hbmFnZXI6T3JhY2xlMTIz' Curl command example for Oracle Unified Directory Data SCIM: curl --noproxy \u0026quot;*\u0026quot; --insecure --location --request GET \\ 'https://\u0026lt;HOSTNAME\u0026gt;:\u0026lt;Https NodePort mapped to 1081\u0026gt;/iam/directory/oud/scim/v1/Schemas/urn:ietf:params:scim:schemas:core:2.0:Schema' \\ --header 'Authorization: Basic Y249RGlyZWN0b3J5IE1hbmFnZXI6T3JhY2xlMTIz' Directory Server/Service added to existing Replication Server/Service (instanceType=AddDS2RS) In this example you will create services, pods and containers, in which Oracle Unified Directory 12c Replication Server instances are deployed. In this case 2 Replication/Directory Server Services are added, in addition the Directory Server created in Directory Server (instanceType=Directory) as a Kubernetes Service (oud-dir-svc-2) is added to the replication group.\nTo create the POD update the oud-ds-plus-rs-svc.yaml file.\nUpdate the following to values specific to your environment:\n   Param Value Example     %NAMESPACE% Namespace myoudns   %IMAGE% Oracle image tag oracle/oud:12.2.1.4.0   %SECRET_NAME% Secret name oudsecret   %PV_NAME% PV name oudpv   %PVC_NAME% PVC name oudpvc    Apply the file:\n$ kubectl apply -f oud-ds-plus-rs-svc.yaml service/oud-dsrs-svc-1 created pod/ouddsrs1 created service/oud-dsrs-svc-2 created pod/ouddsrs2 created Check the status of the new services:\n$ kubectl --namespace myoudns get all NAME READY STATUS RESTARTS AGE pod/oud-dir1 1/1 Running 0 3d pod/oud-dir2 1/1 Running 0 3d pod/ouddsrs1 0/1 Running 0 75s pod/ouddsrs2 0/1 Running 0 75s pod/oudp1 1/1 Running 0 2d21h pod/oudpod1 1/1 Running 0 3d22h pod/oudpodds1a 1/1 Running 0 3h33m pod/oudpodds1b 1/1 Running 0 3h33m pod/oudpodrs1 1/1 Running 0 3h33m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/oud-dir-svc-1 NodePort 10.107.171.235 \u0026lt;none\u0026gt; 1444:30616/TCP,1888:32605/TCP,1389:31405/TCP,1636:32544/TCP,1080:31509/TCP,1081:32395/TCP,1898:31116/TCP 3d service/oud-dir-svc-2 NodePort 10.106.206.229 \u0026lt;none\u0026gt; 1444:30882/TCP,1888:30427/TCP,1389:31299/TCP,1636:31529/TCP,1080:30056/TCP,1081:30458/TCP,1898:31796/TCP 3d service/oud-ds-proxy-svc NodePort 10.103.41.171 \u0026lt;none\u0026gt; 1444:30878/TCP,1888:30847/TCP,1389:31810/TCP,1636:30873/TCP,1080:32076/TCP,1081:30762/TCP,1898:31269/TCP 2d21h service/oud-ds-svc-1a NodePort 10.102.218.25 \u0026lt;none\u0026gt; 1444:30347/TCP,1888:30392/TCP,1389:32482/TCP,1636:31161/TCP,1080:31241/TCP,1081:32597/TCP 3h33m service/oud-ds-svc-1b NodePort 10.104.6.215 \u0026lt;none\u0026gt; 1444:32031/TCP,1888:31621/TCP,1389:32511/TCP,1636:31698/TCP,1080:30737/TCP,1081:30748/TCP 3h33m service/oud-dsrs-svc-1 NodePort 10.102.118.29 \u0026lt;none\u0026gt; 1444:30738/TCP,1888:30935/TCP,1389:32438/TCP,1636:32109/TCP,1080:31776/TCP,1081:31897/TCP,1898:30874/TCP 75s service/oud-dsrs-svc-2 NodePort 10.98.139.53 \u0026lt;none\u0026gt; 1444:32312/TCP,1888:30595/TCP,1389:31376/TCP,1636:30090/TCP,1080:31238/TCP,1081:31174/TCP,1898:31863/TCP 75s service/oud-rs-svc-1 NodePort 10.110.237.193 \u0026lt;none\u0026gt; 1444:32685/TCP,1888:30176/TCP,1898:30543/TCP 3h33m Validation To validate that the Oracle Unified Directory replication group is running, connect to the replication server container (oudrs1):\n$ kubectl --namespace myoudns exec -it -c ouddsrs ouddsrs1 /bin/bash $ cd /u01/oracle/user_projects/ouddsrs1/OUD/bin In the container, run dsreplication to return details of the replication group:\n$ ./dsreplication status --trustAll --hostname localhost --port 1444 --adminUID admin --dataToDisplay compat-view --dataToDisplay rs-connections \u0026gt;\u0026gt;\u0026gt;\u0026gt; Specify Oracle Unified Directory LDAP connection parameters Password for user 'admin': Establishing connections and reading configuration ..... Done. dc=example2,dc=com - Replication Enabled ======================================== Server : Entries : M.C. [1] : A.O.M.C. [2] : Port [3] : Encryption [4] : Trust [5] : U.C. [6] : Status [7] : ChangeLog [8] : Group ID [9] : Connected To [10] --------------------:---------:----------:--------------:----------:----------------:-----------:----------:------------:---------------:--------------:----------------------------- oud-dir-svc-2:1444 : 102 : 0 : 0 : 1898 : Disabled : Trusted : -- : Normal : Enabled : 1 : oud-dir-svc-2:1898 (GID=1) oud-dsrs-svc-1:1444 : 102 : 0 : 0 : 1898 : Disabled : Trusted : -- : Normal : Enabled : 2 : oud-dsrs-svc-1:1898 (GID=2) oud-dsrs-svc-2:1444 : 102 : 0 : 0 : 1898 : Disabled : Trusted : -- : Normal : Enabled : 2 : oud-dsrs-svc-2:1898 (GID=2) Replication Server [11] : RS #1 : RS #2 : RS #3 --------------------------:-------:-------:------- oud-dir-svc-2:1898 (#1) : -- : Yes : Yes oud-dsrs-svc-1:1898 (#2) : Yes : -- : Yes oud-dsrs-svc-2:1898 (#3) : Yes : Yes : -- From outside the cluster, you can invoke curl commands like following for accessing interfaces exposed through NodePort. In this example, there are two services (service/oud-dsrs-svc-1 and service/oud-dsrs-svc-2) exposing set of ports. Following curl commands can be executed against ports exposed through each service.\nCurl command example for Oracle Unified Directory Admin REST: curl --noproxy \u0026quot;*\u0026quot; --insecure --location --request GET \\ 'https://\u0026lt;HOSTNAME\u0026gt;:\u0026lt;AdminHttps NodePort mapped to 1888\u0026gt;/rest/v1/admin/?scope=base\u0026amp;attributes=%2b' \\ --header 'Content-Type: application/json' \\ --header 'Authorization: Basic Y249RGlyZWN0b3J5IE1hbmFnZXI6T3JhY2xlMTIz' Curl command example for Oracle Unified Directory Data REST : curl --noproxy \u0026quot;*\u0026quot; --insecure --location --request GET \\ 'https://\u0026lt;HOSTNAME\u0026gt;:\u0026lt;Https NodePort mapped to 1081\u0026gt;/rest/v1/directory/?scope=base\u0026amp;attributes=%2b' \\ --header 'Authorization: Basic Y249RGlyZWN0b3J5IE1hbmFnZXI6T3JhY2xlMTIz' Curl command example for Oracle Unified Directory Data SCIM: curl --noproxy \u0026quot;*\u0026quot; --insecure --location --request GET \\ 'https://\u0026lt;HOSTNAME\u0026gt;:\u0026lt;Https NodePort mapped to 1081\u0026gt;/iam/directory/oud/scim/v1/Schemas/urn:ietf:params:scim:schemas:core:2.0:Schema' \\ --header 'Authorization: Basic Y249RGlyZWN0b3J5IE1hbmFnZXI6T3JhY2xlMTIz' Appendix A : Reference Before using these sample yaml files, the following variables must be updated:\n %NAMESPACE% - with value for Kubernetes namespace of your choice %IMAGE% - with exact docker image for oracle/oud:12.2.1.x.x %PV_NAME% - with value of the persistent volume name of your choice %PV_HOST_PATH% - with value of the persistent volume Host Path (Directory Path which would be used as storage path for volume) %PVC_NAME% - with value of the persistent volume claim name of your choice %SECRET_NAME% - with value of the secret name which can be created using secrets.yaml file. %rootUserDN% - With Base64 encoded value for rootUserDN parameter. %rootUserPassword% - With Base64 encoded value for rootUserPassword parameter. %adminUID% - With Base64 encoded value for adminUID parameter. %adminPassword% - With Base64 encoded value for adminPassword parameter. %bindDN1% - With Base64 encoded value for bindDN1 parameter. %bindPassword1% - With Base64 encoded value for bindPassword1 parameter. %bindDN2% - With Base64 encoded value for bindDN2 parameter. %bindPassword2% - With Base64 encoded value for bindPassword2 parameter.  oudns.yaml This is a sample file to create a Kubernetes namespace.\npersistent-volume.yaml This is a sample file to create Persistent Volume and Persistent Volume Claim\nsecrets.yaml This is a sample file to create the Kubernetes secrets which can be used to substitute values during Pod creation.\nThe keys below will be honoured by the different Oracle Unified Directory yaml files\n rootUserDN rootUserPassword adminUID adminPassword bindDN1 bindPassword1 bindDN2 bindPassword2  All the values of the keys should be encoded using the command below and the encoded value should be used in the secrets.yaml file.\nTo generate an encoded value for keys in Base64 format, execute the following command:\n$ echo -n 'MyPassword' | base64 TXlQYXNzd29yZA== oud-dir-pod.yaml This is a sample file to create POD (oudpod1) and a container for an Oracle Unified Directory Directory Instance.\noud-ds_proxy-svc.yaml This is a sample file to create:\n POD (oudds1) with container for Oracle Unified Directory Directory Instance (dc=example1,dc=com) POD (oudds2) with container for Oracle Unified Directory Directory Instance (dc=example2,dc=com) POD (oudp1) with container for Oracle Unified Directory Directory Proxy referring to Oracle Unified Directory Directory Instances (oudds1 and oudds2) for dc=example1,dc=com and dc=example2,dc=com Service (oud-ds-proxy-svc) referring to POD with Oracle Unified Directory Directory Proxy (oudp1)  oud-ds_rs_ds-svc.yaml This is a sample file to create:\n POD (oudpodds1) with container for Oracle Unified Directory Directory Instance (dc=example1,dc=com) POD (oudpodrs1) with container for Oracle Unified Directory Replication Server Instance connected to Oracle Unified Directory Directory Instance (oudpodds1) POD (oudpodds1a) with container for Oracle Unified Directory Directory Instance having replication enabled through Replication Server Instance (oudpodrs1) POD (oudpodds1b) with container for Oracle Unified Directory Directory Instance having replication enabled through Replication Server Instance (oudpodrs1) Service (oud-ds-rs-ds-svc) referring to all PODs  The following command can be executed in the container to check the status of the replicated instances:\n$ /u01/oracle/user_projects/oudpodrs1/OUD/bin/dsreplication status \\ --trustAll --hostname oudpodrs1.oud-ds-rs-ds-svc.myoudns.svc.cluster.local --port 1444 \\ --dataToDisplay compat-view oud-ds-plus-rs-svc.yaml This is a sample file to create 3 replicated DS+RS Instances:\n POD (ouddsrs1) with container for Oracle Unified Directory Directory Server (dc=example1,dc=com) and Replication Server POD (ouddsrs2) with container for Oracle Unified Directory Directory Server (dc=example1,dc=com) and Replication Server Service (oud-dsrs-svc) referring to all PODs  The following command can be executed in the container to check the status of the replicated instances:\n$ /u01/oracle/user_projects/ouddsrs1/OUD/bin/dsreplication status \\ --trustAll --hostname ouddsrs1.oud-dsrs-svc.myoudns.svc.cluster.local --port 1444 \\ --dataToDisplay compat-view "
},
{
	"uri": "/fmw-kubernetes/oudsm/create-oudsm-instances/",
	"title": "Create Oracle Unified Directory Services Manager Instances Using Samples",
	"tags": [],
	"description": "Samples for deploying Oracle Unified Directory Services Manager instances to a Kubernetes POD.",
	"content": " Introduction Preparing the Environment for Container Creation  Create Kubernetes Namespace Create Secrets for User IDs and Passwords Prepare a Host Directory to be used for Filesystem Based PersistentVolume Create PersistentVolume (PV) and PersistentVolumeClaim (PVC) for your Namespace   Oracle Unified Directory Services Manager POD Oracle Unified Directory Services Manager Deployment  Introduction The Oracle Unified Directory Services Manager deployment scripts provided in the code repository demonstrate how to deploy Oracle Unified Directory Services Manager in containers within a Kubernetes environment.\nNote: The sample files to assist you in creating and configuring your Oracle Unified Directory Services Manager Kubernetes environment can be found in the project at the following location:\n\u0026lt;work directory\u0026gt;/fmw-kubernetes/OracleUnifiedDirectorySM/kubernetes/samples\nPreparing the Environment for Container Creation In this section you prepare the environment for the Oracle Unified Directory Services Manager container creation. This involves the following steps:\n Create Kubernetes Namespace Create Secrets for User IDs and Passwords Prepare a host directory to be used for Filesystem based PersistentVolume Create PersistentVolume (PV) and PersistentVolumeClaim (PVC) for your Namespace  Create Kubernetes Namespace You should create a Kubernetes namespace to provide a scope for other objects such as pods and services that you create in the environment. To create your namespace you should refer to the oudsmns.yaml file.\nUpdate the oudsmns.yaml file and replace %NAMESPACE% with the value of the namespace you would like to create. In the example below the value \u0026lsquo;myoudsmns\u0026rsquo; is used.\nTo create the namespace apply the file using kubectl:\n$ kubectl apply -f oudsmns.yaml namespace/myoudsmns created Confirm that the namespace is created:\n$ kubectl get namespaces NAME STATUS AGE default Active 4d kube-public Active 4d kube-system Active 4d myoudsmns Active 53s Create Secrets for User IDs and Passwords To protect sensitive information, namely user IDs and passwords, you should create Kubernetes Secrets for the key-value pairs with following keys. The Secret with key-value pairs will be used to pass values to containers created through the OUD image:\n adminUser adminPass  There are two ways by which a Kubernetes secret object can be created with required key-value pairs.\nUsing samples/secrets.yaml file In this method you update the samples/secrets.yaml file with the value for %SECRET_NAME% and %NAMESPACE%, together with the Base64 value for each secret.\n %adminUser% - With Base64 encoded value for adminUser parameter. %adminPass% - With Base64 encoded value for adminPass parameter.  Obtain the base64 value for your secrets, for example:\n$ echo -n weblogic | base64 d2VibG9naWM= $ echo -n Oracle123 | base64 T3JhY2xlMTIz Note: Ensure that you use the -n parameter with the echo command. If the parameter is omitted Base64 values will be generated with a new-line character included.\nUpdate the secrets.yaml file with your values. It should look similar to the file shown below:\napiVersion: v1 kind: Secret metadata: name: oudsmsecret namespace: myoudsmns type: Opaque data: adminUser: d2VibG9naWM= adminPass: T3JhY2xlMTIz Apply the file:\n$ kubectl apply -f secrets.yaml secret/oudsmsecret created Verify that the secret has been created:\n$ kubectl --namespace myoudsmns get secret NAME TYPE DATA AGE default-token-fztcb kubernetes.io/service-account-token 3 15m oudsmsecret Opaque 8 99s Using kubectl create secret command The Kubernetes secret can be created using the command line with the following syntax:\n$ kubectl --namespace %NAMESPACE% create secret generic %SECRET_NAME% \\ --from-literal=adminUser=\u0026quot;%adminUser%\u0026quot; \\ --from-literal=adminPass=\u0026quot;%adminPass%\u0026quot; Update the following placeholders in the command with the relevant value:\n %NAMESPACE% - With name of namespace in which secret is required to be created %SECRET_NAME% - Name for the secret object %adminUser% - With Base64 encoded value for adminUser parameter. %adminPass%- With Base64 encoded value for adminPass parameter.  After executing the kubectl create secret command, verify that the secret has been created:\n$ kubectl --namespace myoudsmns get secret NAME TYPE DATA AGE default-token-fztcb kubernetes.io/service-account-token 3 15m oudsmsecret Opaque 8 99s Prepare a Host Directory to be used for Filesystem Based PersistentVolume It is required to prepare a directory on the Host filesystem to store Oracle Unified Directory Services Manager Instances and other configuration outside the container filesystem. That directory from the Host filesystem will be associated with a PersistentVolume.\nIn the case of a multi-node Kubernetes cluster, the Host directory to be associated with the PersistentVolume should be accessible on all the nodes at the same path.\nTo prepare a Host directory (for example: /scratch/user_projects) for mounting as a file system based PersistentVolume inside your containers, execute the command below on your Host:\n The userid can be anything but it must have uid:guid as 1000:1000, which is the same as the \u0026lsquo;oracle\u0026rsquo; user running in the container. This ensures the \u0026lsquo;oracle\u0026rsquo; user has access to the shared volume/directory.\n $ sudo su - root $ mkdir -p /scratch/user_projects $ chown 1000:1000 /scratch/user_projects $ exit All container operations are performed as the oracle user.\nNote: If a user already exists with -u 1000 -g 1000 then use the same user. Else modify the existing user to have uid-gid as '-u 1000 -g 1000\u0026rsquo;\nCreate PersistentVolume (PV) and PersistentVolumeClaim (PVC) for your Namespace A PersistentVolume (PV) is a storage resource, while a PersistentVolumeClaim (PVC) is a request for that resource. To provide storage for your namespace, update the persistent-volume.yaml file.\nUpdate the following to values specific to your environment:\n   Param Value Example     %PV_NAME% PV name oudsmpv   %PV_HOST_PATH% Valid path on localhost /scratch/user_projects   %PVC_NAME% PVC name oudsmpvc   %NAMESPACE% Namespace myoudsmns    Apply the file:\n$ kubectl apply -f persistent-volume.yaml persistentvolume/oudsmpv created persistentvolumeclaim/oudsmpvc created Verify the PersistentVolume:\n$ kubectl describe persistentvolume oudsmpv Name: oudsmpv Labels: type=oud-pv Annotations: pv.kubernetes.io/bound-by-controller: yes Finalizers: [kubernetes.io/pv-protection] StorageClass: manual Status: Bound Claim: myoudsmns/oudsmpvc Reclaim Policy: Retain Access Modes: RWX VolumeMode: Filesystem Capacity: 10Gi Node Affinity: \u0026lt;none\u0026gt; Message: Source: Type: HostPath (bare host directory volume) Path: /scratch/user_projects HostPathType: Events: \u0026lt;none\u0026gt; Verify the PersistentVolumeClaim:\n$ kubectl --namespace myoudsmns describe pvc oudsmpvc Name: oudsmpvc Namespace: myoudsmns StorageClass: manual Status: Bound Volume: oudsmpv Labels: \u0026lt;none\u0026gt; Annotations: pv.kubernetes.io/bind-completed: yes pv.kubernetes.io/bound-by-controller: yes Finalizers: [kubernetes.io/pvc-protection] Capacity: 10Gi Access Modes: RWX VolumeMode: Filesystem Mounted By: \u0026lt;none\u0026gt; Events: \u0026lt;none\u0026gt; Oracle Unified Directory Services Manager POD In this example you create a POD (oudsmpod) which holds a single container based on an Oracle Unified Directory Services Manager 12c PS4 (12.2.1.4.0) image. This container is configured to run Oracle Unified Directory Services Manager. You also create a service (oudsm) through which you can access the Oracle Unified Directory Services Manager GUI.\nTo create the POD update the samples/oudsm-pod.yaml file.\nUpdate the following parameters to values specific to your environment:\n   Param Value Example     %NAMESPACE% Namespace myoudsmns   %IMAGE% Oracle image tag oracle/oudsm:12.2.1.4.0   %SECRET_NAME% Secret name oudsmsecret   %PV_NAME% PV name oudsmpv   %PVC_NAME% PVC name oudsmpvc    Apply the file:\n$ kubectl apply -f samples/oudsm-pod.yaml service/oudsm-svc created pod/oudsmpod created To check the status of the created pod:\n$ kubectl get pods -n myoudsmns NAME READY STATUS RESTARTS AGE oudsmpod 1/1 Running 0 22m If you see any errors then use the following commands to debug the pod/container.\nTo review issues with the pod e.g. CreateContainerConfigError:\n$ kubectl --namespace \u0026lt;namespace\u0026gt; describe pod \u0026lt;pod\u0026gt; For example:\n$ kubectl --namespace myoudsmns describe pod oudsmpod To tail the container logs while it is initialising use the following command:\n$ kubectl --namespace \u0026lt;namespace\u0026gt; logs -f -c \u0026lt;container\u0026gt; \u0026lt;pod\u0026gt; For example:\n$ kubectl --namespace myoudsmns logs -f -c oudsm oudsmpod To view the full container logs:\n$ kubectl --namespace \u0026lt;namespace\u0026gt; logs -c \u0026lt;container\u0026gt; \u0026lt;pod\u0026gt; To validate that the POD is running:\n$ kubectl --namespace \u0026lt;namespace\u0026gt; get all,pv,pvc,secret For example:\n$ kubectl --namespace myoudsmns get all,pv,pvc,secret NAME READY STATUS RESTARTS AGE pod/oudsmpod 1/1 Running 0 24m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/oudsm-svc NodePort 10.109.142.163 \u0026lt;none\u0026gt; 7001:31674/TCP,7002:31490/TCP 24m NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE persistentvolume/oudsmpv 10Gi RWX Delete Bound myoudsmns/oudsmpvc manual 45m NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistentvolumeclaim/oudsmpvc Bound oudsmpv 10Gi RWX manual 45m NAME TYPE DATA AGE secret/default-token-5kbxk kubernetes.io/service-account-token 3 84m secret/oudsmsecret Opaque 2 80m Once the container is running (READY shows as \u0026lsquo;1/1\u0026rsquo;) check the value of the service port (PORT/s value : here 7001:31674/TCP,7002:31490/TCP) for the Oracle Unified Directory Services Manager service and use this to access Oracle Unified Directory Services Manager in a browser:\nhttp://\u0026lt;hostname\u0026gt;:\u0026lt;svcport\u0026gt;/oudsm  In the case here:\nhttp://\u0026lt;myhost\u0026gt;:31674/oudsm  If you need to release the resources created in this example (POD, service) then issue the following command:\n$ kubectl delete -f samples/oudsm-pod.yaml service \u0026quot;oudsm-svc\u0026quot; deleted pod \u0026quot;oudsmpod\u0026quot; deleted This will avoid conflicts when running the following example for Deployments.\nOracle Unified Directory Services Manager Deployment In this example you create multiple Oracle Unified Directory Services Manager PODs/Services using Kubernetes deployments.\nTo create the deployment update the samples/oudsm-deployment.yaml file.\nUpdate the following to values specific to your environment:\n   Param Value Example     %NAMESPACE% Namespace myoudsmns   %IMAGE% Oracle image tag oracle/oudsm:12.2.1.4.0   %SECRET_NAME% Secret name oudsmsecret    Apply the file:\n$ kubectl apply -f samples/oudsm-deployment.yaml service/oudsm created deployment.apps/oudsmdeploypod created To validate that the POD is running:\n$ kubectl --namespace \u0026lt;namespace\u0026gt; get all,pv,pvc,secret For example:\n$ kubectl --namespace myoudsmns get all,pv,pvc,secret For example:\n$ kubectl --namespace myoudsmns get all,pv,pvc,secret NAME READY STATUS RESTARTS AGE pod/oudsmdeploypod-7c6bb5476-6zcmc 1/1 Running 0 13m pod/oudsmdeploypod-7c6bb5476-nldd8 1/1 Running 0 13m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/oudsm NodePort 10.97.245.58 \u0026lt;none\u0026gt; 7001:31342/TCP,7002:31222/TCP 13m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/oudsmdeploypod 2/2 2 2 13m NAME DESIRED CURRENT READY AGE replicaset.apps/oudsmdeploypod-7c6bb5476 2 2 2 13m NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE persistentvolume/oudsmpv 10Gi RWX Delete Bound myoudsmns/oudsmpvc manual 16h NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistentvolumeclaim/oudsmpvc Bound oudsmpv 10Gi RWX manual 16h NAME TYPE DATA AGE secret/default-token-5kbxk kubernetes.io/service-account-token 3 16h secret/oudsmsecret Opaque 2 16h Once the container is running (READY shows as \u0026lsquo;1/1\u0026rsquo;) check the value of the service port (PORT/s value : here 7001:31421/TCP,7002:31737/TCP) for the Oracle Unified Directory Services Manager service and use this to access Oracle Unified Directory Services Manager in a browser:\n http://\u0026lt;hostname\u0026gt;:\u0026lt;svcport\u0026gt;/oudsm  In the case here:\n http://\u0026lt;myhost\u0026gt;:31342/oudsm  Notice that in the output above we have created 2 Oracle Unified Directory Services Manager PODs (pod/oudsmdeploypod-7bb67b685c-78sq5, pod/oudsmdeploypod-7bb67b685c-xssbq) which are accessed via a service (service/oudsm).\nThe number of PODs is governed by the replicas parameter in the samples/oudsm-deployment.yaml file:\n... kind: Deployment metadata: name: oudsmdeploypod namespace: myoudsmns labels: app: oudsmdeploypod spec: replicas: 2 selector: matchLabels: app: oudsmdeploypod ... If you have a requirement to add additional PODs to your cluster you can update the samples/oudsm-deployment.yaml file with the new value for replicas and apply the file. For example, setting replicas to \u0026lsquo;3\u0026rsquo; would start an additional POD as shown below:\n... kind: Deployment metadata: name: oudsmdeploypod namespace: myoudsmns labels: app: oudsmdeploypod spec: replicas: 3 selector: matchLabels: app: oudsmdeploypod ... $ kubectl apply -f samples/oudsm-deployment.yaml.tmp service/oudsm unchanged deployment.apps/oudsmdeploypod configured Check the number of PODs have increased to 3.\n$ kubectl --namespace myoudsmns get all,pv,pvc,secret NAME READY STATUS RESTARTS AGE pod/oudsmdeploypod-7c6bb5476-6zcmc 1/1 Running 0 17m pod/oudsmdeploypod-7c6bb5476-nldd8 1/1 Running 0 17m pod/oudsmdeploypod-7c6bb5476-vqmz7 0/1 Running 0 26s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/oudsm NodePort 10.97.245.58 \u0026lt;none\u0026gt; 7001:31342/TCP,7002:31222/TCP 17m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/oudsmdeploypod 2/3 3 2 17m NAME DESIRED CURRENT READY AGE replicaset.apps/oudsmdeploypod-7c6bb5476 3 3 2 17m NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE persistentvolume/mike-oud-ds-rs-espv1 20Gi RWX Retain Bound mikens/data-mike-oud-ds-rs-es-cluster-0 elk 4d18h persistentvolume/mike-oud-ds-rs-pv 30Gi RWX Retain Bound mikens/mike-oud-ds-rs-pvc manual 4d18h persistentvolume/oimcluster-oim-pv 10Gi RWX Retain Bound oimcluster/oimcluster-oim-pvc oimcluster-oim-storage-class 69d persistentvolume/oudsmpv 10Gi RWX Delete Bound myoudsmns/oudsmpvc manual 16h NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistentvolumeclaim/oudsmpvc Bound oudsmpv 10Gi RWX manual 16h NAME TYPE DATA AGE secret/default-token-5kbxk kubernetes.io/service-account-token 3 16h secret/oudsmsecret Opaque 2 16h bash-4.2$ In this example, the POD pod/oudsmdeploypod-7c6bb5476-vqmz7 has been added.\nAppendix A : Reference  samples/oudsm-pod.yaml : This yaml file is use to create the pod and bring up the Oracle Unified Directory Services Manager services samples/oudsm-deployment.yaml : This yaml file is used to create replicas of Oracle Unified Directory Services Manager and bring up the Oracle Unified Directory Services Manager services based on the deployment  "
},
{
	"uri": "/fmw-kubernetes/soa-domains/adminguide/",
	"title": "Administration Guide",
	"tags": [],
	"description": "Describes how to use some of the common utility tools and configurations to administer Oracle SOA Suite domains.",
	"content": "Administer Oracle SOA Suite domains in Kubernetes.\n Set up a load balancer  Configure different load balancers for Oracle SOA Suite domains.\n Configure SSL certificates  Create and configure custom SSL certificates for Oracle SOA Suite domains.\n Monitor a domain and publish logs  Monitor an Oracle SOA Suite domain and publish the WebLogic Server logs to Elasticsearch.\n Expose the T3/T3S protocol  Create a T3/T3S channel and the corresponding Kubernetes service to expose the T3/T3S protocol for the Administration Server and Managed Servers in an Oracle SOA Suite domain.\n Deploy composite applications  Deploy composite applications for Oracle SOA Suite and Oracle Service Bus domains.\n Configure Automatic Service Migration  Perform Java Messaging Service (JMS) automatic service migration (ASM) in an Oracle SOA Suite domain setup.\n Persist adapter customizations  Persist the customizations done for Oracle SOA Suite adapters.\n "
},
{
	"uri": "/fmw-kubernetes/soa-domains/",
	"title": "Oracle SOA Suite",
	"tags": [],
	"description": "The Oracle WebLogic Server Kubernetes Operator (the “operator”) supports deployment of Oracle SOA Suite components such as Oracle Service-Oriented Architecture (SOA), Oracle Service Bus (OSB), and Oracle Enterprise Scheduler (ESS). Follow the instructions in this guide to set up these Oracle SOA Suite domains on Kubernetes.",
	"content": "The Oracle WebLogic Server Kubernetes Operator (the “operator”) supports deployment of Oracle SOA Suite components such as Oracle Service-Oriented Architecture (SOA), Oracle Service Bus (OSB), and Oracle Enterprise Scheduler (ESS). Currently the operator supports these domain types:\n soa: Deploys a SOA domain osb: Deploys an OSB domain soaess: Deploys a SOA domain with ESS soaosb: Deploys a domain with SOA and OSB soaessosb: Deploys a domain with SOA, OSB, and ESS  In this release, Oracle SOA Suite domains are supported using the “domain on a persistent volume” model only, where the domain home is located in a persistent volume (PV).\nThe operator has several key features to assist you with deploying and managing Oracle SOA Suite domains in a Kubernetes environment. You can:\n Create Oracle SOA Suite instances in a Kubernetes persistent volume (PV). This PV can reside in an NFS file system or other Kubernetes volume types. Start servers based on declarative startup parameters and desired states. Expose the Oracle SOA Suite services and composites for external access. Scale Oracle SOA Suite domains by starting and stopping Managed Servers on demand, or by integrating with a REST API. Publish operator and WebLogic Server logs to Elasticsearch and interact with them in Kibana. Monitor the Oracle SOA Suite instance using Prometheus and Grafana.  Current production release The current production release for the Oracle SOA Suite domains deployment on Kubernetes is 20.4.2. This release uses the Oracle WebLogic Server Kubernetes Operator version 3.0.1.\nRecent changes and known issues See the Release Notes for recent changes and known issues for Oracle SOA Suite domains deployment on Kubernetes.\nLimitations See here for limitations in this release.\nAbout this documentation This documentation includes sections targeted to different audiences. To help you find what you are looking for more easily, please consult this table of contents:\n  Quick Start explains how to quickly get an Oracle SOA Suite domain instance running, using the defaults, nothing special. Note that this is only for development and test purposes.\n  Install Guide and Administration Guide provide detailed information about all aspects of using the Kubernetes operator including:\n Installing and configuring the operator. Using the operator to create and manage Oracle SOA Suite domains. Configuring Kubernetes load balancers. Configuring Custom SSL certificates. Configuring Elasticsearch and Kibana to access the operator and WebLogic Server log files. Deploying composite applications for Oracle SOA Suite and Oracle Service Bus. Patching an Oracle SOA Suite Docker image. Removing/deleting domains. And much more!    Documentation for earlier releases If you wish to view documentation for an older version, please see\n Version 20.3.3  Additional reading Oracle SOA Suite domains deployment on Kubernetes leverages the Oracle WebLogic Server Kubernetes operator framework.\n To develop an understanding of the operator, including design, architecture, domain life cycle management, and configuration overrides, review the operator documentation. To learn more about the Oracle SOA Suite architecture and components, see Understanding Oracle SOA Suite. To understand the known issues and common questions for Oracle SOA Suite domains deployment on Kubernetes, see the frequently asked questions.  "
},
{
	"uri": "/fmw-kubernetes/soa-domains/adminguide/monitoring-soa-domains/",
	"title": "Monitor a domain and publish logs",
	"tags": [],
	"description": "Monitor an Oracle SOA Suite domain and publish the WebLogic Server logs to Elasticsearch.",
	"content": "After the Oracle SOA Suite domain is set up, you can:\n Monitor the Oracle SOA Suite instance using Prometheus and Grafana Publish WebLogic Server logs into Elasticsearch  Monitor the Oracle SOA Suite instance using Prometheus and Grafana Using the WebLogic Monitoring Exporter you can scrape runtime information from a running Oracle SOA Suite instance and monitor them using Prometheus and Grafana.\nPrerequisite: Before setting up monitoring, make sure that Prometheus and Grafana are deployed on the Kubernetes cluster.\nDeploy Prometheus and Grafana Refer to the compatibility matrix of Kube Prometheus and clone the release version of the kube-prometheus repository according to the Kubernetes version of your cluster.\n  Clone the kube-prometheus repository:\n$ git clone https://github.com/coreos/kube-prometheus.git   Change to folder kube-prometheus and enter the following commands to create the namespace and CRDs, and then wait for their availability before creating the remaining resources:\n$ cd kube-prometheus $ kubectl create -f manifests/setup $ until kubectl get servicemonitors --all-namespaces ; do date; sleep 1; echo \u0026quot;\u0026quot;; done $ kubectl create -f manifests/   kube-prometheus requires all nodes in the Kubernetes cluster to be labeled with kubernetes.io/os=linux. If any node is not labeled with this, then you need to label it using the following command:\n$ kubectl label nodes --all kubernetes.io/os=linux   Enter the following commands to provide external access for Grafana, Prometheus, and Alertmanager:\n$ kubectl patch svc grafana -n monitoring --type=json -p '[{\u0026quot;op\u0026quot;: \u0026quot;replace\u0026quot;, \u0026quot;path\u0026quot;: \u0026quot;/spec/type\u0026quot;, \u0026quot;value\u0026quot;: \u0026quot;NodePort\u0026quot; },{\u0026quot;op\u0026quot;: \u0026quot;replace\u0026quot;, \u0026quot;path\u0026quot;: \u0026quot;/spec/ports/0/nodePort\u0026quot;, \u0026quot;value\u0026quot;: 32100 }]' $ kubectl patch svc prometheus-k8s -n monitoring --type=json -p '[{\u0026quot;op\u0026quot;: \u0026quot;replace\u0026quot;, \u0026quot;path\u0026quot;: \u0026quot;/spec/type\u0026quot;, \u0026quot;value\u0026quot;: \u0026quot;NodePort\u0026quot; },{\u0026quot;op\u0026quot;: \u0026quot;replace\u0026quot;, \u0026quot;path\u0026quot;: \u0026quot;/spec/ports/0/nodePort\u0026quot;, \u0026quot;value\u0026quot;: 32101 }]' $ kubectl patch svc alertmanager-main -n monitoring --type=json -p '[{\u0026quot;op\u0026quot;: \u0026quot;replace\u0026quot;, \u0026quot;path\u0026quot;: \u0026quot;/spec/type\u0026quot;, \u0026quot;value\u0026quot;: \u0026quot;NodePort\u0026quot; },{\u0026quot;op\u0026quot;: \u0026quot;replace\u0026quot;, \u0026quot;path\u0026quot;: \u0026quot;/spec/ports/0/nodePort\u0026quot;, \u0026quot;value\u0026quot;: 32102 }]' Note:\n 32100 is the external port for Grafana 32101 is the external port for Prometheus 32102 is the external port for Alertmanager    Set up monitoring Follow the steps here to set up monitoring for an Oracle SOA Suite instance.\nPublish WebLogic Server logs into Elasticsearch You can publish the WebLogic Server logs to Elasticsearch using the WebLogic logging exporter and interact with them in Kibana. See Publish logs to Elasticsearch.\nWebLogic Server logs can also be published to Elasticsearch using Fluentd. See Fluentd configuration steps.\n"
},
{
	"uri": "/fmw-kubernetes/soa-domains/appendix/docker-k8s-hardening/",
	"title": "Security hardening",
	"tags": [],
	"description": "Review resources for the Docker and Kubernetes cluster hardening.",
	"content": "Securing a Kubernetes cluster involves hardening on multiple fronts - securing the API servers, etcd, nodes, container images, container run-time, and the cluster network. Apply principles of defense in depth, principle of least privilege, and minimize the attack surface. Use security tools such as Kube-Bench to verify the cluster\u0026rsquo;s security posture. Since Kubernetes is evolving rapidly refer to Kubernetes Security Overview for the latest information on securing a Kubernetes cluster. Also ensure the deployed Docker containers follow the Docker Security guidance.\nThis section provides references on how to securely configure Docker and Kubernetes.\nReferences   Docker hardening\n https://docs.docker.com/engine/security/security/ https://blog.aquasec.com/docker-security-best-practices    Kubernetes hardening\n https://kubernetes.io/docs/concepts/security/overview/ https://kubernetes.io/docs/concepts/security/pod-security-standards/ https://blogs.oracle.com/developers/5-best-practices-for-kubernetes-security    Security best practices for Oracle WebLogic Server Running in Docker and Kubernetes\n https://blogs.oracle.com/weblogicserver/security-best-practices-for-weblogic-server-running-in-docker-and-kubernetes    "
},
{
	"uri": "/fmw-kubernetes/soa-domains/patch_and_upgrade/upgrade-k8s-cluster/",
	"title": "Upgrade a Kubernetes cluster",
	"tags": [],
	"description": "Upgrade the underlying Kubernetes cluster version in a running SOA Kubernetes environment.",
	"content": "These instructions describe how to upgrade a Kubernetes cluster created using kubeadm on which an Oracle SOA Suite domain is deployed. A rolling upgrade approach is used to upgrade nodes (master and worker) of the Kubernetes cluster.\nIt is expected that there will be a down time during the upgrade of the Kubernetes cluster as the nodes need to be drained as part of the upgrade process.\n Prerequisites  Review Prerequisites and ensure that your Kubernetes cluster is ready for upgrade. Make sure your environment meets all prerequisites. Make sure the database used for the SOA domain deployment is up and running during the upgrade process.  Upgrade the Kubernetes version An upgrade of Kubernetes is supported from one MINOR version to the next MINOR version, or between PATCH versions of the same MINOR. For example, you can upgrade from 1.x to 1.x+1, but not from 1.x to 1.x+2. To upgrade a Kubernetes version, first all the master nodes of the Kubernetes cluster must be upgraded sequentially, followed by the sequential upgrade of each worker node.\n See here for Kubernetes official documentation to upgrade from v1.14.x to v1.15.x. See here for Kubernetes official documentation to upgrade from v1.15.x to v1.16.x. See here for Kubernetes official documentation to upgrade from v1.16.x to v1.17.x. See here for Kubernetes official documentation to upgrade from v1.17.x to v1.18.x.  "
},
{
	"uri": "/fmw-kubernetes/soa-domains/adminguide/configure-load-balancer/voyager/",
	"title": "Voyager",
	"tags": [],
	"description": "Configure the ingress-based Voyager load balancer for Oracle SOA Suite domains.",
	"content": "Voyager/HAProxy is a popular ingress-based load balancer for production environments. This section provides information about how to install and configure Voyager/HAProxy to load balance Oracle SOA Suite domain clusters. You can configure Voyager for non-SSL, SSL termination, and end-to-end SSL access of the application URL.\nFollow these steps to set up Voyager as a load balancer for an Oracle SOA Suite domain in a Kubernetes cluster:\n  Non-SSL and SSL termination\n Install the Voyager load balancer Configure Voyager to manage ingresses Verify non-SSL and SSL access    End-to-end SSL configuration\n Install Voyager load balancer for end-to-end SSL Deploy tls to access the services Verify end-to-end SSL access    Non-SSL and SSL termination Install the Voyager load balancer   Add the AppsCode chart repository:\n$ helm repo add appscode https://charts.appscode.com/stable/ $ helm repo update   Verify that the chart repository has been added:\n$ helm search repo appscode/voyager  NOTE: After updating the Helm repository, the Voyager version listed may be newer that the one appearing here. Check with the Voyager site for the latest supported versions.\n   Install the Voyager operator:\n NOTE: The Voyager version used for the install should match the version found with helm search.\n $ kubectl create ns voyager $ helm install voyager-operator appscode/voyager --version 10.0.0 \\  --namespace voyager \\  --set cloudProvider=baremetal \\  --set apiserver.enableValidatingWebhook=false Wait until the Voyager operator is running.\n  Check the status of the Voyager operator:\n$ kubectl get all -n voyager    Click here to see the sample output.    NAME READY STATUS RESTARTS AGE pod/voyager-operator-b84f95f8f-4szhl 1/1 Running 0 43h NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/voyager-operator ClusterIP 10.107.201.155 \u0026lt;none\u0026gt; 443/TCP,56791/TCP 43h NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/voyager-operator 1/1 1 1 43h NAME DESIRED CURRENT READY AGE replicaset.apps/voyager-operator-b84f95f8f 1 1 1 43h    See the official installation document for more details.\n  Update the Voyager operator\nAfter the Voyager operator is installed and running, upgrade the Voyager operator using the helm upgrade command, where voyager is the Voyager namespace and soans is the namespace of the domain.\n$ helm upgrade voyager-operator appscode/voyager --namespace voyager    Click here to see the sample output.   Release \u0026quot;voyager-operator\u0026quot; has been upgraded. Happy Helming! NAME: voyager-operator LAST DEPLOYED: Mon Sep 28 11:53:43 2020 NAMESPACE: voyager STATUS: deployed REVISION: 2 TEST SUITE: None NOTES: Set cloudProvider for installing Voyager To verify that Voyager has started, run: kubectl get deployment --namespace voyager -l \u0026quot;app.kubernetes.io/name=voyager,app.kubernetes.io/instance=voyager-operator\u0026quot;      Configure Voyager to manage ingresses   Create an ingress for the domain in the domain namespace by using the sample Helm chart. Here path-based routing is used for ingress. Sample values for default configuration are shown in the file ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain/values.yaml. By default, type is TRAEFIK , tls is Non-SSL, and domainType is soa. These values can be overridden by passing values through the command line or can be edited on the sample file values.yaml.\nIf needed, you can update the ingress yaml file to define more path rules (in the spec.rules.host.http.paths section) based on the domain application URLs that need to be accessed. You need to update the template yaml file for the Voyager (ingress-based) load balancer located at ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain/templates/voyager-ingress.yaml\n$ cd ${WORKDIR}/weblogic-kubernetes-operator $ helm install soa-voyager-ingress kubernetes/samples/charts/ingress-per-domain \\  --namespace soans \\  --values kubernetes/samples/charts/ingress-per-domain/values.yaml \\  --set type=VOYAGER    Click here to check the output of the ingress per domain   NAME: soa-voyager-ingress LAST DEPLOYED: Mon Jul 20 08:20:27 2020 NAMESPACE: soans STATUS: deployed REVISION: 1 TEST SUITE: None      To secure access (SSL) to the Oracle SOA Suite application, create a certificate and generate secrets:\n$ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /tmp/tls1.key -out /tmp/tls1.crt -subj \u0026#34;/CN=*\u0026#34; $ kubectl -n soans create secret tls domain1-tls-cert --key /tmp/tls1.key --cert /tmp/tls1.crt   Deploy ingress-per-domain using Helm for SSL configuration.\nIf needed, you can update the ingress yaml file to define more path rules (in the spec.rules.host.http.paths section) based on the domain application URLs that need to be accessed. You need to update the template yaml file for the Voyager (ingress-based) load balancer located at ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain/templates/voyager-ingress.yaml\n$ cd ${WORKDIR}/weblogic-kubernetes-operator $ helm install soa-voyager-ingress kubernetes/samples/charts/ingress-per-domain \\  --namespace soans \\  --values kubernetes/samples/charts/ingress-per-domain/values.yaml \\  --set type=VOYAGER \\  --set tls=SSL    Click here to see the sample output of the above Commnad.   NAME: soa-voyager-ingress LAST DEPLOYED: Mon Jul 20 08:20:27 2020 NAMESPACE: soans STATUS: deployed REVISION: 1 TEST SUITE: None      For non-SSL access to the Oracle SOA Suite application, get the details of the services deployed by the above ingress:\n$ kubectl describe ingress.voyager.appscode.com/soainfra-voyager -n soans    Click here to see the sample output of the services supported by the above deployed ingress.   Sample output:\nName: soainfra-voyager Namespace: soans Labels: \u0026lt;none\u0026gt; Annotations: ingress.appscode.com/affinity: cookie ingress.appscode.com/default-timeout: {\u0026#34;connect\u0026#34;: \u0026#34;1800s\u0026#34;, \u0026#34;server\u0026#34;: \u0026#34;1800s\u0026#34;} ingress.appscode.com/stats: true ingress.appscode.com/type: NodePort API Version: voyager.appscode.com/v1beta1 Kind: Ingress Metadata: Creation Timestamp: 2020-07-20T08:20:28Z Generation: 1 Managed Fields: API Version: voyager.appscode.com/v1beta1 Fields Type: FieldsV1 fieldsV1: f:metadata: f:annotations: .: f:ingress.appscode.com/affinity: f:ingress.appscode.com/default-timeout: f:ingress.appscode.com/stats: f:ingress.appscode.com/type: f:spec: .: f:rules: Manager: Go-http-client Operation: Update Time: 2020-07-20T08:20:28Z Resource Version: 370484 Self Link: /apis/voyager.appscode.com/v1beta1/namespaces/soans/ingresses/soainfra-voyager UID: bb756966-cd7f-40a5-b08c-79f69e2b9440 Spec: Rules: Host: * Http: Node Port: 30305 Paths: Backend: Service Name: soainfra-adminserver Service Port: 7001 Path: /console Backend: Service Name: soainfra-adminserver Service Port: 7001 Path: /em Backend: Service Name: soainfra-adminserver Service Port: 7001 Path: /weblogic/ready Backend: Service Name: soainfra-cluster-soa-cluster Service Port: 8001 Path: / Backend: Service Name: soainfra-cluster-soa-cluster Service Port: 8001 Path: /soa-infra Backend: Service Name: soainfra-cluster-soa-cluster Service Port: 8001 Path: /soa/composer Backend: Service Name: soainfra-cluster-soa-cluster Service Port: 8001 Path: /integration/worklistapp Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ServiceReconcileSuccessful 4m30s voyager-operator Successfully created NodePort Service voyager-soainfra-voyager Normal ConfigMapReconcileSuccessful 4m30s voyager-operator Successfully created ConfigMap voyager-soainfra-voyager Normal RBACSuccessful 4m30s voyager-operator Successfully created ServiceAccount voyager-soainfra-voyager Normal RBACSuccessful 4m30s voyager-operator Successfully created Role voyager-soainfra-voyager Normal RBACSuccessful 4m30s voyager-operator Successfully created RoleBinding voyager-soainfra-voyager Normal DeploymentReconcileSuccessful 4m30s voyager-operator Successfully created HAProxy Deployment voyager-soainfra-voyager Normal StatsServiceReconcileSuccessful 4m30s voyager-operator Successfully created stats Service voyager-soainfra-voyager-stats      For SSL access to the Oracle SOA Suite application, get the details of the services by the above deployed ingress:\n$ kubectl describe ingress.voyager.appscode.com/soainfra-voyager -n soans    Click here to see all the services configured by the above deployed ingress.   Name: soainfra-voyager Namespace: soans Labels: \u0026lt;none\u0026gt; Annotations: ingress.appscode.com/affinity: cookie ingress.appscode.com/default-timeout: {\u0026#34;connect\u0026#34;: \u0026#34;1800s\u0026#34;, \u0026#34;server\u0026#34;: \u0026#34;1800s\u0026#34;} ingress.appscode.com/stats: true ingress.appscode.com/type: NodePort API Version: voyager.appscode.com/v1beta1 Kind: Ingress Metadata: Creation Timestamp: 2020-07-20T08:20:28Z Generation: 1 Managed Fields: API Version: voyager.appscode.com/v1beta1 Fields Type: FieldsV1 fieldsV1: f:metadata: f:annotations: .: f:ingress.appscode.com/affinity: f:ingress.appscode.com/default-timeout: f:ingress.appscode.com/stats: f:ingress.appscode.com/type: f:spec: .: f:rules: Manager: Go-http-client Operation: Update Time: 2020-07-20T08:20:28Z Resource Version: 370484 Self Link: /apis/voyager.appscode.com/v1beta1/namespaces/soans/ingresses/soainfra-voyager UID: bb756966-cd7f-40a5-b08c-79f69e2b9440 Spec: Frontend Rules: Port: 443 Rules: http-request set-header WL-Proxy-SSL true Rules: Host: * Http: Node Port: 30305 Paths: Backend: Service Name: soainfra-adminserver Service Port: 7001 Path: /console Backend: Service Name: soainfra-adminserver Service Port: 7001 Path : /em Backend: Service Name: soainfra-adminserver Service Port: 7001 Path: /weblogic/ready Backend: Service Name: soainfra-cluster-soa-cluster Service Port: 8001 Path: / Backend: Service Name: soainfra-cluster-soa-cluster Service Port: 8001 Path: /soa-infra Backend: Service Name: soainfra-cluster-soa-cluster Service Port: 8001 Path: /soa/composer Backend: Service Name: soainfra-cluster-soa-cluster Service Port: 8001 Path: /integration/worklistapp Tls: Hosts: * Secret Name: domain1-tls-cert Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ServiceReconcileSuccessful 22s voyager-operator Successfully created NodePort Service voyager-soainfra-voyager Normal ConfigMapReconcileSuccessful 21s voyager-operator Successfully created ConfigMap voyager-soainfra-voyager Normal RBACSuccessful 21s voyager-operator Successfully created ServiceAccount voyager-soainfra-voyager Normal RBACSuccessful 21s voyager-operator Successfully created Role voyager-soainfra-voyager Normal RBACSuccessful 21s voyager-operator Successfully created RoleBinding voyager-soainfra-voyager Normal DeploymentReconcileSuccessful 21s voyager-operator Successfully created HAProxy Deployment voyager-soainfra-voyager Normal StatsServiceReconcileSuccessful 21s voyager-operator Successfully created stats Service voyager-soainfra-voyager-stats      To confirm that the load balancer noticed the new ingress and is successfully routing to the domain\u0026rsquo;s server pods, you can send a request to the URL for the \u0026ldquo;WebLogic ReadyApp framework\u0026rdquo; which should return a HTTP 200 status code, as follows:\n$ curl -v http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-PORT}/weblogic/ready * About to connect() to localhost port 30305 (#0) * Trying 127.0.0.1... * Connected to localhost (127.0.0.1) port 30305 (#0) \u0026gt; GET /weblogic/ready HTTP/1.1 \u0026gt; User-Agent: curl/7.29.0 \u0026gt; Accept: */* \u0026gt; host: *****.com \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; Content-Length: 0 \u0026lt; Date: Thu, 12 Mar 2020 10:16:43 GMT \u0026lt; Vary: Accept-Encoding \u0026lt; * Connection #0 to host localhost left intact   Verify Non-SSL and SSL access After setting up the Voyager (ingress-based) load balancer, verify that the Oracle SOA Suite domain applications are accessible through the load balancer port 30305 (both SSL and non-SSL). The application URLs for Oracle SOA Suite domain of type soa are:\n Note: Port 30305 is the LOADBALANCER-Non-SSLPORT and LOADBALANCER-SSLPORT.\n Non-SSL configuration http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/weblogic/ready http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/console http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/em http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/soa-infra http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/soa/composer http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/integration/worklistapp SSL configuration https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/weblogic/ready https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/console https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/em https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/soa-infra https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/soa/composer https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/integration/worklistapp Uninstalling the chart To uninstall and delete the my-ingress deployment, enter the following command:\n$ helm delete soa-voyager-ingress -n soans End-to-end SSL configuration Install Voyager load balancer for end-to-end SSL Install the Voyager load balancer as described here.\n  Check the status of the Voyager operator.\n$ kubectl get all -n voyager Sample output:\nNAME READY STATUS RESTARTS AGE pod/voyager-operator-b84f95f8f-4szhl 1/1 Running 0 43h NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/voyager-operator ClusterIP 10.107.201.155 \u0026lt;none\u0026gt; 443/TCP,56791/TCP 43h NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/voyager-operator 1/1 1 1 43h NAME DESIRED CURRENT READY AGE replicaset.apps/voyager-operator-b84f95f8f 1 1 1 43h   For secured access (SSL) to the Oracle SOA Suite application, create a certificate and generate Kuberentes secrets:\n$ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /tmp/tls1.key -out /tmp/tls1.crt -subj \u0026#34;/CN=*\u0026#34; $ kubectl -n soans create secret tls domain1-tls-cert --key /tmp/tls1.key --cert /tmp/tls1.crt   Deploy tls to access services   Deploy tls to securely access the services. Only one application can be configured with ssl-passthrough. A sample tls file for Voyager is shown below for the service soainfra-cluster-soa-cluster and port 8002. All the applications running on port 8002 can be securely accessed through this ingress. For each backend service, create different ingresses as Voyager does not support multiple path/rules with annotation ssl-passthrough (that is, for soainfra-cluster-soa-cluster and soainfra-cluster-osb-cluster different ingresses should be created).\n$ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain/tls    Click here to see the content of the file voyager-tls.yaml   apiVersion: voyager.appscode.com/v1beta1 kind: Ingress metadata: name: voyager-ssl namespace: soans annotations: ingress.appscode.com/type: \u0026#39;NodePort\u0026#39; ingress.appscode.com/stats: \u0026#39;true\u0026#39; ingress.appscode.com/affinity: \u0026#39;cookie\u0026#39; ingress.appscode.com/ssl-passthrough: \u0026#34;true\u0026#34; spec: tls: - secretName: domain1-tls-cert hosts: - \u0026#39;*\u0026#39; rules: - host: \u0026#39;*\u0026#39; http: nodePort: \u0026#39;31443\u0026#39; paths: - path: / backend: serviceName: soainfra-cluster-soa-cluster servicePort: \u0026#39;8002\u0026#39;    $ kubectl create -f voyager-tls.yaml    Click here to see the services supported by the ingress   kubectl describe ingress.voyager.appscode.com/voyager-ssl -n soans Name: voyager-ssl Namespace: soans Labels: \u0026lt;none\u0026gt; Annotations: ingress.appscode.com/affinity: cookie ingress.appscode.com/ssl-passthrough: true ingress.appscode.com/stats: true ingress.appscode.com/type: NodePort API Version: voyager.appscode.com/v1beta1 Kind: Ingress Metadata: Creation Timestamp: 2020-07-20T04:34:05Z Generation: 1 Managed Fields: API Version: voyager.appscode.com/v1beta1 Fields Type: FieldsV1 fieldsV1: f:metadata: f:annotations: .: f:ingress.appscode.com/affinity: f:ingress.appscode.com/ssl-passthrough: f:ingress.appscode.com/stats: f:ingress.appscode.com/type: f:spec: .: f:rules: f:tls: Manager: kubectl Operation: Update Time: 2020-07-20T04:34:05Z Resource Version: 340071 Self Link: /apis/voyager.appscode.com/v1beta1/namespaces/soans/ingresses/voyager-ssl UID: 4a9b2e02-1593-45b3-8ac4-ae1ac0f2832c Spec: Rules: Host: * Http: Node Port: 31443 Paths: Backend: Service Name: soainfra-cluster-soa-cluster Service Port: 8002 Path: / Tls: Hosts: * Secret Name: domain1-tls-cert Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ServiceReconcileSuccessful 7m37s voyager-operator Successfully created NodePort Service voyager-voyager-ssl N ormal ConfigMapReconcileSuccessful 7m37s voyager-operator Successfully created ConfigMap voyager-voyager-ssl Normal RBACSuccessful 7m37s voyager-operator Successfully created ServiceAccount voyager-voyager-ssl Normal RBACSuccessful 7m37s voyager-operator Successfully created Role voyager-voyager-ssl Normal RBACSuccessful 7m37s voyager-operator Successfully created RoleBinding voyager-voyager-ssl Normal DeploymentReconcileSuccessful 7m37s voyager-operator Successfully created HAProxy Deployment voyager-voyager-ssl Normal StatsServiceReconcileSuccessful 7m37s voyager-operator Successfully created stats Service voyager-voyager-ssl-stats Normal DeploymentReconcileSuccessful 3m5s voyager-operator Successfully patched HAProxy Deployment voyager-voyager-ssl      Verify end-to-end SSL access Verify that the Oracle SOA Suite domain application URLs are accessible through the SSLPORT 31443:\nhttps://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/soa-infra https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/soa/composer https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/integration/worklistapp Uninstall the Voyager tls $ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain/tls $ kubectl delete -f voyager-tls.yaml "
},
{
	"uri": "/fmw-kubernetes/oam/create-oam-domains/",
	"title": "Create OAM domains",
	"tags": [],
	"description": "Sample for creating an OAM domain home on an existing PV or PVC, and the domain resource YAML file for deploying the generated OAM domain.",
	"content": "The OAM deployment scripts demonstrate the creation of an OAM domain home on an existing Kubernetes persistent volume (PV) and persistent volume claim (PVC). The scripts also generate the domain YAML file, which can then be used to start the Kubernetes artifacts of the corresponding domain.\nPrerequisites Before you begin, perform the following steps:\n Review the Domain resource documentation. Ensure that you have executed all the preliminary steps documented in Prepare your environment. Ensure that the database is up and running.  Prepare to use the create domain script The sample scripts for Oracle Access Management domain deployment are available at \u0026lt;weblogic-kubernetes-operator-project\u0026gt;/kubernetes/samples/scripts/create-access-domain.\n  Make a copy of the create-domain-inputs.yaml file:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-access-domain/domain-home-on-pv $ cp create-domain-inputs.yaml create-domain-inputs.yaml.orig For example:\n$ cd /scratch/OAMDockerK8S/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-access-domain/domain-home-on-pv $ cp create-domain-inputs.yaml create-domain-inputs.yaml.orig   You must edit create-domain-inputs.yaml (or a copy of it) to provide the details for your domain. Please refer to the configuration parameters below to understand the information that you must provide in this file.\nEdit configuration parameters   Edit the create-domain-inputs.yaml and modify the following parameters. Save the file when complete:\ndomainUID: \u0026lt;domain_uid\u0026gt; domainHome: /u01/oracle/user_projects/domains/\u0026lt;domain_uid\u0026gt; image: \u0026lt;image_name\u0026gt; namespace: \u0026lt;domain_namespace\u0026gt; weblogicCredentialsSecretName: \u0026lt;kubernetes_domain_secret\u0026gt; persistentVolumeClaimName: \u0026lt;pvc_name\u0026gt; logHome: /u01/oracle/user_projects/domains/logs/\u0026lt;domain_uid\u0026gt; rcuSchemaPrefix: \u0026lt;rcu_prefix\u0026gt; rcuDatabaseURL: \u0026lt;rcu_db_host\u0026gt;:\u0026lt;rcu_db_port\u0026gt;/\u0026lt;rcu_db_service_name\u0026gt; rcuCredentialsSecret: \u0026lt;kubernetes_rcu_secret\u0026gt; For example:\ndomainUID: accessinfra domainHome: /u01/oracle/user_projects/domains/accessinfra image: oracle/oam:12.2.1.4.0 namespace: accessns weblogicCredentialsSecretName: accessinfra-domain-credentials persistentVolumeClaimName: accessinfra-domain-pvc logHome: /u01/oracle/user_projects/domains/logs/accessinfra rcuSchemaPrefix: OAMK8S rcuDatabaseURL: mydatabasehost.example.com:1521/orcl.example.com rcuCredentialsSecret: accessinfra-rcu-credentials   A full list of parameters in the create-domain-inputs.yaml file are shown below:\n   Parameter Definition Default     adminPort Port number for the Administration Server inside the Kubernetes cluster. 7001   adminNodePort Port number of the Administration Server outside the Kubernetes cluster. 30701   adminServerName Name of the Administration Server. AdminServer   clusterName Name of the WebLogic cluster instance to generate for the domain. By default the cluster name is oam_cluster for the OAM domain. oam_cluster   configuredManagedServerCount Number of Managed Server instances to generate for the domain. 5   createDomainFilesDir Directory on the host machine to locate all the files to create a WebLogic domain, including the script that is specified in the createDomainScriptName property. By default, this directory is set to the relative path wlst, and the create script will use the built-in WLST offline scripts in the wlst directory to create the WebLogic domain. It can also be set to the relative path wdt, and then the built-in WDT scripts will be used instead. An absolute path is also supported to point to an arbitrary directory in the file system. The built-in scripts can be replaced by the user-provided scripts or model files as long as those files are in the specified directory. Files in this directory are put into a Kubernetes config map, which in turn is mounted to the createDomainScriptsMountPath, so that the Kubernetes pod can use the scripts and supporting files to create a domain home. wlst   createDomainScriptsMountPath Mount path where the create domain scripts are located inside a pod. The create-domain.sh script creates a Kubernetes job to run the script (specified in the createDomainScriptName property) in a Kubernetes pod to create a domain home. Files in the createDomainFilesDir directory are mounted to this location in the pod, so that the Kubernetes pod can use the scripts and supporting files to create a domain home. /u01/weblogic   createDomainScriptName Script that the create domain script uses to create a WebLogic domain. The create-domain.sh script creates a Kubernetes job to run this script to create a domain home. The script is located in the in-pod directory that is specified in the createDomainScriptsMountPath property. If you need to provide your own scripts to create the domain home, instead of using the built-it scripts, you must use this property to set the name of the script that you want the create domain job to run. create-domain-job.sh   domainHome Home directory of the OAM domain. If not specified, the value is derived from the domainUID as /shared/domains/\u0026lt;domainUID\u0026gt;. /u01/oracle/user_projects/domains/accessinfra   domainPVMountPath Mount path of the domain persistent volume. /u01/oracle/user_projects   domainUID Unique ID that will be used to identify this particular domain. Used as the name of the generated WebLogic domain as well as the name of the Kubernetes domain resource. This ID must be unique across all domains in a Kubernetes cluster. This ID cannot contain any character that is not valid in a Kubernetes service name. accessinfra   domainType Type of the domain. Mandatory input for OAM domains. You must provide one of the supported domain type value: oam (deploys an OAM domain) oam   exposeAdminNodePort Boolean indicating if the Administration Server is exposed outside of the Kubernetes cluster. false   exposeAdminT3Channel Boolean indicating if the T3 administrative channel is exposed outside the Kubernetes cluster. true   image OAM Docker image. The operator requires OAM 12.2.1.4. Refer to OAM domains for details on how to obtain or create the image. oracle/oam:12.2.1.4.0   imagePullPolicy WebLogic Docker image pull policy. Legal values are IfNotPresent, Always, or Never IfNotPresent   imagePullSecretName Name of the Kubernetes secret to access the Docker Store to pull the WebLogic Server Docker image. The presence of the secret will be validated when this parameter is specified.    includeServerOutInPodLog Boolean indicating whether to include the server .out to the pod\u0026rsquo;s stdout. true   initialManagedServerReplicas Number of Managed Servers to initially start for the domain. 2   javaOptions Java options for starting the Administration Server and Managed Servers. A Java option can have references to one or more of the following pre-defined variables to obtain WebLogic domain information: $(DOMAIN_NAME), $(DOMAIN_HOME), $(ADMIN_NAME), $(ADMIN_PORT), and $(SERVER_NAME). -Dweblogic.StdoutDebugEnabled=false   logHome The in-pod location for the domain log, server logs, server out, and Node Manager log files. If not specified, the value is derived from the domainUID as /shared/logs/\u0026lt;domainUID\u0026gt;. /u01/oracle/user_projects/domains/logs/accessinfra   managedServerNameBase Base string used to generate Managed Server names. oam_server   managedServerPort Port number for each Managed Server. 8001   namespace Kubernetes namespace in which to create the domain. accessns   persistentVolumeClaimName Name of the persistent volume claim created to host the domain home. If not specified, the value is derived from the domainUID as \u0026lt;domainUID\u0026gt;-weblogic-sample-pvc. accessinfra-domain-pvc   productionModeEnabled Boolean indicating if production mode is enabled for the domain. true   serverStartPolicy Determines which WebLogic Server instances will be started. Legal values are NEVER, IF_NEEDED, ADMIN_ONLY. IF_NEEDED   t3ChannelPort Port for the T3 channel of the NetworkAccessPoint. 30012   t3PublicAddress Public address for the T3 channel. This should be set to the public address of the Kubernetes cluster. This would typically be a load balancer address. For development environments only: In a single server (all-in-one) Kubernetes deployment, this may be set to the address of the master, or at the very least, it must be set to the address of one of the worker nodes. If not provided, the script will attempt to set it to the IP address of the Kubernetes cluster   weblogicCredentialsSecretName Name of the Kubernetes secret for the Administration Server\u0026rsquo;s user name and password. If not specified, then the value is derived from the domainUID as \u0026lt;domainUID\u0026gt;-weblogic-credentials. accessinfra-domain-credentials   weblogicImagePullSecretName Name of the Kubernetes secret for the Docker Store, used to pull the WebLogic Server image.    serverPodCpuRequest, serverPodMemoryRequest, serverPodCpuCLimit, serverPodMemoryLimit The maximum amount of compute resources allowed, and minimum amount of compute resources required, for each server pod. Please refer to the Kubernetes documentation on Managing Compute Resources for Containers for details. Resource requests and resource limits are not specified.   rcuSchemaPrefix The schema prefix to use in the database, for example OAM1. You may wish to make this the same as the domainUID in order to simplify matching domains to their RCU schemas. OAM1   rcuDatabaseURL The database URL. oracle-db.default.svc.cluster.local:1521/devpdb.k8s   rcuCredentialsSecret The Kubernetes secret containing the database credentials. accessinfra-rcu-credentials    Note that the names of the Kubernetes resources in the generated YAML files may be formed with the value of some of the properties specified in the create-inputs.yaml file. Those properties include the adminServerName, clusterName and managedServerNameBase. If those values contain any characters that are invalid in a Kubernetes service name, those characters are converted to valid values in the generated YAML files. For example, an uppercase letter is converted to a lowercase letter and an underscore (\u0026quot;_\u0026quot;) is converted to a hyphen (\u0026quot;-\u0026quot;).\nThe sample demonstrates how to create an OAM domain home and associated Kubernetes resources for a domain that has one cluster only. In addition, the sample provides the capability for users to supply their own scripts to create the domain home for other use cases. The generated domain YAML file could also be modified to cover more use cases.\nRun the create domain script   Run the create domain script, specifying your inputs file and an output directory to store the generated artifacts:\ncd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-access-domain/domain-home-on-pv $ ./create-domain.sh -i create-domain-inputs.yaml -o /\u0026lt;path to output-directory\u0026gt; For example:\n$ cd /scratch/OAMDockerK8S/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-access-domain/domain-home-on-pv $ ./create-domain.sh -i create-domain-inputs.yaml -o output_access The output will look similar to the following:\nInput parameters being used export version=\u0026#34;create-weblogic-sample-domain-inputs-v1\u0026#34; export adminPort=\u0026#34;7001\u0026#34; export adminServerName=\u0026#34;AdminServer\u0026#34; export domainUID=\u0026#34;accessinfra\u0026#34; export domainType=\u0026#34;oam\u0026#34; export domainHome=\u0026#34;/u01/oracle/user_projects/domains/accessinfra\u0026#34; export serverStartPolicy=\u0026#34;IF_NEEDED\u0026#34; export clusterName=\u0026#34;oam_cluster\u0026#34; export configuredManagedServerCount=\u0026#34;5\u0026#34; export initialManagedServerReplicas=\u0026#34;2\u0026#34; export managedServerNameBase=\u0026#34;oam_server\u0026#34; export managedServerPort=\u0026#34;14100\u0026#34; export image=\u0026#34;oracle/oam:12.2.1.4.0\u0026#34; export imagePullPolicy=\u0026#34;IfNotPresent\u0026#34; export productionModeEnabled=\u0026#34;true\u0026#34; export weblogicCredentialsSecretName=\u0026#34;accessinfra-domain-credentials\u0026#34; export includeServerOutInPodLog=\u0026#34;true\u0026#34; export logHome=\u0026#34;/u01/oracle/user_projects/domains/logs/accessinfra\u0026#34; export t3ChannelPort=\u0026#34;30012\u0026#34; export exposeAdminT3Channel=\u0026#34;false\u0026#34; export adminNodePort=\u0026#34;30701\u0026#34; export exposeAdminNodePort=\u0026#34;false\u0026#34; export namespace=\u0026#34;accessns\u0026#34; javaOptions=-Dweblogic.StdoutDebugEnabled=false export persistentVolumeClaimName=\u0026#34;accessinfra-domain-pvc\u0026#34; export domainPVMountPath=\u0026#34;/u01/oracle/user_projects/domains\u0026#34; export createDomainScriptsMountPath=\u0026#34;/u01/weblogic\u0026#34; export createDomainScriptName=\u0026#34;create-domain-job.sh\u0026#34; export createDomainFilesDir=\u0026#34;wlst\u0026#34; export rcuSchemaPrefix=\u0026#34;OAMK8S\u0026#34; export rcuDatabaseURL=\u0026#34;mydatabasehost.example.com:1521/orcl.example.com\u0026#34; export rcuCredentialsSecret=\u0026#34;accessinfra-rcu-credentials\u0026#34; Generating output_access/weblogic-domains/accessinfra/create-domain-job.yaml Generating output_access/weblogic-domains/accessinfra/delete-domain-job.yaml Generating output_access/weblogic-domains/accessinfra/domain.yaml Checking to see if the secret accessinfra-domain-credentials exists in namespace accessns configmap/accessinfra-create-oam-infra-domain-job-cm created Checking the configmap accessinfra-create-oam-infra-domain-job-cm was created configmap/accessinfra-create-oam-infra-domain-job-cm labeled Checking if object type job with name accessinfra-create-oam-infra-domain-job exists No resources found in accessns namespace. Creating the domain by creating the job output_access/weblogic-domains/accessinfra/create-domain-job.yaml job.batch/accessinfra-create-oam-infra-domain-job created Waiting for the job to complete... status on iteration 1 of 20 pod accessinfra-create-oam-infra-domain-job-vj69h status is Running status on iteration 2 of 20 pod accessinfra-create-oam-infra-domain-job-vj69h status is Running status on iteration 3 of 20 pod accessinfra-create-oam-infra-domain-job-vj69h status is Running status on iteration 4 of 20 pod accessinfra-create-oam-infra-domain-job-vj69h status is Running status on iteration 5 of 20 pod accessinfra-create-oam-infra-domain-job-vj69h status is Completed Domain accessinfra was created and will be started by the Oracle WebLogic Kubernetes Operator The following files were generated: output_access/weblogic-domains/accessinfra/create-domain-inputs.yaml output_access/weblogic-domains/accessinfra/create-domain-job.yaml output_access/weblogic-domains/accessinfra/domain.yaml Completed Note: If the domain creation fails, refer to the Troubleshooting section.\nThe command creates a domain.yaml file required for domain creation.\n  Navigate to the /output_access/weblogic-domains/\u0026lt;domain_uid\u0026gt; directory:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-access-domain/domain-home-on-pv/output_access/weblogic-domains/\u0026lt;domain_uid\u0026gt; For example:\n$ cd /scratch/OAMDockerK8S/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-access-domain/domain-home-on-pv/output_access/weblogic-domains/accessinfra Edit the domain.yaml file, increase the min and max heap size save the file. Change the following value from:\n- name: USER_MEM_ARGS\u0026quot; value: \u0026quot;-Djava.security.egd=file:/dev/./urandom -Xms256m -Xmx1024m \u0026quot; to:\n- name: USER_MEM_ARGS\u0026quot; value: \u0026quot;-XX:+UseContainerSupport -Djava.security.egd=file:/dev/./urandom -Xms8192m -Xmx8192m\u0026quot;   If required, you can add the optional parameter maxClusterConcurrentStartup to the spec section of the domain.yaml. This parameter specifies the number of managed servers to be started in sequence per cluster. For example if you updated the initialManagedServerReplicas to 4 in create-domain-inputs.yaml and only had 2 nodes, then setting maxClusterConcurrentStartup: 1 will start one managed server at a time on each node, rather than starting them all at once. This can be useful to take the strain off individual nodes at startup. Below is an example with the parameter added:\napiVersion: \u0026quot;weblogic.oracle/v8\u0026quot; kind: Domain metadata: name: accessinfra namespace: accessns labels: weblogic.domainUID: accessinfra spec: # The WebLogic Domain Home domainHome: /u01/oracle/user_projects/domains/accessinfra maxClusterConcurrentStartup: 1 # The domain home source type # Set to PersistentVolume for domain-in-pv, Image for domain-in-image, or FromModel for model-in-image domainHomeSourceType: PersistentVolume # The WebLogic Server Docker image that the Operator uses to start the domain image: \u0026quot;oracle/oam:12.2.1.4.0\u0026quot; ....   Create the Kubernetes resource using the following command:\n$ kubectl apply -f \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-access-domain/domain-home-on-pv/output_access/weblogic-domains/accessinfra/domain.yaml For example:\n$ kubectl apply -f /scratch/OAMDockerK8S/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-access-domain/domain-home-on-pv/output_access/weblogic-domains/accessinfra/domain.yaml The output will look similar to the following:\ndomain.weblogic.oracle/accessinfra created   Verify the domain   Verify the domain, servers pods and services are created and in the READY state with a status of 1/1, by running the following command:\n$ kubectl get all,domains -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get all,domains -n accessns The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE pod/accessinfra-adminserver 1/1 Running 0 17m pod/accessinfra-create-oam-infra-domain-job-vj69h 0/1 Completed 0 42m pod/accessinfra-oam-policy-mgr1 1/1 Running 0 9m7s pod/accessinfra-oam-server1 1/1 Running 0 9m7s pod/accessinfra-oam-server2 1/1 Running 0 9m7s pod/helper 1/1 Running 0 23h NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/accessinfra-adminserver ClusterIP None \u0026lt;none\u0026gt; 7001/TCP 17m service/accessinfra-cluster-oam-cluster ClusterIP 10.110.50.168 \u0026lt;none\u0026gt; 14100/TCP 9m8s service/accessinfra-cluster-policy-cluster ClusterIP 10.102.32.247 \u0026lt;none\u0026gt; 15100/TCP 9m8s service/accessinfra-oam-policy-mgr1 ClusterIP None \u0026lt;none\u0026gt; 15100/TCP 9m8s service/accessinfra-oam-policy-mgr2 ClusterIP 10.104.147.108 \u0026lt;none\u0026gt; 15100/TCP 9m8s service/accessinfra-oam-policy-mgr3 ClusterIP 10.108.233.86 \u0026lt;none\u0026gt; 15100/TCP 9m8s service/accessinfra-oam-policy-mgr4 ClusterIP 10.105.15.228 \u0026lt;none\u0026gt; 15100/TCP 9m7s service/accessinfra-oam-policy-mgr5 ClusterIP 10.99.66.92 \u0026lt;none\u0026gt; 15100/TCP 9m8s service/accessinfra-oam-server1 ClusterIP None \u0026lt;none\u0026gt; 14100/TCP 9m8s service/accessinfra-oam-server2 ClusterIP None \u0026lt;none\u0026gt; 14100/TCP 9m8s service/accessinfra-oam-server3 ClusterIP 10.111.231.33 \u0026lt;none\u0026gt; 14100/TCP 9m8s service/accessinfra-oam-server4 ClusterIP 10.110.10.183 \u0026lt;none\u0026gt; 14100/TCP 9m7s service/accessinfra-oam-server5 ClusterIP 10.103.192.174 \u0026lt;none\u0026gt; 14100/TCP 9m8s NAME COMPLETIONS DURATION AGE job.batch/accessinfra-create-oam-infra-domain-job 1/1 2m14s 42m NAME AGE domain.weblogic.oracle/accessinfra 25m Note: It will take several minutes before all the services listed above show. When a pod has a STATUS of 0/1 the pod is started but the OAM server associated with it is currently starting. While the pods are starting you can check the startup status in the pod logs, by running the following command:\n$ kubectl logs accessinfra-adminserver -n accessns $ kubectl logs accessinfra-oam-policy-mgr1 -n accessns $ kubectl logs accessinfra-oam-server1 -n accessns etc.. The default domain created by the script has the following characteristics:\n An Administration Server named AdminServer listening on port 7001. A configured OAM cluster named oam_cluster of size 5. A configured Policy Manager cluster named policy_cluster of size 5. Two started OAM managed Servers, named oam_server1 and oam_server2, listening on port 14100. One started Policy Manager managed server named oam-policy-mgr1, listening on port 15100. Log files that are located in \u0026lt;persistent_volume\u0026gt;/logs/\u0026lt;domainUID\u0026gt;.    Run the following command to describe the domain:\n$ kubectl describe domain \u0026lt;domain_uid\u0026gt; -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl describe domain accessinfra -n accessns The output will look similar to the following:\nName: accessinfra Namespace: accessns Labels: weblogic.domainUID=accessinfra Annotations: API Version: weblogic.oracle/v8 Kind: Domain Metadata: Creation Timestamp: 2020-09-24T14:00:34Z Generation: 1 Managed Fields: API Version: weblogic.oracle/v8 Fields Type: FieldsV1 fieldsV1: f:metadata: f:annotations: .: f:kubectl.kubernetes.io/last-applied-configuration: f:labels: .: f:weblogic.domainUID: Manager: kubectl Operation: Update Time: 2020-09-24T14:00:34Z API Version: weblogic.oracle/v8 Fields Type: FieldsV1 fieldsV1: f:status: .: f:clusters: f:conditions: f:servers: f:startTime: Manager: OpenAPI-Generator Operation: Update Time: 2020-09-24T14:12:51Z Resource Version: 244336 Self Link: /apis/weblogic.oracle/v8/namespaces/accessns/domains/accessinfra UID: 0edf8266-4419-45f1-bd50-e26ac41340e5 Spec: Admin Server: Server Pod: Env: Name: USER_MEM_ARGS Value: -Djava.security.egd=file:/dev/./urandom -Xms512m -Xmx1024m Server Start State: RUNNING Clusters: Cluster Name: policy_cluster Replicas: 1 Server Pod: Affinity: Pod Anti Affinity: Preferred During Scheduling Ignored During Execution: Pod Affinity Term: Label Selector: Match Expressions: Key: weblogic.clusterName Operator: In Values: $(CLUSTER_NAME) Topology Key: kubernetes.io/hostname Weight: 100 Server Service: Precreate Service: true Server Start State: RUNNING Cluster Name: oam_cluster Replicas: 2 Server Pod: Affinity: Pod Anti Affinity: Preferred During Scheduling Ignored During Execution: Pod Affinity Term: Label Selector: Match Expressions: Key: weblogic.clusterName Operator: In Values: $(CLUSTER_NAME) Topology Key: kubernetes.io/hostname Weight: 100 Server Service: Precreate Service: true Server Start State: RUNNING Data Home: Domain Home: /u01/oracle/user_projects/domains/accessinfra Domain Home Source Type: PersistentVolume Http Access Log In Log Home: true Image: oracle/oam:12.2.1.4.0 Image Pull Policy: IfNotPresent Include Server Out In Pod Log: true Log Home: /u01/oracle/user_projects/domains/logs/accessinfra Log Home Enabled: true Server Pod: Env: Name: JAVA_OPTIONS Value: -Dweblogic.StdoutDebugEnabled=false Name: USER_MEM_ARGS Value: -Djava.security.egd=file:/dev/./urandom -Xms256m -Xmx1024m Volume Mounts: Mount Path: /u01/oracle/user_projects/domains Name: weblogic-domain-storage-volume Volumes: Name: weblogic-domain-storage-volume Persistent Volume Claim: Claim Name: accessinfra-domain-pvc Server Start Policy: IF_NEEDED Web Logic Credentials Secret: Name: accessinfra-domain-credentials Status: Clusters: Cluster Name: oam_cluster Maximum Replicas: 5 Minimum Replicas: 0 Ready Replicas: 2 Replicas: 2 Replicas Goal: 2 Cluster Name: policy_cluster Maximum Replicas: 5 Minimum Replicas: 0 Ready Replicas: 1 Replicas: 1 Replicas Goal: 1 Conditions: Last Transition Time: 2020-09-24T14:12:02.037Z Reason: ServersReady Status: True Type: Available Servers: Desired State: RUNNING Health: Activation Time: 2020-09-24T14:09:01.164Z Overall Health: ok Subsystems: Subsystem Name: ServerRuntime Symptoms: Node Name: 10.250.111.112 Server Name: AdminServer State: RUNNING Cluster Name: oam_cluster Desired State: RUNNING Health: Activation Time: 2020-09-24T14:11:06.015Z Overall Health: ok Subsystems: Subsystem Name: ServerRuntime Symptoms: Node Name: 10.250.111.111 Server Name: oam_server1 State: RUNNING Cluster Name: oam_cluster Desired State: RUNNING Health: Activation Time: 2020-09-24T14:11:35.454Z Overall Health: ok Subsystems: Subsystem Name: ServerRuntime Symptoms: Node Name: 10.250.111.112 Server Name: oam_server2 State: RUNNING Cluster Name: oam_cluster Desired State: SHUTDOWN Server Name: oam_server3 Cluster Name: oam_cluster Desired State: SHUTDOWN Server Name: oam_server4 Cluster Name: oam_cluster Desired State: SHUTDOWN Server Name: oam_server5 Cluster Name: policy_cluster Desired State: RUNNING Health: Activation Time: 2020-09-24T14:11:54.938Z Overall Health: ok Subsystems: Subsystem Name: ServerRuntime Symptoms: Node Name: 10.250.111.112 Server Name: oam_policy_mgr1 State: RUNNING Cluster Name: policy_cluster Desired State: SHUTDOWN Server Name: oam_policy_mgr2 Cluster Name: policy_cluster Desired State: SHUTDOWN Server Name: oam_policy_mgr3 Cluster Name: policy_cluster Desired State: SHUTDOWN Server Name: oam_policy_mgr4 Cluster Name: policy_cluster Desired State: SHUTDOWN Server Name: oam_policy_mgr5 Start Time: 2020-09-24T14:00:34.395Z Events: \u0026lt;none\u0026gt; In the Status section of the output, the available servers and clusters are listed.\n  Run the following command to see the pods running the servers and which nodes they are running on:\n$ kubectl get pods -n \u0026lt;domain_namespace\u0026gt; -o wide For example:\n$ kubectl get pods -n accessns -o wide The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES accessinfra-adminserver 1/1 Running 0 26m 10.244.1.7 10.250.111.112 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; accessinfra-create-oam-infra-domain-job-vj69h 0/1 Completed 0 5h55m 10.244.1.5 10.250.111.112 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; accessinfra-oam-policy-mgr1 1/1 Running 0 18m 10.244.1.9 10.250.111.112 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; accessinfra-oam-server1 1/1 Running 0 18m 10.244.2.3 10.250.111.111 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; accessinfra-oam-server2 1/1 Running 0 18m 10.244.1.8 10.250.111.112 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; helper 1/1 Running 0 22h 10.244.1.4 10.250.111.112 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; You are now ready to configure an Ingress to direct traffic for your OAM domain as per Configure an Ingress for an OAM domain.\n  "
},
{
	"uri": "/fmw-kubernetes/oig/create-oig-domains/",
	"title": "Create OIG domains",
	"tags": [],
	"description": "Sample for creating an OIG domain home on an existing PV or PVC, and the domain resource YAML file for deploying the generated OIG domain.",
	"content": " Introduction Prerequisites Prepare the Create Domain Script  Edit Configuration Parameters   Run the Create Domain Script  Generate the Create Domain Script Create Docker Registry Secret Run the Create Domain Scripts   Verify the Results  Verify the Domain, Pods and Services Verify the Domain Verify the Pods    Introduction The OIG deployment scripts demonstrate the creation of an OIG domain home on an existing Kubernetes persistent volume (PV) and persistent volume claim (PVC). The scripts also generate the domain YAML file, which can then be used to start the Kubernetes artifacts of the corresponding domain.\nPrerequisites Before you begin, perform the following steps:\n Review the Domain resource documentation. Ensure that you have executed all the preliminary steps documented in Prepare your environment. Ensure that the database is up and running.  Prepare the Create Domain Script The sample scripts for Oracle Identity Governance domain deployment are available at \u0026lt;weblogic-kubernetes-operator-project\u0026gt;/kubernetes/samples/scripts/create-oim-domain.\n  Make a copy of the create-domain-inputs.yaml file:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-oim-domain/domain-home-on-pv $ cp create-domain-inputs.yaml create-domain-inputs.yaml.orig For example:\n$ cd /scratch/OIGDockerK8S/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-oim-domain/domain-home-on-pv $ cp create-domain-inputs.yaml create-domain-inputs.yaml.orig You must edit create-domain-inputs.yaml (or a copy of it) to provide the details for your domain. Please refer to the configuration parameters below to understand the information that you must provide in this file.\n  Edit Configuration Parameters   Edit the create-domain-inputs.yaml and modify the following parameters. Save the file when complete:\ndomainUID: \u0026lt;domain_uid\u0026gt; domainHome: /u01/oracle/user_projects/domains/\u0026lt;domain_uid\u0026gt; image: \u0026lt;image_name\u0026gt; namespace: \u0026lt;domain_namespace\u0026gt; weblogicCredentialsSecretName: \u0026lt;kubernetes_domain_secret\u0026gt; persistentVolumeClaimName: \u0026lt;pvc_name\u0026gt; logHome: /u01/oracle/user_projects/domains/logs/\u0026lt;domain_id\u0026gt; rcuSchemaPrefix: \u0026lt;rcu_prefix\u0026gt; rcuDatabaseURL: \u0026lt;rcu_db_host\u0026gt;:\u0026lt;rcu_db_port\u0026gt;/\u0026lt;rcu_db_service_name\u0026gt; rcuCredentialsSecret: \u0026lt;kubernetes_rcu_secret\u0026gt; For example:\ndomainUID: oimcluster domainHome: /u01/oracle/user_projects/domains/oimcluster image: oracle/oig:12.2.1.4.0 namespace: oimcluster weblogicCredentialsSecretName: oimcluster-domain-credentials persistentVolumeClaimName: oimcluster-domain-pvc logHome: /u01/oracle/user_projects/domains/logs/oimcluster rcuSchemaPrefix: OIGK8S rcuDatabaseURL: mydatabasehost.example.com:1521/orcl.example.com rcuCredentialsSecret: oimcluster-rcu-credentials   A full list of parameters in the create-domain-inputs.yaml file are shown below:\n   Parameter Definition Default     adminPort Port number for the Administration Server inside the Kubernetes cluster. 7001   adminNodePort Port number of the Administration Server outside the Kubernetes cluster. 30701   adminServerName Name of the Administration Server. AdminServer   clusterName Name of the WebLogic cluster instance to generate for the domain. By default the cluster name is oimcluster for the OIG domain. oimcluster   configuredManagedServerCount Number of Managed Server instances to generate for the domain. 5   createDomainFilesDir Directory on the host machine to locate all the files to create a WebLogic domain, including the script that is specified in the createDomainScriptName property. By default, this directory is set to the relative path wlst, and the create script will use the built-in WLST offline scripts in the wlst directory to create the WebLogic domain. It can also be set to the relative path wdt, and then the built-in WDT scripts will be used instead. An absolute path is also supported to point to an arbitrary directory in the file system. The built-in scripts can be replaced by the user-provided scripts or model files as long as those files are in the specified directory. Files in this directory are put into a Kubernetes config map, which in turn is mounted to the createDomainScriptsMountPath, so that the Kubernetes pod can use the scripts and supporting files to create a domain home. wlst   createDomainScriptsMountPath Mount path where the create domain scripts are located inside a pod. The create-domain.sh script creates a Kubernetes job to run the script (specified in the createDomainScriptName property) in a Kubernetes pod to create a domain home. Files in the createDomainFilesDir directory are mounted to this location in the pod, so that the Kubernetes pod can use the scripts and supporting files to create a domain home. /u01/weblogic   createDomainScriptName Script that the create domain script uses to create a WebLogic domain. The create-domain.sh script creates a Kubernetes job to run this script to create a domain home. The script is located in the in-pod directory that is specified in the createDomainScriptsMountPath property. If you need to provide your own scripts to create the domain home, instead of using the built-it scripts, you must use this property to set the name of the script that you want the create domain job to run. create-domain-job.sh   domainHome Home directory of the OIG domain. If not specified, the value is derived from the domainUID as /shared/domains/\u0026lt;domainUID\u0026gt;. /u01/oracle/user_projects/domains/oimcluster   domainPVMountPath Mount path of the domain persistent volume. /u01/oracle/user_projects   domainUID Unique ID that will be used to identify this particular domain. Used as the name of the generated WebLogic domain as well as the name of the Kubernetes domain resource. This ID must be unique across all domains in a Kubernetes cluster. This ID cannot contain any character that is not valid in a Kubernetes service name. oimcluster   exposeAdminNodePort Boolean indicating if the Administration Server is exposed outside of the Kubernetes cluster. false   exposeAdminT3Channel Boolean indicating if the T3 administrative channel is exposed outside the Kubernetes cluster. true   image OIG Docker image. The operator requires OIG 12.2.1.4. Refer to OIG domains for details on how to obtain or create the image. oracle/oig:12.2.1.4.0   imagePullPolicy WebLogic Docker image pull policy. Legal values are IfNotPresent, Always, or Never IfNotPresent   imagePullSecretName Name of the Kubernetes secret to access the Docker Store to pull the WebLogic Server Docker image. The presence of the secret will be validated when this parameter is specified.    includeServerOutInPodLog Boolean indicating whether to include the server .out to the pod\u0026rsquo;s stdout. true   initialManagedServerReplicas Number of Managed Servers to initially start for the domain. 2   javaOptions Java options for starting the Administration Server and Managed Servers. A Java option can have references to one or more of the following pre-defined variables to obtain WebLogic domain information: $(DOMAIN_NAME), $(DOMAIN_HOME), $(ADMIN_NAME), $(ADMIN_PORT), and $(SERVER_NAME). -Dweblogic.StdoutDebugEnabled=false   logHome The in-pod location for the domain log, server logs, server out, and Node Manager log files. If not specified, the value is derived from the domainUID as /shared/logs/\u0026lt;domainUID\u0026gt;. /u01/oracle/user_projects/domains/logs/oimcluster   managedServerNameBase Base string used to generate Managed Server names. oim_server   managedServerPort Port number for each Managed Server. 8001   namespace Kubernetes namespace in which to create the domain. oimcluster   persistentVolumeClaimName Name of the persistent volume claim created to host the domain home. If not specified, the value is derived from the domainUID as \u0026lt;domainUID\u0026gt;-weblogic-sample-pvc. oimcluster-domain-pvc   productionModeEnabled Boolean indicating if production mode is enabled for the domain. true   serverStartPolicy Determines which WebLogic Server instances will be started. Legal values are NEVER, IF_NEEDED, ADMIN_ONLY. IF_NEEDED   t3ChannelPort Port for the T3 channel of the NetworkAccessPoint. 30012   t3PublicAddress Public address for the T3 channel. This should be set to the public address of the Kubernetes cluster. This would typically be a load balancer address. For development environments only: In a single server (all-in-one) Kubernetes deployment, this may be set to the address of the master, or at the very least, it must be set to the address of one of the worker nodes. If not provided, the script will attempt to set it to the IP address of the Kubernetes cluster   weblogicCredentialsSecretName Name of the Kubernetes secret for the Administration Server\u0026rsquo;s user name and password. If not specified, then the value is derived from the domainUID as \u0026lt;domainUID\u0026gt;-weblogic-credentials. oimcluster-domain-credentials   weblogicImagePullSecretName Name of the Kubernetes secret for the Docker Store, used to pull the WebLogic Server image.    serverPodCpuRequest, serverPodMemoryRequest, serverPodCpuCLimit, serverPodMemoryLimit The maximum amount of compute resources allowed, and minimum amount of compute resources required, for each server pod. Please refer to the Kubernetes documentation on Managing Compute Resources for Containers for details. Resource requests and resource limits are not specified.   rcuSchemaPrefix The schema prefix to use in the database, for example OIGK8S. You may wish to make this the same as the domainUID in order to simplify matching domains to their RCU schemas. OIGK8S   rcuDatabaseURL The database URL. oracle-db.default.svc.cluster.local:1521/devpdb.k8s   rcuCredentialsSecret The Kubernetes secret containing the database credentials. oimcluster-rcu-credentials    Note that the names of the Kubernetes resources in the generated YAML files may be formed with the value of some of the properties specified in the create-inputs.yaml file. Those properties include the adminServerName, clusterName and managedServerNameBase. If those values contain any characters that are invalid in a Kubernetes service name, those characters are converted to valid values in the generated YAML files. For example, an uppercase letter is converted to a lowercase letter and an underscore (\u0026quot;_\u0026quot;) is converted to a hyphen (\u0026quot;-\u0026quot;).\nThe sample demonstrates how to create an OIG domain home and associated Kubernetes resources for a domain that has one cluster only. In addition, the sample provides the capability for users to supply their own scripts to create the domain home for other use cases. The generated domain YAML file could also be modified to cover more use cases.\nRun the Create Domain Script Generate the Create Domain Script   Run the create domain script, specifying your inputs file and an output directory to store the generated artifacts:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-oim-domain/domain-home-on-pv $ mkdir output_oimcluster $ ./create-domain.sh -i create-domain-inputs.yaml -o /\u0026lt;path to output-directory\u0026gt; For example:\n$ cd /scratch/OIGDockerK8S/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-oim-domain/domain-home-on-pv $ mkdir output_oimcluster $ ./create-domain.sh -i create-domain-inputs.yaml -o output_oimcluster The output will look similar to the following:\n$ ./create-domain.sh -i create-domain-inputs.yaml -o output_oimcluster Input parameters being used export version=\u0026#34;create-weblogic-sample-domain-inputs-v1\u0026#34; export adminPort=\u0026#34;7001\u0026#34; export adminServerName=\u0026#34;AdminServer\u0026#34; export domainUID=\u0026#34;oimcluster\u0026#34; export domainHome=\u0026#34;/u01/oracle/user_projects/domains/oimcluster\u0026#34; export serverStartPolicy=\u0026#34;IF_NEEDED\u0026#34; export clusterName=\u0026#34;oim_cluster\u0026#34; export configuredManagedServerCount=\u0026#34;5\u0026#34; export initialManagedServerReplicas=\u0026#34;1\u0026#34; export managedServerNameBase=\u0026#34;oim_server\u0026#34; export managedServerPort=\u0026#34;14000\u0026#34; export image=\u0026#34;oracle/oig:12.2.1.4.0\u0026#34; export imagePullPolicy=\u0026#34;IfNotPresent\u0026#34; export imagePullSecretName=\u0026#34;oig-docker\u0026#34; export productionModeEnabled=\u0026#34;true\u0026#34; export weblogicCredentialsSecretName=\u0026#34;oimcluster-domain-credentials\u0026#34; export includeServerOutInPodLog=\u0026#34;true\u0026#34; export logHome=\u0026#34;/u01/oracle/user_projects/domains/logs/oimcluster\u0026#34; export t3ChannelPort=\u0026#34;30012\u0026#34; export exposeAdminT3Channel=\u0026#34;false\u0026#34; export adminNodePort=\u0026#34;30701\u0026#34; export exposeAdminNodePort=\u0026#34;false\u0026#34; export namespace=\u0026#34;oimcluster\u0026#34; javaOptions=-Dweblogic.StdoutDebugEnabled=false export persistentVolumeClaimName=\u0026#34;oimcluster-oim-pvc\u0026#34; export domainPVMountPath=\u0026#34;/u01/oracle/user_projects/domains\u0026#34; export createDomainScriptsMountPath=\u0026#34;/u01/weblogic\u0026#34; export createDomainScriptName=\u0026#34;create-domain-job.sh\u0026#34; export createDomainFilesDir=\u0026#34;wlst\u0026#34; export rcuSchemaPrefix=\u0026#34;OIGK8S\u0026#34; export rcuDatabaseURL=\u0026#34;mydatabasehost.example.com:1521/orcl.example.com\u0026#34; export rcuCredentialsSecret=\u0026#34;oimcluster-rcu-credentials\u0026#34; export frontEndHost=\u0026#34;100.102.48.49\u0026#34; export frontEndPort=\u0026#34;80\u0026#34; Generating output_oimcluster/weblogic-domains/oimcluster/create-domain-job.yaml Generating output_oimcluster/weblogic-domains/oimcluster/delete-domain-job.yaml Generating output_oimcluster/weblogic-domains/oimcluster/domain.yaml Checking to see if the secret oimcluster-domain-credentials exists in namespace oimcluster configmap/oimcluster-create-fmw-infra-sample-domain-job-cm created Checking the configmap oimcluster-create-fmw-infra-sample-domain-job-cm was created configmap/oimcluster-create-fmw-infra-sample-domain-job-cm labeled Checking if object type job with name oimcluster-create-fmw-infra-sample-domain-job exists No resources found in oimcluster namespace. Creating the domain by creating the job output_oimcluster/weblogic-domains/oimcluster/create-domain-job.yaml job.batch/oimcluster-create-fmw-infra-sample-domain-job created Waiting for the job to complete... status on iteration 1 of 40 pod oimcluster-create-fmw-infra-sample-domain-job-dktkk status is Running status on iteration 2 of 40 pod oimcluster-create-fmw-infra-sample-domain-job-dktkk status is Running status on iteration 3 of 40 pod oimcluster-create-fmw-infra-sample-domain-job-dktkk status is Running status on iteration 4 of 40 pod oimcluster-create-fmw-infra-sample-domain-job-dktkk status is Running status on iteration 5 of 40 pod oimcluster-create-fmw-infra-sample-domain-job-dktkk status is Running status on iteration 6 of 40 pod oimcluster-create-fmw-infra-sample-domain-job-dktkk status is Running status on iteration 7 of 40 pod oimcluster-create-fmw-infra-sample-domain-job-dktkk status is Running status on iteration 8 of 40 pod oimcluster-create-fmw-infra-sample-domain-job-dktkk status is Running status on iteration 9 of 40 pod oimcluster-create-fmw-infra-sample-domain-job-dktkk status is Running status on iteration 10 of 40 pod oimcluster-create-fmw-infra-sample-domain-job-dktkk status is Running status on iteration 11 of 40 pod oimcluster-create-fmw-infra-sample-domain-job-dktkk status is Completed Domain oimcluster was created and will be started by the WebLogic Kubernetes Operator The following files were generated: output_oimcluster/weblogic-domains/oimcluster/create-domain-inputs.yaml output_oimcluster/weblogic-domains/oimcluster/create-domain-job.yaml output_oimcluster/weblogic-domains/oimcluster/domain.yaml sed Completed $ Note: If the create domain script creation fails, refer to the Troubleshooting section.\n  Create Docker Registry Secret   Create a Docker Registry Secret with name oig-docker. The operator validates the presence of this secret. The OIG image has been manually loaded in Install the OIG Docker Image so you can run this command as is. The presence of the secret is sufficient for creating the Kubernetes resource in the next step.\n$ kubectl create secret docker-registry oig-docker -n \u0026lt;domain_namespace\u0026gt; --docker-username=\u0026#39;\u0026lt;user_name\u0026gt;\u0026#39; --docker-password=\u0026#39;\u0026lt;password\u0026gt;\u0026#39; --docker-server=\u0026#39;\u0026lt;docker_registry_url\u0026gt;\u0026#39; --docker-email=\u0026#39;\u0026lt;email_address\u0026gt;\u0026#39; For example:\n$ kubectl create secret docker-registry oig-docker -n oimcluster --docker-username=\u0026#39;\u0026lt;user_name\u0026gt;\u0026#39; --docker-password=\u0026#39;\u0026lt;password\u0026gt;\u0026#39; --docker-server=\u0026#39;\u0026lt;docker_registry_url\u0026gt;\u0026#39; --docker-email=\u0026#39;\u0026lt;email_address\u0026gt;\u0026#39; Note: The above command should be run as described. Do not change anything other than the \u0026lt;domain_namespace\u0026gt;.\nThe output will look similar to the following:\nsecret/oig-docker created   Run the Create Domain Scripts   Create the Kubernetes resource using the following command:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-oim-domain/domain-home-on-pv/output_oimcluster/weblogic-domains/oimcluster $ kubectl apply -f domain.yaml For example:\n$ cd /scratch/OIGDockerK8S/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-oim-domain/domain-home-on-pv/output_oimcluster/weblogic-domains/oimcluster $ kubectl apply -f domain.yaml The output will look similar to the following:\ndomain.weblogic.oracle/oimcluster created   Run the following command to view the status of the OIG pods:\n$ kubectl get pods -n oimcluster The output will initially look similar to the following:\nNAME READY STATUS RESTARTS AGE helper 1/1 Running 0 3h30m oimcluster-create-fmw-infra-sample-domain-job-dktkk 0/1 Completed 0 27m oimcluster-introspect-domain-job-p4brt 1/1 Running 0 6s The introspect-domain-job pod will be displayed first. Run the command again after several minutes and check to see that the AdminServer and SOA Server are both started. When started they should have STATUS = Running and READY = 1/1.\nNAME READY STATUS RESTARTS AGE helper 1/1 Running 0 3h38m oimcluster-adminserver 1/1 Running 0 7m30s oimcluster-create-fmw-infra-sample-domain-job-dktkk 0/1 Completed 0 35m oimcluster-soa-server1 1/1 Running 0 4m Note: It will take several minutes before all the pods listed above show. When a pod has a STATUS of 0/1 the pod is started but the OIG server associated with it is currently starting. While the pods are starting you can check the startup status in the pod logs, by running the following command:\n$ kubectl logs oimcluster-adminserver -n oimcluster $ kubectl logs oimcluster-soa-server1 -n oimcluster   Once both pods are running, start the OIM Server using the following command:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-oim-domain/domain-home-on-pv/output_oimcluster/weblogic-domains/oimcluster $ kubectl apply -f domain_oim_soa.yaml For example:\n$ cd /scratch/OIGDockerK8S/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-oim-domain/domain-home-on-pv/output_oimcluster/weblogic-domains/oimcluster $ kubectl apply -f domain_oim_soa.yaml The output will look similar to the following:\ndomain.weblogic.oracle/oimcluster configured   Verify the Results Verify the Domain, Pods and Services   Verify the domain, servers pods and services are created and in the READY state with a STATUS of 1/1, by running the following command:\n$ kubectl get all,domains -n oimcluster The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE pod/helper 1/1 Running 0 3h40m pod/oimcluster-adminserver 1/1 Running 0 16m pod/oimcluster-create-fmw-infra-sample-domain-job-dktkk 0/1 Completed 0 36m pod/oimcluster-oim-server1 1/1 Running 0 5m57s pod/oimcluster-soa-server1 1/1 Running 0 13m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/oimcluster-adminserver ClusterIP None \u0026lt;none\u0026gt; 7001/TCP 16m service/oimcluster-cluster-oim-cluster ClusterIP 10.97.121.159 \u0026lt;none\u0026gt; 14000/TCP 13m service/oimcluster-cluster-soa-cluster ClusterIP 10.111.231.242 \u0026lt;none\u0026gt; 8001/TCP 13m service/oimcluster-oim-server1 ClusterIP None \u0026lt;none\u0026gt; 14000/TCP 5m57s service/oimcluster-oim-server2 ClusterIP 10.108.139.30 \u0026lt;none\u0026gt; 14000/TCP 5m57s service/oimcluster-oim-server3 ClusterIP 10.97.170.104 \u0026lt;none\u0026gt; 14000/TCP 5m57s service/oimcluster-oim-server4 ClusterIP 10.99.82.214 \u0026lt;none\u0026gt; 14000/TCP 5m57s service/oimcluster-oim-server5 ClusterIP 10.98.75.228 \u0026lt;none\u0026gt; 14000/TCP 5m57s service/oimcluster-soa-server1 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP 13m service/oimcluster-soa-server2 ClusterIP 10.107.232.220 \u0026lt;none\u0026gt; 8001/TCP 13m service/oimcluster-soa-server3 ClusterIP 10.108.203.6 \u0026lt;none\u0026gt; 8001/TCP 13m service/oimcluster-soa-server4 ClusterIP 10.96.178.0 \u0026lt;none\u0026gt; 8001/TCP 13m service/oimcluster-soa-server5 ClusterIP 10.107.83.62 \u0026lt;none\u0026gt; 8001/TCP 13m NAME COMPLETIONS DURATION AGE job.batch/oimcluster-create-fmw-infra-sample-domain-job 1/1 5m30s 36m NAME AGE domain.weblogic.oracle/oimcluster 17m Note: It will take several minutes before all the services listed above show. While the oimcluster-oim-server1 pod has a STATUS of 0/1 the pod is started but the OIG server associated with it is currently starting. While the pod is starting you can check the startup status in the pod logs, by running the following command:\n$ kubectl logs oimcluster-soa-server1 -n oimcluster   The default domain created by the script has the following characteristics:\n An Administration Server named AdminServer listening on port 7001. A configured OIG cluster named oig_cluster of size 5. A configured SOA cluster named soa_cluster of size 5. One started OIG managed Server, named oim_server1, listening on port 14000. One started SOA managed Server, named soa_server1, listening on port 8001. Log files that are located in \u0026lt;persistent_volume\u0026gt;/logs/\u0026lt;domainUID\u0026gt;  Verify the Domain To confirm that the domain was created, use this command:\n$ kubectl describe domain \u0026lt;domain_uid\u0026gt; -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl describe domain oimcluster -n oimcluster Here is an example of the output of this command:\nName: oimcluster Namespace: oimcluster Labels: weblogic.domainUID=oimcluster Annotations: API Version: weblogic.oracle/v8 Kind: Domain Metadata: Creation Timestamp: 2020-09-29T14:08:09Z Generation: 2 Managed Fields: API Version: weblogic.oracle/v8 Fields Type: FieldsV1 fieldsV1: f:metadata: f:annotations: .: f:kubectl.kubernetes.io/last-applied-configuration: f:labels: .: f:weblogic.domainUID: Manager: kubectl Operation: Update Time: 2020-09-29T14:19:58Z API Version: weblogic.oracle/v8 Fields Type: FieldsV1 fieldsV1: f:status: .: f:clusters: f:conditions: f:servers: f:startTime: Manager: OpenAPI-Generator Operation: Update Time: 2020-09-29T14:27:30Z Resource Version: 1278400 Self Link: /apis/weblogic.oracle/v8/namespaces/oimcluster/domains/oimcluster UID: 94604c47-6995-43c5-8848-5c5975ba5ace Spec: Admin Server: Server Pod: Env: Name: USER_MEM_ARGS Value: -Djava.security.egd=file:/dev/./urandom -Xms512m -Xmx1024m Server Start State: RUNNING Clusters: Cluster Name: soa_cluster Replicas: 1 Server Pod: Affinity: Pod Anti Affinity: Preferred During Scheduling Ignored During Execution: Pod Affinity Term: Label Selector: Match Expressions: Key: weblogic.clusterName Operator: In Values: $(CLUSTER_NAME) Topology Key: kubernetes.io/hostname Weight: 100 Server Service: Precreate Service: true Server Start State: RUNNING Cluster Name: oim_cluster Replicas: 1 Server Pod: Affinity: Pod Anti Affinity: Preferred During Scheduling Ignored During Execution: Pod Affinity Term: Label Selector: Match Expressions: Key: weblogic.clusterName Operator: In Values: $(CLUSTER_NAME) Topology Key: kubernetes.io/hostname Weight: 100 Server Service: Precreate Service: true Server Start State: RUNNING Data Home: Domain Home: /u01/oracle/user_projects/domains/oimcluster Domain Home Source Type: PersistentVolume Http Access Log In Log Home: true Image: oracle/oig:12.2.1.4.0 Image Pull Policy: IfNotPresent Image Pull Secrets: Name: oig-docker Include Server Out In Pod Log: true Log Home: /u01/oracle/user_projects/domains/logs/oimcluster Log Home Enabled: true Server Pod: Env: Name: JAVA_OPTIONS Value: -Dweblogic.StdoutDebugEnabled=false Name: USER_MEM_ARGS Value: -Djava.security.egd=file:/dev/./urandom -Xms256m -Xmx1024m Volume Mounts: Mount Path: /u01/oracle/user_projects/domains Name: weblogic-domain-storage-volume Volumes: Name: weblogic-domain-storage-volume Persistent Volume Claim: Claim Name: oimcluster-oim-pvc Server Start Policy: IF_NEEDED Web Logic Credentials Secret: Name: oimcluster-domain-credentials Status: Clusters: Cluster Name: oim_cluster Maximum Replicas: 5 Minimum Replicas: 0 Ready Replicas: 1 Replicas: 1 Replicas Goal: 1 Cluster Name: soa_cluster Maximum Replicas: 5 Minimum Replicas: 0 Ready Replicas: 1 Replicas: 1 Replicas Goal: 1 Conditions: Last Transition Time: 2020-09-29T14:25:51.338Z Reason: ServersReady Status: True Type: Available Servers: Desired State: RUNNING Health: Activation Time: 2020-09-29T14:12:23.439Z Overall Health: ok Subsystems: Subsystem Name: ServerRuntime Symptoms: Node Name: 10.250.111.112 Server Name: AdminServer State: RUNNING Cluster Name: oim_cluster Desired State: RUNNING Health: Activation Time: 2020-09-29T14:25:46.339Z Overall Health: ok Subsystems: Subsystem Name: ServerRuntime Symptoms: Node Name: 10.250.111.112 Server Name: oim_server1 State: RUNNING Cluster Name: oim_cluster Desired State: SHUTDOWN Server Name: oim_server2 Cluster Name: oim_cluster Desired State: SHUTDOWN Server Name: oim_server3 Cluster Name: oim_cluster Desired State: SHUTDOWN Server Name: oim_server4 Cluster Name: oim_cluster Desired State: SHUTDOWN Server Name: oim_server5 Cluster Name: soa_cluster Desired State: RUNNING Health: Activation Time: 2020-09-29T14:15:11.288Z Overall Health: ok Subsystems: Subsystem Name: ServerRuntime Symptoms: Node Name: 10.250.111.112 Server Name: soa_server1 State: RUNNING Cluster Name: soa_cluster Desired State: SHUTDOWN Server Name: soa_server2 Cluster Name: soa_cluster Desired State: SHUTDOWN Server Name: soa_server3 Cluster Name: soa_cluster Desired State: SHUTDOWN Server Name: soa_server4 Cluster Name: soa_cluster Desired State: SHUTDOWN Server Name: soa_server5 Start Time: 2020-09-29T14:08:10.085Z Events: \u0026lt;none\u0026gt; In the Status section of the output, the available servers and clusters are listed.\nVerify the Pods Use the following command to see the pods running the servers and which nodes they are running on:\n$ kubectl get pods -n \u0026lt;namespace\u0026gt; -o wide For example:\n$ kubectl get pods -n oimcluster -o wide The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES helper 1/1 Running 0 3h50m 10.244.1.39 10.250.111.112 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; oimcluster-adminserver 1/1 Running 0 27m 10.244.1.42 10.250.111.112 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; oimcluster-create-fmw-infra-sample-domain-job-dktkk 0/1 Completed 0 47m 10.244.1.40 10.250.111.112 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; oimcluster-oim-server1 1/1 Running 0 16m 10.244.1.44 10.250.111.112 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; oimcluster-soa-server1 1/1 Running 0 24m 10.244.1.43 10.250.111.112 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; You are now ready to configure an Ingress to direct traffic for your OIG domain as per Configure an Ingress for an OIG domain.\n"
},
{
	"uri": "/fmw-kubernetes/wcsites-domains/create-wcsites-domains/",
	"title": "Create WebCenter Sites domains",
	"tags": [],
	"description": "Sample for creating a WebCenter Sites domain home on an existing PV or PVC, and the domain resource YAML file for deploying the generated WebCenter Sites domain.",
	"content": " Oracle WebCenter Sites is currently supported for non-production use in Docker and Kubernetes. The information provided in this document is a preview for early adopters who wish to experiment with Oracle WebCenter Sites in Kubernetes before it is supported for production use.\n Contents  Introduction Prerequisites Prepare the WebCenter Sites Domain Creation Input File Create the WebCenter Sites Domain Initialize the WebCenter Sites Domain Verify the WebCenter Sites Domain Expose WebCenter Sites Services Load Balance With an Ingress Controller or A Web Server Configure WebCenter Sites Settings in WebCenter Sites Property Management For Publishing Setting in WebCenter Sites  Introduction This document details on how to use sample scripts to demonstrate the creation of a WebCenter Sites domain home on an existing Kubernetes persistent volume (PV) and persistent volume claim (PVC). The scripts also generate the domain YAML file, which can then be used to start the Kubernetes artifacts of the corresponding domain.\nPrerequisites  Ensure that you have completed all of the steps under prepare-your-environment. Ensure that the database and the WebLogic Kubernetes operator is up.  Prepare the WebCenter Sites Domain Creation Input File If required, domain creation inputs can be customized by editing create-domain-inputs.yaml as described below:\nPlease note that the sample scripts for the WebCenter Sites domain deployment are available from the previously downloaded repository at kubernetes/samples/scripts/create-wcsites-domain/domain-home-on-pv/.\nMake a copy of the create-domain-inputs.yaml file before updating the default values.\nThe default domain created by the script has the following characteristics:\n An Administration Server named AdminServer listening on port 7001. A configured cluster named wcsites_cluster of size 5. Managed Server, named wcsites_server1, listening on port 8001. Log files that are located in /shared/logs/\u0026lt;domainUID\u0026gt;.  Configuration parameters The following parameters can be provided in the inputs file:\n   Parameter Definition Default     adminPort Port number for the Administration Server inside the Kubernetes cluster. 7001   adminServerName Name of the Administration Server. AdminServer   clusterName Name of the WebLogic cluster instance to generate for the domain. By default the cluster name is wcsites_cluster for the WebCenter Sites domain. wcsites_cluster   configuredManagedServerCount Number of Managed Server instances for the domain. 3   createDomainFilesDir Directory on the host machine to locate all the files that you need to create a WebLogic domain, including the script that is specified in the createDomainScriptName property. By default, this directory is set to the relative path wlst, and the create script will use the built-in WLST offline scripts in the wlst directory to create the WebLogic domain. It can also be set to the relative path wdt, and then the built-in WDT scripts will be used instead. An absolute path is also supported to point to an arbitrary directory in the file system. The built-in scripts can be replaced by the user-provided scripts or model files as long as those files are in the specified directory. Files in this directory are put into a Kubernetes config map, which in turn is mounted to the createDomainScriptsMountPath, so that the Kubernetes pod can use the scripts and supporting files to create a domain home. wlst   createDomainScriptsMountPath Mount path where the create domain scripts are located inside a pod. The create-domain.sh script creates a Kubernetes job to run the script (specified in the createDomainScriptName property) in a Kubernetes pod to create a domain home. Files in the createDomainFilesDir directory are mounted to this location in the pod, so that the Kubernetes pod can use the scripts and supporting files to create a domain home. /u01/weblogic   createDomainScriptName Script that the create domain script uses to create a WebLogic domain. The create-domain.sh script creates a Kubernetes job to run this script to create a domain home. The script is located in the in-pod directory that is specified in the createDomainScriptsMountPath property. If you need to provide your own scripts to create the domain home, instead of using the built-it scripts, you must use this property to set the name of the script that you want the create domain job to run. create-domain-job.sh   domainHome Home directory of the WebCenter Sites domain. If not specified, the value is derived from the domainUID as /shared/domains/\u0026lt;domainUID\u0026gt;. /u01/oracle/user_projects/domains/wcsitesinfra   domainPVMountPath Mount path of the domain persistent volume. /u01/oracle/user_projects/domains   domainUID Unique ID that will be used to identify this particular domain. Used as the name of the generated WebLogic domain as well as the name of the Kubernetes domain resource. This ID must be unique across all domains in a Kubernetes cluster. This ID cannot contain any character that is not valid in a Kubernetes service name. wcsitesinfra   exposeAdminNodePort Boolean indicating if the Administration Server is exposed outside of the Kubernetes cluster. false   exposeAdminT3Channel Boolean indicating if the T3 administrative channel is exposed outside the Kubernetes cluster. false   image WebCenter Sites Docker image. The Operator requires WebCenter Sites release 12.2.1.4.0. Refer to WebCenter Sites Docker Image for details on how to obtain or create the image. oracle/wcsites:12.2.1.4   imagePullPolicy WebLogic Docker image pull policy. Legal values are IfNotPresent, Always, or Never IfNotPresent   imagePullSecretName Name of the Kubernetes secret to access the Docker Store to pull the WebLogic Server Docker image. The presence of the secret will be validated when this parameter is specified.    includeServerOutInPodLog Boolean indicating whether to include the server.out to the pod\u0026rsquo;s stdout. true   initialManagedServerReplicas Number of Managed Server to initially start for the domain. 1   javaOptions Java options for starting the Administration Server and Managed Servers. A Java option can include references to one or more of the following pre-defined variables to obtain WebLogic domain information: $(DOMAIN_NAME), $(DOMAIN_HOME), $(ADMIN_NAME), $(ADMIN_PORT), and $(SERVER_NAME). -Dweblogic.StdoutDebugEnabled=false   logHome The in-pod location for the domain log, server logs, server out, and Node Manager log files. If not specified, the value is derived from the domainUID as /shared/logs/\u0026lt;domainUID\u0026gt;. /u01/oracle/user_projects/domains/logs/wcsitesinfra   managedServerNameBase Base string used to generate Managed Server names. wcsites_server   managedServerPort Port number for each Managed Server. 8001   namespace Kubernetes namespace in which to create the domain. wcsites-ns   persistentVolumeClaimName Name of the persistent volume claim created to host the domain home. If not specified, the value is derived from the domainUID as \u0026lt;domainUID\u0026gt;-weblogic-sample-pvc. wcsitesinfra-domain-pvc   productionModeEnabled Boolean indicating if production mode is enabled for the domain. true   serverStartPolicy Determines which WebLogic Server instances will be started. Legal values are NEVER, IF_NEEDED, ADMIN_ONLY. IF_NEEDED   t3ChannelPort Port for the T3 channel of the NetworkAccessPoint. 30012   t3PublicAddress Public address for the T3 channel. This should be set to the public address of the Kubernetes cluster. This would typically be a load balancer address. For development environments only: In a single server (all-in-one) Kubernetes deployment, this may be set to the address of the master, or at the very least, it must be set to the address of one of the worker nodes. If not provided, the script will attempt to set it to the IP address of the Kubernetes cluster.   weblogicCredentialsSecretName Name of the Kubernetes secret for the Administration Server\u0026rsquo;s user name and password. If not specified, then the value is derived from the domainUID as \u0026lt;domainUID\u0026gt;-weblogic-credentials. wcsites-domain-credentials   weblogicImagePullSecretName Name of the Kubernetes secret for the Docker Store, used to pull the WebLogic Server image.    serverPodCpuRequest, serverPodMemoryRequest, serverPodCpuCLimit, serverPodMemoryLimit The maximum amount of compute resources allowed and minimum amount of compute resources required for each server pod. Please refer to the Kubernetes documentation on Managing Compute Resources for Containers for details. Resource requests and resource limits are not specified. Refer to WebCenter Sites Cluster Sizing Recommendations for more details.   rcuSchemaPrefix The schema prefix to use in the database, for example WCS1. You may wish to make this the same as the domainUID in order to simplify matching domains to their RCU schemas. WCS1   rcuDatabaseURL The database URL. oracle-db.wcsitesdb-ns.svc.cluster.local:1521/devpdb.k8s   rcuCredentialsSecret The loadbalancer hostname to be provided. wcsites-rcu-credentials   loadBalancerHostName Hostname for the final url accessible outside K8S environment. abc.def.com   loadBalancerPortNumber Port for the final url accessible outside K8S environment. 30305   loadBalancerProtocol Protocol for the final url accessible outside K8S environment. http   loadBalancerType Loadbalancer name that will be used. Example: Traefik or \u0026quot;\u0026rdquo; traefik   unicastPort Starting range of uniciast port that application will use. 50000   sitesSamples Sites to be installed without samples sites by default, else true. false    You can form the names of the Kubernetes resources in the generated YAML files with the value of these properties specified in the create-domain-inputs.yaml file: adminServerName , clusterName and managedServerNameBase . Characters that are invalid in a Kubernetes service name are converted to valid values in the generated YAML files. For example, an uppercase letter is converted to a lowercase letter and an underscore (\u0026quot;_\u0026quot;) is converted to a hyphen (\u0026quot;-\u0026quot;) .\nThe sample demonstrates how to create a WebCenter Sites domain home and associated Kubernetes resources for a domain that has one cluster only. In addition, the sample provides the capability for users to supply their own scripts to create the domain home for other use cases. You can modify the generated domain YAML file to include more use cases.\nCreate the WebCenter Sites Domain   Understanding the syntax of the create-domain.sh script:\n$ ./create-domain.sh \\ -i create-domain-inputs.yaml \\ -o /\u0026lt;path to output-directory\u0026gt; The script performs the following functions:\n  Creates a directory for the generated Kubernetes YAML files for this domain if it does not already exist. The path name is /\u0026lt;path to output-directory\u0026gt;/weblogic-domains/\u0026lt;domainUID\u0026gt;. If the directory already exists, remove its content before using this script.\n  Creates a Kubernetes job that will start up a utility WebCenter Sites container and run offline WLST scripts to create the domain on the shared storage.\n  Runs and waits for the job to finish.\n  Creates a Kubernetes domain YAML file, domain.yaml, in the directory that is created above. This YAML file can be used to create the Kubernetes resource using the kubectl create -f or kubectl apply -f command:\n$ kubectl apply -f ../\u0026lt;path to output-directory\u0026gt;/weblogic-domains/\u0026lt;domainUID\u0026gt;/domain.yaml   Creates a convenient utility script, delete-domain-job.yaml, to clean up the domain home created by the create script.\n    Now, run the create-domain.sh sample script below, pointing it at the create-domain-inputs inputs file and an output directory like below:\nbash-4.2$ rm -rf kubernetes/samples/scripts/create-wcsites-domain/output/weblogic-domains bash-4.2$ sh kubernetes/samples/scripts/create-wcsites-domain/domain-home-on-pv/create-domain.sh \\  -i kubernetes/samples/scripts/create-wcsites-domain/domain-home-on-pv/create-domain-inputs.yaml \\  -o kubernetes/samples/scripts/create-wcsites-domain/output Input parameters being used export version=\u0026#34;create-weblogic-sample-domain-inputs-v1\u0026#34; export adminPort=\u0026#34;7001\u0026#34; export adminServerName=\u0026#34;adminserver\u0026#34; export domainUID=\u0026#34;wcsitesinfra\u0026#34; export domainHome=\u0026#34;/u01/oracle/user_projects/domains/$domainUID\u0026#34; export serverStartPolicy=\u0026#34;IF_NEEDED\u0026#34; export clusterName=\u0026#34;wcsites_cluster\u0026#34; export configuredManagedServerCount=\u0026#34;3\u0026#34; export initialManagedServerReplicas=\u0026#34;1\u0026#34; export managedServerNameBase=\u0026#34;wcsites_server\u0026#34; export managedServerPort=\u0026#34;8001\u0026#34; export image=\u0026#34;oracle/wcsites:12.2.1.4\u0026#34; export imagePullPolicy=\u0026#34;IfNotPresent\u0026#34; export productionModeEnabled=\u0026#34;true\u0026#34; export weblogicCredentialsSecretName=\u0026#34;wcsitesinfra-domain-credentials\u0026#34; export includeServerOutInPodLog=\u0026#34;true\u0026#34; export logHome=\u0026#34;/u01/oracle/user_projects/domains/logs/$domainUID\u0026#34; export t3ChannelPort=\u0026#34;30012\u0026#34; export exposeAdminT3Channel=\u0026#34;false\u0026#34; export adminNodePort=\u0026#34;30701\u0026#34; export exposeAdminNodePort=\u0026#34;false\u0026#34; export namespace=\u0026#34;wcsites-ns\u0026#34; javaOptions=-Dweblogic.StdoutDebugEnabled=false -Xms2g export persistentVolumeClaimName=\u0026#34;wcsitesinfra-domain-pvc\u0026#34; export domainPVMountPath=\u0026#34;/u01/oracle/user_projects/domains\u0026#34; export createDomainScriptsMountPath=\u0026#34;/u01/weblogic\u0026#34; export createDomainScriptName=\u0026#34;create-domain-job.sh\u0026#34; export createDomainFilesDir=\u0026#34;wlst\u0026#34; export rcuSchemaPrefix=\u0026#34;WCS1\u0026#34; export rcuDatabaseURL=\u0026#34;oracle-db.wcsitesdb-ns.svc.cluster.local:1521/devpdb.k8s\u0026#34; export rcuCredentialsSecret=\u0026#34;wcsitesinfra-rcu-credentials\u0026#34; export loadBalancerHostName=\u0026#34;abc.def.com\u0026#34; export loadBalancerPortNumber=\u0026#34;30305\u0026#34; export loadBalancerProtocol=\u0026#34;http\u0026#34; export loadBalancerType=\u0026#34;traefik\u0026#34; export unicastPort=\u0026#34;50000\u0026#34; export sitesSamples=\u0026#34;true\u0026#34; Generating kubernetes/samples/scripts/create-wcsites-domain/output/weblogic-domains/wcsitesinfra/create-domain-job.yaml Generating kubernetes/samples/scripts/create-wcsites-domain/output/weblogic-domains/wcsitesinfra/delete-domain-job.yaml Generating kubernetes/samples/scripts/create-wcsites-domain/output/weblogic-domains/wcsitesinfra/domain.yaml Checking to see if the secret wcsitesinfra-domain-credentials exists in namespace wcsites-ns configmap/wcsitesinfra-create-fmw-infra-sample-domain-job-cm created Checking the configmap wcsitesinfra-create-fmw-infra-sample-domain-job-cm was created configmap/wcsitesinfra-create-fmw-infra-sample-domain-job-cm labeled Checking if object type job with name wcsitesinfra-create-fmw-infra-sample-domain-job exists No resources found. $loadBalancerType is NOT empty Creating the domain by creating the job kubernetes/samples/scripts/create-wcsites-domain/output/weblogic-domains/wcsitesinfra/create-domain-job.yaml job.batch/wcsitesinfra-create-fmw-infra-sample-domain-job created Waiting for the job to complete... status on iteration 1 of 20 pod wcsitesinfra-create-fmw-infra-sample-domain-job-6l7zh status is Running status on iteration 2 of 20 pod wcsitesinfra-create-fmw-infra-sample-domain-job-6l7zh status is Running status on iteration 3 of 20 pod wcsitesinfra-create-fmw-infra-sample-domain-job-6l7zh status is Running status on iteration 4 of 20 pod wcsitesinfra-create-fmw-infra-sample-domain-job-6l7zh status is Running status on iteration 5 of 20 pod wcsitesinfra-create-fmw-infra-sample-domain-job-6l7zh status is Running status on iteration 6 of 20 pod wcsitesinfra-create-fmw-infra-sample-domain-job-6l7zh status is Running status on iteration 7 of 20 pod wcsitesinfra-create-fmw-infra-sample-domain-job-6l7zh status is Running status on iteration 8 of 20 pod wcsitesinfra-create-fmw-infra-sample-domain-job-6l7zh status is Running status on iteration 9 of 20 pod wcsitesinfra-create-fmw-infra-sample-domain-job-6l7zh status is Running status on iteration 10 of 20 pod wcsitesinfra-create-fmw-infra-sample-domain-job-6l7zh status is Running status on iteration 11 of 20 pod wcsitesinfra-create-fmw-infra-sample-domain-job-6l7zh status is Running status on iteration 12 of 20 pod wcsitesinfra-create-fmw-infra-sample-domain-job-6l7zh status is Running status on iteration 13 of 20 pod wcsitesinfra-create-fmw-infra-sample-domain-job-6l7zh status is Running status on iteration 14 of 20 pod wcsitesinfra-create-fmw-infra-sample-domain-job-6l7zh status is Running status on iteration 15 of 20 pod wcsitesinfra-create-fmw-infra-sample-domain-job-6l7zh status is Running status on iteration 16 of 20 pod wcsitesinfra-create-fmw-infra-sample-domain-job-6l7zh status is Completed Domain wcsitesinfra was created and will be started by the WebLogic Kubernetes Operator The following files were generated: kubernetes/samples/scripts/create-wcsites-domain/output/weblogic-domains/wcsitesinfra/create-domain-inputs.yaml kubernetes/samples/scripts/create-wcsites-domain/output/weblogic-domains/wcsitesinfra/create-domain-job.yaml kubernetes/samples/scripts/create-wcsites-domain/output/weblogic-domains/wcsitesinfra/domain.yaml Completed\t  To monitor the above domain creation logs:\n$ kubectl get pods -n wcsites-ns |grep wcsitesinfra-create wcsitesinfra-create-fmw-infra-sample-domain-job-6l7zh 1/1 Running 0 6s $ kubectl get pods -n wcsites-ns | grep wcsitesinfra-create | awk \u0026#39;{print $1}\u0026#39; | xargs kubectl -n wcsites-ns logs -f SAMPLE OUTPUT:\nThe domain will be created using the script /u01/weblogic/createSitesDomain.sh Install Automation -\u0026gt; Starting automation script [mkdir] Created dir: /u01/wcs-wls-docker-install/work [echo] [3/14/20 7:54 AM] Work Directory=/u01/wcs-wls-docker-install/work [echo] [3/14/20 7:54 AM] DB URL: jdbc:oracle:thin:@ [echo] [3/14/20 7:54 AM] Info -\u0026gt; The script.db.connectstring has been set. [echo] [3/14/20 7:54 AM] Info.setDBConnectStringPropertey -\u0026gt; setting oracle-db.wcsitesdb-ns.svc.cluster.local:1521/devpdb.k8s [echo] [3/14/20 7:54 AM] Validation -\u0026gt; Checking if full path to JAVA executable is correctly specified [exec] java version \u0026quot;1.8.0_241\u0026quot; [exec] Java(TM) SE Runtime Environment (build 1.8.0_241-b07) [exec] Java HotSpot(TM) 64-Bit Server VM (build 25.241-b07, mixed mode) [echo] [3/14/20 7:54 AM] Validation -\u0026gt; Checking database connection [echo] [3/14/20 7:54 AM] dbUrl-----------------: jdbc:oracle:thin:@oracle-db.wcsitesdb-ns.svc.cluster.local:1521/devpdb.k8s [echo] [3/14/20 7:54 AM] Database Connection --\u0026gt; Success! [echo] [3/14/20 7:54 AM] 1st phase: WebCenter Sites installation started... [copy] Copying 1 file to /u01/wcs-wls-docker-install/work [copy] Copying /u01/wcs-wls-docker-install/rcu.rsp to /u01/wcs-wls-docker-install/work/rcu.rsp [echo] [3/14/20 7:54 AM] 1st phase: WebCenter Sites installation completed [echo] [3/14/20 7:54 AM] 2nd phase: WebCenter Sites RCU configuration started... [echo] [3/14/20 7:54 AM] Installation -\u0026gt; Repository Creation Utility - creates schema [echo] [3/14/20 7:54 AM] connectString-----------------: oracle-db.wcsitesdb-ns.svc.cluster.local:1521/devpdb.k8s [replace] Replaced 1 occurrences in 1 files. [replace] Replaced 1 occurrences in 1 files. [replace] Replaced 1 occurrences in 1 files. [replace] Replaced 1 occurrences in 1 files. [replace] Replaced 1 occurrences in 1 files. [echo] [3/14/20 7:54 AM] Create schema using command: /u01/oracle/oracle_common/bin/rcu -silent -responseFile /u01/wcs-wls-docker-install/work/rcu.rsp -f \u0026lt; /u01/wcs-wls-docker-install/work/rcuPasswords8852085298596415722.txt \u0026gt;/u01/wcs-wls-docker-install/work/rcu_output.log [echo] [3/14/20 7:54 AM] RCU Create Schema -\u0026gt; Please wait ... may take several minutes [echo] [3/14/20 8:00 AM] [echo] RCU Logfile: /u01/wcs-wls-docker-install/work/rcu/RCU2020-03-14_07-54_2112542638/logs/rcu.log [echo] Processing command line .... [echo] Repository Creation Utility - Checking Prerequisites [echo] Checking Global Prerequisites [echo] Repository Creation Utility - Checking Prerequisites [echo] Checking Component Prerequisites [echo] Repository Creation Utility - Creating Tablespaces [echo] Validating and Creating Tablespaces [echo] Create tablespaces in the repository database [echo] Repository Creation Utility - Create [echo] Repository Create in progress. [echo] Executing pre create operations [echo] Percent Complete: 20 [echo] Percent Complete: 20 [echo] Percent Complete: 22 [echo] Percent Complete: 24 [echo] Percent Complete: 26 [echo] Percent Complete: 26 [echo] Percent Complete: 28 [echo] Percent Complete: 28 [echo] Creating Common Infrastructure Services(STB) [echo] Percent Complete: 36 [echo] Percent Complete: 36 [echo] Percent Complete: 46 [echo] Percent Complete: 46 [echo] Percent Complete: 46 [echo] Creating Audit Services Append(IAU_APPEND) [echo] Percent Complete: 54 [echo] Percent Complete: 54 [echo] Percent Complete: 64 [echo] Percent Complete: 64 [echo] Percent Complete: 64 [echo] Creating Audit Services Viewer(IAU_VIEWER) [echo] Percent Complete: 72 [echo] Percent Complete: 72 [echo] Percent Complete: 72 [echo] Percent Complete: 73 [echo] Percent Complete: 73 [echo] Percent Complete: 74 [echo] Percent Complete: 74 [echo] Percent Complete: 74 [echo] Creating Weblogic Services(WLS) [echo] Percent Complete: 79 [echo] Percent Complete: 79 [echo] Percent Complete: 83 [echo] Percent Complete: 83 [echo] Percent Complete: 92 [echo] Percent Complete: 99 [echo] Percent Complete: 99 [echo] Creating Audit Services(IAU) [echo] Percent Complete: 100 [echo] Creating Oracle Platform Security Services(OPSS) [echo] Creating WebCenter Sites(WCSITES) [echo] Executing post create operations [echo] Repository Creation Utility: Create - Completion Summary [echo] Database details: [echo] ----------------------------- [echo] Host Name : oracle-db.wcsitesdb-ns.svc.cluster.local:1521/devpdb.k8s [echo] Port : 1521 [echo] Service Name : devpdb.k8s [echo] Connected As : sys [echo] Prefix for (prefixable) Schema Owners : WCS1 [echo] RCU Logfile : /u01/wcs-wls-docker-install/work/rcu/RCU2020-03-14_07-54_2112542638/logs/rcu.log [echo] Component schemas created: [echo] ----------------------------- [echo] Component Status Logfile [echo] Common Infrastructure Services Success /u01/wcs-wls-docker-install/work/rcu/RCU2020-03-14_07-54_2112542638/logs/stb.log [echo] Oracle Platform Security Services Success /u01/wcs-wls-docker-install/work/rcu/RCU2020-03-14_07-54_2112542638/logs/opss.log [echo] WebCenter Sites Success /u01/wcs-wls-docker-install/work/rcu/RCU2020-03-14_07-54_2112542638/logs/wcsites.log [echo] Audit Services Success /u01/wcs-wls-docker-install/work/rcu/RCU2020-03-14_07-54_2112542638/logs/iau.log [echo] Audit Services Append Success /u01/wcs-wls-docker-install/work/rcu/RCU2020-03-14_07-54_2112542638/logs/iau_append.log [echo] Audit Services Viewer Success /u01/wcs-wls-docker-install/work/rcu/RCU2020-03-14_07-54_2112542638/logs/iau_viewer.log [echo] WebLogic Services Success /u01/wcs-wls-docker-install/work/rcu/RCU2020-03-14_07-54_2112542638/logs/wls.log [echo] Repository Creation Utility - Create : Operation Completed [echo] [3/14/20 8:00 AM] Successfully created schemas [echo] [3/14/20 8:00 AM] 2nd phase: WebCenter Sites RCU configuration completed successfully. [echo] [3/14/20 8:00 AM] Oracle WebCenter Sites Installation complete. You can connect to the WebCenter Sites instance at http://10.244.0.252:7002/sites/ Sites RCU Phase completed successfull!!! Sites Installation completed in 366 seconds. --------------------------------------------------------- The domain will be created using the script /u01/weblogic/create-domain-script.sh wlst.sh -skipWLSModuleScanning /u01/weblogic/createSitesDomain.py -oh /u01/oracle -jh /u01/jdk -parent /u01/oracle/user_projects/domains/wcsitesinfra/.. -name wcsitesinfra -user weblogic -password Welcome1 -rcuDb oracle-db.wcsitesdb-ns.svc.cluster.local:1521/devpdb.k8s -rcuPrefix WCS1 -rcuSchemaPwd Welcome1 -adminListenPort 7001 -adminName adminserver -managedNameBase wcsites_server -managedServerPort 8001 -prodMode true -managedServerCount 3 -clusterName wcsites_cluster -exposeAdminT3Channel false -t3ChannelPublicAddress 10.123.152.96 -t3ChannelPort 30012 -domainType wcsites -machineName wcsites_machine Initializing WebLogic Scripting Tool (WLST) ... Welcome to WebLogic Server Administration Scripting Shell Type help() for help on available commands Creating Admin Server... Creating cluster... Creating Node Managers... managed server name is wcsites_server1 managed server name is wcsites_server2 managed server name is wcsites_server3 ['wcsites_server1', 'wcsites_server2', 'wcsites_server3'] Will create Base domain at /u01/oracle/user_projects/domains/wcsitesinfra Writing base domain... Base domain created at /u01/oracle/user_projects/domains/wcsitesinfra Extending domain at /u01/oracle/user_projects/domains/wcsitesinfra Database oracle-db.wcsitesdb-ns.svc.cluster.local:1521/devpdb.k8s ExposeAdminT3Channel false with 10.123.152.96:30012 Applying JRF templates... Extension Templates added Applying WCSITES templates... Extension Templates added Configuring the Service Table DataSource... fmwDb...jdbc:oracle:thin:@oracle-db.wcsitesdb-ns.svc.cluster.local:1521/devpdb.k8s Set user...WCS140_OPSS Set user...WCS140_IAU_APPEND Set user...WCS140_IAU_VIEWER Set user...WCS140_STB Set user...WCS140_WCSITES Getting Database Defaults... Targeting Server Groups... Targeting Server Groups... Set CoherenceClusterSystemResource to defaultCoherenceCluster for server:wcsites_server1 Set CoherenceClusterSystemResource to defaultCoherenceCluster for server:wcsites_server2 Set CoherenceClusterSystemResource to defaultCoherenceCluster for server:wcsites_server3 Targeting Cluster ... Set CoherenceClusterSystemResource to defaultCoherenceCluster for cluster:wcsites_cluster Set WLS clusters as target of defaultCoherenceCluster:[wcsites_cluster] Preparing to update domain... Mar 14, 2020 8:01:52 AM oracle.security.jps.az.internal.runtime.policy.AbstractPolicyImpl initializeReadStore INFO: Property for read store in parallel: oracle.security.jps.az.runtime.readstore.threads = null Domain updated successfully Copying /u01/weblogic/server-config-update.sh to PV /u01/oracle/user_projects/domains/wcsitesinfra Copying /u01/weblogic/unicast.py to PV /u01/oracle/user_projects/domains/wcsitesinfra replacing tokens in /u01/oracle/user_projects/domains/wcsitesinfra/server-config-update.sh Successfully Completed   Initialize the WebCenter Sites Domain To start the domain, apply the above domain.yaml:\n$ kubectl apply -f kubernetes/samples/scripts/create-wcsites-domain/output/weblogic-domains/wcsitesinfra/domain.yaml domain.weblogic.oracle/wcsitesinfra created Verify the WebCenter Sites Domain Verify that the domain and servers pods and services are created and in the READY state:\nSample run below:\n-bash-4.2$ kubectl get pods -n wcsites-ns -w NAME READY STATUS RESTARTS\tAGE wcsitesinfra-create-fmw-infra-sample-domain-job-6l7zh 0/1 Completed 0 15m wcsitesinfra-introspect-domain-job-7tvdt 1/1 Running 0 15s wcsitesinfra-introspect-domain-job-7tvdt 0/1 Completed 0 25s wcsitesinfra-introspect-domain-job-7tvdt 0/1 Terminating 0 5s wcsitesinfra-adminserver 0/1 Pending 0 0s wcsitesinfra-adminserver 0/1 Init:0/1 0 0s wcsitesinfra-adminserver 0/1 PodInitializing 0 12s wcsitesinfra-adminserver 0/1 Running 0 13s wcsitesinfra-adminserver 1/1 Running 0 108s wcsitesinfra-wcsites-server1 0/1 Pending 0 0s wcsitesinfra-wcsites-server1 0/1 Init:0/1 0 1s wcsitesinfra-wcsites-server1 0/1 PodInitializing 0 13s wcsitesinfra-wcsites-server1 0/1 Running 0 14s wcsitesinfra-wcsites-server1 1/1 Running 0 96s -bash-4.2$ kubectl get all -n wcsites-ns NAME READY STATUS RESTARTS AGE pod/wcsitesinfra-adminserver 1/1 Running 0 7m5s pod/wcsitesinfra-create-fmw-infra-sample-domain-job-6l7zh 0/1 Completed 0 22m pod/wcsitesinfra-wcsites-server1 1/1 Running 0 5m17s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/wcsitesinfra-adminserver ClusterIP None \u0026lt;none\u0026gt; 7001/TCP 7m5s service/wcsitesinfra-cluster-wcsites-cluster ClusterIP 10.109.210.3 \u0026lt;none\u0026gt; 8001/TCP 5m17s service/wcsitesinfra-wcsites-server1 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP 5m17s NAME COMPLETIONS DURATION AGE job.batch/wcsitesinfra-create-fmw-infra-sample-domain-job 1/1 7m40s 22m To see the Admin and Managed Servers logs, you can check the pod logs:\n$ kubectl logs -f wcsitesinfra-adminserver -n wcsites-ns $ kubectl exec -it wcsitesinfra-adminserver -n wcsites-ns -- /bin/bash $ kubectl logs -f wcsitesinfra-wcsites-server1 -n wcsites-ns $ kubectl exec -it wcsitesinfra-wcsites-server1 -n wcsites-ns -- /bin/bash Verify the Pods Use the following command to see the pods running the servers:\n$ kubectl get pods -n NAMESPACE Here is an example of the output of this command:\n-bash-4.2$ kubectl get pods -n wcsites-ns NAME READY STATUS RESTARTS AGE wcsitesinfra-adminserver 1/1 Running 0 56m wcsitesinfra-create-fmw-infra-sample-domain-job-rq4xv 0/1 Completed 0 65m wcsitesinfra-wcsites-server1 1/1 Running 0 41m wcsitesinfra-wcsites-server2 1/1 Running 0 41m Verify the Services Use the following command to see the services for the domain:\n$ kubectl get services -n NAMESPACE Here is an example of the output of this command:\n-bash-4.2$ kubectl get services -n wcsites-ns NAME READY STATUS RESTARTS AGE wcsitesinfra-adminserver 1/1 Running 0 7m38s wcsitesinfra-create-fmw-infra-sample-domain-job-6l7zh 0/1 Completed 0 23m wcsitesinfra-wcsites-server1 1/1 Running 0 5m50s Expose WebCenter Sites Services Below are the default values for exposing services required for all the WebCenter Sites Managed Servers. Reset them if any values are modified.\nDetails on kubernetes/samples/scripts/create-wcsites-domain/utils/wcs-services.yaml:\n name: wcsitesinfra-wcsites-server1-mcp namespace: wcsites-ns weblogic.domainUID: wcsitesinfra weblogic.serverName: wcsites_server1  Execute the below command for exposing the services: (If domain is configured for more than 3 Managed Servers then add the service yaml for additional servers.)\n$ kubectl apply -f kubernetes/samples/scripts/create-wcsites-domain/utils/wcs-services.yaml service/wcsitesinfra-wcsites-server1-np created service/wcsitesinfra-wcsites-server1-svc created service/wcsitesinfra-wcsites-server2-svc created service/wcsitesinfra-wcsites-server3-svc created To verify the services created, here is an example of the output of this command:\n-bash-4.2$ kubectl get services -n wcsites-ns NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE wcsitesinfra-adminserver ClusterIP None \u0026lt;none\u0026gt; 7001/TCP 11m wcsitesinfra-cluster-wcsites-cluster ClusterIP 10.109.210.3 \u0026lt;none\u0026gt; 8001/TCP 9m14s wcsitesinfra-wcsites-server1 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP 9m14s wcsitesinfra-wcsites-server1-np NodePort 10.105.167.205 \u0026lt;none\u0026gt; 8001:30155/TCP 2m47s wcsitesinfra-wcsites-server1-svc ClusterIP None \u0026lt;none\u0026gt; 50000/TCP,50001/TCP,50002/TCP,50003/TCP,50004/TCP,50005/TCP,50006/TCP,50007/TCP,50008/TCP,50009/TCP 2m47s wcsitesinfra-wcsites-server2-svc ClusterIP None \u0026lt;none\u0026gt; 50000/TCP,50001/TCP,50002/TCP,50003/TCP,50004/TCP,50005/TCP,50006/TCP,50007/TCP,50008/TCP,50009/TCP 2m47s wcsitesinfra-wcsites-server3-svc ClusterIP None \u0026lt;none\u0026gt; 50000/TCP,50001/TCP,50002/TCP,50003/TCP,50004/TCP,50005/TCP,50006/TCP,50007/TCP,50008/TCP,50009/TCP 2m47s Load Balance With an Ingress Controller or A Web Server You can choose a load balancer provider for your WebLogic domains running in a Kubernetes cluster. Please refer to the WebLogic Kubernetes Operator Load Balancer Samples for information about the current capabilities and setup instructions for each of the supported load balancers.\nFor information on how to set up Loadbalancer for setting up WebCenter Sites domain on K8S:\nFor Traefik, see Setting Up Loadbalancer Traefik for the WebCenter Sites Domain on K8S\nFor Voyager, see Setting Up Loadbalancer Voyager for the WebCenter Sites Domain on K8S\nConfigure WebCenter Sites   Configure WebCenter Sites by hitting url http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-PORT}/sites/sitesconfigsetup\nWhen installing, select sample sites to be installed and enter the required passwords. Do not change the sites-config location. If you change the location, installation will fail.\n  After the configuration is complete, edit the domain, and restart the Managed Server.\n  To stop Managed Servers:\n$ kubectl patch domain wcsitesinfra -n wcsites-ns --type=\u0026#39;json\u0026#39; -p=\u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/clusters/0/replicas\u0026#34;, \u0026#34;value\u0026#34;: 0 }]\u0026#39; To start all configured Managed Servers:\n$ kubectl patch domain wcsitesinfra -n wcsites-ns --type=\u0026#39;json\u0026#39; -p=\u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/clusters/0/replicas\u0026#34;, \u0026#34;value\u0026#34;: 3 }]\u0026#39; Wait till the Managed Server pod is killed and then restart it. Monitor with below command: -bash-4.2$ kubectl get pods -n wcsites-ns -w NAME READY STATUS RESTARTS AGE wcsitesinfra-adminserver 1/1 Running 0 111m wcsitesinfra-create-fmw-infra-sample-domain-job-6l7zh 0/1 Completed 0 126m wcsitesinfra-wcsites-server1 1/1 Running 0 3m7s wcsitesinfra-wcsites-server2 1/1 Running 0 3m7s wcsitesinfra-wcsites-server3 1/1 Running 0 3m7s   Settings in WebCenter Sites Property Management Incase of Voyager Load Balancer: Use Property Management Tool and update cookieserver.validnames property with value JSESSIONID,SERVERID.\nIncase of Traefik Load Balancer: Use Property Management Tool and update cookieserver.validnames property with value JSESSIONID,sticky.\nFor Publishing Setting in WebCenter Sites While configuring publishing destination use NodePort port of target cluster which can be found by executing below command:\n(In this example for publishihng the port 30155 has to be used.)\n-bash-4.2$ kubectl get service/wcsitesinfra-wcsites-server1-np -n wcsites-ns NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE wcsitesinfra-wcsites-server1-np NodePort 10.105.167.205 \u0026lt;none\u0026gt; 8001:30155/TCP 32h "
},
{
	"uri": "/fmw-kubernetes/oam/manage-oam-domains/logging-and-visualization/",
	"title": "Logging and Visualization",
	"tags": [],
	"description": "Describes the steps for logging and visualization with Elasticsearch and Kibana.",
	"content": "After the OAM domain is set up you can publish operator and WebLogic Server logs into Elasticsearch and interact with them in Kibana.\nIn Prepare your environment if you decided to use the Elasticsearch and Kibana by setting the parameter elkIntegrationEnabled to true, then the steps below must be followed to complete the setup.\nIf you did not set elkIntegrationEnabled to true and want to do so post configuration, run the following command:\n$ helm upgrade --reuse-values --namespace operator --set \u0026#34;elkIntegrationEnabled=true\u0026#34; --set \u0026#34;logStashImage=logstash:6.6.0\u0026#34; --set \u0026#34;elasticSearchHost=elasticsearch.default.svc.cluster.local\u0026#34; --set \u0026#34;elasticSearchPort=9200\u0026#34; --wait weblogic-kubernetes-operator kubernetes/charts/weblogic-operator The output will look similar to the following:\nRelease \u0026#34;weblogic-kubernetes-operator\u0026#34; has been upgraded. Happy Helming! NAME: weblogic-kubernetes-operator LAST DEPLOYED: Fri Sep 25 09:57:11 2020 NAMESPACE: operator STATUS: deployed REVISION: 3 TEST SUITE: None Install Elasticsearch and Kibana   Create the Kubernetes resource using the following command:\n$ kubectl apply -f \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/scripts/elasticsearch-and-kibana/elasticsearch_and_kibana.yaml For example:\n$ kubectl apply -f /scratch/OAMDockerK8S/weblogic-kubernetes-operator/kubernetes/samples/scripts/elasticsearch-and-kibana/elasticsearch_and_kibana.yaml The output will look similar to the following:\ndeployment.apps/elasticsearch created service/elasticsearch created deployment.apps/kibana created service/kibana created   Run the following command to ensure Elasticsearch is used by the operator:\n$ helm get values --all weblogic-kubernetes-operator -n opns The output will look similar to the following:\nCOMPUTED VALUES: dedicated: false domainNamespaces: - accessns elasticSearchHost: elasticsearch.default.svc.cluster.local elasticSearchPort: 9200 elkIntegrationEnabled: true externalDebugHttpPort: 30999 externalRestEnabled: false externalRestHttpsPort: 31001 image: weblogic-kubernetes-operator:3.0.1 imagePullPolicy: IfNotPresent internalDebugHttpPort: 30999 istioEnabled: false javaLoggingLevel: FINE logStashImage: logstash:6.6.0 remoteDebugNodePortEnabled: false serviceAccount: op-sa suspendOnDebugStartup: false   To check that Elasticsearch and Kibana are deployed in the Kubernetes cluster, run the following command:\n$ kubectl get pods The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE elasticsearch-857bd5ff6b-h8bxm 1/1 Running 0 67s kibana-594465687d-84hxz 1/1 Running 0 67s   Create the logstash pod OAM Server logs can be pushed to the Elasticsearch server using the logstash pod. The logstash pod needs access to the persistent volume of the OAM domain created previously, for example accessinfra-domain-pv. The steps to create the logstash pod are as follows:\n  Obtain the OAM domain persistence volume details:\n$ kubectl get pv -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get pv -n accessns The output will look similar to the following:\nNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE accessinfra-domain-pv 10Gi RWX Retain Bound accessns/accessinfra-domain-pvc accessinfra-domain-storage-class 1h12m Make note of the CLAIM value, for example in this case accessinfra-domain-pvc\n  Run the following command to get the mountPath of your domain:\n$ kubectl describe domains \u0026lt;domain_uid\u0026gt; -n \u0026lt;domain_namespace\u0026gt; | grep \u0026#34;Mount Path\u0026#34; For example:\n$ kubectl describe domains accessinfra -n accessns | grep \u0026#34;Mount Path\u0026#34; The output will look similar to the following:\nMount Path: /u01/oracle/user_projects/domains   Navigate to the \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/scripts/elasticsearch-and-kibana directory and create a logstash.yaml file as follows. Change the claimName and mountPath values to match the values returned in the previous commands:\napiVersion: apps/v1 kind: Deployment metadata: name: logstash-wls namespace: accessns spec: selector: matchLabels: k8s-app: logstash-wls template: # create pods using pod definition in this template metadata: labels: k8s-app: logstash-wls spec: volumes: - name: weblogic-domain-storage-volume persistentVolumeClaim: claimName: accessinfra-domain-pvc - name: shared-logs emptyDir: {} containers: - name: logstash image: logstash:6.6.0 command: [\u0026quot;/bin/sh\u0026quot;] args: [\u0026quot;/usr/share/logstash/bin/logstash\u0026quot;, \u0026quot;-f\u0026quot;, \u0026quot;/u01/oracle/user_projects/domains/logstash/logstash.conf\u0026quot;] imagePullPolicy: IfNotPresent volumeMounts: - mountPath: /u01/oracle/user_projects/domains name: weblogic-domain-storage-volume - name: shared-logs mountPath: /shared-logs ports: - containerPort: 5044 name: logstash   In the NFS persistent volume directory that corresponds to the mountPath /u01/oracle/user_projects/domains, create a logstash directory. For example:\nmkdir -p /scratch/OAMDockerK8S/accessdomainpv/logstash   Create a logstash.conf in the newly created logstash directory that contains the following. Make sure the paths correspond to your mountPath and domain name:\ninput { file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/logs/accessinfra/AdminServer*.log\u0026quot; tags =\u0026gt; \u0026quot;Adminserver_log\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/logs/accessinfra/oam_policy_mgr*.log\u0026quot; tags =\u0026gt; \u0026quot;Policymanager_log\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/logs/accessinfra/oam_server*.log\u0026quot; tags =\u0026gt; \u0026quot;Oamserver_log\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/accessinfra/servers/AdminServer/logs/AdminServer-diagnostic.log\u0026quot; tags =\u0026gt; \u0026quot;Adminserver_diagnostic\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/accessinfra/servers/**/logs/oam_policy_mgr*-diagnostic.log\u0026quot; tags =\u0026gt; \u0026quot;Policy_diagnostic\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/accessinfra/servers/**/logs/oam_server*-diagnostic.log\u0026quot; tags =\u0026gt; \u0026quot;Oamserver_diagnostic\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/accessinfra/servers/**/logs/access*.log\u0026quot; tags =\u0026gt; \u0026quot;Access_logs\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/accessinfra/servers/AdminServer/logs/auditlogs/OAM/audit.log\u0026quot; tags =\u0026gt; \u0026quot;Audit_logs\u0026quot; start_position =\u0026gt; beginning } } filter { grok { match =\u0026gt; [ \u0026quot;message\u0026quot;, \u0026quot;\u0026lt;%{DATA:log_timestamp}\u0026gt; \u0026lt;%{WORD:log_level}\u0026gt; \u0026lt;%{WORD:thread}\u0026gt; \u0026lt;%{HOSTNAME:hostname}\u0026gt; \u0026lt;%{HOSTNAME:servername}\u0026gt; \u0026lt;%{DATA:timer}\u0026gt; \u0026lt;\u0026lt;%{DATA:kernel}\u0026gt;\u0026gt; \u0026lt;\u0026gt; \u0026lt;%{DATA:uuid}\u0026gt; \u0026lt;%{NUMBER:timestamp}\u0026gt; \u0026lt;%{DATA:misc}\u0026gt; \u0026lt;%{DATA:log_number}\u0026gt; \u0026lt;%{DATA:log_message}\u0026gt;\u0026quot; ] } if \u0026quot;_grokparsefailure\u0026quot; in [tags] { mutate { remove_tag =\u0026gt; [ \u0026quot;_grokparsefailure\u0026quot; ] } } } output { elasticsearch { hosts =\u0026gt; [\u0026quot;elasticsearch.default.svc.cluster.local:9200\u0026quot;] } }   Deploy the logstash pod by executing the following command:\n$ kubectl create -f \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/scripts/elasticsearch-and-kibana/logstash.yaml The output will look similar to the following:\ndeployment.apps/logstash-wls created   Run the following command to check the logstash pod is created correctly:\n$ kubectl get pods -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl get pods -n accessns The output should look similar to the following:\nNAME READY STATUS RESTARTS AGE accessinfra-adminserver 1/1 Running 0 36m accessinfra-create-oam-infra-domain-job-vj69h 0/1 Completed 0 24h accessinfra-oam-policy-mgr1 1/1 Running 0 33m accessinfra-oam-server1 1/1 Running 0 33m accessinfra-oam-server2 1/1 Running 0 33m helper 1/1 Running 0 41h logstash-wls-7957897645-67c4k 1/1 Running 0 7s voyager-accessinfra-voyager-698764d6d-w8pbt 1/1 Running 0 66m Then run the following to get the Elasticsearch pod name:\n$ kubectl get pods The output should look similar to the following:\nNAME READY STATUS RESTARTS AGE elasticsearch-857bd5ff6b-h8bxm 1/1 Running 0 5m45s kibana-594465687d-84hxz 1/1 Running 0 5m45s   Verify and access the Kibana console   Check if the indices are created correctly in the elasticsearch pod:\n$ kubectl exec -it elasticsearch-857bd5ff6b-h8bxm -- /bin/bash This will take you into a bash shell in the elasticsearch pod:\n[root@elasticsearch-857bd5ff6b-h8bxm elasticsearch]#   In the elasticsearch bash shell, run the following to check the indices:\n[root@elasticsearch-857bd5ff6b-h8bxm elasticsearch]# curl -i \u0026#34;127.0.0.1:9200/_cat/indices?v\u0026#34; The output will look similar to the following:\nHTTP/1.1 200 OK content-type: text/plain; charset=UTF-8 content-length: 696 health status index uuid pri rep docs.count docs.deleted store.size pri.store.size yellow open logstash-2020.09.23 -kVgdpB7TPSwnjvhEDD2RA 5 1 825 0 406.6kb 406.6kb green open .kibana_1 F6DNmwQ5SZaOM7I2LonEVw 1 0 2 0 7.6kb 7.6kb yellow open logstash-2020.09.25 9QQA-DwvQay8uOAe3dvKuQ 5 1 149293 0 39.3mb 39.3mb yellow open logstash-2020.09.24 t5N8O0LxRRabND6StHFgSg 5 1 69748 0 21.1mb 21.1mb green open .kibana_task_manager kt1uSgpnSGWgWR8nKDuiVA 1 0 2 0 12.5kb 12.5kb Exit the bash shell by typing exit.\n  Find the Kibana port by running the following command:\n$ kubectl get svc The output will look similar to the following:\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE elasticsearch ClusterIP 10.97.144.163 \u0026lt;none\u0026gt; 9200/TCP,9300/TCP 9m25s kibana NodePort 10.103.150.116 \u0026lt;none\u0026gt; 5601:30707/TCP 9m25s kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 47h In the example above the Kibana port is 30707.\n  Access the Kibana console with http://${MASTERNODE-HOSTNAME}:${KIBANA-PORT}/app/kibana.\n  Click Dashboard and in the Create index pattern page enter logstash*. Click Next Step.\n  From the Time Filter field name drop down menu select @timestamp and click Create index pattern.\n  Once the index pattern is created click on Discover in the navigation menu to view the logs.\n  For more details on how to use the Kibana console see the Kibana Guide\n"
},
{
	"uri": "/fmw-kubernetes/oig/manage-oig-domains/running-oig-utilities/",
	"title": "Runnning OIG Utilities",
	"tags": [],
	"description": "Describes the steps for running OIG utilities in Kubernetes.",
	"content": "Run OIG utlities inside the OIG Kubernetes cluster.\nRun utilities in an interactive bash shell   Access a bash shell inside the oimcluster-oim-server1 pod:\n$ kubectl -n oimcluster exec -it oimcluster-oim-server1 -- bash This will take you into a bash shell in the running oimcluster-oim-server1 pod:\n[oracle@oimcluster-oim-server1 oracle]$   Navigate to the /u01/oracle/idm/server/bin directory and execute the utility as required. For example:\n[oracle@oimcluster-oim-server1 oracle] cd /u01/oracle/idm/server/bin [oracle@oimcluster-oim-server1 bin]$ ./\u0026lt;filename\u0026gt;.sh   Passing inputs as a jar/xml file   Copy the input file to pass to a directory of your choice.\n  Run the following command to copy the input file to the running oimcluster-oim-server1 pod.\n$ kubectl -n oimcluster cp /\u0026lt;path\u0026gt;/\u0026lt;inputFile\u0026gt; oimcluster-oim-server1:/u01/oracle/idm/server/bin/   Access a bash shell inside the oimcluster-oim-server1 pod:\n$ kubectl -n oimcluster exec -it oimcluster-oim-server1 -- bash This will take you into a bash shell in the running oimcluster-oim-server1 pod:\n[oracle@oimcluster-oim-server1 oracle]$   Navigate to the /u01/oracle/idm/server/bin directory and execute the utility as required, passing the input file. For example:\n[oracle@oimcluster-oim-server1 oracle] cd /u01/oracle/idm/server/bin [oracle@oimcluster-oim-server1 bin]$ ./\u0026lt;filename\u0026gt;.sh -inputFile \u0026lt;inputFile\u0026gt; Note As pods are stateless the copied input file will remain until the pod restarts.\n  Editing property/profile files To edit a property/profile file in the Kubernetes cluster:\n  Copy the input file from the pod to a on the local system, for example:\n$ kubectl -n oimcluster cp oimcluster-oim-server1:/u01/oracle/idm/server/bin/\u0026lt;file.properties_profile\u0026gt; /\u0026lt;path\u0026gt;/\u0026lt;file.properties_profile\u0026gt; Note: If you see the message tar: Removing leading '/' from member names this can be ignored.\n  Edit the \u0026lt;/path\u0026gt;/\u0026lt;file.properties_profile\u0026gt; in an editor of your choice.\n  Copy the file back to the pod:\n$ kubectl -n oimcluster cp /\u0026lt;path\u0026gt;/\u0026lt;file.properties_profile\u0026gt; oimcluster-oim-server1:/u01/oracle/idm/server/bin/ Note: As pods are stateless the copied input file will remain until the pod restarts. Preserve a local copy in case you need to copy files back after pod restart.\n  "
},
{
	"uri": "/fmw-kubernetes/soa-domains/patch_and_upgrade/",
	"title": "Patch and upgrade",
	"tags": [],
	"description": "",
	"content": "Patch an existing Oracle SOA Suite image or upgrade the infrastructure, such as upgrading the underlying Kubernetes cluster to a new release and upgrading the WebLogic Kubernetes operator release.\n Patch an image  Create a patched Oracle SOA Suite image using the WebLogic Image Tool.\n Upgrade an operator release  Upgrade the WebLogic Kubernetes operator release to a newer version.\n Upgrade a Kubernetes cluster  Upgrade the underlying Kubernetes cluster version in a running SOA Kubernetes environment.\n "
},
{
	"uri": "/fmw-kubernetes/wcsites-domains/manage-wcsites-domains/",
	"title": "Manage WebCenter Sites domains",
	"tags": [],
	"description": "Sample for managing a WebCenter Sites domain home on an existing PV or PVC, and the domain resource YAML file for deploying the generated WebCenter Sites domain.",
	"content": "Contents  Introduction Integrate Logstash, Elasticsearch and Kibana Set Up WebLogic Logging Exporter Set Up WebLogic Monitoring Exporter Delete the Generated Domain Home Clean Up the create-domain-job Script After Execution Failure  Introduction This document provides instructions to delete or clear an environment in case of errors and steps to integrate with some of the common utility tools to manage the domains created.\nIntegrate Logstash, Elasticsearch and Kibana You can send the operator logs to Elasticsearch, to be displayed in Kibana. Use this sample script to configure Elasticsearch and Kibana deployments and services. For sample configurations on WebCenter Sites, see Elasticsearch integration for the WebLogic Kubernetes Operator\nSet Up WebLogic Logging Exporter After the WebCenter Sites domain is set up, you can publish WebLogic Kubernetes Operator and WebLogic Server logs into Elasticsearch and interact with them in Kibana. Follow the steps described in this document to set up the Weblogic Logging Exporter and publish the logs to Elasticsearch.\nSet Up WebLogic Monitoring Exporter The WebCenter Sites instance can be monitored using Prometheus and Grafana. The WebLogic Monitoring Exporter uses the WebLogic Server RESTful Management API to scrape runtime information and then exports Prometheus-compatible metrics. It is deployed as a web application in a WebLogic Server (WLS) instance, version 12.2.1 or later, typically, in the instance from which you want to get metrics. For information, see Set Up WebLogic Monitoring Exporter.\nDelete the Generated Domain Home Sometimes in production, but most likely in testing environments, you might want to remove the domain home that is generated using the create-domain.sh script. Do this by running the generated delete domain job script in the /\u0026lt;path to weblogic-operator-output-directory\u0026gt;/weblogic-domains/\u0026lt;domainUID\u0026gt; directory.\n$ kubectl create -f delete-domain-job.yaml Clean Up the create-domain-job script After Execution Failure To clean up the create-domain-job script:\n  Get the create domain job and configmaps:\n$ kubectl get configmaps,jobs -n wcsites-ns |grep \u0026#34;create-domain-job\u0026#34;   Delete the job and configmap:\n$ kubectl delete job wcsitesinfra-create-fmw-infra-sample-domain-job -n wcsites-ns $ kubectl delete configmap wcsitesinfra-create-fmw-infra-sample-domain-job-cm -n wcsites-ns   Delete the contents of the PV, if any:\n$ sudo rm -rf /scratch/K8SVolume/WCSites   "
},
{
	"uri": "/fmw-kubernetes/oud/create-oud-instances-helm/",
	"title": "Create Oracle Unified Directory Instances Using Helm",
	"tags": [],
	"description": "This document provides steps to create Oracle Unified Directory instances using Helm Charts.",
	"content": " Introduction Install Helm Deploy an Application using the Helm Chart Undeploy an Application using the Helm Chart Helm Chart(s) for Oracle Unified Directory  Introduction This chapter demonstrates how to deploy Oracle Unified Directory 12c instance(s) using the Helm package manager for Kubernetes. Helm Chart(s) described here can be used to facilitate installation, configuration, and environment setup within a Kubernetes environment.\nInstall Helm Helm can be used to create and deploy the Oracle Unified Directory resources in a Kubernetes cluster. For Helm installation and usage information, refer to the README.\nDeploy an Application using the Helm Chart The helm install command is used to deploy applications to a Kubernetes environment, using the Helm Chart supplied.\n$ helm install [Deployment NAME] [CHART Reference] [flags] For example:\n$ helm install my-oud-ds-rs oud-ds-rs --namespace myhelmns Undeploy an Application using the Helm Chart To uninstall an application deployed using a Helm chart you need to identify the release name and then issue a delete command:\nTo get the release name:\n$ helm --namespace \u0026lt;namespace\u0026gt; list For example:\n$ helm --namespace myhelmns list NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION my-oud-ds-rs- myhelmns 1 2020-03-31 10:37:30.616927678 -0700 PDT deployed oud-ds-rs-12.2.1.4.0 12.2.1.4.0 To delete the chart:\n$ helm uninstall --namespace \u0026lt;namespace\u0026gt; \u0026lt;release\u0026gt; For example:\n$ helm uninstall --namespace myhelmns my-oud-ds-rs release \u0026quot;my-oud-ds-rs\u0026quot; uninstalled Helm Chart(s) for Oracle Unified Directory The following list provides Helm charts for deploying Oracle Unified Directory in a Kubernetes environment. Helm charts provided can be found in the project at the following location:\nhttps://github.com/oracle/fmw-kubernetes/tree/master/OracleUnifiedDirectory/kubernetes/helm\nDetails about each Helm Chart can be found in the relevant README listed below:\n oud-ds-rs : A Helm chart for deployment of Oracle Unified Directory Directory (DS+RS) instances on Kubernetes.  "
},
{
	"uri": "/fmw-kubernetes/oudsm/create-oudsm-instances-helm/",
	"title": "Create Oracle Unified Directory Services Manager Instances Using Helm",
	"tags": [],
	"description": "This document provides steps to create OUDSM instances using Helm Charts.",
	"content": " Introduction Install Helm Deploy an Application using the Helm Chart Undeploy an Application using the Helm Chart Helm Chart(s) for Oracle Unified Directory Services Manager  Introduction This chapter demonstrates how to deploy Oracle Unified Directory Services Manager 12c instance(s) using the Helm package manager for Kubernetes. Helm Chart(s) described here can be used to facilitate installation, configuration, and environment setup within a Kubernetes environment.\nInstall Helm Helm can be used to create and deploy the Oracle Unified Directory Services Manager resources in a Kubernetes cluster. For Helm installation and usage information, refer to the README.\nDeploy an Application using the Helm Chart The helm install command is used to deploy applications to a Kubernetes environment, using the Helm Chart supplied.\n$ helm install [Deployment NAME] [CHART Reference] [flags] For example:\n$ helm install my-oudsm oudsm --namespace myhelmns Undeploy an Application using the Helm Chart To uninstall an application deployed using a Helm chart you need to identify the release name and then issue a delete command:\nTo get the release name:\n$ helm --namespace \u0026lt;namespace\u0026gt; list For example:\n$ helm --namespace myhelmns list NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION my-oudsm myhelmns 1 2020-03-31 10:37:30.616927678 -0700 PDT deployed my-oudsm-12.2.1.4.0 12.2.1.4.0 To delete the chart:\n$ helm uninstall --namespace \u0026lt;namespace\u0026gt; \u0026lt;release\u0026gt; For example:\n$ helm uninstall --namespace myhelmns my-oudsm release \u0026quot;my-oudsm\u0026quot; uninstalled Helm Chart(s) for Oracle Unified Directory Services Manager The following list provides Helm charts for deploying Oracle Unified Directory Services Manager in a Kubernetes environment. The following list provides Helm charts for deploying Oracle Unified Directory Services Manager in a Kubernetes environment. Helm charts provided can be found in the project at the following location:\nhttps://github.com/oracle/fmw-kubernetes/tree/master/OracleUnifiedDirectorySM/kubernetes/helm\nDetails about each Helm Chart can be found in the relevant README listed below:\n oudsm : A Helm chart for deployment of Oracle Unified Directory Services Manager instances on Kubernetes.  "
},
{
	"uri": "/fmw-kubernetes/oud/",
	"title": "Oracle Unified Directory",
	"tags": [],
	"description": "Oracle Unified Directory provides a comprehensive Directory Solution for robust Identity Management",
	"content": "Oracle Unified Directory provides a comprehensive Directory Solution for robust Identity Management. Oracle Unified Directory is an all-in-one directory solution with storage, proxy, synchronization and virtualization capabilities. While unifying the approach, it provides all the services required for high-performance Enterprise and carrier-grade environments. Oracle Unified Directory ensures scalability to billions of entries, ease of installation, elastic deployments, enterprise manageability and effective monitoring.\nThis project supports deployment of Oracle Unified Directory (OUD) Docker images based on the 12cPS4 (12.2.1.4.0) release within a Kubernetes environment. The OUD Docker Image refers to binaries for OUD Release 12.2.1.4.0 and it has the capability to create different types of OUD Instances (Directory Service, Proxy, Replication) in containers.\nImage: oracle/oud:12.2.1.4.0\nThis project has several key features to assist you with deploying and managing Oracle Unified Directory in a Kubernetes environment. You can:\n Create Oracle Unified Directory instances in a Kubernetes persistent volume (PV). This PV can reside in an NFS file system or other Kubernetes volume types. Start servers based on declarative startup parameters and desired states. Expose the Oracle Unified Directory services for external access. Scale Oracle Unified Directory by starting and stopping servers on demand. Monitor the Oracle Unified Directory instance using Prometheus and Grafana.  Follow the instructions in this guide to set up Oracle Unified Directory on Kubernetes.\nGetting started For detailed information about deploying Oracle Unified Directory, start at Prerequisites and follow this documentation sequentially.\nCurrent release The current supported release of Oracle Unified Directory is OUD 12c PS4 (12.2.1.4.0)\n"
},
{
	"uri": "/fmw-kubernetes/soa-domains/adminguide/configure-load-balancer/apache/",
	"title": "Apache webtier",
	"tags": [],
	"description": "Configure the Apache webtier load balancer for Oracle SOA Suite domains.",
	"content": "This section provides information about how to install and configure Apache webtier to load balance Oracle SOA Suite domain clusters. You can configure Apache webtier for non-SSL and SSL termination access of the application URL.\nFollow these steps to set up Apache webtier as a load balancer for an Oracle SOA Suite domain in a Kubernetes cluster:\n Build the Apache webtier image Create the Apache plugin configuration file Prepare the certificate and private key Install the Apache webtier Helm chart Verify domain application URL access Uninstall Apache webtier  Build the Apache webtier image Refer to the sample, to build the Apache webtier Docker image.\nCreate the Apache plugin configuration file   The configuration file named custom_mod_wl_apache.conf should have all the URL routing rules for the Oracle SOA Suite applications deployed in the domain that needs to be accessible externally. Update this file with values based on your environment. The file content is similar to below. This directory location of this configuration file is referred as host-config-dir in input.yaml.\n  Click here to see the sample content of the configuration file custom_mod_wl_apache.conf for soa domain   # Copyright (c) 2020 Oracle and/or its affiliates. # # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # \u0026lt;IfModule mod_weblogic.c\u0026gt; WebLogicHost \u0026lt;WEBLOGIC_HOST\u0026gt; WebLogicPort 7001 \u0026lt;/IfModule\u0026gt; # Directive for weblogic admin Console deployed on WebLogic Admin Server \u0026lt;Location /console\u0026gt; SetHandler weblogic-handler WebLogicHost soainfra-adminserver WebLogicPort 7001 \u0026lt;/Location\u0026gt; \u0026lt;Location /em\u0026gt; SetHandler weblogic-handler WebLogicHost soainfra-adminserver WebLogicPort 7001 \u0026lt;/Location\u0026gt; \u0026lt;Location /servicebus\u0026gt; SetHandler weblogic-handler WebLogicHost soainfra-adminserver WebLogicPort 7001 \u0026lt;/Location\u0026gt; \u0026lt;Location /lwpfconsole\u0026gt; SetHandler weblogic-handler WebLogicHost soainfra-adminserver WebLogicPort 7001 \u0026lt;/Location\u0026gt; \u0026lt;Location /weblogic/ready\u0026gt; SetHandler weblogic-handler WebLogicHost soainfra-adminserver WebLogicPort 7001 \u0026lt;/Location\u0026gt; # Directive for all application deployed on weblogic cluster with a prepath defined by LOCATION variable # For example, if the LOCAITON is set to \u0026#39;/weblogic\u0026#39;, all applications deployed on the cluster can be accessed via # http://myhost:myport/weblogic/application_end_url # where \u0026#39;myhost\u0026#39; is the IP of the machine that runs the Apache web tier, and # \u0026#39;myport\u0026#39; is the port that the Apache web tier is publicly exposed to. # Note that LOCATION cannot be set to \u0026#39;/\u0026#39; unless this is the only Location module configured. \u0026lt;Location /soa-infra\u0026gt; WLSRequest On WebLogicCluster soainfra-cluster-soa-cluster:8001 PathTrim /weblogic1 \u0026lt;/Location\u0026gt; \u0026lt;Location /soa/composer\u0026gt; WLSRequest On WebLogicCluster soainfra-cluster-soa-cluster:8001 PathTrim /weblogic1 \u0026lt;/Location\u0026gt; \u0026lt;Location /integration/worklistapp\u0026gt; WLSRequest On WebLogicCluster soainfra-cluster-soa-cluster:8001 PathTrim /weblogic1 \u0026lt;/Location\u0026gt; \u0026lt;Location /ess\u0026gt; WLSRequest On WebLogicCluster soainfra-cluster-soa-cluster:8001 PathTrim /weblogic1 \u0026lt;/Location\u0026gt; \u0026lt;Location /EssHealthCheck\u0026gt; WLSRequest On WebLogicCluster soainfra-cluster-soa-cluster:8001 PathTrim /weblogic1 \u0026lt;/Location\u0026gt; # Directive for all application deployed on weblogic cluster with a prepath defined by LOCATION2 variable # For example, if the LOCAITON2 is set to \u0026#39;/weblogic2\u0026#39;, all applications deployed on the cluster can be accessed via # http://myhost:myport/weblogic2/application_end_url # where \u0026#39;myhost\u0026#39; is the IP of the machine that runs the Apache web tier, and # \u0026#39;myport\u0026#39; is the port that the Apache webt ier is publicly exposed to. #\u0026lt;Location /weblogic2\u0026gt; #WLSRequest On #WebLogicCluster domain2-cluster-cluster-1:8021 #PathTrim /weblogic2 #\u0026lt;/Location\u0026gt;      Update volume path with the directory location of the file custom_mod_wl_apache.conf in file kubernetes/samples/charts/apache-samples/custom-sample/input.yaml\n  Prepare the certificate and private key   (For the SSL termination configuration only) Run the following commands to generate your own certificate and private key using openssl.\n$ cd ${WORKDIR}/weblogic-kubernetes-operator $ cd kubernetes/samples/charts/apache-samples/custom-sample $ export VIRTUAL_HOST_NAME=WEBLOGIC_HOST $ export SSL_CERT_FILE=WEBLOGIC_HOST.crt $ export SSL_CERT_KEY_FILE=WEBLOGIC_HOST.key $ sh certgen.sh  NOTE: Replace WEBLOGIC_HOST with the host name on which Apache webtier is to be installed.\n   Click here to see the output of the certifcate generation   $ls certgen.sh custom_mod_wl_apache.conf custom_mod_wl_apache.conf_orig input.yaml README.md $ sh certgen.sh Generating certs for WEBLOGIC_HOST Generating a 2048 bit RSA private key ........................+++ .......................................................................+++ unable to write \u0026#39;random state\u0026#39; writing new private key to \u0026#39;apache-sample.key\u0026#39; ----- $ ls certgen.sh custom_mod_wl_apache.conf_orig WEBLOGIC_HOST.info config.txt input.yaml WEBLOGIC_HOST.key custom_mod_wl_apache.conf WEBLOGIC_HOST.crt README.md      Prepare input values for the Apache webtier Helm chart.\nRun the following commands to prepare the input value file for the Apache webtier Helm chart.\n$ base64 -i ${SSL_CERT_FILE} | tr -d \u0026#39;\\n\u0026#39; $ base64 -i ${SSL_CERT_KEY_FILE} | tr -d \u0026#39;\\n\u0026#39; $ touch input.yaml Update virtualHostName with the value of the WEBLOGIC_HOST in file kubernetes/samples/charts/apache-samples/custom-sample/input.yaml\n  Click here to see the snapshot of the sample input.yaml file   $ cat apache-samples/custom-sample/input.yaml # Use this to provide your own Apache webtier configuration as needed; simply define this # path and put your own custom_mod_wl_apache.conf file under this path. volumePath: ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/charts/apache-samples/custom-sample # The VirtualHostName of the Apache HTTP server. It is used to enable custom SSL configuration. virtualHostName: \u0026lt;WEBLOGIC_HOST\u0026gt;      Install the Apache webtier Helm chart   Install the Apache webtier Helm chart to the domain soans namespace with the specified input parameters:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/charts $ kubectl create namespace apache-webtier $ helm install apache-webtier --values apache-samples/custom-sample/input.yaml --namespace soans apache-webtier --set image=oracle/apache:12.2.1.3   Check the status of the Apache webtier:\n$ kubectl get all -n soans | grep apache Sample output of the status of the apache webtier:\npod/apache-webtier-apache-webtier-65f69dc6bc-zg5pj 1/1 Running 0 22h service/apache-webtier-apache-webtier NodePort 10.108.29.98 \u0026lt;none\u0026gt; 80:30305/TCP,4433:30443/TCP 22h deployment.apps/apache-webtier-apache-webtier 1/1 1 1 22h replicaset.apps/apache-webtier-apache-webtier-65f69dc6bc 1 1 1 22h   Verify domain application URL access Post the Apache webtier load balancer is up, verify that the domain applications are accessible through the load balancer port 30305/30443. The application URLs for domain of type soa are:\n Note: Port 30305 is the LOADBALANCER-Non-SSLPORT and Port 30443 is LOADBALANCER-SSLPORT.\n Non-SSL configuration http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/weblogic/ready http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/console http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/em http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/soa-infra http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/soa/composer http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/integration/worklistapp SSL configuration https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/weblogic/ready https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/console https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/em https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/soa-infra https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/soa/composer https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/integration/worklistapp Uninstall Apache webtier $ helm delete apache-webtier -n soans "
},
{
	"uri": "/fmw-kubernetes/soa-domains/adminguide/enablingt3/",
	"title": "Expose the T3/T3S protocol",
	"tags": [],
	"description": "Create a T3/T3S channel and the corresponding Kubernetes service to expose the T3/T3S protocol for the Administration Server and Managed Servers in an Oracle SOA Suite domain.",
	"content": " Oracle strongly recommends that you do not expose non-HTTPS traffic (T3/T3s/LDAP/IIOP/IIOPs) outside of the external firewall. You can control this access using a combination of network channels and firewalls.\n You can create T3/T3S channels and the corresponding Kubernetes service to expose the T3/T3S protocol for the Administration Server and Managed Servers in an Oracle SOA Suite domain.\nThe WebLogic Kubernetes operator provides an option to expose a T3 channel for the Administration Server using the exposeAdminT3Channel setting during domain creation, then the matching T3 service can be used to connect. By default, when exposeAdminT3Channel is set, the WebLogic Kubernetes operator environment exposes the NodePort for the T3 channel of the NetworkAccessPoint at 30012 (use t3ChannelPort to configure the port to a different value).\nIf you miss enabling exposeAdminT3Channel during domain creation, follow these steps to create a T3 channel for Managed Servers manually.\nExpose a T3/T3S Channel for the Administration Server To create a custom T3/T3S channel for the Administration Server that has a listen port listen_port and a paired public port public_port:\n  Create t3_admin_config.py with the following content:\nadmin_pod_name = sys.argv[1] admin_port = sys.argv[2] user_name = sys.argv[3] password = sys.argv[4] listen_port = sys.argv[5] public_port = sys.argv[6] public_address = sys.argv[7] AdminServerName = sys.argv[8] channelType = sys.argv[9] print(\u0026#39;custom admin_pod_name : [%s]\u0026#39; % admin_pod_name); print(\u0026#39;custom admin_port : [%s]\u0026#39; % admin_port); print(\u0026#39;custom user_name : [%s]\u0026#39; % user_name); print(\u0026#39;custom password : ********\u0026#39;); print(\u0026#39;public address : [%s]\u0026#39; % public_address); print(\u0026#39;channel listen port : [%s]\u0026#39; % listen_port); print(\u0026#39;channel public listen port : [%s]\u0026#39; % public_port); connect(user_name, password, \u0026#39;t3://\u0026#39; + admin_pod_name + \u0026#39;:\u0026#39; + admin_port) edit() startEdit() cd(\u0026#39;/\u0026#39;) cd(\u0026#39;Servers/%s/\u0026#39; % AdminServerName ) if channelType == \u0026#39;t3\u0026#39;: create(\u0026#39;T3Channel_AS\u0026#39;,\u0026#39;NetworkAccessPoint\u0026#39;) cd(\u0026#39;NetworkAccessPoints/T3Channel_AS\u0026#39;) set(\u0026#39;Protocol\u0026#39;,\u0026#39;t3\u0026#39;) set(\u0026#39;ListenPort\u0026#39;,int(listen_port)) set(\u0026#39;PublicPort\u0026#39;,int(public_port)) set(\u0026#39;PublicAddress\u0026#39;, public_address) print(\u0026#39;Channel T3Channel_AS added\u0026#39;) elif channelType == \u0026#39;t3s\u0026#39;:\tcreate(\u0026#39;T3SChannel_AS\u0026#39;,\u0026#39;NetworkAccessPoint\u0026#39;) cd(\u0026#39;NetworkAccessPoints/T3SChannel_AS\u0026#39;) set(\u0026#39;Protocol\u0026#39;,\u0026#39;t3s\u0026#39;) set(\u0026#39;ListenPort\u0026#39;,int(listen_port)) set(\u0026#39;PublicPort\u0026#39;,int(public_port)) set(\u0026#39;PublicAddress\u0026#39;, public_address) set(\u0026#39;HttpEnabledForThisProtocol\u0026#39;, true) set(\u0026#39;OutboundEnabled\u0026#39;, false) set(\u0026#39;Enabled\u0026#39;, true) set(\u0026#39;TwoWaySSLEnabled\u0026#39;, true) set(\u0026#39;ClientCertificateEnforced\u0026#39;, false) else: print(\u0026#39;channelType [%s] not supported\u0026#39;,channelType) activate() disconnect()   Copy t3_admin_config.py into the domain home (for example, /u01/oracle/user_projects/domains/soainfra) of the Administration Server pod (for example, soainfra-adminserver in soans namespace).\n $ kubectl cp t3_admin_config.py soans/soainfra-adminserver:/u01/oracle/user_projects/domains/soainfra    Run wlst.sh t3_admin_config.py by using exec into the Administration Server pod with the following parameters:\n admin_pod_name: soainfra-adminserver # Administration Server pod admin_port: 7001 user_name: weblogic password: Welcome1 # weblogic password listen_port: 30014 # New port for T3 Administration Server public_port: 30014 # Kubernetes NodePort which will be used to expose T3 port externally public_address:  AdminServerName: AdminServer # Give administration Server name channelType: t3 # t3 or t3s protocol channel  $ kubectl exec -it \u0026lt;Administration Server pod\u0026gt; -n \u0026lt;namespace\u0026gt; -- /u01/oracle/oracle_common/common/bin/wlst.sh \u0026lt;domain_home\u0026gt;/t3_admin_config.py \u0026lt;Administration Server pod\u0026gt; \u0026lt;Administration Server port\u0026gt; weblogic \u0026lt;password for weblogic\u0026gt; \u0026lt;t3 port on Administration Server\u0026gt; \u0026lt;t3 nodeport\u0026gt; \u0026lt;master_ip\u0026gt; \u0026lt;AdminServerName\u0026gt; \u0026lt;channelType t3 or t3s\u0026gt; For example:\n$ kubectl exec -it soainfra-adminserver -n soans -- /u01/oracle/oracle_common/common/bin/wlst.sh /u01/oracle/user_projects/domains/soainfra/t3_admin_config.py soainfra-adminserver 7001 weblogic Welcome1 30014 30014 xxx.xxx.xxx.xxx AdminServer t3   Create t3_admin_svc.yaml with the following contents to expose T3 at NodePort 30014 for domainName and domainUID as soainfra and domain deployed in soans namespace:\n Note: For T3S, replace NodePort 30014 with the appropriate value used with public_port while creating the T3S channel using wlst.sh in the previous step.\n apiVersion: v1 kind: Service metadata: name: soainfra-adminserver-t3-external namespace: soans labels: weblogic.serverName: AdminServer weblogic.domainName: soainfra weblogic.domainUID: soainfra spec: type: NodePort selector: weblogic.domainName: soainfra weblogic.domainUID: soainfra weblogic.serverName: AdminServer ports: - name: t3adminport protocol: TCP port: 30014 targetPort: 30014 nodePort: 30014   Create the NodePort Service for port 30014:\n$ kubectl create -f t3_admin_svc.yaml   Verify that you can access T3 for the Administration Server with the following URL:\nt3://\u0026lt;master_ip\u0026gt;:30014   Similarly, you can access T3S as follows:\na. First get the certificates from the Administration Server to be used for secured (T3S) connection from the client. You can export the certificate from the Administration Server with WLST commands. For example, to export the default demoidentity:\n Note: If you are using the custom SSL certificate, replace the steps accordingly.\n $ kubectl exec -it soainfra-adminserver -n soans -- bash $ /u01/oracle/oracle_common/common/bin/wlst.sh $ connect('weblogic','Welcome1','t3://soainfra-adminserver:7001') $ svc = getOpssService(name='KeyStoreService') $ svc.exportKeyStoreCertificate(appStripe='system', name='demoidentity', password='DemoIdentityKeyStorePassPhrase', alias='DemoIdetityKeyStorePassPhrase', type='Certificate', filepath='/tmp/cert.txt/') These steps download the certificate at /tmp/cert.txt.\nb. Use the same certificates from the client side and connect using t3s. For example:\n$ export JAVA_HOME=/u01/jdk $ keytool -import -v -trustcacerts -alias soadomain -file cert.txt -keystore $JAVA_HOME/jre/lib/security/cacerts -keypass changeit -storepass changeit $ export WLST_PROPERTIES=\u0026quot;-Dweblogic.security.SSL.ignoreHostnameVerification=true\u0026quot; $ cd $ORACLE_HOME/oracle_common/common/bin $ ./wlst.sh Initializing WebLogic Scripting Tool (WLST) ... Welcome to WebLogic Server Administration Scripting Shell Type help() for help on available commands $ wls:/offline\u0026gt; connect('weblogic','Welcome1','t3s://\u0026lt;Master IP address\u0026gt;:30014')   Expose T3/T3S for Managed Servers To create a custom T3/T3S channel for all Managed Servers, with a listen port listen_port and a paired public port public_port:\n  Create t3_ms_config.py with the following content:\nadmin_pod_name = sys.argv[1] admin_port = sys.argv[2] user_name = sys.argv[3] password = sys.argv[4] listen_port = sys.argv[5] public_port = sys.argv[6] public_address = sys.argv[7] managedNameBase = sys.argv[8] ms_count = sys.argv[9] channelType = sys.argv[10] print(\u0026#39;custom host : [%s]\u0026#39; % admin_pod_name); print(\u0026#39;custom port : [%s]\u0026#39; % admin_port); print(\u0026#39;custom user_name : [%s]\u0026#39; % user_name); print(\u0026#39;custom password : ********\u0026#39;); print(\u0026#39;public address : [%s]\u0026#39; % public_address); print(\u0026#39;channel listen port : [%s]\u0026#39; % listen_port); print(\u0026#39;channel public listen port : [%s]\u0026#39; % public_port); connect(user_name, password, \u0026#39;t3://\u0026#39; + admin_pod_name + \u0026#39;:\u0026#39; + admin_port) edit() startEdit() for index in range(0, int(ms_count)): cd(\u0026#39;/\u0026#39;) msIndex = index+1 cd(\u0026#39;/\u0026#39;) name = \u0026#39;%s%s\u0026#39; % (managedNameBase, msIndex) cd(\u0026#39;Servers/%s/\u0026#39; % name ) if channelType == \u0026#39;t3\u0026#39;: create(\u0026#39;T3Channel_MS\u0026#39;,\u0026#39;NetworkAccessPoint\u0026#39;) cd(\u0026#39;NetworkAccessPoints/T3Channel_MS\u0026#39;) set(\u0026#39;Protocol\u0026#39;,\u0026#39;t3\u0026#39;) set(\u0026#39;ListenPort\u0026#39;,int(listen_port)) set(\u0026#39;PublicPort\u0026#39;,int(public_port)) set(\u0026#39;PublicAddress\u0026#39;, public_address) print(\u0026#39;Channel T3Channel_MS added ...for \u0026#39; + name) elif channelType == \u0026#39;t3s\u0026#39;:\tcreate(\u0026#39;T3SChannel_MS\u0026#39;,\u0026#39;NetworkAccessPoint\u0026#39;) cd(\u0026#39;NetworkAccessPoints/T3SChannel_MS\u0026#39;) set(\u0026#39;Protocol\u0026#39;,\u0026#39;t3s\u0026#39;) set(\u0026#39;ListenPort\u0026#39;,int(listen_port)) set(\u0026#39;PublicPort\u0026#39;,int(public_port)) set(\u0026#39;PublicAddress\u0026#39;, public_address) set(\u0026#39;HttpEnabledForThisProtocol\u0026#39;, true) set(\u0026#39;OutboundEnabled\u0026#39;, false) set(\u0026#39;Enabled\u0026#39;, true) set(\u0026#39;TwoWaySSLEnabled\u0026#39;, true) set(\u0026#39;ClientCertificateEnforced\u0026#39;, false) print(\u0026#39;Channel T3SChannel_MS added ...for \u0026#39; + name) else: print(\u0026#39;Protocol [%s] not supported\u0026#39; % channelType) activate() disconnect()   Copy t3_ms_config.py into the domain home (for example, /u01/oracle/user_projects/domains/soainfra) of the Administration Server pod (for example, soainfra-adminserver in soans namespace).\n$ kubectl cp t3_ms_config.py soans/soainfra-adminserver:/u01/oracle/user_projects/domains/soainfra   Run wlst.sh t3_ms_config.py by exec into the Administration Server pod with the following parameters:\n admin_pod_name: soainfra-adminserver # Administration Server pod admin_port: 7001 user_name: weblogic password: Welcome1 # weblogic password listen_port: 30016 # New port for T3 Managed Servers public_port: 30016 # Kubernetes NodePort which will be used to expose T3 port externally public_address:  managedNameBase: soa_server # Give Managed Server base name. For osb_cluster this will be osb_server ms_count: 5 # Number of configured Managed Servers channelType: t3 # channelType is t3 or t3s  $ kubectl exec -it \u0026lt;Administration Server pod\u0026gt; -n \u0026lt;namespace\u0026gt; -- /u01/oracle/oracle_common/common/bin/wlst.sh \u0026lt;domain_home\u0026gt;/t3_ms_config.py \u0026lt;Administration Server pod\u0026gt; \u0026lt;Administration Server port\u0026gt; weblogic \u0026lt;password for weblogic\u0026gt; \u0026lt;t3 port on Managed Server\u0026gt; \u0026lt;t3 nodeport\u0026gt; \u0026lt;master_ip\u0026gt; \u0026lt;managedNameBase\u0026gt; \u0026lt;ms_count\u0026gt; \u0026lt;channelType t3 or t3s\u0026gt; For example:\n$ kubectl exec -it soainfra-adminserver -n soans -- /u01/oracle/oracle_common/common/bin/wlst.sh /u01/oracle/user_projects/domains/soainfra/t3_ms_config.py soainfra-adminserver 7001 weblogic Welcome1 30016 30016 xxx.xxx.xxx.xxx soa_server 5 t3   Create t3_ms_svc.yaml with the following contents to expose T3 at Managed Server port 30016 for domainName, domainUID as soainfra, and clusterName as soa_cluster for SOA Cluster. Similarly, you can create Kubernetes Service with clusterName as osb_cluster for OSB Cluster:\n Note: For T3S, replace NodePort 30016 with the appropriate value used with public_port while creating the T3S channel using wlst.sh in the previous step.\n apiVersion: v1 kind: Service metadata: name: soainfra-soa-cluster-t3-external namespace: soans labels: weblogic.clusterName: soa_cluster weblogic.domainName: soainfra weblogic.domainUID: soainfra spec: type: NodePort selector: weblogic.domainName: soainfra weblogic.domainUID: soainfra weblogic.clusterName: soa_cluster ports: - name: t3soaport protocol: TCP port: 30016 targetPort: 30016 nodePort: 30016   Create the NodePort Service for port 30016:\n$ kubectl create -f t3_ms_svc.yaml   Verify that you can access T3 for the Managed Server with the following URL:\nt3://\u0026lt;master_ip\u0026gt;:30016   Similarly, you can access T3S as follows:\na. First get the certificates from the Administration Server to be used for secured (t3s) connection from client. You can export the certificate from the Administration Server with wlst commands. Sample commands to export the default demoidentity:\n Note: In case you are using the custom SSL certificate, replaces the steps accordingly\n $ kubectl exec -it soainfra-adminserver -n soans -- bash $ /u01/oracle/oracle_common/common/bin/wlst.sh $ connect('weblogic','Welcome1','t3://soainfra-adminserver:7001') $ svc = getOpssService(name='KeyStoreService') $ svc.exportKeyStoreCertificate(appStripe='system', name='demoidentity', password='DemoIdentityKeyStorePassPhrase', alias='DemoIdetityKeyStorePassPhrase', type='Certificate', filepath='/tmp/cert.txt/') The above steps download the certificate at /tmp/cert.txt.\nb. Use the same certificates from the client side and connect using t3s. For example:\n$ export JAVA_HOME=/u01/jdk $ keytool -import -v -trustcacerts -alias soadomain -file cert.txt -keystore $JAVA_HOME/jre/lib/security/cacerts -keypass changeit -storepass changeit $ export WLST_PROPERTIES=\u0026quot;-Dweblogic.security.SSL.ignoreHostnameVerification=true\u0026quot; $ cd $ORACLE_HOME/oracle_common/common/bin $ ./wlst.sh Initializing WebLogic Scripting Tool (WLST) ... Welcome to WebLogic Server Administration Scripting Shell Type help() for help on available commands $ wls:/offline\u0026gt; connect('weblogic','Welcome1','t3s://\u0026lt;Master IP address\u0026gt;:30016')   Remove T3/T3S configuration For Administration Server   Create t3_admin_delete.py with the following content:\nadmin_pod_name = sys.argv[1] admin_port = sys.argv[2] user_name = sys.argv[3] password = sys.argv[4] AdminServerName = sys.argv[5] channelType = sys.argv[6] print(\u0026#39;custom admin_pod_name : [%s]\u0026#39; % admin_pod_name); print(\u0026#39;custom admin_port : [%s]\u0026#39; % admin_port); print(\u0026#39;custom user_name : [%s]\u0026#39; % user_name); print(\u0026#39;custom password : ********\u0026#39;); connect(user_name, password, \u0026#39;t3://\u0026#39; + admin_pod_name + \u0026#39;:\u0026#39; + admin_port) edit() startEdit() cd(\u0026#39;/\u0026#39;) cd(\u0026#39;Servers/%s/\u0026#39; % AdminServerName ) if channelType == \u0026#39;t3\u0026#39;: delete(\u0026#39;T3Channel_AS\u0026#39;,\u0026#39;NetworkAccessPoint\u0026#39;) elif channelType == \u0026#39;t3s\u0026#39;: delete(\u0026#39;T3SChannel_AS\u0026#39;,\u0026#39;NetworkAccessPoint\u0026#39;) else: print(\u0026#39;channelType [%s] not supported\u0026#39;,channelType) activate() disconnect()   Copy t3_admin_delete.py into the domain home (for example, /u01/oracle/user_projects/domains/soainfra) of the Administration Server pod (for example, soainfra-adminserver in soans namespace).\n$ kubectl cp t3_admin_delete.py soans/soainfra-adminserver:/u01/oracle/user_projects/domains/soainfra   Run wlst.sh t3_admin_delete.py by exec into the Administration Server pod with the following parameters:\n admin_pod_name: soainfra-adminserver # Administration Server pod admin_port: 7001 user_name: weblogic password: Welcome1 # weblogic password AdminServerName: AdminServer # Give administration Server name channelType: t3 # T3 channel  $ kubectl exec -it \u0026lt;Administration Server pod\u0026gt; -n \u0026lt;namespace\u0026gt; -- /u01/oracle/oracle_common/common/bin/wlst.sh \u0026lt;domain_home\u0026gt;/t3_admin_delete.py \u0026lt;Administration Server pod\u0026gt; \u0026lt;Administration Server port\u0026gt; weblogic \u0026lt;password for weblogic\u0026gt; \u0026lt;AdminServerName\u0026gt; \u0026lt;protocol t3 or t3s\u0026gt; For example:\n$ kubectl exec -it soainfra-adminserver -n soans -- /u01/oracle/oracle_common/common/bin/wlst.sh /u01/oracle/user_projects/domains/soainfra/t3_admin_delete.py soainfra-adminserver 7001 weblogic Welcome1 AdminServer t3   Delete the NodePort Service for port 30014:\n$ kubectl delete -f t3_admin_svc.yaml   For Managed Servers These steps delete the custom T3/T3S channel created by Expose T3/T3S for Managed Servers for all Managed Servers.\n  Create t3_ms_delete.py with the following content:\nadmin_pod_name = sys.argv[1] admin_port = sys.argv[2] user_name = sys.argv[3] password = sys.argv[4] managedNameBase = sys.argv[5] ms_count = sys.argv[6] channelType = sys.argv[7] print(\u0026#39;custom host : [%s]\u0026#39; % admin_pod_name); print(\u0026#39;custom port : [%s]\u0026#39; % admin_port); print(\u0026#39;custom user_name : [%s]\u0026#39; % user_name); print(\u0026#39;custom password : ********\u0026#39;); connect(user_name, password, \u0026#39;t3://\u0026#39; + admin_pod_name + \u0026#39;:\u0026#39; + admin_port) edit() startEdit() for index in range(0, int(ms_count)): cd(\u0026#39;/\u0026#39;) msIndex = index+1 cd(\u0026#39;/\u0026#39;) name = \u0026#39;%s%s\u0026#39; % (managedNameBase, msIndex) cd(\u0026#39;Servers/%s/\u0026#39; % name ) if channelType == \u0026#39;t3\u0026#39;: delete(\u0026#39;T3Channel_MS\u0026#39;,\u0026#39;NetworkAccessPoint\u0026#39;) elif channelType == \u0026#39;t3s\u0026#39;: delete(\u0026#39;T3SChannel_MS\u0026#39;,\u0026#39;NetworkAccessPoint\u0026#39;) else: print(\u0026#39;Protocol [%s] not supported\u0026#39; % channelType) activate() disconnect()   Copy t3_ms_delete.py into the domain home (for example, /u01/oracle/user_projects/domains/soainfra) of the Administration Server pod (for example, soainfra-adminserver in soans namespace).\n$ kubectl cp t3_ms_delete.py soans/soainfra-adminserver:/u01/oracle/user_projects/domains/soainfra   Run wlst.sh t3_ms_delete.py by exec into the Administration Server pod with the following parameters:\n admin_pod_name: soainfra-adminserver # Administration Server pod admin_port: 7001 user_name: weblogic password: Welcome1 # weblogic password managedNameBase: soa_server # Give Managed Server base name. For osb_cluster this will be osb_server ms_count: 5 # Number of configured Managed Servers channelType: t3 # channelType is t3 or t3s  $ kubectl exec -it \u0026lt;Administration Server pod\u0026gt; -n \u0026lt;namespace\u0026gt; -- /u01/oracle/oracle_common/common/bin/wlst.sh \u0026lt;domain_home\u0026gt;/t3_ms_delete.py \u0026lt;Administration Server pod\u0026gt; \u0026lt;Administration Server port\u0026gt; weblogic \u0026lt;password for weblogic\u0026gt; \u0026lt;t3 port on Managed Server\u0026gt; \u0026lt;t3 nodeport\u0026gt; \u0026lt;master_ip\u0026gt; \u0026lt;managedNameBase\u0026gt; \u0026lt;ms_count\u0026gt; \u0026lt;channelType t3 or t3s\u0026gt; For example:\n$ kubectl exec -it soainfra-adminserver -n soans -- /u01/oracle/oracle_common/common/bin/wlst.sh /u01/oracle/user_projects/domains/soainfra/t3_ms_delete.py soainfra-adminserver 7001 weblogic Welcome1 soa_server 5 t3   Delete the NodePort Service for port 30016 (or the NodePort used while creating the Kubernetes Service):\n$ kubectl delete -f t3_ms_svc.yaml   "
},
{
	"uri": "/fmw-kubernetes/oam/configure-ingress/",
	"title": "Configure an Ingress for an OAM domain",
	"tags": [],
	"description": "This document provides steps to configure an Ingress to direct traffic to the OAM domain.",
	"content": "Choose one of the following supported methods to configure an Ingress to direct traffic for your OAM domain.\n a. Using an Ingress with NGINX  Steps to set up an Ingress for NGINX to direct traffic to the OAM domain.\n b. Using an Ingress with Voyager  Steps to set up an Ingress for Voyager to direct traffic to the OAM domain.\n "
},
{
	"uri": "/fmw-kubernetes/oig/configure-ingress/",
	"title": "Configure an Ingress for an OIG domain",
	"tags": [],
	"description": "This document provides steps to configure an Ingress to direct traffic to the OIG domain.",
	"content": "Choose one of the following supported methods to configure an Ingress to direct traffic for your OIG domain.\n a. Using an Ingress with NGINX (non-SSL)  Steps to set up an Ingress for NGINX to direct traffic to the OIG domain (non-SSL).\n b. Using an Ingress with NGINX (SSL)  Steps to set up an Ingress for NGINX to direct traffic to the OIG domain (SSL).\n c. Using an Ingress with Voyager (non-SSL)  Steps to set up an Ingress for Voyager to direct traffic to the OIG domain (non-SSL).\n d. Using an Ingress with Voyager (SSL)  Steps to set up an Ingress for Voyager to direct traffic to the OIG domain (SSL).\n "
},
{
	"uri": "/fmw-kubernetes/oig/manage-oig-domains/logging-and-visualization/",
	"title": "Logging and Visualization",
	"tags": [],
	"description": "Describes the steps for logging and visualization with Elasticsearch and Kibana.",
	"content": "After the OIG domain is set up you can publish operator and WebLogic Server logs into Elasticsearch and interact with them in Kibana.\nIn Prepare your environment if you decided to use the Elasticsearch and Kibana by setting the parameter elkIntegrationEnabled to true, then the steps below must be followed to complete the setup.\nIf you did not set elkIntegrationEnabled to true and want to do so post configuration, run the following command:\n$ helm upgrade --reuse-values --namespace operator --set \u0026#34;elkIntegrationEnabled=true\u0026#34; --set \u0026#34;logStashImage=logstash:6.6.0\u0026#34; --set \u0026#34;elasticSearchHost=elasticsearch.default.svc.cluster.local\u0026#34; --set \u0026#34;elasticSearchPort=9200\u0026#34; --wait weblogic-kubernetes-operator kubernetes/charts/weblogic-operator The output will look similar to the following:\nRelease \u0026#34;weblogic-kubernetes-operator\u0026#34; has been upgraded. Happy Helming! NAME: weblogic-kubernetes-operator LAST DEPLOYED: Tue Aug 18 05:57:11 2020 NAMESPACE: operator STATUS: deployed REVISION: 3 TEST SUITE: None Install Elasticsearch and Kibana   Create the Kubernetes resource using the following command:\n$ kubectl apply -f \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/scripts/elasticsearch-and-kibana/elasticsearch_and_kibana.yaml For example:\n$ kubectl apply -f /scratch/OIGDockerK8S/weblogic-kubernetes-operator/kubernetes/samples/scripts/elasticsearch-and-kibana/elasticsearch_and_kibana.yaml The output will look similar to the following:\ndeployment.apps/elasticsearch created service/elasticsearch created deployment.apps/kibana created service/kibana created   Run the following command to ensure Elasticsearch is used by the operator:\n$ helm get values --all weblogic-kubernetes-operator -n operator The output will look similar to the following:\nCOMPUTED VALUES: dedicated: false domainNamespaces: - oimcluster elasticSearchHost: elasticsearch.default.svc.cluster.local elasticSearchPort: 9200 elkIntegrationEnabled: true externalDebugHttpPort: 30999 externalRestEnabled: false externalRestHttpsPort: 31001 image: weblogic-kubernetes-operator:3.0.1 imagePullPolicy: IfNotPresent internalDebugHttpPort: 30999 istioEnabled: false javaLoggingLevel: INFO logStashImage: logstash:6.6.0 remoteDebugNodePortEnabled: false serviceAccount: operator-serviceaccount suspendOnDebugStartup: false   To check that Elasticsearch and Kibana are deployed in the Kubernetes cluster, run the following command:\n$ kubectl get pods The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE elasticsearch-857bd5ff6b-tvqdn 1/1 Running 0 2m9s kibana-594465687d-zc2rt 1/1 Running 0 2m9s   Create the logstash pod OIG Server logs can be pushed to the Elasticsearch server using the logstash pod. The logstash pod needs access to the persistent volume of the OIG domain created previously, for example oimcluster-oim-pv. The steps to create the logstash pod are as follows:\n  Obtain the OIG domain persistence volume details:\n$ kubectl get pv -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get pv -n oimcluster The output will look similar to the following:\nNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE oimcluster-oim-pv 10Gi RWX Retain Bound oimcluster/oimcluster-oim-pvc oimcluster-oim-storage-class 28h Make note of the CLAIM value, for example in this case oimcluster-oim-pvc\n  Run the following command to get the mountPath of your domain:\n$ kubectl describe domains \u0026lt;domain_uid\u0026gt; -n \u0026lt;domain_namespace\u0026gt; | grep \u0026#34;Mount Path\u0026#34; For example:\n$ kubectl describe domains oimcluster -n oimcluster | grep \u0026#34;Mount Path\u0026#34; The output will look similar to the following:\nMount Path: /u01/oracle/user_projects/domains   Navigate to the \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/scripts/elasticsearch-and-kibana directory and create a logstash.yaml file as follows. Change the claimName and mountPath values to match the values returned in the previous commands:\napiVersion: apps/v1 kind: Deployment metadata: name: logstash-wls namespace: oimcluster spec: selector: matchLabels: k8s-app: logstash-wls template: # create pods using pod definition in this template metadata: labels: k8s-app: logstash-wls spec: volumes: - name: weblogic-domain-storage-volume persistentVolumeClaim: claimName: oimcluster-oim-pvc - name: shared-logs emptyDir: {} containers: - name: logstash image: logstash:6.6.0 command: [\u0026quot;/bin/sh\u0026quot;] args: [\u0026quot;/usr/share/logstash/bin/logstash\u0026quot;, \u0026quot;-f\u0026quot;, \u0026quot;/u01/oracle/user_projects/domains/logstash/logstash.conf\u0026quot;] imagePullPolicy: IfNotPresent volumeMounts: - mountPath: /u01/oracle/user_projects/domains name: weblogic-domain-storage-volume - name: shared-logs mountPath: /shared-logs ports: - containerPort: 5044 name: logstash   In the NFS persistent volume directory that corresponds to the mountPath /u01/oracle/user_projects/domains, create a logstash directory. For example:\n$ mkdir -p /scratch/OIGDockerK8S/oimclusterdomainpv/logstash   Create a logstash.conf in the newly created logstash directory that contains the following. Make sure the paths correspond to your mountPath and domain name:\ninput { file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/logs/oimcluster/AdminServer*.log\u0026quot; tags =\u0026gt; \u0026quot;Adminserver_log\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/logs/oimcluster/soa_server*.log\u0026quot; tags =\u0026gt; \u0026quot;soaserver_log\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/logs/oimcluster/oim_server*.log\u0026quot; tags =\u0026gt; \u0026quot;Oimserver_log\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/oimcluster/servers/AdminServer/logs/AdminServer-diagnostic.log\u0026quot; tags =\u0026gt; \u0026quot;Adminserver_diagnostic\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/oimcluster/servers/**/logs/soa_server*-diagnostic.log\u0026quot; tags =\u0026gt; \u0026quot;Soa_diagnostic\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/oimcluster/servers/**/logs/oim_server*-diagnostic.log\u0026quot; tags =\u0026gt; \u0026quot;Oimserver_diagnostic\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/oimcluster/servers/**/logs/access*.log\u0026quot; tags =\u0026gt; \u0026quot;Access_logs\u0026quot; start_position =\u0026gt; beginning } } filter { grok { match =\u0026gt; [ \u0026quot;message\u0026quot;, \u0026quot;\u0026lt;%{DATA:log_timestamp}\u0026gt; \u0026lt;%{WORD:log_level}\u0026gt; \u0026lt;%{WORD:thread}\u0026gt; \u0026lt;%{HOSTNAME:hostname}\u0026gt; \u0026lt;%{HOSTNAME:servername}\u0026gt; \u0026lt;%{DATA:timer}\u0026gt; \u0026lt;\u0026lt;%{DATA:kernel}\u0026gt;\u0026gt; \u0026lt;\u0026gt; \u0026lt;%{DATA:uuid}\u0026gt; \u0026lt;%{NUMBER:timestamp}\u0026gt; \u0026lt;%{DATA:misc}\u0026gt; \u0026lt;%{DATA:log_number}\u0026gt; \u0026lt;%{DATA:log_message}\u0026gt;\u0026quot; ] } if \u0026quot;_grokparsefailure\u0026quot; in [tags] { mutate { remove_tag =\u0026gt; [ \u0026quot;_grokparsefailure\u0026quot; ] } } } output { elasticsearch { hosts =\u0026gt; [\u0026quot;elasticsearch.default.svc.cluster.local:9200\u0026quot;] } }   Deploy the logstash pod by executing the following command:\n$ kubectl create -f \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/scripts/elasticsearch-and-kibana/logstash.yaml The output will look similar to the following:\ndeployment.apps/logstash-wls created   Run the following command to check the logstash pod is created correctly:\n$ kubectl get pods -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl get pods -n oimcluster The output should look similar to the following:\nNAME READY STATUS RESTARTS AGE logstash-wls-85867765bc-bhs54 1/1 Running 0 9s oig-design-console 1/1 Running 1 160m oimcluster-adminserver 1/1 Running 0 90m oimcluster-create-fmw-infra-sample-domain-job-dktkk 0/1 Completed 0 25h oimcluster-oim-server1 1/1 Running 0 87m oimcluster-soa-server1 1/1 Running 0 87m Then run the following to get the Elasticsearch pod name:\n$ kubectl get pods The output should look similar to the following:\nNAME READY STATUS RESTARTS AGE elasticsearch-857bd5ff6b-tvqdn 1/1 Running 0 7m48s kibana-594465687d-zc2rt 1/1 Running 0 7m48s   Verify and access the Kibana console   Check if the indices are created correctly in the elasticsearch pod:\n$ kubectl exec -it elasticsearch-857bd5ff6b-tvqdn -- /bin/bash This will take you into a bash shell in the elasticsearch pod:\n[root@elasticsearch-857bd5ff6b-tvqdn elasticsearch]#   In the elasticsearch bash shell run the following to check the indices:\n[root@elasticsearch-857bd5ff6b-tvqdn elasticsearch]# curl -i \u0026quot;127.0.0.1:9200/_cat/indices?v\u0026quot; The output will look similar to the following:\nHTTP/1.1 200 OK content-type: text/plain; charset=UTF-8 content-length: 580 health status index uuid pri rep docs.count docs.deleted store.size pri.store.size green open .kibana_task_manager 1qQ-C21GQJa38lAR28_7iA 1 0 2 0 12.6kb 12.6kb green open .kibana_1 TwIdqENXTqm6mZlBRVy__A 1 0 2 0 7.6kb 7.6kb yellow open logstash-2020.09.30 6LuZLYgARYCGGN-yZT5bJA 5 1 90794 0 22mb 22mb yellow open logstash-2020.09.29 QBYQrolXRiW9l8Ct3DrSyQ 5 1 38 0 86.3kb 86.3kb Exit the bash shell by typing exit.\n  Find the Kibana port by running the following command:\n$ kubectl get svc The output will look similar to the following:\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE elasticsearch ClusterIP 10.107.79.44 \u0026lt;none\u0026gt; 9200/TCP,9300/TCP 11m kibana NodePort 10.103.60.126 \u0026lt;none\u0026gt; 5601:31490/TCP 11m kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 7d5h In the example above the Kibana port is 31490.\n  Access the Kibana console with http://${MASTERNODE-HOSTNAME}:${KIBANA-PORT}/app/kibana.\n  Click on Dashboard in the left hand Navigation Menu.\n  In the Create index pattern page enter logstash* and click Next Step.\n  From the Time Filter field name drop down menu select @timestamp and click Create index pattern.\n  Once the index pattern is created click on Discover in the navigation menu to view the logs.\n  For more details on how to use the Kibana console see the Kibana Guide\n"
},
{
	"uri": "/fmw-kubernetes/oam/manage-oam-domains/monitoring-oam-domains/",
	"title": "Monitoring an OAM domain",
	"tags": [],
	"description": "Describes the steps for Monitoring the OAM domain.",
	"content": "After the OAM domain is set up you can monitor the OAM instance using Prometheus and Grafana. See Monitoring a domain.\nThe WebLogic Monitoring Exporter uses the WLS RESTful Management API to scrape runtime information and then exports Prometheus-compatible metrics. It is deployed as a web application in a WebLogic Server (WLS) instance, version 12.2.1 or later, typically, in the instance from which you want to get metrics.\nDeploy the Prometheus operator   Clone Prometheus by running the following commands:\n$ cd \u0026lt;work directory\u0026gt; $ git clone https://github.com/coreos/kube-prometheus.git Note: Please refer the compatibility matrix of Kube Prometheus. Please download the release of the repository according to the Kubernetes version of your cluster. In the above example the latest release will be downloaded.\nFor example:\n$ cd /scratch/OAMDockerK8S $ git clone https://github.com/coreos/kube-prometheus.git   Run the following command to create the namespace and custom resource definitions:\n$ cd kube-prometheus $ kubectl create -f manifests/setup The output will look similar to the following:\nkubectl create -f manifests/setup namespace/monitoring created customresourcedefinition.apiextensions.k8s.io/alertmanagers.monitoring.coreos.com created customresourcedefinition.apiextensions.k8s.io/podmonitors.monitoring.coreos.com created customresourcedefinition.apiextensions.k8s.io/probes.monitoring.coreos.com created customresourcedefinition.apiextensions.k8s.io/prometheuses.monitoring.coreos.com created customresourcedefinition.apiextensions.k8s.io/prometheusrules.monitoring.coreos.com created customresourcedefinition.apiextensions.k8s.io/servicemonitors.monitoring.coreos.com created customresourcedefinition.apiextensions.k8s.io/thanosrulers.monitoring.coreos.com created clusterrole.rbac.authorization.k8s.io/prometheus-operator created clusterrolebinding.rbac.authorization.k8s.io/prometheus-operator created deployment.apps/prometheus-operator created service/prometheus-operator created serviceaccount/prometheus-operator created   Run the following command to created the rest of the resources:\n$ kubectl create -f manifests/ The output will look similar to the following:\nalertmanager.monitoring.coreos.com/main created secret/alertmanager-main created service/alertmanager-main created serviceaccount/alertmanager-main created servicemonitor.monitoring.coreos.com/alertmanager created secret/grafana-datasources created configmap/grafana-dashboard-apiserver created configmap/grafana-dashboard-cluster-total created configmap/grafana-dashboard-controller-manager created configmap/grafana-dashboard-k8s-resources-cluster created configmap/grafana-dashboard-k8s-resources-namespace created configmap/grafana-dashboard-k8s-resources-node created configmap/grafana-dashboard-k8s-resources-pod created configmap/grafana-dashboard-k8s-resources-workload created configmap/grafana-dashboard-k8s-resources-workloads-namespace created configmap/grafana-dashboard-kubelet created configmap/grafana-dashboard-namespace-by-pod created configmap/grafana-dashboard-namespace-by-workload created configmap/grafana-dashboard-node-cluster-rsrc-use created configmap/grafana-dashboard-node-rsrc-use created configmap/grafana-dashboard-nodes created configmap/grafana-dashboard-persistentvolumesusage created configmap/grafana-dashboard-pod-total created configmap/grafana-dashboard-prometheus-remote-write created configmap/grafana-dashboard-prometheus created configmap/grafana-dashboard-proxy created configmap/grafana-dashboard-scheduler created configmap/grafana-dashboard-statefulset created configmap/grafana-dashboard-workload-total created configmap/grafana-dashboards created deployment.apps/grafana created service/grafana created serviceaccount/grafana created servicemonitor.monitoring.coreos.com/grafana created clusterrole.rbac.authorization.k8s.io/kube-state-metrics created clusterrolebinding.rbac.authorization.k8s.io/kube-state-metrics created deployment.apps/kube-state-metrics created service/kube-state-metrics created serviceaccount/kube-state-metrics created servicemonitor.monitoring.coreos.com/kube-state-metrics created clusterrole.rbac.authorization.k8s.io/node-exporter created clusterrolebinding.rbac.authorization.k8s.io/node-exporter created daemonset.apps/node-exporter created service/node-exporter created serviceaccount/node-exporter created servicemonitor.monitoring.coreos.com/node-exporter created apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created clusterrole.rbac.authorization.k8s.io/prometheus-adapter created clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created clusterrolebinding.rbac.authorization.k8s.io/prometheus-adapter created clusterrolebinding.rbac.authorization.k8s.io/resource-metrics:system:auth-delegator created clusterrole.rbac.authorization.k8s.io/resource-metrics-server-resources created configmap/adapter-config created deployment.apps/prometheus-adapter created rolebinding.rbac.authorization.k8s.io/resource-metrics-auth-reader created service/prometheus-adapter created serviceaccount/prometheus-adapter created servicemonitor.monitoring.coreos.com/prometheus-adapter created clusterrole.rbac.authorization.k8s.io/prometheus-k8s created clusterrolebinding.rbac.authorization.k8s.io/prometheus-k8s created servicemonitor.monitoring.coreos.com/prometheus-operator created prometheus.monitoring.coreos.com/k8s created rolebinding.rbac.authorization.k8s.io/prometheus-k8s-config created rolebinding.rbac.authorization.k8s.io/prometheus-k8s created rolebinding.rbac.authorization.k8s.io/prometheus-k8s created rolebinding.rbac.authorization.k8s.io/prometheus-k8s created role.rbac.authorization.k8s.io/prometheus-k8s-config created role.rbac.authorization.k8s.io/prometheus-k8s created role.rbac.authorization.k8s.io/prometheus-k8s created role.rbac.authorization.k8s.io/prometheus-k8s created prometheusrule.monitoring.coreos.com/prometheus-k8s-rules created service/prometheus-k8s created serviceaccount/prometheus-k8s created servicemonitor.monitoring.coreos.com/prometheus created servicemonitor.monitoring.coreos.com/kube-apiserver created servicemonitor.monitoring.coreos.com/coredns created servicemonitor.monitoring.coreos.com/kube-controller-manager created servicemonitor.monitoring.coreos.com/kube-scheduler created servicemonitor.monitoring.coreos.com/kubelet created   Kube-Prometheus requires all nodes to be labelled with kubernetes.io/os=linux. To check if your nodes are labelled, run the following:\n$ kubectl get nodes --show-labels If the nodes are labelled the output will look similar to the following:\nNAME STATUS ROLES AGE VERSION LABELS worker-node1 Ready \u0026lt;none\u0026gt; 42d v1.18.4 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=worker-node1,kubernetes.io/os=linux worker-node2 Ready \u0026lt;none\u0026gt; 42d v1.18.4 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=worker-node2,kubernetes.io/os=linux master-node Ready master 42d v1.18.4 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=master-node,kubernetes.io/os=linux,node-role.kubernetes.io/master= If the nodes are not labelled, run the following command:\n$ kubectl label nodes --all kubernetes.io/os=linux   Provide external access for Grafana, Prometheus, and Alertmanager, by running the following commands:\n$ kubectl patch svc grafana -n monitoring --type=json -p \u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/type\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;NodePort\u0026#34; },{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/ports/0/nodePort\u0026#34;, \u0026#34;value\u0026#34;: 32100 }]\u0026#39; $ kubectl patch svc prometheus-k8s -n monitoring --type=json -p \u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/type\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;NodePort\u0026#34; },{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/ports/0/nodePort\u0026#34;, \u0026#34;value\u0026#34;: 32101 }]\u0026#39; $ kubectl patch svc alertmanager-main -n monitoring --type=json -p \u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/type\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;NodePort\u0026#34; },{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/ports/0/nodePort\u0026#34;, \u0026#34;value\u0026#34;: 32102 }]\u0026#39; Note: This assigns port 32100 to Grafana, 32101 to Prometheus, and 32102 to Alertmanager.\nThe output will look similar to the following:\nservice/grafana patched service/prometheus-k8s patched service/alertmanager-main patched   Verify that the Prometheus, Grafana, and Alertmanager pods are running in the monitoring namespace and the respective services have the exports configured correctly:\n$ kubectl get pods,services -o wide -n monitoring The output should look similar to the following:\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/alertmanager-main-0 2/2 Running 0 62s 10.244.2.10 worker-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/alertmanager-main-1 2/2 Running 0 62s 10.244.1.19 worker-node1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/alertmanager-main-2 2/2 Running 0 62s 10.244.2.11 worker-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/grafana-86445dccbb-xz5d5 1/1 Running 0 62s 10.244.1.20 worker-node1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/kube-state-metrics-b5b74495f-8bglg 3/3 Running 0 62s 10.244.1.21 worker-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/node-exporter-wj4jw 2/2 Running 0 62s 10.196.4.112 master-node \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/node-exporter-wl2jv 2/2 Running 0 62s 10.250.111.112 worker-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/node-exporter-wt88k 2/2 Running 0 62s 10.250.111.111 worker-node1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/prometheus-adapter-66b855f564-4pmwk 1/1 Running 0 62s 10.244.2.12 worker-node1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/prometheus-k8s-0 3/3 Running 1 62s 10.244.2.13 worker-node1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/prometheus-k8s-1 3/3 Running 1 62s 10.244.1.22 worker-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/prometheus-operator-8ff9cc68-6q9lc 2/2 Running 0 69s 10.244.2.18 worker-node1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service/alertmanager-main NodePort 10.106.217.213 \u0026lt;none\u0026gt; 9093:32102/TCP 62s alertmanager=main,app=alertmanager service/alertmanager-operated ClusterIP None \u0026lt;none\u0026gt; 9093/TCP,9094/TCP,9094/UDP 62s app=alertmanager service/grafana NodePort 10.97.246.92 \u0026lt;none\u0026gt; 3000:32100/TCP 62s app=grafana service/kube-state-metrics ClusterIP None \u0026lt;none\u0026gt; 8443/TCP,9443/TCP 62s app.kubernetes.io/name=kube-state-metrics service/node-exporter ClusterIP None \u0026lt;none\u0026gt; 9100/TCP 62s app.kubernetes.io/name=node-exporter service/prometheus-adapter ClusterIP 10.109.14.232 \u0026lt;none\u0026gt; 443/TCP 62s name=prometheus-adapter service/prometheus-k8s NodePort 10.101.68.142 \u0026lt;none\u0026gt; 9090:32101/TCP 62s app=prometheus,prometheus=k8s service/prometheus-operated ClusterIP None \u0026lt;none\u0026gt; 9090/TCP 62s app=prometheus service/prometheus-operator ClusterIP None \u0026lt;none\u0026gt; 8443/TCP 70 app.kubernetes.io/component=controller,app.kubernetes.io/name=prometheus-operator   Deploy WebLogic Monitoring Exporter   Download WebLogic Monitoring Exporter:\n$ mkdir -p \u0026lt;work_directory\u0026gt;/wls_exporter $ cd \u0026lt;work_directory\u0026gt;/wls_exporter $ wget https://github.com/oracle/weblogic-monitoring-exporter/releases/download/\u0026lt;version\u0026gt;/wls-exporter.war $ wget https://github.com/oracle/weblogic-monitoring-exporter/releases/download/\u0026lt;version\u0026gt;/get\u0026lt;version\u0026gt;.sh Note: see WebLogic Monitoring Exporter Releases for latest releases.\nFor example:\n$ mkdir -p /scratch/OAMDockerK8S/wls_exporter $ cd /scratch/OAMDockerK8S/wls_exporter $ wget https://github.com/oracle/weblogic-monitoring-exporter/releases/download/v1.2.0/wls-exporter.war $ wget https://github.com/oracle/weblogic-monitoring-exporter/releases/download/v1.2.0/get1.2.0.sh   Create a configuration file config-admin.yaml in the \u0026lt;work_directory\u0026gt;/wls_exporter directory that contains the following. Modify the restPort to match the server port for the OAM Administration Server:\nmetricsNameSnakeCase: true restPort: 7001 queries: - key: name keyName: location prefix: wls_server_ applicationRuntimes: key: name keyName: app componentRuntimes: prefix: wls_webapp_config_ type: WebAppComponentRuntime key: name values: [deploymentState, contextRoot, sourceInfo, openSessionsHighCount, openSessionsCurrentCount, sessionsOpenedTotalCount, sessionCookieMaxAgeSecs, sessionInvalidationIntervalSecs, sessionTimeoutSecs, singleThreadedServletPoolSize, sessionIDLength, servletReloadCheckSecs, jSPPageCheckSecs] servlets: prefix: wls_servlet_ key: servletName - JVMRuntime: prefix: wls_jvm_ key: name - executeQueueRuntimes: prefix: wls_socketmuxer_ key: name values: [pendingRequestCurrentCount] - workManagerRuntimes: prefix: wls_workmanager_ key: name values: [stuckThreadCount, pendingRequests, completedRequests] - threadPoolRuntime: prefix: wls_threadpool_ key: name values: [executeThreadTotalCount, queueLength, stuckThreadCount, hoggingThreadCount] - JMSRuntime: key: name keyName: jmsruntime prefix: wls_jmsruntime_ JMSServers: prefix: wls_jms_ key: name keyName: jmsserver destinations: prefix: wls_jms_dest_ key: name keyName: destination - persistentStoreRuntimes: prefix: wls_persistentstore_ key: name - JDBCServiceRuntime: JDBCDataSourceRuntimeMBeans: prefix: wls_datasource_ key: name - JTARuntime: prefix: wls_jta_ key: name   Create a configuration file config-oamserver.yaml in the \u0026lt;work_directory\u0026gt;/wls_exporter directory that contains the following. Modify the restPort to match the server port for the OAM Managed Servers:\nmetricsNameSnakeCase: true restPort: 14100 queries: - key: name keyName: location prefix: wls_server_ applicationRuntimes: key: name keyName: app componentRuntimes: prefix: wls_webapp_config_ type: WebAppComponentRuntime key: name values: [deploymentState, contextRoot, sourceInfo, openSessionsHighCount, openSessionsCurrentCount, sessionsOpenedTotalCount, sessionCookieMaxAgeSecs, sessionInvalidationIntervalSecs, sessionTimeoutSecs, singleThreadedServletPoolSize, sessionIDLength, servletReloadCheckSecs, jSPPageCheckSecs] servlets: prefix: wls_servlet_ key: servletName - JVMRuntime: prefix: wls_jvm_ key: name - executeQueueRuntimes: prefix: wls_socketmuxer_ key: name values: [pendingRequestCurrentCount] - workManagerRuntimes: prefix: wls_workmanager_ key: name values: [stuckThreadCount, pendingRequests, completedRequests] - threadPoolRuntime: prefix: wls_threadpool_ key: name values: [executeThreadTotalCount, queueLength, stuckThreadCount, hoggingThreadCount] - JMSRuntime: key: name keyName: jmsruntime prefix: wls_jmsruntime_ JMSServers: prefix: wls_jms_ key: name keyName: jmsserver destinations: prefix: wls_jms_dest_ key: name keyName: destination - persistentStoreRuntimes: prefix: wls_persistentstore_ key: name - JDBCServiceRuntime: JDBCDataSourceRuntimeMBeans: prefix: wls_datasource_ key: name - JTARuntime: prefix: wls_jta_ key: name   Create a configuration file config-policyserver.yaml in the \u0026lt;work_directory\u0026gt;/wls_exporter directory that contains the following. Modify the restPort to match the server port for the OAM Policy Manager Servers:\nmetricsNameSnakeCase: true restPort: 15100 queries: - key: name keyName: location prefix: wls_server_ applicationRuntimes: key: name keyName: app componentRuntimes: prefix: wls_webapp_config_ type: WebAppComponentRuntime key: name values: [deploymentState, contextRoot, sourceInfo, openSessionsHighCount, openSessionsCurrentCount, sessionsOpenedTotalCount, sessionCookieMaxAgeSecs, sessionInvalidationIntervalSecs, sessionTimeoutSecs, singleThreadedServletPoolSize, sessionIDLength, servletReloadCheckSecs, jSPPageCheckSecs] servlets: prefix: wls_servlet_ key: servletName - JVMRuntime: prefix: wls_jvm_ key: name - executeQueueRuntimes: prefix: wls_socketmuxer_ key: name values: [pendingRequestCurrentCount] - workManagerRuntimes: prefix: wls_workmanager_ key: name values: [stuckThreadCount, pendingRequests, completedRequests] - threadPoolRuntime: prefix: wls_threadpool_ key: name values: [executeThreadTotalCount, queueLength, stuckThreadCount, hoggingThreadCount] - JMSRuntime: key: name keyName: jmsruntime prefix: wls_jmsruntime_ JMSServers: prefix: wls_jms_ key: name keyName: jmsserver destinations: prefix: wls_jms_dest_ key: name keyName: destination - persistentStoreRuntimes: prefix: wls_persistentstore_ key: name - JDBCServiceRuntime: JDBCDataSourceRuntimeMBeans: prefix: wls_datasource_ key: name - JTARuntime: prefix: wls_jta_ key: name   Generate the deployment package for the OAM Administration Server:\n$ chmod 777 get\u0026lt;version\u0026gt;.sh $ ./get\u0026lt;version\u0026gt; config-admin.yaml For example:\n$ chmod 777 get1.2.0.sh $ ./get1.2.0.sh config-admin.yaml The output will look similar to the following:\n% Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 642 100 642 0 0 1272 0 --:--:-- --:--:-- --:--:-- 1273 100 2033k 100 2033k 0 0 1224k 0 0:00:01 0:00:01 --:--:-- 2503k created /tmp/ci-AcBAO1eTer /tmp/ci-AcBAO1eTer /scratch/OAMDockerK8S/wls_exporter in temp dir adding: config.yml (deflated 65%) /scratch/OAMDockerK8S/wls_exporter This will generate a wls-exporter.war file in the same directory. This war file contains a config.yml that corresponds to config-admin.yaml. Rename the file as follows:\nmv wls-exporter.war wls-exporter-admin.war   Generate the deployment package for the OAM Managed Server and Policy Manager Server, for example:\n$ ./get1.2.0.sh config-oamserver.yaml $ mv wls-exporter.war wls-exporter-oamserver.war $ ./get1.2.0.sh config-policyserver.yaml $ mv wls-exporter.war wls-exporter-policyserver.war   Copy the war files to the persistent volume directory:\n$ cp wls-exporter*.war \u0026lt;work_directory\u0026gt;/\u0026lt;persistent_volume\u0026gt;/ For example:\n$ cp wls-exporter*.war /scratch/OAMDockerK8S/accessdomainpv/   Deploy the wls-exporter war files in OAM WebLogic server   Login to the Oracle Enterprise Manager Console using the URL https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/em.\n  Navigate to WebLogic Domain \u0026gt; Deployments. Click on the padlock in the upper right hand corner and select Lock and Edit.\n  From the \u0026lsquo;Deployment\u0026rsquo; drop down menu select Deploy.\n  In the Select Archive screen, under Archive or exploded directory is on the server where Enterprise Manager is running, click Browse. Navigate to the /u01/oracle/user_projects/domains directory and select wls-exporter-admin.war. Click OK and then Next.\n  In Select Target check AdminServer and click Next.\n  In Application Attributes set the following and click Next:\n Application Name: wls-exporter-admin Context Root: wls-exporter Distribution: Install and start application (servicing all requests)    In Deployment Settings click Deploy.\n  Once you see the message Deployment Succeeded, click Close.\n  Click on the padlock in the upper right hand corner and select Activate Changes.\n  Repeat the above steps to deploy wls-exporter-oamserver.war with the following caveats:\n In Select Target choose oam_cluster In Application Attributes set Application Name: wls-exporter-oamserver, Context Root: wls-exporter In Distribution select Install and start application (servicing all requests)    Repeat the above steps to deploy wls-exporter-policyserver.war with the following caveats:\n In Select Target choose policy_cluster In Application Attributes set Application Name: wls-exporter-policyserver, Context Root: wls-exporter In Distribution select Install and start application (servicing all requests)    Check the wls-exporter is accessible using the URL: https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/wls-exporter.\nYou should see a page saying This is the WebLogic Monitoring Exporter.\n  Prometheus Operator Configuration Prometheus has to be configured to collect the metrics from the weblogic-monitor-exporter. The Prometheus operator identifies the targets using service discovery. To get the weblogic-monitor-exporter end point discovered as a target, you will need to create a service monitor to point to the service.\n  Create a wls-exporter-service-monitor.yaml in the \u0026lt;work_directory\u0026gt;/wls_exporter directory with the following contents:\napiVersion: v1 kind: Secret metadata: name: basic-auth namespace: monitoring data: password: V2VsY29tZTE= ## \u0026lt;password\u0026gt; base64 user: d2VibG9naWM= ## weblogic base64 type: Opaque --- apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: wls-exporter-accessinfra namespace: monitoring labels: k8s-app: wls-exporter spec: namespaceSelector: matchNames: - accessns selector: matchLabels: weblogic.domainName: accessinfra endpoints: - basicAuth: password: name: basic-auth key: password username: name: basic-auth key: user port: default relabelings: - action: labelmap regex: __meta_kubernetes_service_label_(.+) interval: 10s honorLabels: true path: /wls-exporter/metrics Note: In the above example, change the password: V2VsY29tZTE= value to the base64 encoded version of your weblogic password. To find the base64 value run the following:\n$ echo -n \u0026#34;\u0026lt;password\u0026gt;\u0026#34; | base64 If using a different namespace from accessns or a different domain_UID from accessinfra, then change accordingly.\n  Add Rolebinding for the WebLogic OAM domain namespace:\n$ cd \u0026lt;work_directory\u0026gt;/kube-prometheus/manifests Edit the prometheus-roleBindingSpecificNamespaces.yaml file and add the following to the file for your OAM domain namespace, for example accessns.\n- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: prometheus-k8s namespace: accessns roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: prometheus-k8s subjects: - kind: ServiceAccount name: prometheus-k8s namespace: monitoring For example the file should now read:\napiVersion: rbac.authorization.k8s.io/v1 items: - apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: prometheus-k8s namespace: accessns roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: prometheus-k8s subjects: - kind: ServiceAccount name: prometheus-k8s namespace: monitoring - apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: prometheus-k8s namespace: default ....   Add the Role for WebLogic OAM domain namespace. Edit the prometheus-roleSpecificNamespaces.yaml and change the namespace to your OAM domain namespace, for example accessns.\n- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: prometheus-k8s namespace: accessns rules: - apiGroups: - \u0026quot;\u0026quot; resources: - services - endpoints - pods verbs: - get - list - watch ....   Apply the yaml files as follows:\n$ kubectl apply -f prometheus-roleBindingSpecificNamespaces.yaml $ kubectl apply -f prometheus-roleSpecificNamespaces.yaml The output should look similar to the following:\nkubectl apply -f prometheus-roleBindingSpecificNamespaces.yaml rolebinding.rbac.authorization.k8s.io/prometheus-k8s created Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply rolebinding.rbac.authorization.k8s.io/prometheus-k8s configured Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply rolebinding.rbac.authorization.k8s.io/prometheus-k8s configured Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply rolebinding.rbac.authorization.k8s.io/prometheus-k8s configured $ kubectl apply -f prometheus-roleSpecificNamespaces.yaml role.rbac.authorization.k8s.io/prometheus-k8s created Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply role.rbac.authorization.k8s.io/prometheus-k8s configured Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply role.rbac.authorization.k8s.io/prometheus-k8s configured Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply role.rbac.authorization.k8s.io/prometheus-k8s configured   Deploy the ServiceMonitor   Run the following command to create the ServiceMonitor:\n$ cd \u0026lt;work_directory\u0026gt;/wls_exporter $ kubectl create -f wls-exporter-service-monitor.yaml The output will look similar to the following:\nservicemonitor.monitoring.coreos.com/wls-exporter-accessinfra created   Prometheus Service Discovery After ServiceMonitor is deployed, the wls-exporter should be discovered by Prometheus and be able to scrape metrics.\n  Access the following URL to view Prometheus service discovery: http://${MASTERNODE-HOSTNAME}:32101/service-discovery\n  Click on monitoring/wls-exporter-accessinfra/0  and then show more. Verify all the targets are mentioned.\n  Grafana Dashboard   Access the Grafana dashboard with the following URL: http://${MASTERNODE-HOSTNAME}:32100 and login with admin/admin. Change your password when prompted.\n  Import the Grafana dashboard by navigating on the left hand menu to Create \u0026gt; Import. Copy the content from \u0026lt;work_directory\u0026gt;/FMW-DockerImages/OracleAccessManagement/kubernetes/3.0.1/grafana/weblogic_dashboard.json and paste. Then click Load and Import.\n  "
},
{
	"uri": "/fmw-kubernetes/soa-domains/create-or-update-image/",
	"title": "Create or update an image",
	"tags": [],
	"description": "Create or update an Oracle SOA Suite Docker image used for deploying Oracle SOA Suite domains. An Oracle SOA Suite Docker image can be created using the WebLogic Image Tool or using the Dockerfile approach.",
	"content": "If you have access to the My Oracle Support (MOS), and there is a need to build a new image with a patch (bundle or interim), it is recommended to use the WebLogic Image Tool to build an Oracle SOA Suite image for production deployments.\n Create or update an Oracle SOA Suite Docker image using the WebLogic Image Tool  Set up the WebLogic Image Tool Create an image Update an image   Create an Oracle SOA Suite Docker image using Dockerfile  Create or update an Oracle SOA Suite Docker image using the WebLogic Image Tool Using the WebLogic Image Tool, you can create a new Oracle SOA Suite Docker image (can include patches as well) or update an existing image with one or more patches (bundle patch and interim patches).\n Recommendations:\n Use create for creating a new Oracle SOA Suite Docker image either:  without any patches or, containing the Oracle SOA Suite binaries, bundle patch and interim patches. This is the recommended approach if you have access to the Oracle SOA Suite patches because it optimizes the size of the image.   Use update for patching an existing Oracle SOA Suite Docker image with a single interim patch. Note that the patched image size may increase considerably due to additional image layers introduced by the patch application tool.   Set up the WebLogic Image Tool  Prerequisites Set up the WebLogic Image Tool Validate setup WebLogic Image Tool build directory WebLogic Image Tool cache Set up additional build scripts  Prerequisites Verify that your environment meets the following prerequisites:\n Docker client and daemon on the build machine, with minimum Docker version 18.03.1.ce. Bash version 4.0 or later, to enable the command complete feature. JAVA_HOME environment variable set to the appropriate JDK location.  Set up the WebLogic Image Tool To set up the WebLogic Image Tool:\n  Create a working directory and change to it. In these steps, this directory is imagetool-setup.\n$ mkdir imagetool-setup $ cd imagetool-setup   Download the latest version of the WebLogic Image Tool from the releases page.\n  Unzip the release ZIP file to the imagetool-setup directory.\n  Execute the following commands to set up the WebLogic Image Tool on a Linux environment:\n$ cd imagetool-setup/imagetool/bin $ source setup.sh   Validate setup To validate the setup of the WebLogic Image Tool:\n  Enter the following command to retrieve the version of the WebLogic Image Tool:\n$ imagetool --version   Enter imagetool then press the Tab key to display the available imagetool commands:\n$ imagetool \u0026lt;TAB\u0026gt; cache create help rebase update   WebLogic Image Tool build directory The WebLogic Image Tool creates a temporary Docker context directory, prefixed by wlsimgbuilder_temp, every time the tool runs. Under normal circumstances, this context directory will be deleted. However, if the process is aborted or the tool is unable to remove the directory, it is safe for you to delete it manually. By default, the WebLogic Image Tool creates the Docker context directory under the user\u0026rsquo;s home directory. If you prefer to use a different directory for the temporary context, set the environment variable WLSIMG_BLDDIR:\n$ export WLSIMG_BLDDIR=\u0026#34;/path/to/buid/dir\u0026#34; WebLogic Image Tool cache The WebLogic Image Tool maintains a local file cache store. This store is used to look up where the Java, WebLogic Server installers, and WebLogic Server patches reside in the local file system. By default, the cache store is located in the user\u0026rsquo;s $HOME/cache directory. Under this directory, the lookup information is stored in the .metadata file. All automatically downloaded patches also reside in this directory. You can change the default cache store location by setting the environment variable WLSIMG_CACHEDIR:\n$ export WLSIMG_CACHEDIR=\u0026#34;/path/to/cachedir\u0026#34; Set up additional build scripts Creating an Oracle SOA Suite Docker image using the WebLogic Image Tool requires additional container scripts for Oracle SOA Suite domains.\n  Clone the docker-images repository to set up those scripts. In these steps, this directory is DOCKER_REPO:\n$ cd imagetool-setup $ git clone https://github.com/oracle/docker-images.git   Copy the additional WebLogic Image Tool build files from the operator source repository to the imagetool-setup location:\n$ mkdir -p imagetool-setup/docker-images/OracleSOASuite/imagetool/12.2.1.4.0 $ cd imagetool-setup/docker-images/OracleSOASuite/imagetool/12.2.1.4.0 $ cp -rf ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/imagetool-scripts/* .   Create an image After setting up the WebLogic Image Tool and required build scripts, follow these steps to use the WebLogic Image Tool to create a new Oracle SOA Suite Docker image.\nDownload the Oracle SOA Suite installation binaries and patches You must download the required Oracle SOA Suite installation binaries and patches as listed below from the Oracle Software Delivery Cloud and save them in a directory of your choice. In these steps, this directory is download location.\n  Click here to see the sample list of installation binaries and patches:     JDK:\n jdk-8u241-linux-x64.tar.gz    Fusion MiddleWare Infrastructure installer:\n fmw_12.2.1.4.0_infrastructure.jar    Fusion MiddleWare Infrastructure patches:\n p28186730_139422_Generic.zip (Opatch) p30432881_122140_Generic.zip (OWSM) p30513324_122140_Linux-x86-64.zip (OSS) p30581253_122140_Generic.zip (ADF) p30689820_122140_Generic.zip (WLS) p30729380_122140_Generic.zip (COH)    SOA and OSB installers:\n fmw_12.2.1.4.0_soa.jar fmw_12.2.1.4.0_osb.jar    SOA and OSB patches:\n p30749990_122140_Generic.zip (SOA) p30779352_122140_Generic.zip (OSB)       Note: This is a sample list of patches. You must get the appropriate list of patches for your Oracle SOA Suite image.\n Update required build files The following files available in the code repository location ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/imagetool-scripts are used for creating the image.\n additionalBuildCmds.txt buildArgs    In the buildArgs file, update all the occurrences of %DOCKER_REPO% with the docker-images repository location, which is the complete path of imagetool-setup/docker-images.\nFor example, update:\n%DOCKER_REPO%/OracleSOASuite/imagetool/12.2.1.4.0/\nto:\n\u0026lt;imagetool-setup-location\u0026gt;/docker-images/OracleSOASuite/imagetool/12.2.1.4.0/\n  Similarly, update the placeholders %JDK_VERSION% and %BUILDTAG% with appropriate values.\n  Create the image   Add a JDK package to the WebLogic Image Tool cache:\n$ imagetool cache addInstaller --type jdk --version 8u241 --path \u0026lt;download location\u0026gt;/jdk-8u241-linux-x64.tar.gz   Add the downloaded installation binaries to the WebLogic Image Tool cache:\n$ imagetool cache addInstaller --type fmw --version 12.2.1.4.0 --path \u0026lt;download location\u0026gt;/fmw_12.2.1.4.0_infrastructure.jar $ imagetool cache addInstaller --type soa --version 12.2.1.4.0 --path \u0026lt;download location\u0026gt;/fmw_12.2.1.4.0_soa.jar $ imagetool cache addInstaller --type osb --version 12.2.1.4.0 --path \u0026lt;download location\u0026gt;/fmw_12.2.1.4.0_osb.jar   Add the downloaded patches to the WebLogic Image Tool cache:\n  Click here to see the commands to add patches in to the cache:   ``` bash $ imagetool cache addEntry --key 28186730_13.9.4.2.2 --path \u0026lt;download location\u0026gt;/p28186730_139422_Generic.zip $ imagetool cache addEntry --key 30432881_12.2.1.4.0 --path \u0026lt;download location\u0026gt;/p30432881_122140_Generic.zip $ imagetool cache addEntry --key 30513324_12.2.1.4.0 --path \u0026lt;download location\u0026gt;/p30513324_122140_Linux-x86-64.zip $ imagetool cache addEntry --key 30581253_12.2.1.4.0 --path \u0026lt;download location\u0026gt;/p30581253_122140_Generic.zip $ imagetool cache addEntry --key 30689820_12.2.1.4.0 --path \u0026lt;download location\u0026gt;/p30689820_122140_Generic.zip $ imagetool cache addEntry --key 30729380_12.2.1.4.0 --path \u0026lt;download location\u0026gt;/p30729380_122140_Generic.zip $ imagetool cache addEntry --key 30749990_12.2.1.4.0 --path \u0026lt;download location\u0026gt;/p30749990_122140_Generic.zip $ imagetool cache addEntry --key 30779352_12.2.1.4.0 --path \u0026lt;download location\u0026gt;/p30779352_122140_Generic.zip ```      Update the patches list to buildArgs.\nTo the create command in the buildArgs file, append the Oracle SOA Suite and Oracle Service Bus patches list using the --patches flag and Opatch patch using the --opatchBugNumber flag. Sample options for the list of patches above are:\n--patches 30432881_12.2.1.4.0,30513324_12.2.1.4.0,30581253_12.2.1.4.0,30689820_12.2.1.4.0,30729380_12.2.1.4.0,30749990_12.2.1.4.0,30779352_12.2.1.4.0 --opatchBugNumber=28186730_13.9.4.2.2 Example buildArgs file after appending product\u0026rsquo;s list of patches and Opatch patch:\ncreate --jdkVersion=8u241 --type soa_osb --version=12.2.1.4.0 --tag=localhost/oracle/soasuite:12.2.1.4 --pull --additionalBuildCommands \u0026lt;imagetool-setup-location\u0026gt;/docker-images/OracleSOASuite/imagetool/12.2.1.4.0/additionalBuildCmds.txt --additionalBuildFiles \u0026lt;imagetool-setup-location\u0026gt;/docker-images/OracleSOASuite/dockerfiles/12.2.1.4.0/container-scripts --patches 30432881_12.2.1.4.0,30513324_12.2.1.4.0,30581253_12.2.1.4.0,30689820_12.2.1.4.0,30729380_12.2.1.4.0,30749990_12.2.1.4.0,30779352_12.2.1.4.0 --opatchBugNumber=28186730_13.9.4.2.2 Refer to this page for the complete list of options available with the WebLogic Image Tool create command.\n  Enter the following command to create the Oracle SOA Suite image:\n$ imagetool @\u0026lt;absolute path to `buildargs` file\u0026gt;\u0026#34;    Click here to see the sample Dockerfile generated with the imagetool command.    ```bash ########## BEGIN DOCKERFILE ########## # # Copyright (c) 2019, 2020, Oracle and/or its affiliates. # # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # # FROM oraclelinux:7-slim as OS_UPDATE LABEL com.oracle.weblogic.imagetool.buildid=\u0026quot;ee25d9b6-7656-41c2-ad9d-a7ed80ef1e91\u0026quot; USER root RUN yum -y --downloaddir= install gzip tar unzip libaio \\ \u0026amp;\u0026amp; yum -y --downloaddir= clean all \\ \u0026amp;\u0026amp; rm -rf /var/cache/yum/* \\ \u0026amp;\u0026amp; rm -rf ## Create user and group RUN if [ -z \u0026quot;$(getent group oracle)\u0026quot; ]; then hash groupadd \u0026amp;\u0026gt; /dev/null \u0026amp;\u0026amp; groupadd oracle || exit -1 ; fi \\ \u0026amp;\u0026amp; if [ -z \u0026quot;$(getent passwd oracle)\u0026quot; ]; then hash useradd \u0026amp;\u0026gt; /dev/null \u0026amp;\u0026amp; useradd -g oracle oracle || exit -1; fi \\ \u0026amp;\u0026amp; mkdir /u01 \\ \u0026amp;\u0026amp; chown oracle:oracle /u01 # Install Java FROM OS_UPDATE as JDK_BUILD LABEL com.oracle.weblogic.imagetool.buildid=\u0026quot;ee25d9b6-7656-41c2-ad9d-a7ed80ef1e91\u0026quot; ENV JAVA_HOME=/u01/jdk COPY --chown=oracle:oracle jdk-8u231-linux-x64.tar.gz /tmp/imagetool/ USER oracle RUN tar xzf /tmp/imagetool/jdk-8u231-linux-x64.tar.gz -C /u01 \\ \u0026amp;\u0026amp; mv /u01/jdk* /u01/jdk \\ \u0026amp;\u0026amp; rm -rf /tmp/imagetool # Install Middleware FROM OS_UPDATE as WLS_BUILD LABEL com.oracle.weblogic.imagetool.buildid=\u0026quot;ee25d9b6-7656-41c2-ad9d-a7ed80ef1e91\u0026quot; ENV JAVA_HOME=/u01/jdk \\ ORACLE_HOME=/u01/oracle \\ OPATCH_NO_FUSER=true RUN mkdir -p /u01/oracle \\ \u0026amp;\u0026amp; mkdir -p /u01/oracle/oraInventory \\ \u0026amp;\u0026amp; chown oracle:oracle /u01/oracle/oraInventory \\ \u0026amp;\u0026amp; chown oracle:oracle /u01/oracle COPY --from=JDK_BUILD --chown=oracle:oracle /u01/jdk /u01/jdk/ COPY --chown=oracle:oracle fmw_12.2.1.4.0_infrastructure.jar fmw.rsp /tmp/imagetool/ COPY --chown=oracle:oracle fmw_12.2.1.4.0_soa.jar soa.rsp /tmp/imagetool/ COPY --chown=oracle:oracle fmw_12.2.1.4.0_osb.jar osb.rsp /tmp/imagetool/ COPY --chown=oracle:oracle oraInst.loc /u01/oracle/ COPY --chown=oracle:oracle p28186730_139422_Generic.zip /tmp/imagetool/opatch/ COPY --chown=oracle:oracle patches/* /tmp/imagetool/patches/ USER oracle RUN \\ /u01/jdk/bin/java -Xmx1024m -jar /tmp/imagetool/fmw_12.2.1.4.0_infrastructure.jar -silent ORACLE_HOME=/u01/oracle \\ -responseFile /tmp/imagetool/fmw.rsp -invPtrLoc /u01/oracle/oraInst.loc -ignoreSysPrereqs -force -novalidation RUN \\ /u01/jdk/bin/java -Xmx1024m -jar /tmp/imagetool/fmw_12.2.1.4.0_soa.jar -silent ORACLE_HOME=/u01/oracle \\ -responseFile /tmp/imagetool/soa.rsp -invPtrLoc /u01/oracle/oraInst.loc -ignoreSysPrereqs -force -novalidation RUN \\ /u01/jdk/bin/java -Xmx1024m -jar /tmp/imagetool/fmw_12.2.1.4.0_osb.jar -silent ORACLE_HOME=/u01/oracle \\ -responseFile /tmp/imagetool/osb.rsp -invPtrLoc /u01/oracle/oraInst.loc -ignoreSysPrereqs -force -novalidation RUN cd /tmp/imagetool/opatch \\ \u0026amp;\u0026amp; /u01/jdk/bin/jar -xf /tmp/imagetool/opatch/p28186730_139422_Generic.zip \\ \u0026amp;\u0026amp; /u01/jdk/bin/java -jar /tmp/imagetool/opatch/6880880/opatch_generic.jar -silent -ignoreSysPrereqs -force -novalidation oracle_home=/u01/oracle RUN /u01/oracle/OPatch/opatch napply -silent -oh /u01/oracle -phBaseDir /tmp/imagetool/patches \\ \u0026amp;\u0026amp; /u01/oracle/OPatch/opatch util cleanup -silent -oh /u01/oracle FROM OS_UPDATE as FINAL_BUILD ARG ADMIN_NAME ARG ADMIN_HOST ARG ADMIN_PORT ARG MANAGED_SERVER_PORT ENV ORACLE_HOME=/u01/oracle \\ JAVA_HOME=/u01/jdk \\ LC_ALL=${DEFAULT_LOCALE:-en_US.UTF-8} \\ PATH=${PATH}:/u01/jdk/bin:/u01/oracle/oracle_common/common/bin:/u01/oracle/wlserver/common/bin:/u01/oracle LABEL com.oracle.weblogic.imagetool.buildid=\u0026quot;ee25d9b6-7656-41c2-ad9d-a7ed80ef1e91\u0026quot; COPY --from=JDK_BUILD --chown=oracle:oracle /u01/jdk /u01/jdk/ COPY --from=WLS_BUILD --chown=oracle:oracle /u01/oracle /u01/oracle/ USER oracle WORKDIR /u01/oracle #ENTRYPOINT /bin/bash ENV ORACLE_HOME=/u01/oracle \\ VOLUME_DIR=/u01/oracle/user_projects \\ SCRIPT_FILE=/u01/oracle/container-scripts/* \\ JAVA_OPTIONS=\u0026quot;-Doracle.jdbc.fanEnabled=false -Dweblogic.StdoutDebugEnabled=false\u0026quot; \\ PATH=$PATH:/usr/java/default/bin:/u01/oracle/oracle_common/common/bin:/u01/oracle/wlserver/common/bin:/u01/oracle/container-scripts USER root RUN mkdir -p $VOLUME_DIR \u0026amp;\u0026amp; chown oracle:oracle /u01 $VOLUME_DIR \u0026amp;\u0026amp; \\ mkdir -p /u01/oracle/container-scripts \u0026amp;\u0026amp; \\ yum install -y hostname ant \u0026amp;\u0026amp; \\ rm -rf /var/cache/yum #COPY container-scripts/* /u01/oracle/container-scripts/ COPY --chown=oracle:oracle files/build.xml files/createDomainAndStart.sh files/createDomain.py files/soaExtFun.sh files/startAS.sh files/startMS.sh files/updListenAddress.py /u01/oracle/container-scripts/ RUN chmod +xr $SCRIPT_FILE USER oracle WORKDIR ${ORACLE_HOME} CMD [\u0026quot;/u01/oracle/container-scripts/createDomainAndStart.sh\u0026quot;] ########## END DOCKERFILE ########## ```      Check the created image using the docker images command:\n$ docker images | grep soa   Update an image After setting up the WebLogic Image Tool and required build scripts, use the WebLogic Image Tool to update an existing Oracle SOA Suite Docker image:\n  Enter the following command for each patch to add the required patch(es) to the WebLogic Image Tool cache:\n$ cd \u0026lt;imagetool-setup\u0026gt; $ imagetool cache addEntry --key=30761841_12.2.1.4.0 --value \u0026lt;downloaded-patches-location\u0026gt;/p30761841_122140_Generic.zip [INFO ] Added entry 30761841_12.2.1.4.0=\u0026lt;downloaded-patches-location\u0026gt;/p30761841_122140_Generic.zip   Provide the following arguments to the WebLogic Image Tool update command:\n –-fromImage - Identify the image that needs to be updated. In the example below, the image to be updated is soasuite:12.2.1.4. –-patches - Multiple patches can be specified as a comma-separated list. --tag - Specify the new tag to be applied for the image being built.  Refer here for the complete list of options available with the WebLogic Image Tool update command.\n Note: The WebLogic Image Tool cache should have the latest OPatch zip. The WebLogic Image Tool will update the OPatch if it is not already updated in the image.\n Examples   Click here to see the example `update` command:   $ imagetool update --fromImage soasuite:12.2.1.4 --tag=soasuite:12.2.1.4-30761841 --patches=30761841_12.2.1.4.0 [INFO ] Image Tool build ID: bd21dc73-b775-4186-ae03-8219bf02113e [INFO ] Temporary directory used for docker build context: \u0026lt;work-directory\u0026gt;/wlstmp/wlsimgbuilder_temp1117031733123594064 [INFO ] Using patch 28186730_13.9.4.2.2 from cache: \u0026lt;downloaded-patches-location\u0026gt;/p28186730_139422_Generic.zip [WARNING] skipping patch conflict check, no support credentials provided [WARNING] No credentials provided, skipping validation of patches [INFO ] Using patch 30761841_12.2.1.4.0 from cache: \u0026lt;downloaded-patches-location\u0026gt;/p30761841_122140_Generic.zip [INFO ] docker cmd = docker build --force-rm=true --no-cache --tag soasuite:12.2.1.4-30761841 --build-arg http_proxy=http://\u0026lt;YOUR-COMPANY-PROXY\u0026gt; --build-arg https_proxy=http://\u0026lt;YOUR-COMPANY-PROXY\u0026gt; --build-arg no_proxy=\u0026lt;IP addresses and Domain address for no_proxy\u0026gt;,/var/run/docker.sock \u0026lt;work-directory\u0026gt;/wlstmp/wlsimgbuilder_temp1117031733123594064 Sending build context to Docker daemon 53.47MB Step 1/7 : FROM soasuite:12.2.1.4 as FINAL_BUILD ---\u0026gt; 445b649a3459 Step 2/7 : USER root ---\u0026gt; Running in 27f45e6958c3 Removing intermediate container 27f45e6958c3 ---\u0026gt; 150ae0161d46 Step 3/7 : ENV OPATCH_NO_FUSER=true ---\u0026gt; Running in daddfbb8fd9e Removing intermediate container daddfbb8fd9e ---\u0026gt; a5fc6b74be39 Step 4/7 : LABEL com.oracle.weblogic.imagetool.buildid=\u0026quot;bd21dc73-b775-4186-ae03-8219bf02113e\u0026quot; ---\u0026gt; Running in cdfec79c3fd4 Removing intermediate container cdfec79c3fd4 ---\u0026gt; 4c773aeb956f Step 5/7 : USER oracle ---\u0026gt; Running in ed3432e43e89 Removing intermediate container ed3432e43e89 ---\u0026gt; 54fe6b07c447 Step 6/7 : COPY --chown=oracle:oracle patches/* /tmp/imagetool/patches/ ---\u0026gt; d6d12f02a9be Step 7/7 : RUN /u01/oracle/OPatch/opatch napply -silent -oh /u01/oracle -phBaseDir /tmp/imagetool/patches \u0026amp;\u0026amp; /u01/oracle/OPatch/opatch util cleanup -silent -oh /u01/oracle \u0026amp;\u0026amp; rm -rf /tmp/imagetool ---\u0026gt; Running in a79addca4d2f Oracle Interim Patch Installer version 13.9.4.2.2 Copyright (c) 2020, Oracle Corporation. All rights reserved. Oracle Home : /u01/oracle Central Inventory : /u01/oracle/oraInventory from : /u01/oracle/oraInst.loc OPatch version : 13.9.4.2.2 OUI version : 13.9.4.0.0 Log file location : /u01/oracle/cfgtoollogs/opatch/opatch2020-06-01_10-56-13AM_1.log OPatch detects the Middleware Home as \u0026quot;/u01/oracle\u0026quot; Verifying environment and performing prerequisite checks... OPatch continues with these patches: 30761841 Do you want to proceed? [y|n] Y (auto-answered by -silent) User Responded with: Y All checks passed. Please shutdown Oracle instances running out of this ORACLE_HOME on the local system. (Oracle Home = '/u01/oracle') Is the local system ready for patching? [y|n] Y (auto-answered by -silent) User Responded with: Y Backing up files... Applying interim patch '30761841' to OH '/u01/oracle' ApplySession: Optional component(s) [ oracle.org.bouncycastle.bcprov.ext.jdk15on, 1.55.0.0.0 ] , [ oracle.org.bouncycastle.bcprov.ext.jdk15on, 1.55.0.0.0 ] , [ oracle.org.bouncycastle.bcprov.ext.jdk15on, 1.5.0.0.0 ] , [ oracle.org.bouncycastle.bcprov.ext.jdk15on, 1.5.0.0.0 ] , [ oracle.org.bouncycastle.bcprov.jdk15on, 1.55.0.0.0 ] , [ oracle.org.bouncycastle.bcprov.jdk15on, 1.55.0.0.0 ] , [ oracle.org.bouncycastle.bcprov.jdk15on, 1.52.0.0.0 ] , [ oracle.org.bouncycastle.bcprov.jdk15on, 1.52.0.0.0 ] , [ oracle.org.bouncycastle.bcprov.ext.jdk15on, 1.48.0.0.0 ] , [ oracle.org.bouncycastle.bcprov.ext.jdk15on, 1.48.0.0.0 ] , [ oracle.org.bouncycastle.bcpkix.jdk15on, 1.49.0.0.0 ] , [ oracle.org.bouncycastle.bcpkix.jdk15on, 1.49.0.0.0 ] , [ oracle.org.bouncycastle.bcprov.jdk15on, 1.51.0.0.0 ] , [ oracle.org.bouncycastle.bcprov.jdk15on, 1.51.0.0.0 ] , [ oracle.org.bouncycastle.bcprov.jdk15on, 1.54.0.0.0 ] , [ oracle.org.bouncycastle.bcprov.jdk15on, 1.54.0.0.0 ] , [ oracle.org.bouncycastle.bcprov.ext.jdk15on, 1.54.0.0.0 ] , [ oracle.org.bouncycastle.bcprov.ext.jdk15on, 1.54.0.0.0 ] , [ oracle.org.bouncycastle.bcpkix.jdk15on, 1.5.0.0.0 ] , [ oracle.org.bouncycastle.bcpkix.jdk15on, 1.5.0.0.0 ] , [ oracle.org.bouncycastle.bcpkix.jdk15on, 1.54.0.0.0 ] , [ oracle.org.bouncycastle.bcpkix.jdk15on, 1.54.0.0.0 ] , [ oracle.org.bouncycastle.bcpkix.jdk15on, 1.55.0.0.0 ] , [ oracle.org.bouncycastle.bcpkix.jdk15on, 1.55.0.0.0 ] , [ oracle.org.bouncycastle.bcprov.jdk15on, 1.49.0.0.0 ] , [ oracle.org.bouncycastle.bcprov.jdk15on, 1.49.0.0.0 ] , [ oracle.org.bouncycastle.bcprov.jdk15on, 1.5.0.0.0 ] , [ oracle.org.bouncycastle.bcprov.jdk15on, 1.5.0.0.0 ] not present in the Oracle Home or a higher version is found. Patching component oracle.org.bouncycastle.bcprov.jdk15on, 1.60.0.0.0... Patching component oracle.org.bouncycastle.bcprov.jdk15on, 1.60.0.0.0... Patching component oracle.org.bouncycastle.bcprov.ext.jdk15on, 1.60.0.0.0... Patching component oracle.org.bouncycastle.bcprov.ext.jdk15on, 1.60.0.0.0... Patching component oracle.org.bouncycastle.bcpkix.jdk15on, 1.60.0.0.0... Patching component oracle.org.bouncycastle.bcpkix.jdk15on, 1.60.0.0.0... Patch 30761841 successfully applied. Log file location: /u01/oracle/cfgtoollogs/opatch/opatch2020-06-01_10-56-13AM_1.log OPatch succeeded. Oracle Interim Patch Installer version 13.9.4.2.2 Copyright (c) 2020, Oracle Corporation. All rights reserved. Oracle Home : /u01/oracle Central Inventory : /u01/oracle/oraInventory from : /u01/oracle/oraInst.loc OPatch version : 13.9.4.2.2 OUI version : 13.9.4.0.0 Log file location : /u01/oracle/cfgtoollogs/opatch/opatch2020-06-01_10-57-19AM_1.log OPatch detects the Middleware Home as \u0026quot;/u01/oracle\u0026quot; Invoking utility \u0026quot;cleanup\u0026quot; OPatch will clean up 'restore.sh,make.txt' files and 'scratch,backup' directories. You will be still able to rollback patches after this cleanup. Do you want to proceed? [y|n] Y (auto-answered by -silent) User Responded with: Y Backup area for restore has been cleaned up. For a complete list of files/directories deleted, Please refer log file. OPatch succeeded. Removing intermediate container a79addca4d2f ---\u0026gt; 2ef2a67a685b Successfully built 2ef2a67a685b Successfully tagged soasuite:12.2.1.4-30761841 [INFO ] Build successful. Build time=112s. Image tag=soasuite:12.2.1.4-30761841      Click here to see the example Dockerfile generated by the WebLogic Image Tool with the `–-dryRun` option:   $ imagetool update --fromImage soasuite:12.2.1.4 --tag=soasuite:12.2.1.4-30761841 --patches=30761841_12.2.1.4.0 --dryRun [INFO ] Image Tool build ID: f9feea35-c52c-4974-b155-eb7f34d95892 [INFO ] Temporary directory used for docker build context: \u0026lt;work-directory\u0026gt;/wlstmp/wlsimgbuilder_temp1799120592903014749 [INFO ] Using patch 28186730_13.9.4.2.2 from cache: \u0026lt;downloaded-patches-location\u0026gt;/p28186730_139422_Generic.zip [WARNING] skipping patch conflict check, no support credentials provided [WARNING] No credentials provided, skipping validation of patches [INFO ] Using patch 30761841_12.2.1.4.0 from cache: \u0026lt;downloaded-patches-location\u0026gt;/p30761841_122140_Generic.zip [INFO ] docker cmd = docker build --force-rm=true --no-cache --tag soasuite:12.2.1.4-30761841 --build-arg http_proxy=http://www.yourcompany.proxy.com:80 --build-arg https_proxy=http://www.yourcompany.proxy.com:80 --build-arg no_proxy=10.250.109.251,localhost,127.0.0.1,/var/run/docker.sock \u0026lt;work-directory\u0026gt;/wlstmp/wlsimgbuilder_temp1799120592903014749 ########## BEGIN DOCKERFILE ########## # # Copyright (c) 2019, 2020, Oracle and/or its affiliates. # # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # # FROM soasuite:12.2.1.4 as FINAL_BUILD USER root ENV OPATCH_NO_FUSER=true LABEL com.oracle.weblogic.imagetool.buildid=\u0026#34;f9feea35-c52c-4974-b155-eb7f34d95892\u0026#34; USER oracle COPY --chown=oracle:oracle patches/* /tmp/imagetool/patches/ RUN /u01/oracle/OPatch/opatch napply -silent -oh /u01/oracle -phBaseDir /tmp/imagetool/patches \\  \u0026amp;\u0026amp; /u01/oracle/OPatch/opatch util cleanup -silent -oh /u01/oracle \\  \u0026amp;\u0026amp; rm -rf /tmp/imagetool ########## END DOCKERFILE ##########      Check the built image using the docker images command:\n$ docker images | grep soa soasuite 12.2.1.4-30761841 2ef2a67a685b About a minute ago 4.84GB   Create an Oracle SOA Suite Docker image using Dockerfile For test and development purposes, you can create an Oracle SOA Suite image using the Dockerfile. Consult the README file for important prerequisite steps, such as building or pulling the Server JRE Docker image, Oracle FMW Infrastructure Docker image, and downloading the Oracle SOA Suite installer and bundle patch binaries.\nA prebuilt Oracle Fusion Middleware Infrastructure image, container-registry.oracle.com/middleware/fmw-infrastructure:12.2.1.4, is available at container-registry.oracle.com. We recommend that you pull and rename this image to build the Oracle SOA Suite image.\n$ docker pull container-registry.oracle.com/middleware/fmw-infrastructure:12.2.1.4 $ docker tag container-registry.oracle.com/middleware/fmw-infrastructure:12.2.1.4 oracle/fmw-infrastructure:12.2.1.4 Follow these steps to build an Oracle Fusion Middleware Infrastructure image, and then the Oracle SOA Suite image as a layer on top of that:\n  Make a local clone of the sample repository:\n$ git clone https://github.com/oracle/docker-images   Build the oracle/fmw-infrastructure:12.2.1.4 image:\n$ cd docker-images/OracleFMWInfrastructure/dockerfiles $ sh buildDockerImage.sh -v 12.2.1.4 -s This will produce an image named oracle/fmw-infrastructure:12.2.1.4.\n  Download the Oracle SOA Suite installer from the Oracle Technology Network or e-delivery.\n Note: Copy the installer binaries to the same location as the Dockerfile.\n   To build the Oracle SOA Suite image with patches, you must download and drop the patch zip files (for example, p29928100_122140_Generic.zip) into the patches/ folder under the version that is required. For example, for 12.2.1.4.0 the folder is 12.2.1.4/patches. Similarly, to build the image by including the OPatch patch, download and drop the OPatch patch zip file (for e.g. p28186730_139424_Generic.zip) into the opatch_patch/ folder.\n  Create the Oracle SOA Suite image by running the provided script:\n$ cd docker-images/OracleSOASuite/dockerfiles $ ./buildDockerImage.sh -v 12.2.1.4 -s The image produced will be named oracle/soasuite:12.2.1.4. The samples and instructions assume the Oracle SOA Suite image is named soasuite:12.2.1.4. You must rename your image to match this name, or update the samples to refer to the image you created.\n$ docker tag oracle/soasuite:12.2.1.4 soasuite:12.2.1.4   "
},
{
	"uri": "/fmw-kubernetes/soa-domains/adminguide/deploying-composites/",
	"title": "Deploy composite applications",
	"tags": [],
	"description": "Deploy composite applications for Oracle SOA Suite and Oracle Service Bus domains.",
	"content": "Learn how to deploy the composite applications for Oracle SOA Suite and Oracle Service Bus domains.\n Deploy using JDeveloper  Deploy Oracle SOA Suite and Oracle Service Bus composite applications from Oracle JDeveloper to Oracle SOA Suite in the WebLogic Kubernetes operator environment.\n Deploy using Maven and Ant  Deploy Oracle SOA Suite and Oracle Service Bus composite applications using the Maven and Ant based approach in an Oracle SOA Suite deployment.\n "
},
{
	"uri": "/fmw-kubernetes/oud/manage-oud-containers/",
	"title": "Manage Oracle Unified Directory Containers",
	"tags": [],
	"description": "This document provides steps manage Oracle Unified Directory containers.",
	"content": "Important considerations for Oracle Unified Directory instances in Kubernetes.\n a) Logging and Visualization for Helm Chart oud-ds-rs Deployment  Describes the steps for logging and visualization with Elasticsearch and Kibana.\n b) Monitoring an Oracle Unified Directory Instance  Describes the steps for Monitoring the Oracle Unified Directory environment.\n "
},
{
	"uri": "/fmw-kubernetes/oudsm/manage-oudsm-containers/",
	"title": "Manage Oracle Unified Directory Services Manager Containers",
	"tags": [],
	"description": "This document provides steps to manage Oracle Unified Directory Services Manager containers.",
	"content": "Important considerations for Oracle Unified Directory Services Manager instances in Kubernetes.\n a) Logging and Visualization for Helm Chart oudsm Deployment  Describes the steps for logging and visualization with Elasticsearch and Kibana.\n b) Monitoring an Oracle Unified Directory Services Manager Instance  Describes the steps for Monitoring the Oracle Unified Directory Services Manager environment.\n "
},
{
	"uri": "/fmw-kubernetes/oudsm/",
	"title": "Oracle Unified Directory Services Manager",
	"tags": [],
	"description": "Oracle Unified Directory Services Manager provides an interface for managing instances of Oracle Unified Directory",
	"content": "Oracle Unified Directory Services Manager is an interface for managing instances of Oracle Unified Directory. Oracle Unified Directory Services Manager enables you to configure the structure of the directory, define objects in the directory, add and configure users, groups, and other entries. Oracle Unified Directory Services Manager is also the interface you use to manage entries, schema, security, and other directory features.\nThis project supports deployment of Oracle Unified Directory Services Manager images based on the 12cPS4 (12.2.1.4.0) release within a Kubernetes environment. The Oracle Unified Directory Services Manager Image refers to binaries for Oracle Unified Directory Services Manager Release 12.2.1.4.0.\nImage: oracle/oudsm:12.2.1.4.0\nFollow the instructions in this guide to set up Oracle Unified Directory Services Manager on Kubernetes.\nGetting started For detailed information about deploying Oracle Unified Directory Services Manager, start at Prerequisites and follow this documentation sequentially.\nCurrent release The current supported release of Oracle Unified Directory Services Manager is OUD 12c PS4 (12.2.1.4.0)\n"
},
{
	"uri": "/fmw-kubernetes/oam/manage-oam-domains/delete-domain-home/",
	"title": "Delete the OAM domain home",
	"tags": [],
	"description": "Learn about the steps to cleanup the OAM domain home.",
	"content": "Sometimes in production, but most likely in testing environments, you might want to remove the domain home that is generated using the create-domain.sh script.\n  Run the following command to delete the jobs, domain, and configmaps:\n$ kubectl delete jobs \u0026lt;domain_job\u0026gt; -n \u0026lt;domain_namespace\u0026gt; $ kubectl delete domain \u0026lt;domain_uid\u0026gt; -n \u0026lt;domain_namespace\u0026gt; $ kubectl delete configmaps \u0026lt;domain_job\u0026gt;-cm -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl delete jobs accessinfra-create-oam-infra-domain-job -n accessns $ kubectl delete domain accessinfra -n accessns $ kubectl delete configmaps accessinfra-create-oam-infra-domain-job-cm -n accessns   Drop the RCU schemas as follows:\n$ kubectl exec -it helper -n \u0026lt;domain_namespace\u0026gt; -- /bin/bash [oracle@helper ~]$ [oracle@helper ~]$ export CONNECTION_STRING=\u0026lt;db_host.domain\u0026gt;:\u0026lt;db_port\u0026gt;/\u0026lt;service_name\u0026gt; [oracle@helper ~]$ export RCUPREFIX=\u0026lt;rcu_schema_prefix\u0026gt; /u01/oracle/oracle_common/bin/rcu -silent -dropRepository -databaseType ORACLE -connectString $CONNECTION_STRING \\ -dbUser sys -dbRole sysdba -selectDependentsForComponents true -schemaPrefix $RCUPREFIX \\ -component MDS -component IAU -component IAU_APPEND -component IAU_VIEWER -component OPSS \\ -component WLS -component STB -component OAM -f \u0026lt; /tmp/pwd.txt For example:\n$ kubectl exec -it helper -n accessns -- /bin/bash [oracle@helper ~]$ export CONNECTION_STRING=mydatabasehost.example.com:1521/orcl.example.com [oracle@helper ~]$ export RCUPREFIX=OAMK8S /u01/oracle/oracle_common/bin/rcu -silent -dropRepository -databaseType ORACLE -connectString $CONNECTION_STRING \\ -dbUser sys -dbRole sysdba -selectDependentsForComponents true -schemaPrefix $RCUPREFIX \\ -component MDS -component IAU -component IAU_APPEND -component IAU_VIEWER -component OPSS \\ -component WLS -component STB -component OAM -f \u0026lt; /tmp/pwd.txt   Delete the Persistent Volume and Persistent Volume Claim:\n$ kubectl delete pv \u0026lt;pv-name\u0026gt; -n \u0026lt;domain_namespace\u0026gt; $ kubectl delete pvc \u0026lt;pvc-name\u0026gt; -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl delete pv accessinfra-domain-pv -n accessns $ kubectl delete pvc accessinfra-domain-pvc -n accessns   Delete the contents of the persistent volume, for example:\n$ rm -rf \u0026lt;work directory\u0026gt;/accessdomainpv/* For example:\n$ rm -rf /scratch/OAMDockerK8S/accessdomainpv/*   Delete the Oracle WebLogic Server Kubernetes Operator, by running the following command:\n$ helm delete weblogic-kubernetes-operator   To delete NGINX:\ncd \u0026lt;work_directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain $ kubectl delete -f ssl-nginx-ingress.yaml Then run:\n$ helm delete nginx-ingress   To delete Voyager:\nhelm delete voyager-operator -n voyager then:\n$ helm delete oam-voyager-ingress -n \u0026lt;domain_namespace\u0026gt; For example:\n$ helm delete oam-voyager-ingress -n accessns   "
},
{
	"uri": "/fmw-kubernetes/wcsites-domains/known-issues/",
	"title": "Known Issues in WCSites",
	"tags": [],
	"description": "Known Issues for Oracle WebCenter Sites domains",
	"content": "Known Issues for Oracle WebCenter Sites domains\nKnown issues    Issue Description     Publishing via LoadBalancer Endpoint Currenly publishihng is only supported via NodePort as described in section For Publishing Setting in WebCenter Sites on page.    "
},
{
	"uri": "/fmw-kubernetes/oig/manage-oig-domains/monitoring-oim-domains/",
	"title": "Monitoring an OIG domain",
	"tags": [],
	"description": "Describes the steps for Monitoring the OIG domain and Publising the logs to Elasticsearch.",
	"content": "After the OIG domain is set up you can monitor the OIG instance using Prometheus and Grafana. See Monitoring a domain.\nThe WebLogic Monitoring Exporter uses the WLS RESTful Management API to scrape runtime information and then exports Prometheus-compatible metrics. It is deployed as a web application in a WebLogic Server (WLS) instance, version 12.2.1 or later, typically, in the instance from which you want to get metrics.\nDeploy the Prometheus operator   Clone Prometheus by running the following commands:\n$ cd \u0026lt;work directory\u0026gt; $ git clone https://github.com/coreos/kube-prometheus.git Note: Please refer the compatibility matrix of Kube Prometheus. Please download the release of the repository according to the Kubernetes version of your cluster. In the above example the latest release will be downloaded.\nFor example:\n$ cd /scratch/OIGDockerK8S $ git clone https://github.com/coreos/kube-prometheus.git   Run the following command to create the namespace and custom resource definitions:\n$ cd kube-prometheus $ kubectl create -f manifests/setup The output will look similar to the following:\nkubectl create -f manifests/setup namespace/monitoring created customresourcedefinition.apiextensions.k8s.io/alertmanagers.monitoring.coreos.com created customresourcedefinition.apiextensions.k8s.io/podmonitors.monitoring.coreos.com created customresourcedefinition.apiextensions.k8s.io/probes.monitoring.coreos.com created customresourcedefinition.apiextensions.k8s.io/prometheuses.monitoring.coreos.com created customresourcedefinition.apiextensions.k8s.io/prometheusrules.monitoring.coreos.com created customresourcedefinition.apiextensions.k8s.io/servicemonitors.monitoring.coreos.com created customresourcedefinition.apiextensions.k8s.io/thanosrulers.monitoring.coreos.com created clusterrole.rbac.authorization.k8s.io/prometheus-operator created clusterrolebinding.rbac.authorization.k8s.io/prometheus-operator created deployment.apps/prometheus-operator created service/prometheus-operator created serviceaccount/prometheus-operator created   Run the following command to created the rest of the resources:\n$ kubectl create -f manifests/ The output will look similar to the following:\nalertmanager.monitoring.coreos.com/main created secret/alertmanager-main created service/alertmanager-main created serviceaccount/alertmanager-main created servicemonitor.monitoring.coreos.com/alertmanager created secret/grafana-datasources created configmap/grafana-dashboard-apiserver created configmap/grafana-dashboard-cluster-total created configmap/grafana-dashboard-controller-manager created configmap/grafana-dashboard-k8s-resources-cluster created configmap/grafana-dashboard-k8s-resources-namespace created configmap/grafana-dashboard-k8s-resources-node created configmap/grafana-dashboard-k8s-resources-pod created configmap/grafana-dashboard-k8s-resources-workload created configmap/grafana-dashboard-k8s-resources-workloads-namespace created configmap/grafana-dashboard-kubelet created configmap/grafana-dashboard-namespace-by-pod created configmap/grafana-dashboard-namespace-by-workload created configmap/grafana-dashboard-node-cluster-rsrc-use created configmap/grafana-dashboard-node-rsrc-use created configmap/grafana-dashboard-nodes created configmap/grafana-dashboard-persistentvolumesusage created configmap/grafana-dashboard-pod-total created configmap/grafana-dashboard-prometheus-remote-write created configmap/grafana-dashboard-prometheus created configmap/grafana-dashboard-proxy created configmap/grafana-dashboard-scheduler created configmap/grafana-dashboard-statefulset created configmap/grafana-dashboard-workload-total created configmap/grafana-dashboards created deployment.apps/grafana created service/grafana created serviceaccount/grafana created servicemonitor.monitoring.coreos.com/grafana created clusterrole.rbac.authorization.k8s.io/kube-state-metrics created clusterrolebinding.rbac.authorization.k8s.io/kube-state-metrics created deployment.apps/kube-state-metrics created service/kube-state-metrics created serviceaccount/kube-state-metrics created servicemonitor.monitoring.coreos.com/kube-state-metrics created clusterrole.rbac.authorization.k8s.io/node-exporter created clusterrolebinding.rbac.authorization.k8s.io/node-exporter created daemonset.apps/node-exporter created service/node-exporter created serviceaccount/node-exporter created servicemonitor.monitoring.coreos.com/node-exporter created apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created clusterrole.rbac.authorization.k8s.io/prometheus-adapter created clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created clusterrolebinding.rbac.authorization.k8s.io/prometheus-adapter created clusterrolebinding.rbac.authorization.k8s.io/resource-metrics:system:auth-delegator created clusterrole.rbac.authorization.k8s.io/resource-metrics-server-resources created configmap/adapter-config created deployment.apps/prometheus-adapter created rolebinding.rbac.authorization.k8s.io/resource-metrics-auth-reader created service/prometheus-adapter created serviceaccount/prometheus-adapter created servicemonitor.monitoring.coreos.com/prometheus-adapter created clusterrole.rbac.authorization.k8s.io/prometheus-k8s created clusterrolebinding.rbac.authorization.k8s.io/prometheus-k8s created servicemonitor.monitoring.coreos.com/prometheus-operator created prometheus.monitoring.coreos.com/k8s created rolebinding.rbac.authorization.k8s.io/prometheus-k8s-config created rolebinding.rbac.authorization.k8s.io/prometheus-k8s created rolebinding.rbac.authorization.k8s.io/prometheus-k8s created rolebinding.rbac.authorization.k8s.io/prometheus-k8s created role.rbac.authorization.k8s.io/prometheus-k8s-config created role.rbac.authorization.k8s.io/prometheus-k8s created role.rbac.authorization.k8s.io/prometheus-k8s created role.rbac.authorization.k8s.io/prometheus-k8s created prometheusrule.monitoring.coreos.com/prometheus-k8s-rules created service/prometheus-k8s created serviceaccount/prometheus-k8s created servicemonitor.monitoring.coreos.com/prometheus created servicemonitor.monitoring.coreos.com/kube-apiserver created servicemonitor.monitoring.coreos.com/coredns created servicemonitor.monitoring.coreos.com/kube-controller-manager created servicemonitor.monitoring.coreos.com/kube-scheduler created servicemonitor.monitoring.coreos.com/kubelet created   Kube-Prometheus requires all nodes to be labelled with kubernetes.io/os=linux. To check if your nodes are labelled, run the following:\n$ kubectl get nodes --show-labels If the nodes are labelled the output will look similar to the following:\nNAME STATUS ROLES AGE VERSION LABELS worker-node1 Ready \u0026lt;none\u0026gt; 42d v1.18.4 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=worker-node1,kubernetes.io/os=linux worker-node2 Ready \u0026lt;none\u0026gt; 42d v1.18.4 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=worker-node2,kubernetes.io/os=linux master-node Ready master 42d v1.18.4 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=masternode,kubernetes.io/os=linux,node-role.kubernetes.io/master= If the nodes are not labelled, run the following command:\n$ kubectl label nodes --all kubernetes.io/os=linux   Provide external access for Grafana, Prometheus, and Alertmanager, by running the following commands:\n$ kubectl patch svc grafana -n monitoring --type=json -p \u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/type\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;NodePort\u0026#34; },{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/ports/0/nodePort\u0026#34;, \u0026#34;value\u0026#34;: 32100 }]\u0026#39; $ kubectl patch svc prometheus-k8s -n monitoring --type=json -p \u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/type\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;NodePort\u0026#34; },{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/ports/0/nodePort\u0026#34;, \u0026#34;value\u0026#34;: 32101 }]\u0026#39; $ kubectl patch svc alertmanager-main -n monitoring --type=json -p \u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/type\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;NodePort\u0026#34; },{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/ports/0/nodePort\u0026#34;, \u0026#34;value\u0026#34;: 32102 }]\u0026#39; Note: This assigns port 32100 to Grafana, 32101 to Prometheus, and 32102 to Alertmanager.\nThe output will look similar to the following:\nservice/grafana patched service/prometheus-k8s patched service/alertmanager-main patched   Verify that the Prometheus, Grafana, and Alertmanager pods are running in the monitoring namespace and the respective services have the exports configured correctly:\n$ kubectl get pods,services -o wide -n monitoring The output should look similar to the following:\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/alertmanager-main-0 2/2 Running 0 97s 10.244.2.52 worker-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/alertmanager-main-1 2/2 Running 0 97s 10.244.1.61 worker-node1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/alertmanager-main-2 2/2 Running 0 97s 10.244.2.53 worker-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/grafana-86445dccbb-dln2l 1/1 Running 0 96s 10.244.2.55 worker-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/kube-state-metrics-5b67d79459-k7xrb 3/3 Running 0 96s 10.244.1.63 worker-node1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/node-exporter-dhp4k 2/2 Running 0 96s 10.250.111.111 worker-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/node-exporter-jknkv 2/2 Running 0 96s 10.196.4.112 masternode \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/node-exporter-vpn9l 2/2 Running 0 96s 10.250.111.112 worker-node1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/prometheus-adapter-66b855f564-snkjb 1/1 Running 0 96s 10.244.2.56 worker-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/prometheus-k8s-0 3/3 Running 0 96s 10.244.2.54 worker-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/prometheus-k8s-1 3/3 Running 0 96s 10.244.1.62 worker-node1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/prometheus-operator-78fcb48ccf-gcgc5 2/2 Running 0 107s 10.244.1.60 worker-node1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service/alertmanager-main NodePort 10.107.184.118 \u0026lt;none\u0026gt; 9093:32102/TCP 98s alertmanager=main,app=alertmanager service/alertmanager-operated ClusterIP None \u0026lt;none\u0026gt; 9093/TCP,9094/TCP,9094/UDP 97s app=alertmanager service/grafana NodePort 10.96.249.254 \u0026lt;none\u0026gt; 3000:32100/TCP 97s app=grafana service/kube-state-metrics ClusterIP None \u0026lt;none\u0026gt; 8443/TCP,9443/TCP 97s app.kubernetes.io/name=kube-state-metrics service/node-exporter ClusterIP None \u0026lt;none\u0026gt; 9100/TCP 97s app.kubernetes.io/name=node-exporter service/prometheus-adapter ClusterIP 10.100.222.239 \u0026lt;none\u0026gt; 443/TCP 97s name=prometheus-adapter service/prometheus-k8s NodePort 10.106.163.78 \u0026lt;none\u0026gt; 9090:32101/TCP 96s app=prometheus,prometheus=k8s service/prometheus-operated ClusterIP None \u0026lt;none\u0026gt; 9090/TCP 96s app=prometheus service/prometheus-operator ClusterIP None \u0026lt;none\u0026gt; 8443/TCP 108s app.kubernetes.io/component=contr oller,app.kubernetes.io/name=prometheus-operator   Deploy WebLogic Monitoring Exporter   Download WebLogic Monitoring Exporter:\n$ mkdir -p \u0026lt;work_directory\u0026gt;/wls_exporter $ cd \u0026lt;work_directory\u0026gt;/wls_exporter $ wget https://github.com/oracle/weblogic-monitoring-exporter/releases/download/\u0026lt;version\u0026gt;/wls-exporter.war $ wget https://github.com/oracle/weblogic-monitoring-exporter/releases/download/\u0026lt;version\u0026gt;/get\u0026lt;version\u0026gt;.sh For example:\n$ mkdir -p /scratch/OIGDockerK8S/wls_exporter $ cd /scratch/OIGDockerK8S/wls_exporter $ wget https://github.com/oracle/weblogic-monitoring-exporter/releases/download/v1.2.0/wls-exporter.war $ wget https://github.com/oracle/weblogic-monitoring-exporter/releases/download/v1.2.0/get1.2.0.sh   Create a configuration file config-admin.yaml in the \u0026lt;work_directory\u0026gt;/wls_exporter directory that contains the following. Modify the restPort to match the server port for the OIG Administration Server:\nmetricsNameSnakeCase: true restPort: 7001 queries: - key: name keyName: location prefix: wls_server_ applicationRuntimes: key: name keyName: app componentRuntimes: prefix: wls_webapp_config_ type: WebAppComponentRuntime key: name values: [deploymentState, contextRoot, sourceInfo, openSessionsHighCount, openSessionsCurrentCount, sessionsOpenedTotalCount, sessionCookieMaxAgeSecs, sessionInvalidationIntervalSecs, sessionTimeoutSecs, singleThreadedServletPoolSize, sessionIDLength, servletReloadCheckSecs, jSPPageCheckSecs] servlets: prefix: wls_servlet_ key: servletName - JVMRuntime: prefix: wls_jvm_ key: name - executeQueueRuntimes: prefix: wls_socketmuxer_ key: name values: [pendingRequestCurrentCount] - workManagerRuntimes: prefix: wls_workmanager_ key: name values: [stuckThreadCount, pendingRequests, completedRequests] - threadPoolRuntime: prefix: wls_threadpool_ key: name values: [executeThreadTotalCount, queueLength, stuckThreadCount, hoggingThreadCount] - JMSRuntime: key: name keyName: jmsruntime prefix: wls_jmsruntime_ JMSServers: prefix: wls_jms_ key: name keyName: jmsserver destinations: prefix: wls_jms_dest_ key: name keyName: destination - persistentStoreRuntimes: prefix: wls_persistentstore_ key: name - JDBCServiceRuntime: JDBCDataSourceRuntimeMBeans: prefix: wls_datasource_ key: name - JTARuntime: prefix: wls_jta_ key: name   Create a configuration file config-oimserver.yaml in the \u0026lt;work_directory\u0026gt;/wls_exporter directory that contains the following. Modify the restPort to match the server port for the OIG Managed Servers:\nmetricsNameSnakeCase: true restPort: 14000 queries: - key: name keyName: location prefix: wls_server_ applicationRuntimes: key: name keyName: app componentRuntimes: prefix: wls_webapp_config_ type: WebAppComponentRuntime key: name values: [deploymentState, contextRoot, sourceInfo, openSessionsHighCount, openSessionsCurrentCount, sessionsOpenedTotalCount, sessionCookieMaxAgeSecs, sessionInvalidationIntervalSecs, sessionTimeoutSecs, singleThreadedServletPoolSize, sessionIDLength, servletReloadCheckSecs, jSPPageCheckSecs] servlets: prefix: wls_servlet_ key: servletName - JVMRuntime: prefix: wls_jvm_ key: name - executeQueueRuntimes: prefix: wls_socketmuxer_ key: name values: [pendingRequestCurrentCount] - workManagerRuntimes: prefix: wls_workmanager_ key: name values: [stuckThreadCount, pendingRequests, completedRequests] - threadPoolRuntime: prefix: wls_threadpool_ key: name values: [executeThreadTotalCount, queueLength, stuckThreadCount, hoggingThreadCount] - JMSRuntime: key: name keyName: jmsruntime prefix: wls_jmsruntime_ JMSServers: prefix: wls_jms_ key: name keyName: jmsserver destinations: prefix: wls_jms_dest_ key: name keyName: destination - persistentStoreRuntimes: prefix: wls_persistentstore_ key: name - JDBCServiceRuntime: JDBCDataSourceRuntimeMBeans: prefix: wls_datasource_ key: name - JTARuntime: prefix: wls_jta_ key: name   Create a configuration file config-soaserver.yaml in the \u0026lt;work_directory\u0026gt;/wls_exporter directory that contains the following. Modify the restPort to match the server port for the SOA Managed Servers:\nmetricsNameSnakeCase: true restPort: 8001 queries: - key: name keyName: location prefix: wls_server_ applicationRuntimes: key: name keyName: app componentRuntimes: prefix: wls_webapp_config_ type: WebAppComponentRuntime key: name values: [deploymentState, contextRoot, sourceInfo, openSessionsHighCount, openSessionsCurrentCount, sessionsOpenedTotalCount, sessionCookieMaxAgeSecs, sessionInvalidationIntervalSecs, sessionTimeoutSecs, singleThreadedServletPoolSize, sessionIDLength, servletReloadCheckSecs, jSPPageCheckSecs] servlets: prefix: wls_servlet_ key: servletName - JVMRuntime: prefix: wls_jvm_ key: name - executeQueueRuntimes: prefix: wls_socketmuxer_ key: name values: [pendingRequestCurrentCount] - workManagerRuntimes: prefix: wls_workmanager_ key: name values: [stuckThreadCount, pendingRequests, completedRequests] - threadPoolRuntime: prefix: wls_threadpool_ key: name values: [executeThreadTotalCount, queueLength, stuckThreadCount, hoggingThreadCount] - JMSRuntime: key: name keyName: jmsruntime prefix: wls_jmsruntime_ JMSServers: prefix: wls_jms_ key: name keyName: jmsserver destinations: prefix: wls_jms_dest_ key: name keyName: destination - persistentStoreRuntimes: prefix: wls_persistentstore_ key: name - JDBCServiceRuntime: JDBCDataSourceRuntimeMBeans: prefix: wls_datasource_ key: name - JTARuntime: prefix: wls_jta_ key: name   Generate the deployment package for the OIG Administration Server:\n$ chmod 777 get\u0026lt;version\u0026gt;.sh $ ./get\u0026lt;version\u0026gt; config-admin.yaml For example:\n$ chmod 777 get1.2.0.sh $ ./get1.2.0.sh config-admin.yaml The output will look similar to the following:\n% Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 629 100 629 0 0 1241 0 --:--:-- --:--:-- --:--:-- 1240 100 2033k 100 2033k 0 0 1219k 0 0:00:01 0:00:01 --:--:-- 2882k created /tmp/ci-lKm0dOnLwU /tmp/ci-lKm0dOnLwU /scratch/OIGDockerK8S/wls_exporter in temp dir adding: config.yml (deflated 65%) /scratch/OIGDockerK8S/wls_exporter This will generate a wls-exporter.war file in the same directory that contains a config.yml that corresponds to config-admin.yaml. Rename the file as follows:\nmv wls-exporter.war wls-exporter-admin.war   Generate the deployment package for the OIG Managed Server and Policy Manager Server, for example:\n$ ./get1.2.0.sh config-oimserver.yaml $ mv wls-exporter.war wls-exporter-oimserver.war $ ./get1.2.0.sh config-soaserver.yaml $ mv wls-exporter.war wls-exporter-soaserver.war   Copy the war files to the persistent volume directory:\ncp wls-exporter*.war \u0026lt;work_directory\u0026gt;/\u0026lt;persistent_volume\u0026gt;/ For example:\n$ cp wls-exporter*.war /scratch/OIGDockerK8S/oimclusterdomainpv/   Deploy the wls-exporter war files in OIG WebLogic server   Login to the Oracle Enterprise Manager Console using the URL https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/em.\n  Navigate to WebLogic Domain \u0026gt; Deployments. Click on the padlock in the upper right hand corner and select Lock and Edit.\n  From the \u0026lsquo;Deployment\u0026rsquo; drop down menu select Deploy.\n  In the Select Archive screen, under Archive or exploded directory is on the server where Enterprise Manager is running, click Browse. Navigate to the /u01/oracle/user_projects/domains directory and select wls-exporter-admin.war. Click OK and then Next.\n  In Select Target check AdminServer and click Next.\n  In Application Attributes set the following and click Next:\n Application Name: wls-exporter-admin Context Root: wls-exporter Distribution: Install and start application (servicing all requests)    In Deployment Settings click Deploy.\n  Once you see the message Deployment Succeeded, click Close.\n  Click on the padlock in the upper right hand corner and select Activate Changes.\n  Repeat the above steps to deploy wls-exporter-oimserver.war with the following caveats:\n In Select Target choose oim_cluster In Application Attributes set Application Name: wls-exporter-oimserver, Context Root: wls-exporter In Distribution select Install and start application (servicing all requests)    Repeat the above steps to deploy wls-exporter-soaserver.war with the following caveats:\n In Select Target choose soa_cluster In Application Attributes set Application Name: wls-exporter-soaserver, Context Root: wls-exporter In Distribution select Install and start application (servicing all requests)    Check the wls-exporter is accessible using the URL: https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/wls-exporter.\nYou should see a page saying This is the WebLogic Monitoring Exporter.\n  Prometheus Operator Configuration Prometheus has to be configured to collect the metrics from the weblogic-monitor-exporter. The Prometheus operator identifies the targets using service discovery. To get the weblogic-monitor-exporter end point discovered as a target, you will need to create a service monitor to point to the service as follows:\n  Create a wls-exporter-service-monitor.yaml in the \u0026lt;work_directory\u0026gt;/wls_exporter directory with the following contents:\napiVersion: v1 kind: Secret metadata: name: basic-auth namespace: monitoring data: password: V2VsY29tZTE= ## \u0026lt;password\u0026gt; base64 user: d2VibG9naWM= ## weblogic base64 type: Opaque --- apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: wls-exporter-oimcluster namespace: monitoring labels: k8s-app: wls-exporter spec: namespaceSelector: matchNames: - oimcluster selector: matchLabels: weblogic.domainName: oimcluster endpoints: - basicAuth: password: name: basic-auth key: password username: name: basic-auth key: user port: default relabelings: - action: labelmap regex: __meta_kubernetes_service_label_(.+) interval: 10s honorLabels: true path: /wls-exporter/metrics Note: In the above example, change the password value to the base64 encoded version of your weblogic password. To find the base64 value run the following:\n$ echo -n \u0026#34;\u0026lt;password\u0026gt;\u0026#34; | base64 If using a different namespace from oimcluster or a different domain_UID from oimcluster, then change accordingly.\n  Add Rolebinding for the WebLogic OIG domain namespace:\n$ cd \u0026lt;work_directory\u0026gt;/kube-prometheus/manifests Edit the prometheus-roleBindingSpecificNamespaces.yaml file and add the following to the file for your OIG domain namespace, for example oimcluster:\n- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: prometheus-k8s namespace: oimcluster roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: prometheus-k8s subjects: - kind: ServiceAccount name: prometheus-k8s namespace: monitoring For example the file should now read:\napiVersion: rbac.authorization.k8s.io/v1 items: - apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: prometheus-k8s namespace: oimcluster roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: prometheus-k8s subjects: - kind: ServiceAccount name: prometheus-k8s namespace: monitoring - apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: prometheus-k8s namespace: default ....   Add the Role for WebLogic OIG domain namespace. Edit the prometheus-roleSpecificNamespaces.yaml and change the namespace to your OIG domain namespace, for example oimcluster:\n- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: prometheus-k8s namespace: oimcluster rules: - apiGroups: - \u0026quot;\u0026quot; resources: - services - endpoints - pods verbs: - get - list - watch ....\t  Apply the yaml files as follows:\n$ kubectl apply -f prometheus-roleBindingSpecificNamespaces.yaml $ kubectl apply -f prometheus-roleSpecificNamespaces.yaml The output should look similar to the following:\nkubectl apply -f prometheus-roleBindingSpecificNamespaces.yaml rolebinding.rbac.authorization.k8s.io/prometheus-k8s created Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply rolebinding.rbac.authorization.k8s.io/prometheus-k8s configured Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply rolebinding.rbac.authorization.k8s.io/prometheus-k8s configured Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply rolebinding.rbac.authorization.k8s.io/prometheus-k8s configured $ kubectl apply -f prometheus-roleSpecificNamespaces.yaml role.rbac.authorization.k8s.io/prometheus-k8s created Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply role.rbac.authorization.k8s.io/prometheus-k8s configured Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply role.rbac.authorization.k8s.io/prometheus-k8s configured Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply role.rbac.authorization.k8s.io/prometheus-k8s configured   Deploy the ServiceMonitor   Run the following command to create the ServiceMonitor:\n$ cd \u0026lt;work_directory\u0026gt;/wls_exporter $ kubectl create -f wls-exporter-service-monitor.yaml The output will look similar to the following:\nservicemonitor.monitoring.coreos.com/wls-exporter-oim-cluster created   Prometheus Service Discovery After ServiceMonitor is deployed, the wls-exporter should be discovered by Prometheus and be able to scrape metrics.\n  Access the following URL to view Prometheus service discovery: http://${MASTERNODE-HOSTNAME}:32101/service-discovery\n  Click on monitoring/wls-exporter-oimcluster/0  and then show more. Verify all the targets are mentioned.\n  Grafana Dashboard   Access the Grafana dashboard with the following URL: http://${MASTERNODE-HOSTNAME}:32100 and login with admin/admin. Change your password when prompted.\n  Import the Grafana dashboard by navigating on the left hand menu to Create \u0026gt; Import. Copy the content from \u0026lt;work_directory\u0026gt;/fmw-kubernetes/OracleIdentityGovernance/kubernetes/3.0.1/grafana/weblogic_dashboard.json and paste. Then click Load and Import.\n  "
},
{
	"uri": "/fmw-kubernetes/oam/validate-domain-urls/",
	"title": "Validate Domain URLs",
	"tags": [],
	"description": "Sample for validating domain urls.",
	"content": "In this section you validate the OAM domain URLs are accessible via the NGINX or Voyager ingress.\nMake sure you know the master hostname and ingress port for NGINX or Voyager before proceeding.\nValidate the OAM domain urls via the Ingress Launch a browser and access the following URL\u0026rsquo;s. Login with the weblogic username and password (weblogic/\u0026lt;password\u0026gt;).\n   Console or Page URL     WebLogic Administration Console https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/console   Oracle Enterprise Manager Console https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/em   Oracle Access Management Console https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/oamconsole   Oracle Access Management Console https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/access   Logout URL https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/oam/server/logout    Note: WebLogic Administration Console and Oracle Enterprise Manager Console should only be used to monitor the servers in the OAM domain. To control the Administration Server and OAM Managed Servers (start/stop) you must use Kubernetes. See Domain Life Cycle  for more information.\nThe browser will give certificate errors if you used a self signed certifcate and have not imported it into the browsers Certificate Authority store. If this occurs you can proceed with the connection and ignore the errors.\n"
},
{
	"uri": "/fmw-kubernetes/oig/validate-domain-urls/",
	"title": "Validate Domain URLs",
	"tags": [],
	"description": "Sample for validating domain urls.",
	"content": "In this section you validate the OIG domain URLs that are accessible via the NGINX or Voyager ingress.\nMake sure you know the master hostname and port before proceeding.\nValidate the OIG domain urls via the Ingress Launch a browser and access the following URL\u0026rsquo;s. Use http or https depending on whether you configured your ingress for non-ssl or ssl.\nLogin to the WebLogic Administration Console and Oracle Enterprise Manager Console with the WebLogic username and password (weblogic/\u0026lt;password\u0026gt;).\nLogin to Oracle Identity Governance with the xelsysadm username and password (xelsysadm/\u0026lt;password\u0026gt;).\n   Console or Page URL     WebLogic Administration Console https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/console   Oracle Enterprise Manager Console https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/em   Oracle Identity System Administration https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/sysadmin   Oracle Identity Self Service https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/identity    Note: WebLogic Administration Console and Oracle Enterprise Manager Console should only be used to monitor the servers in the OIG domain. To control the Administration Server and OIG Managed Servers (start/stop) you must use Kubernetes. See Domain Life Cycle  for more information.\nThe browser will give certificate errors if you used a self signed certifcate and have not imported it into the browsers Certificate Authority store. If this occurs you can proceed with the connection and ignore the errors.\n"
},
{
	"uri": "/fmw-kubernetes/oud/patch-and-upgrade/",
	"title": "Upgrade",
	"tags": [],
	"description": "This document provides steps to upgrade a Kubernetes Cluster.",
	"content": "Upgrade the underlying Kubernetes cluster to a new release.\n a) Upgrade a Kubernetes cluster  Instructions on how to upgrade a Kubernetes cluster.\n "
},
{
	"uri": "/fmw-kubernetes/oudsm/patch-and-upgrade/",
	"title": "Upgrade",
	"tags": [],
	"description": "This document provides steps to upgrade a Kubernetes Cluster.",
	"content": "Upgrade the underlying Kubernetes cluster to a new release.\n b) Upgrade a Kubernetes cluster  Instructions on how to upgrade a Kubernetes cluster.\n "
},
{
	"uri": "/fmw-kubernetes/wcsites-domains/",
	"title": "Oracle WebCenter Sites",
	"tags": [],
	"description": "The WebLogic Kubernetes operator supports deployment of Oracle WebCenter Sites. Follow the instructions in this guide to set up Oracle WebCenter Sites domains on Kubernetes.",
	"content": "The WebLogic Kubernetes operator supports deployment of Oracle WebCenter Sites.\nOracle WebCenter Sites is currently supported only for non-production use in Docker and Kubernetes. The information provided in this document is a preview for early adopters who wish to experiment with Oracle WebCenter Sites in Kubernetes before it is supported for production use.\n In this release, Oracle WebCenter Sites domains are supported using the domain on a persistent volume model only, where the domain home is located in a persistent volume (PV).\nThe operator has several key features to assist you with deploying and managing Oracle WebCenter Sites domains in a Kubernetes environment. You can:\n Create Oracle WebCenter Sites instances in a Kubernetes persistent volume. This persistent volume can reside in an NFS file system or other Kubernetes volume types. Start servers based on declarative startup parameters and desired states. Expose the WebCenter Sites Services and Composites for external access. Scale WebCenter Sites domains by starting and stopping Managed Servers on demand, or by integrating with a REST API to initiate scaling based on WLDF, Prometheus, Grafana, or other rules. Publish operator and WebLogic Server logs into Elasticsearch and interact with them in Kibana. Monitor the WebCenter Sites instance using Prometheus and Grafana.  Limitations Refer here for limitations in this release.\n"
},
{
	"uri": "/fmw-kubernetes/soa-domains/adminguide/jms-asm-migration/",
	"title": "Configure Automatic Service Migration",
	"tags": [],
	"description": "Perform Java Messaging Service (JMS) automatic service migration (ASM) in an Oracle SOA Suite domain setup.",
	"content": "Singleton services offer one of the highest qualities of service (QOS) but are very susceptible to single point of failure. To address this issue, WebLogic Server offers a solution called migration. The process of moving the entire server instance from one physical machine to another upon failure is called Whole Server Migration (WSM). On the other hand, moving only the affected subsystem services from one server instance to another running server instance is called Service Migration. For more information on migration, see here.\nIn an Oracle SOA Suite domain deployed in Kubernetes cluster, the Whole Server migration is not supported, Service level migration is supported with DB leasing, however Consensus leasing is not supported.\nThis section describes the steps to perform Java Messaging Service (JMS) automatic service migration (ASM) in an Oracle SOA Suite domain setup. See here for more information on this process.\nConfigure Automatic Service Migration for Oracle SOA Suite Servers   Log in to the Administration Console URL of the domain.\nFor example: http://\u0026lt;LOADBALANCER-HOST\u0026gt;:\u0026lt;port\u0026gt;/console\n  In the home page, click Clusters. Then click the cluster that you want to use for migration. For example, soa_cluster.\n  Click the Migration tab and then click Lock \u0026amp; Edit in the Change Center panel. Verify that:\n Migration Basis is set to Database. Auto Migration Table Name is set to ACTIVE. Data Source For Automatic Migration drop down shows the appropriate data source (for example, WLSSchemaDataSource) for migration.    (Optional) Click New to create a new data source.\n  Click Save.\n  Browse to Environment -\u0026gt; Clusters -\u0026gt; Migratable Targets.\n  For each migratable Managed Server (for example, soa_server1), go to the Migration tab and set Service Migration Policy to Auto-Migrate Exactly-Once-Services.\n  Click Save.\n  Click Activate Changes in the Change Center panel.\n  Restart the entire Oracle SOA Suite domain using these steps.\n   Note: Do not restart servers from the Administration Console.\n "
},
{
	"uri": "/fmw-kubernetes/soa-domains/cleanup-domain-setup/",
	"title": "Uninstall",
	"tags": [],
	"description": "Clean up the Oracle SOA Suite domain setup.",
	"content": "Learn how to clean up the Oracle SOA Suite domain setup.\nRemove the domain   Remove the domain\u0026rsquo;s ingress (for example, Traefik ingress) using Helm:\n$ helm uninstall soa-domain-ingress -n sample-domain1-ns For example:\n$ helm uninstall soainfra-traefik -n soans   Remove the domain resources by using the sample delete-weblogic-domain-resources.sh script present at ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/delete-domain:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/delete-domain $ ./delete-weblogic-domain-resources.sh -d sample-domain1 For example:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/delete-domain $ ./delete-weblogic-domain-resources.sh -d soainfra   Use kubectl to confirm that the server pods and domain resource are deleted:\n$ kubectl get pods -n sample-domain1-ns $ kubectl get domains -n sample-domain1-ns For example:\n$ kubectl get pods -n soans $ kubectl get domains -n soans   Drop the RCU schemas Follow these steps to drop the RCU schemas created for Oracle SOA Suite domains.\nRemove the domain namespace   Configure the installed ingress load balancer (for example, Traefik) to stop managing the ingresses in the domain namespace:\n$ helm upgrade traefik-operator stable/traefik \\  --namespace traefik \\  --reuse-values \\  --set \u0026#34;kubernetes.namespaces={traefik}\u0026#34; \\  --wait   Configure the operator to stop managing the domain:\n$ helm upgrade sample-weblogic-operator \\  kubernetes/charts/weblogic-operator \\  --namespace sample-weblogic-operator-ns \\  --reuse-values \\  --set \u0026#34;domainNamespaces={}\u0026#34; \\  --wait For example:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator $ helm upgrade weblogic-kubernetes-operator \\  kubernetes/charts/weblogic-operator \\  --namespace opns \\  --reuse-values \\  --set \u0026#34;domainNamespaces={}\u0026#34; \\  --wait   Delete the domain namespace:\n$ kubectl delete namespace sample-domain1-ns For example:\n$ kubectl delete namespace soans   Remove the operator   Remove the operator:\n$ helm uninstall sample-weblogic-operator -n sample-weblogic-operator-ns For example:\n$ helm uninstall weblogic-kubernetes-operator -n opns   Remove the operator\u0026rsquo;s namespace:\n$ kubectl delete namespace sample-weblogic-operator-ns For example:\n$ kubectl delete namespace opns   Remove the load balancer   Remove the installed ingress based load balancer (for example, Traefik):\n$ helm uninstall traefik -n traefik   Remove the Traefik namespace:\n$ kubectl delete namespace traefik   Delete the domain home To remove the domain home that is generated using the create-domain.sh script, with appropriate privileges manually delete the contents of the storage attached to the domain home persistent volume (PV).\nFor example, for the domain\u0026rsquo;s persistent volume of type host_path:\n$ rm -rf /scratch/k8s_dir/SOA "
},
{
	"uri": "/fmw-kubernetes/oig/manage-oig-domains/delete-domain-home/",
	"title": "Delete the OIG domain home",
	"tags": [],
	"description": "Learn about the steps to cleanup the OIG domain home.",
	"content": "Sometimes in production, but most likely in testing environments, you might want to remove the domain home that is generated using the create-domain.sh script.\n  Run the following command to delete the jobs, domain, and configmaps:\n$ kubectl delete jobs \u0026lt;domain_job\u0026gt; -n \u0026lt;domain_namespace\u0026gt; $ kubectl delete domain \u0026lt;domain_uid\u0026gt; -n \u0026lt;domain_namespace\u0026gt; $ kubectl delete configmaps \u0026lt;domain_job\u0026gt;-cm -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl delete jobs oimcluster-create-fmw-infra-sample-domain-job -n oimcluster $ kubectl delete domain oimcluster -n oimcluster $ kubectl delete configmaps oimcluster-create-fmw-infra-sample-domain-job-cm -n oimcluster   Drop the RCU schemas as follows:\n$ kubectl exec -it helper -n \u0026lt;domain_namespace\u0026gt; -- /bin/bash [oracle@helper ~]$ [oracle@helper ~]$ export CONNECTION_STRING=\u0026lt;db_host.domain\u0026gt;:\u0026lt;db_port\u0026gt;/\u0026lt;service_name\u0026gt; [oracle@helper ~]$ export RCUPREFIX=\u0026lt;rcu_schema_prefix\u0026gt; /u01/oracle/oracle_common/bin/rcu -silent -dropRepository -databaseType ORACLE -connectString $CONNECTION_STRING \\ -dbUser sys -dbRole sysdba -selectDependentsForComponents true -schemaPrefix $RCUPREFIX \\ -component MDS -component IAU -component IAU_APPEND -component IAU_VIEWER -component OPSS \\ -component WLS -component STB -component OIM -component SOAINFRA -component UCSUMS -f \u0026lt; /tmp/pwd.txt For example:\n$ kubectl exec -it helper -n oimcluster -- /bin/bash [oracle@helper ~]$ export CONNECTION_STRING=mydatabasehost.example.com:1521/orcl.example.com [oracle@helper ~]$ export RCUPREFIX=OIGK8S /u01/oracle/oracle_common/bin/rcu -silent -dropRepository -databaseType ORACLE -connectString $CONNECTION_STRING \\ -dbUser sys -dbRole sysdba -selectDependentsForComponents true -schemaPrefix $RCUPREFIX \\ -component MDS -component IAU -component IAU_APPEND -component IAU_VIEWER -component OPSS \\ -component WLS -component STB -component OIM -component SOAINFRA -component UCSUMS -f \u0026lt; /tmp/pwd.txt   Delete the Persistent Volume and Persistent Volume Claim:\n$ kubectl delete pv \u0026lt;pv-name\u0026gt; -n \u0026lt;domain_namespace\u0026gt; $ kubectl delete pvc \u0026lt;pvc-name\u0026gt; -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl delete pv oimcluster-oim-pv -n oimcluster $ kubectl delete pvc oimcluster-oim-pvc -n oimcluster   Delete the contents of the persistent volume, for example:\n$ rm -rf /\u0026lt;work directory\u0026gt;/oimclusterdomainpv/* For example:\n$ rm -rf /scratch/OIGDockerK8S/oimclusterdomainpv/*   Delete the Oracle WebLogic Kubernetes Operator, by running the following command:\n$ helm delete weblogic-kubernetes-operator -n operator   To delete NGINX:\n$ helm delete oimcluster-nginx -n oimcluster $ helm delete nginx-ingress -n nginx $ kubectl delete namespace nginx or if using SSL:\n$ helm delete oimcluster-nginx -n oimcluster $ helm delete nginx-ingress -n nginxssl $ kubectl delete namespace nginxssl   To delete Voyager:\n$ helm delete oimcluster-voyager -n oimcluster $ helm delete voyager-ingress -n voyager $ kubectl delete namespace voyager or if using SSL:\n$ helm delete oimcluster-voyager -n oimcluster $ helm delete voyager-ingress -n voyagerssl $ kubectl delete namespace voyagerssl   "
},
{
	"uri": "/fmw-kubernetes/oam/post-install-config/",
	"title": "Post Install Configuration",
	"tags": [],
	"description": "Post install configuration.",
	"content": "Follow these mandatory post install configuration steps.\n WebLogic Server Tuning Modify oamconfig.properties  WebLogic Server Tuning For production environments, the following WebLogic Server tuning parameters must be set:\nAdd Minimum Thread constraint to worker manager \u0026ldquo;OAPOverRestWM\u0026rdquo;  Login to the WebLogic Server Console at https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/console. Click Lock \u0026amp; Edit. In Domain Structure, click Deployments. On the Deployments page click Next until you see oam_server. Expand oam_server by clicking on the + icon, then click /iam/access/binding. Click the Configuration tab, followed by the Workload tab. Click wm/OAPOverRestWM Under Application Scoped Work Managed Components, click New. In Create a New Work Manager Component, select Minumum Threads Constraint and click Next. In Minimum Threads Constraint Properties enter the Count as 400 and click Finish. In the Save Deployment Plan change the Path to the value /u01/oracle/user_projects/domains/accessinfra/Plan.xml, where accessinfra is your domain_UID. Click OK and then Activate Changes.  Remove Max Thread Constraint and Capacity Constraint  Repeat steps 1-7 above. Under Application Scoped Work Managed Components select the check box for Capacity and MaxThreadsCount. Click Delete. In the Delete Work Manage Components screen, click OK to delete. Click on Release Configuration and then Log Out.  oamDS DataSource Tuning  Login to the WebLogic Server Console at https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/console. Click Lock \u0026amp; Edit. In Domain Structure, Expand Data Sources and click Data Sources. Click on oamDS. In Settings for oamDS, select the Configuration tab, and then the Connection Pool tab. Change Initial Capacity, Maximum Capacity, and Minimum Capacity to 800 and click Save. Click Activate Changes.  Modify oamconfig.properties   Navigate to the following directory and change permissions for the oamconfig_modify.sh:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-access-domain/domain-home-on-pv/common $ chmod 777 oamconfig_modify.sh For example:\n$ cd /scratch/OAMDockerK8S/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-access-domain/domain-home-on-pv/common $ chmod 777 oamconfig_modify.sh   Edit the oamconfig.properties and change the OAM_NAMESPACE, INGRESS, INGRESS_NAME, and LBR_HOST to match the values for your OAM Kubernetes environment. For example:\n#Below are only the sample values, please modify them as per your setup # The name space where OAM servers are created OAM_NAMESPACE='accessns' # Define the INGRESS CONTROLLER used. typical values are voyager/nginx INGRESS=\u0026quot;nginx\u0026quot; # Define the INGRESS CONTROLLER name used during installation. INGRESS_NAME=\u0026quot;nginx-ingress\u0026quot; # FQDN of the LBR Host i.e the host from where you access oam console LBR_HOST=\u0026quot;masternode.example.com\u0026quot;   Run the oamconfig_modify.sh script as follows:\n$ ./oamconfig_modify.sh \u0026lt;OAM_ADMIN_USER\u0026gt;:\u0026lt;OAM_ADMIN_PASSWORD\u0026gt; where:\nOAM_ADMIN_USER is the OAM administrator username\nOAM_ADMIN_PASSWORD is the OAM administrator password\nFor example:\n$ ./oamconfig_modify.sh weblogic:\u0026lt;password\u0026gt; Note: Make sure port 30540 is free before running the command.\nThe output will look similar to the following:\nLBR_PROTOCOL: https domainUID: accessinfra OAM_SERVER: accessinfra-oam-server OAM_NAMESPACE: accessns INGRESS: nginx INGRESS_NAME: nginx-ingress ING_TYPE : NodePort LBR_HOST: masternode.example.com LBR_PORT: 32190 Started Executing Command % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 762k 0 762k 0 0 3276k 0 --:--:-- --:--:-- --:--:-- 3273k new_cluster_id: abcd-accessinfra oamoap-service NodePort 10.96.63.13 \u0026lt;none\u0026gt; 5575:30540/TCP 5h9m HTTP/1.1 100 Continue HTTP/1.1 201 Created Date: Thu, 15 Oct 2020 11:22:46 GMT Content-Type: text/plain Content-Length: 76 Connection: keep-alive X-ORACLE-DMS-ECID: 9aadbcc3-e0a5-46d7-882a-484b17587cf2-00005839 X-ORACLE-DMS-RID: 0 Set-Cookie: JSESSIONID=XmIr_3-T4iesEkMJxp5NCqZxjr5M-0icByML5hkwMQn9-KCg_zno!-992913931; path=/; HttpOnly Set-Cookie: _WL_AUTHCOOKIE_JSESSIONID=t2cmxfVqXI.sZLm.8tPo; path=/; secure; HttpOnly Strict-Transport-Security: max-age=15724800; includeSubDomains https://masternode.example.com:32190/iam/admin/config/api/v1/config?path=%2F /home/rest/output/oamconfig_modify.xml executed successfully --------------------------------------------------------------------------- Initializing WebLogic Scripting Tool (WLST) ... Welcome to WebLogic Server Administration Scripting Shell Type help() for help on available commands Connecting to t3://accessinfra-adminserver:7001 with userid weblogic ... Successfully connected to Admin Server \u0026quot;AdminServer\u0026quot; that belongs to domain \u0026quot;accessinfra\u0026quot;. Warning: An insecure protocol was used to connect to the server. To ensure on-the-wire security, the SSL port or Admin port should be used instead. Location changed to domainRuntime tree. This is a read-only tree with DomainMBean as the root MBean. For more help, use help('domainRuntime') Exiting WebLogic Scripting Tool. Please wait for some time for the server to restart pod \u0026quot;accessinfra-oam-server1\u0026quot; deleted pod \u0026quot;accessinfra-oam-server2\u0026quot; deleted The script will delete the accessinfra-oam-server1 and accessinfra-oam-server2 pods and then create new ones. Check the pods are running again by issuing the following command:\n$ kubectl gets pods -n accessns The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE pod/accessinfra-adminserver 1/1 Running 0 1h17m pod/accessinfra-create-oam-infra-domain-job-vj69h 0/1 Completed 0 1h42m pod/accessinfra-oam-policy-mgr1 1/1 Running 0 1h9m pod/accessinfra-oam-server1 0/1 Running 0 31s pod/accessinfra-oam-server2 0/1 Running 0 31s The accessinfra-oam-server1 and accessinfra-oam-server2 are started, but currently have a READY status of 0/1. This means oam_server1 and oam_server2 are not currently running but are in the process of starting. The servers will take several minutes to start so keep executing the command until READY shows 1/1:\nNAME READY STATUS RESTARTS AGE pod/accessinfra-adminserver 1/1 Running 0 1h23m pod/accessinfra-create-oam-infra-domain-job-vj69h 0/1 Completed 0 1h48m pod/accessinfra-oam-policy-mgr1 1/1 Running 0 1h15m pod/accessinfra-oam-server1 1/1 Running 0 6m pod/accessinfra-oam-server2 1/1 Running 0 6m   "
},
{
	"uri": "/fmw-kubernetes/oig/post-install-config/",
	"title": "Post Install Configuration",
	"tags": [],
	"description": "Post install configuration.",
	"content": "Follow these post install configuration steps.\n a. Set OIMfrontendURL  Set the OIMfrontendURL in Oracle Enterprise Manager.\n b. Install and Configure Connectors  Install and Configure Connectors.\n "
},
{
	"uri": "/fmw-kubernetes/oud/security-hardening/",
	"title": "Security Hardening",
	"tags": [],
	"description": "Review resources for the Docker and Kubernetes cluster hardening.",
	"content": "Securing a Kubernetes cluster involves hardening on multiple fronts - securing the API servers, etcd, nodes, container images, container run-time, and the cluster network. Apply principles of defense in depth, principle of least privilege, and minimize the attack surface. Use security tools such as Kube-Bench to verify the cluster’s security posture. Since Kubernetes is evolving rapidly, refer to Kubernetes Security Overview for the latest information on securing a Kubernetes cluster. Also ensure the deployed Docker containers follow the Docker Security guidance.\nThis section provides references on how to securely configure Docker and Kubernetes.\nReferences   Docker hardening:\n https://docs.docker.com/engine/security/security/ https://blog.aquasec.com/docker-security-best-practices    Kubernetes hardening:\n https://kubernetes.io/docs/concepts/security/overview/ https://kubernetes.io/docs/concepts/security/pod-security-standards/ https://blogs.oracle.com/developers/5-best-practices-for-kubernetes-security    "
},
{
	"uri": "/fmw-kubernetes/oudsm/security-hardening/",
	"title": "Security Hardening",
	"tags": [],
	"description": "Review resources for the Docker and Kubernetes cluster hardening.",
	"content": "Securing a Kubernetes cluster involves hardening on multiple fronts - securing the API servers, etcd, nodes, container images, container run-time, and the cluster network. Apply principles of defense in depth, principle of least privilege, and minimize the attack surface. Use security tools such as Kube-Bench to verify the cluster’s security posture. Since Kubernetes is evolving rapidly, refer to Kubernetes Security Overview for the latest information on securing a Kubernetes cluster. Also ensure the deployed Docker containers follow the Docker Security guidance.\nThis section provides references on how to securely configure Docker and Kubernetes.\nReferences   Docker hardening:\n https://docs.docker.com/engine/security/security/ https://blog.aquasec.com/docker-security-best-practices    Kubernetes hardening:\n https://kubernetes.io/docs/concepts/security/overview/ https://kubernetes.io/docs/concepts/security/pod-security-standards/ https://blogs.oracle.com/developers/5-best-practices-for-kubernetes-security    "
},
{
	"uri": "/fmw-kubernetes/soa-domains/faq/",
	"title": "Frequently Asked Questions",
	"tags": [],
	"description": "This section describes known issues for Oracle SOA Suite domains deployment on Kubernetes. Also, provides answers to frequently asked questions.",
	"content": "Overriding tuning parameters is not supported using configuration overrides The Oracle WebLogic Server Kubernetes operator enables you to override some of the domain configuration using configuration overrides (also called situational configuration). See supported overrides. Overriding the tuning parameters such as MaxMessageSize and PAYLOAD, for Oracle SOA Suite domains is not supported using the configuration overrides feature. However, you can override them using the following steps:\n  Specify the new value using the environment variable K8S_REFCONF_OVERRIDES in serverPod.env section in domain.yaml configuration file (example path: \u0026lt;domain-creation-output-directory\u0026gt;/weblogic-domains/soainfra/domain.yaml) based on the servers to which the changes are to be applied.\nFor example, to override the value at the Administration Server pod level:\nspec: adminServer: serverPod: env: - name: K8S_REFCONF_OVERRIDES value: \u0026#34;-Dweblogic.MaxMessageSize=78787878\u0026#34; - name: USER_MEM_ARGS value: \u0026#39;-Djava.security.egd=file:/dev/./urandom -Xms512m -Xmx1024m \u0026#39; serverStartState: RUNNING For example, to override the value at a specific cluster level (soa_cluster or osb_cluster):\n- clusterName: soa_cluster serverService: precreateService: true serverStartState: \u0026#34;RUNNING\u0026#34; serverPod: env: - name: K8S_REFCONF_OVERRIDES value: \u0026#34;-Dsoa.payload.threshold.kb=102410\u0026#34;  Note: When multiple system properties are specified for serverPod.env.value, make sure each system property is separated by a space.\n   Apply the updated domain.yaml file:\n$ kubectl apply -f domain.yaml  Note: The server pod(s) will be automatically restarted (rolling restart).\n   Deployments in the WebLogic Server Administration Console may display unexpected error In an Oracle SOA Suite environment deployed using the operator, accessing Deployments from the WebLogic Server Administration Console home page may display the error message Unexpected error encountered while obtaining monitoring information for applications. This error does not have any functional impact and can be ignored. You can verify that the applications are in Active state from the Control tab in Summary of deployments page.\nEnterprise Manager Console may display ADF_FACES-30200 error In an Oracle SOA Suite environment deployed using the operator, the Enterprise Manager Console may intermittently display the following error when the domain servers are restarted:\nADF_FACES-30200: For more information, please see the server\u0026#39;s error log for an entry beginning with: The UIViewRoot is null. Fatal exception during PhaseId: RESTORE_VIEW 1. You can refresh the Enterprise Manager Console URL to successfully log in to the Console.\nConfigure the external URL access for Oracle SOA Suite composite applications For Oracle SOA Suite composite applications to access the external URLs over the internet (if your cluster is behind a http proxy server), you must configure the following proxy parameters for Administration Server and Managed Server pods.\n-Dhttp.proxyHost=www-your-proxy.com -Dhttp.proxyPort=proxy-port -Dhttps.proxyHost=www-your-proxy.com -Dhttps.proxyPort=proxy-port -Dhttp.nonProxyHosts=\u0026#34;localhost|soainfra-adminserver|soainfra-soa-server1|soainfra-osb-server1|...soainfra-soa-serverN|*.svc.cluster.local|*.your.domain.com|/var/run/docker.sock\u0026#34; To do this, edit the domain.yaml configuration file and append the proxy parameters to the spec.serverPod.env.JAVA_OPTIONS environment variable value.\nFor example:\nserverPod: env: - name: JAVA_OPTIONS value: -Dweblogic.StdoutDebugEnabled=false -Dweblogic.ssl.Enabled=true -Dweblogic.security.SSL.ignoreHostnameVerification=true -Dhttp.proxyHost=www-your-proxy.com -Dhttp.proxyPort=proxy-port -Dhttps.proxyHost=www-your-proxy.com -Dhttps.proxyPort=proxy-port -Dhttp.nonProxyHosts=\u0026#34;localhost|soainfra-adminserver|soainfra-soa-server1|soainfra-osb-server1|...soainfra-soa-serverN|*.svc.cluster.local|*.your.domain.com|/var/run/docker.sock\u0026#34; - name: USER_MEM_ARGS value: \u0026#39;-Djava.security.egd=file:/dev/./urandom -Xms256m -Xmx1024m \u0026#39; volumeMounts:  Note: The -Dhttp.nonProxyHosts parameter must have the pod names of the Administration Server and each Managed Server. For example: soainfra-adminserver, soainfra-soa-server1, soainfra-osb-server1, and so on.\n Apply the updated domain.yaml file:\n$ kubectl apply -f domain.yaml  Note: The server pod(s) will be automatically restarted (rolling restart).\n Configure the external access for the Oracle Enterprise Scheduler WebServices WSDL URLs In an Oracle SOA Suite domain deployed including the Oracle Enterprise Scheduler (ESS) component, the following ESS WebServices WSDL URLs shown in the table format in the ess/essWebServicesWsdl.jsp page are not reachable outside the Kubernetes cluster.\nESSWebService EssAsyncCallbackService EssWsJobAsyncCallbackService Follow these steps to configure the external access for the Oracle Enterprise Scheduler WebServices WSDL URLs:\n Log in to the Administration Console URL of the domain.\nFor example: http://\u0026lt;LOADBALANCER-HOST\u0026gt;:\u0026lt;port\u0026gt;/console In the Home Page, click Clusters. Then click the soa_cluster. Click the HTTP tab and then click Lock \u0026amp; Edit in the Change Center panel. Update the following values:  Frontend Host: host name of the load balancer. For example, domain1.example.com. Frontend HTTP Port: load balancer port. For example, 30305. Frontend HTTPS Port: load balancer https port. For example, 30443.   Click Save. Click Activate Changes in the Change Center panel. Restart the servers in the SOA cluster.   Note: Do not restart servers from the Administration Console.\n Oracle WebLogic Server Kubernetes Operator FAQs See the general frequently asked questions for using the Oracle WebLogic Server Kubernetes operator.\n"
},
{
	"uri": "/fmw-kubernetes/soa-domains/adminguide/persisting-soa-adapters-customizations/",
	"title": "Persist adapter customizations",
	"tags": [],
	"description": "Persist the customizations done for Oracle SOA Suite adapters.",
	"content": "The lifetime for any customization done in a file on a server pod is up to the lifetime of that pod. The changes are not persisted once the pod goes down or is restarted.\nFor example, the following configuration updates DbAdapter.rar to create a new connection instance and creates data source CoffeeShop on the Administration Console for the same with jdbc/CoffeeShopDS.\nFile location: /u01/oracle/soa/soa/connectors/DbAdapter.rar\n\u0026lt;connection-instance\u0026gt; \u0026lt;jndi-name\u0026gt;eis/DB/CoffeeShop\u0026lt;/jndi-name\u0026gt; \u0026lt;connection-properties\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;XADataSourceName\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;jdbc/CoffeeShopDS\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;DataSourceName\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;PlatformClassName\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;org.eclipse.persistence.platform.database.Oracle10Platform\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;/connection-properties\u0026gt; \u0026lt;/connection-instance\u0026gt; If you need to persist the customizations for any of the adapter files under the SOA Oracle Home in the server pod, use one of the following methods.\nMethod 1: Customize the Adapter file using the WebLogic Administration Console:   Log in to the WebLogic Administration Console, and go to Deployments \u0026gt; ABC.rar \u0026gt; Configuration \u0026gt; Outbound Connection Pools.\n  Click New to create a new connection, then provide a new connection name, and click Finish.\n  Go back to the new connection, update the properties as required, and save.\n  Under Deployments, select ABC.rar, then Update.\nThis step asks for the Plan.xml location. This location by default will be in ${ORACLE_HOME}/soa/soa which is not under Persistent Volume (PV). Therefore, provide the domain\u0026rsquo;s PV location such as {DOMAIN_HOME}/soainfra/servers.\nNow the Plan.xml will be persisted under this location for each Managed Server.\n  Method 2: Customize the Adapter file on the Worker Node:   Copy ABC.rar from the server pod to a PV path:\n$ kubectl cp \u0026lt;namespace\u0026gt;/\u0026lt;SOA Managed Server pod name\u0026gt;:\u0026lt;full path of .rar file\u0026gt; \u0026lt;destination path inside PV\u0026gt; For example:\n$ kubectl cp soans/soainfra-soa-server1:/u01/oracle/soa/soa/connectors/ABC.rar ${DockerVolume}/domains/soainfra/servers/ABC.rar or do a normal file copy between these locations after entering (using kubectl exec) in to the Managed Server pod.\n  Unrar ABC.rar.\n  Update the new connection details in the weblogic-ra.xml file under META_INF.\n  In the WebLogic Administration Console, under Deployments, select ABC.rar, then Update.\n  Select the ABC.rar path as the new location, which is ${DOMAIN_HOME}/user_projects/domains/soainfra/servers/ABC.rar and click Update.\n  Verify that the plan.xml or updated .rar should be persisted in the PV.\n  "
},
{
	"uri": "/fmw-kubernetes/oig/configure-design-console/",
	"title": "Configure Design Console",
	"tags": [],
	"description": "Configure Design Console.",
	"content": "Configure an Ingress to allow Design Console to connect to your Kubernetes cluster.\n a. Using Design Console with NGINX(non-SSL)  Configure Design Console with NGINX(non-SSL).\n a. Using Design Console with NGINX(SSL)  Configure Design Console with NGINX(SSL).\n a. Using Design Console with Voyager(non-SSL)  Configure Design Console with Voyager(non-SSL).\n a. Using Design Console with Voyager(SSL)  Configure Design Console with Voyager(SSL).\n "
},
{
	"uri": "/fmw-kubernetes/oig/manage-oig-domains/",
	"title": "Manage OIG Domains",
	"tags": [],
	"description": "This document provides steps to manage the OIG domain.",
	"content": "Important considerations for Oracle Identity Governance domains in Kubernetes.\n Domain Life Cycle  Learn about the domain life cyle of an OIG domain.\n WLST Administration Operations  Describes the steps for WLST administration using helper pod running in the same Kubernetes Cluster as OIG Domain.\n Runnning OIG Utilities  Describes the steps for running OIG utilities in Kubernetes.\n Logging and Visualization  Describes the steps for logging and visualization with Elasticsearch and Kibana.\n Monitoring an OIG domain  Describes the steps for Monitoring the OIG domain and Publising the logs to Elasticsearch.\n Delete the OIG domain home  Learn about the steps to cleanup the OIG domain home.\n "
},
{
	"uri": "/fmw-kubernetes/oam/validate-sso-using-webgate/",
	"title": "Validate a Basic SSO Flow using WebGate Registration ",
	"tags": [],
	"description": "Sample for validating a basic SSO flow using WebGate registration.",
	"content": "In this section you validate single-sign on works to the OAM Kubernetes cluster via Oracle WebGate. The instructions below assume you have a running Oracle HTTP Server (for example ohs_k8s) and Oracle WebGate installed on an independent server. The instructions also assume basic knowledge of how to register a WebGate agent.\nNote: At present Oracle HTTP Server and Oracle WebGate are not supported on a Kubernetes cluster.\nUpdate the OAM Hostname and Port for the Loadbalancer If using an NGINX or Voyager ingress with no load balancer, change {LOADBALANCER-HOSTNAME}:${LOADBALANCER-PORT} to {MASTERNODE-HOSTNAME}:${MASTERNODE-PORT} when referenced below.\n  Launch a browser and access the OAM console (https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-PORT}/oamconsole). Login with the weblogic username and password (weblogic/\u0026lt;password\u0026gt;)\n  Navigate to Configuration → Settings ( View ) → Access Manager.\n  Under Load Balancing modify the OAM Server Host and OAM Server Port, to point to the Loadbalancer HTTP endpoint (e.g loadbalancer.example.com and \u0026lt;port\u0026gt; respectively). In the OAM Server Protocol drop down list select https.\n  Under WebGate Traffic Load Balancer modify the OAM Server Host and OAM Server Port, to point to the Loadbalancer HTTP endpoint (e.g loadbalancer.example.com and \u0026lt;port\u0026gt; repectively). In the OAM Server Protocol drop down list select https.\n  Click Apply.\n  Register a WebGate Agent In all the examples below, change the directory path as appropriate for your installation.\n  Run the following command on the server with Oracle HTTP Server and WebGate installed:\n$ cd /scratch/export/home/oracle/product/middleware/webgate/ohs/tools/deployWebGate $ ./deployWebGateInstance.sh -w /scratch/export/home/oracle/admin/domains/oam_domain/config/fmwconfig/components/OHS/ohs_k8s -oh /scratch/export/home/oracle/product/middleware -ws ohs The output will look similar to the following:\nCopying files from WebGate Oracle Home to WebGate Instancedir   Run the following command to update the OHS configuration files appropriately:\n$ export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/scratch/export/home/oracle/product/middleware/lib $ cd /scratch/export/home/oracle/product/middleware/webgate/ohs/tools/setup/InstallTools/ $ ./EditHttpConf -w /scratch/export/home/oracle/admin/domains/oam_domain/config/fmwconfig/components/OHS/ohs_k8s -oh /scratch/export/home/oracle/product/middleware The output will look similar to the following:\nThe web server configuration file was successfully updated /scratch/export/home/oracle/admin/domains/oam_domain/config/fmwconfig/components/OHS/ohs_k8s/httpd.conf has been backed up as /scratch/export/home/oracle/admin/domains/oam_domain/config/fmwconfig/components/OHS/ohs_k8s/httpd.conf.ORIG   Launch a browser, and access the OAM console. Navigate to Application Security → Quick Start Wizards → SSO Agent Registration. Register the agent in the usual way, download the configuration zip file and copy to the OHS WebGate server, for example: /scratch/export/home/oracle/admin/domains/oam_domain/config/fmwconfig/components/OHS/ohs_k8/webgate/config. Extract the zip file.\n  Copy the Certificate Authority (CA) certificate (cacert.pem) for the load balancer/ingress certificate to the same directory e.g: /scratch/export/home/oracle/admin/domains/oam_domain/config/fmwconfig/components/OHS/ohs_k8/webgate/config.\nIf you used a self signed certificate for the ingress, instead copy the self signed certificate (e.g: /scratch/ssl/tls.crt) to the above directory. Rename the certificate to cacert.pem.\n  Restart Oracle HTTP Server.\n  Access the configured OHS e.g http://ohs.example.com:7778, and check you are redirected to the SSO login page. Login and make sure you are redirected successfully to the home page.\n  Changing WebGate agent to use OAP Note: This section should only be followed if you need to change the OAM/WebGate Agent communication from HTTPS to OAP.\nTo change the WebGate agent to use OAP:\n  In the OAM Console click Application Security and then Agents.\n  Search for the agent you want modify and select it.\n  In the User Defined Parameters change:\na) OAMServerCommunicationMode from HTTPS to OAP. For example OAMServerCommunicationMode=OAP\nb) OAMRestEndPointHostName=\u0026lt;hostname\u0026gt; to the {$MASTERNODE-HOSTNAME}. For example OAMRestEndPointHostName=masternode.example.com\n  In the Server Lists section click Add to a add new server with the following values:\n Access Server: oam_server Host Name: \u0026lt;{$MASTERNODE-HOSTNAME}\u0026gt; Host Port: \u0026lt;oamoap-service NodePort\u0026gt;  Note: To find the value for Host Port run the following:\n$ kubectl describe svc oamoap-service -n accessns The output will look similar to the following:\nName: oamoap-service Namespace: accessns Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Selector: weblogic.clusterName=oam_cluster Type: NodePort IP: 10.96.63.13 Port: \u0026lt;unset\u0026gt; 5575/TCP TargetPort: 5575/TCP NodePort: \u0026lt;unset\u0026gt; 30540/TCP Endpoints: 10.244.0.30:5575,10.244.0.31:5575 Session Affinity: None External Traffic Policy: Cluster Events: \u0026lt;none\u0026gt;   Delete all servers in Server Lists except for the one just created, and click Apply.\n  Click Download to download the webgate zip file. Copy the zip file to the desired WebGate.\n  "
},
{
	"uri": "/fmw-kubernetes/soa-domains/appendix/",
	"title": "Appendix",
	"tags": [],
	"description": "",
	"content": "This section provides information on miscellaneous tasks related to Oracle SOA Suite domains deployment on Kubernetes.\n Domain resource sizing  Describes the resourse sizing information for Oracle SOA Suite domains setup on Kubernetes cluster.\n Quick start deployment on-premise  Describes how to quickly get an Oracle SOA Suite domain instance running (using the defaults, nothing special) for development and test purposes.\n Security hardening  Review resources for the Docker and Kubernetes cluster hardening.\n "
},
{
	"uri": "/fmw-kubernetes/oud/troubleshooting/",
	"title": "Troubleshooting",
	"tags": [],
	"description": "How to Troubleshoot issues.",
	"content": " Check the Status of a Namespace View POD Logs View Pod Description  Check the Status of a Namespace To check the status of objects in a namespace use the following command:\n$ kubectl --namespace \u0026lt;namespace\u0026gt; get nodes,pod,service,secret,pv,pvc,ingress -o wide Output will be similar to the following:\n$ kubectl --namespace myhelmns get nodes,pod,service,secret,pv,pvc,ingress -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME node/10.89.73.203 Ready \u0026lt;none\u0026gt; 75d v1.18.4 10.89.73.203 \u0026lt;none\u0026gt; Oracle Linux Server 7.5 4.1.12-124.35.2.el7uek.x86_64 docker://19.3.11 node/10.89.73.204 Ready \u0026lt;none\u0026gt; 75d v1.18.4 10.89.73.204 \u0026lt;none\u0026gt; Oracle Linux Server 7.5 4.1.12-124.38.1.el7uek.x86_64 docker://19.3.11 node/10.89.73.42 Ready master 76d v1.18.4 10.89.73.42 \u0026lt;none\u0026gt; Oracle Linux Server 7.5 4.1.12-124.35.2.el7uek.x86_64 docker://19.3.11 NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/my-oud-ds-rs-0 1/1 Running 0 83m 10.244.1.90 10.89.73.203 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/my-oud-ds-rs-1 1/1 Running 0 83m 10.244.1.91 10.89.73.203 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/my-oud-ds-rs-2 1/1 Running 0 83m 10.244.1.89 10.89.73.203 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service/my-oud-ds-rs-0 ClusterIP 10.100.226.50 \u0026lt;none\u0026gt; 1444/TCP,1888/TCP,1898/TCP 83m app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=my-oud-ds-rs-0 service/my-oud-ds-rs-1 ClusterIP 10.96.231.214 \u0026lt;none\u0026gt; 1444/TCP,1888/TCP,1898/TCP 83m app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=my-oud-ds-rs-1 service/my-oud-ds-rs-2 ClusterIP 10.99.254.14 \u0026lt;none\u0026gt; 1444/TCP,1888/TCP,1898/TCP 83m app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=my-oud-ds-rs-2 service/my-oud-ds-rs-http-0 ClusterIP 10.109.186.111 \u0026lt;none\u0026gt; 1080/TCP,1081/TCP 83m app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=my-oud-ds-rs-0 service/my-oud-ds-rs-http-1 ClusterIP 10.101.227.72 \u0026lt;none\u0026gt; 1080/TCP,1081/TCP 83m app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=my-oud-ds-rs-1 service/my-oud-ds-rs-http-2 ClusterIP 10.103.18.99 \u0026lt;none\u0026gt; 1080/TCP,1081/TCP 83m app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=my-oud-ds-rs-2 service/my-oud-ds-rs-lbr-admin ClusterIP 10.105.211.54 \u0026lt;none\u0026gt; 1888/TCP,1444/TCP 83m app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs service/my-oud-ds-rs-lbr-http ClusterIP 10.99.23.245 \u0026lt;none\u0026gt; 1080/TCP,1081/TCP 83m app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs service/my-oud-ds-rs-lbr-ldap ClusterIP 10.103.171.90 \u0026lt;none\u0026gt; 1389/TCP,1636/TCP 83m app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs service/my-oud-ds-rs-ldap-0 ClusterIP 10.107.250.130 \u0026lt;none\u0026gt; 1389/TCP,1636/TCP 83m app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=my-oud-ds-rs-0 service/my-oud-ds-rs-ldap-1 ClusterIP 10.100.73.198 \u0026lt;none\u0026gt; 1389/TCP,1636/TCP 83m app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=my-oud-ds-rs-1 service/my-oud-ds-rs-ldap-2 ClusterIP 10.98.176.118 \u0026lt;none\u0026gt; 1389/TCP,1636/TCP 83m app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=my-oud-ds-rs-2 NAME TYPE DATA AGE secret/default-token-kfmhq kubernetes.io/service-account-token 3 84m secret/my-oud-ds-rs-creds opaque 8 83m secret/my-oud-ds-rs-tls-cert kubernetes.io/tls 2 83m secret/my-oud-ds-rs-token-c4tg4 kubernetes.io/service-account-token 3 83m secret/sh.helm.release.v1.my-oud-ds-rs.v1 helm.sh/release.v1 1 83m NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE VOLUMEMODE persistentvolume/my-oud-ds-rs-espv1 20Gi RWX Retain Available elk 83m Filesystem persistentvolume/my-oud-ds-rs-pv 30Gi RWX Retain Bound myhelmns/my-oud-ds-rs-pvc manual 83m Filesystem persistentvolume/oimcluster-oim-pv 10Gi RWX Retain Bound oimcluster/oimcluster-oim-pvc oimcluster-oim-storage-class 63d Filesystem NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE VOLUMEMODE persistentvolumeclaim/my-oud-ds-rs-pvc Bound my-oud-ds-rs-pv 30Gi RWX manual 83m Filesystem NAME CLASS HOSTS ADDRESS PORTS AGE ingress.extensions/my-oud-ds-rs-admin-ingress-nginx \u0026lt;none\u0026gt; my-oud-ds-rs-admin-0,my-oud-ds-rs-admin-1,my-oud-ds-rs-admin-2 + 2 more... 80, 443 83m ingress.extensions/my-oud-ds-rs-http-ingress-nginx \u0026lt;none\u0026gt; my-oud-ds-rs-http-0,my-oud-ds-rs-http-1,my-oud-ds-rs-http-2 + 3 more... 80, 443 83m Include/exclude elements (nodes,pod,service,secret,pv,pvc,ingress) as required.\nView POD Logs To view logs for a POD use the following command:\n$ kubectl logs \u0026lt;pod\u0026gt; -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl logs my-oud-ds-rs-0 -n myhelmns Output will depend on the application running in the POD.\nView Pod Description Details about a POD can be viewed using the kubectl describe command:\n$ kubectl describe pod \u0026lt;pod\u0026gt; -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl describe pod my-oud-ds-rs-0 -n myhelmns Name: my-oud-ds-rs-0 Namespace: myhelmns Priority: 0 Node: 10.89.73.203/10.89.73.203 Start Time: Wed, 07 Oct 2020 07:30:27 -0700 Labels: app.kubernetes.io/instance=my-oud-ds-rs app.kubernetes.io/managed-by=Helm app.kubernetes.io/name=oud-ds-rs app.kubernetes.io/version=12.2.1.4.0 helm.sh/chart=oud-ds-rs-0.1 oud/instance=my-oud-ds-rs-0 Annotations: meta.helm.sh/release-name: my-oud-ds-rs meta.helm.sh/release-namespace: myhelmns Status: Running IP: 10.244.1.90 IPs: IP: 10.244.1.90 Containers: oud-ds-rs: Container ID: docker://e3b79a283f56870e6d702cf8c2cc7aafa09a242f7a2cd543d8014a24aa219903 Image: oracle/oud:12.2.1.4.0 Image ID: docker://sha256:8a937042bef357fdeb09ce20d34332b14d1f1afe3ccb9f9b297f6940fdf32a76 Ports: 1444/TCP, 1888/TCP, 1389/TCP, 1636/TCP, 1080/TCP, 1081/TCP, 1898/TCP Host Ports: 0/TCP, 0/TCP, 0/TCP, 0/TCP, 0/TCP, 0/TCP, 0/TCP State: Running Started: Wed, 07 Oct 2020 07:30:28 -0700 Ready: True Restart Count: 0 Liveness: tcp-socket :ldap delay=900s timeout=15s period=30s #success=1 #failure=1 Readiness: exec [/u01/oracle/container-scripts/checkOUDInstance.sh] delay=180s timeout=30s period=60s #success=1 #failure=10 Environment: instanceType: Directory sleepBeforeConfig: 3 OUD_INSTANCE_NAME: my-oud-ds-rs-0 hostname: my-oud-ds-rs-0 baseDN: dc=example,dc=com rootUserDN: \u0026lt;set to the key 'rootUserDN' in secret 'my-oud-ds-rs-creds'\u0026gt; Optional: false rootUserPassword: \u0026lt;set to the key 'rootUserPassword' in secret 'my-oud-ds-rs-creds'\u0026gt; Optional: false adminConnectorPort: 1444 httpAdminConnectorPort: 1888 ldapPort: 1389 ldapsPort: 1636 httpPort: 1080 httpsPort: 1081 replicationPort: 1898 sampleData: 10 Mounts: /u01/oracle/user_projects from my-oud-ds-rs-pv (rw) /var/run/secrets/kubernetes.io/serviceaccount from my-oud-ds-rs-token-c4tg4 (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: my-oud-ds-rs-pv: Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace) ClaimName: my-oud-ds-rs-pvc ReadOnly: false my-oud-ds-rs-token-c4tg4: Type: Secret (a volume populated by a Secret) SecretName: my-oud-ds-rs-token-c4tg4 Optional: false QoS Class: BestEffort Node-Selectors: \u0026lt;none\u0026gt; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300s Events: \u0026lt;none\u0026gt; "
},
{
	"uri": "/fmw-kubernetes/oudsm/troubleshooting/",
	"title": "Troubleshooting",
	"tags": [],
	"description": "How to Troubleshoot issues.",
	"content": " Check the Status of a Namespace View POD Logs View Pod Description  Check the Status of a Namespace To check the status of objects in a namespace use the following command:\n$ kubectl --namespace \u0026lt;namespace\u0026gt; get nodes,pod,service,secret,pv,pvc,ingress -o wide Output will be similar to the following:\n$ kubectl --namespace myhelmns get nodes,pod,service,secret,pv,pvc,ingress -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME node/10.89.73.203 Ready \u0026lt;none\u0026gt; 75d v1.18.4 10.89.73.203 \u0026lt;none\u0026gt; Oracle Linux Server 7.5 4.1.12-124.35.2.el7uek.x86_64 docker://19.3.11 node/10.89.73.204 Ready \u0026lt;none\u0026gt; 75d v1.18.4 10.89.73.204 \u0026lt;none\u0026gt; Oracle Linux Server 7.5 4.1.12-124.38.1.el7uek.x86_64 docker://19.3.11 node/10.89.73.42 Ready master 76d v1.18.4 10.89.73.42 \u0026lt;none\u0026gt; Oracle Linux Server 7.5 4.1.12-124.35.2.el7uek.x86_64 docker://19.3.11 NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/my-oud-ds-rs-0 1/1 Running 0 83m 10.244.1.90 10.89.73.203 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/my-oud-ds-rs-1 1/1 Running 0 83m 10.244.1.91 10.89.73.203 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/my-oud-ds-rs-2 1/1 Running 0 83m 10.244.1.89 10.89.73.203 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service/my-oud-ds-rs-0 ClusterIP 10.100.226.50 \u0026lt;none\u0026gt; 1444/TCP,1888/TCP,1898/TCP 83m app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=my-oud-ds-rs-0 service/my-oud-ds-rs-1 ClusterIP 10.96.231.214 \u0026lt;none\u0026gt; 1444/TCP,1888/TCP,1898/TCP 83m app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=my-oud-ds-rs-1 service/my-oud-ds-rs-2 ClusterIP 10.99.254.14 \u0026lt;none\u0026gt; 1444/TCP,1888/TCP,1898/TCP 83m app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=my-oud-ds-rs-2 service/my-oud-ds-rs-http-0 ClusterIP 10.109.186.111 \u0026lt;none\u0026gt; 1080/TCP,1081/TCP 83m app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=my-oud-ds-rs-0 service/my-oud-ds-rs-http-1 ClusterIP 10.101.227.72 \u0026lt;none\u0026gt; 1080/TCP,1081/TCP 83m app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=my-oud-ds-rs-1 service/my-oud-ds-rs-http-2 ClusterIP 10.103.18.99 \u0026lt;none\u0026gt; 1080/TCP,1081/TCP 83m app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=my-oud-ds-rs-2 service/my-oud-ds-rs-lbr-admin ClusterIP 10.105.211.54 \u0026lt;none\u0026gt; 1888/TCP,1444/TCP 83m app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs service/my-oud-ds-rs-lbr-http ClusterIP 10.99.23.245 \u0026lt;none\u0026gt; 1080/TCP,1081/TCP 83m app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs service/my-oud-ds-rs-lbr-ldap ClusterIP 10.103.171.90 \u0026lt;none\u0026gt; 1389/TCP,1636/TCP 83m app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs service/my-oud-ds-rs-ldap-0 ClusterIP 10.107.250.130 \u0026lt;none\u0026gt; 1389/TCP,1636/TCP 83m app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=my-oud-ds-rs-0 service/my-oud-ds-rs-ldap-1 ClusterIP 10.100.73.198 \u0026lt;none\u0026gt; 1389/TCP,1636/TCP 83m app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=my-oud-ds-rs-1 service/my-oud-ds-rs-ldap-2 ClusterIP 10.98.176.118 \u0026lt;none\u0026gt; 1389/TCP,1636/TCP 83m app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=my-oud-ds-rs-2 NAME TYPE DATA AGE secret/default-token-kfmhq kubernetes.io/service-account-token 3 84m secret/my-oud-ds-rs-creds opaque 8 83m secret/my-oud-ds-rs-tls-cert kubernetes.io/tls 2 83m secret/my-oud-ds-rs-token-c4tg4 kubernetes.io/service-account-token 3 83m secret/sh.helm.release.v1.my-oud-ds-rs.v1 helm.sh/release.v1 1 83m NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE VOLUMEMODE persistentvolume/my-oud-ds-rs-espv1 20Gi RWX Retain Available elk 83m Filesystem persistentvolume/my-oud-ds-rs-pv 30Gi RWX Retain Bound myhelmns/my-oud-ds-rs-pvc manual 83m Filesystem persistentvolume/oimcluster-oim-pv 10Gi RWX Retain Bound oimcluster/oimcluster-oim-pvc oimcluster-oim-storage-class 63d Filesystem NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE VOLUMEMODE persistentvolumeclaim/my-oud-ds-rs-pvc Bound my-oud-ds-rs-pv 30Gi RWX manual 83m Filesystem NAME CLASS HOSTS ADDRESS PORTS AGE ingress.extensions/my-oud-ds-rs-admin-ingress-nginx \u0026lt;none\u0026gt; my-oud-ds-rs-admin-0,my-oud-ds-rs-admin-1,my-oud-ds-rs-admin-2 + 2 more... 80, 443 83m ingress.extensions/my-oud-ds-rs-http-ingress-nginx \u0026lt;none\u0026gt; my-oud-ds-rs-http-0,my-oud-ds-rs-http-1,my-oud-ds-rs-http-2 + 3 more... 80, 443 83m Include/exclude elements (nodes,pod,service,secret,pv,pvc,ingress) as required.\nView POD Logs To view logs for a POD use the following command:\n$ kubectl logs \u0026lt;pod\u0026gt; -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl logs my-oudsm -n myhelmns Output will depend on the application running in the POD.\nView Pod Description Details about a POD can be viewed using the kubectl describe command:\n$ kubectl describe pod \u0026lt;pod\u0026gt; -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl describe pod my-oud-ds-rs-0 -n myhelmns Name: my-oud-ds-rs-0 Namespace: myhelmns Priority: 0 Node: 10.89.73.203/10.89.73.203 Start Time: Wed, 07 Oct 2020 07:30:27 -0700 Labels: app.kubernetes.io/instance=my-oud-ds-rs app.kubernetes.io/managed-by=Helm app.kubernetes.io/name=oud-ds-rs app.kubernetes.io/version=12.2.1.4.0 helm.sh/chart=oud-ds-rs-0.1 oud/instance=my-oud-ds-rs-0 Annotations: meta.helm.sh/release-name: my-oud-ds-rs meta.helm.sh/release-namespace: myhelmns Status: Running IP: 10.244.1.90 IPs: IP: 10.244.1.90 Containers: oud-ds-rs: Container ID: docker://e3b79a283f56870e6d702cf8c2cc7aafa09a242f7a2cd543d8014a24aa219903 Image: oracle/oud:12.2.1.4.0 Image ID: docker://sha256:8a937042bef357fdeb09ce20d34332b14d1f1afe3ccb9f9b297f6940fdf32a76 Ports: 1444/TCP, 1888/TCP, 1389/TCP, 1636/TCP, 1080/TCP, 1081/TCP, 1898/TCP Host Ports: 0/TCP, 0/TCP, 0/TCP, 0/TCP, 0/TCP, 0/TCP, 0/TCP State: Running Started: Wed, 07 Oct 2020 07:30:28 -0700 Ready: True Restart Count: 0 Liveness: tcp-socket :ldap delay=900s timeout=15s period=30s #success=1 #failure=1 Readiness: exec [/u01/oracle/container-scripts/checkOUDInstance.sh] delay=180s timeout=30s period=60s #success=1 #failure=10 Environment: instanceType: Directory sleepBeforeConfig: 3 OUD_INSTANCE_NAME: my-oud-ds-rs-0 hostname: my-oud-ds-rs-0 baseDN: dc=example,dc=com rootUserDN: \u0026lt;set to the key 'rootUserDN' in secret 'my-oud-ds-rs-creds'\u0026gt; Optional: false rootUserPassword: \u0026lt;set to the key 'rootUserPassword' in secret 'my-oud-ds-rs-creds'\u0026gt; Optional: false adminConnectorPort: 1444 httpAdminConnectorPort: 1888 ldapPort: 1389 ldapsPort: 1636 httpPort: 1080 httpsPort: 1081 replicationPort: 1898 sampleData: 10 Mounts: /u01/oracle/user_projects from my-oud-ds-rs-pv (rw) /var/run/secrets/kubernetes.io/serviceaccount from my-oud-ds-rs-token-c4tg4 (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: my-oud-ds-rs-pv: Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace) ClaimName: my-oud-ds-rs-pvc ReadOnly: false my-oud-ds-rs-token-c4tg4: Type: Secret (a volume populated by a Secret) SecretName: my-oud-ds-rs-token-c4tg4 Optional: false QoS Class: BestEffort Node-Selectors: \u0026lt;none\u0026gt; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300s Events: \u0026lt;none\u0026gt; "
},
{
	"uri": "/fmw-kubernetes/oam/manage-oam-domains/",
	"title": "Manage OAM Domains",
	"tags": [],
	"description": "This document provides steps to manage the OAM domain.",
	"content": "Important considerations for Oracle Access Management domains in Kubernetes.\n Domain Life Cycle  Learn about the domain life cyle of an OAM domain.\n WLST Administration Operations  Describes the steps for WLST administration using helper pod running in the same Kubernetes Cluster as OAM Domain.\n Logging and Visualization  Describes the steps for logging and visualization with Elasticsearch and Kibana.\n Monitoring an OAM domain  Describes the steps for Monitoring the OAM domain.\n Delete the OAM domain home  Learn about the steps to cleanup the OAM domain home.\n "
},
{
	"uri": "/fmw-kubernetes/oig/patch-and-upgrade/",
	"title": "Patch and Upgrade",
	"tags": [],
	"description": "This document provides steps to patch or upgrade an OIG image, Oracle WebLogic Kubernetes Operator or Kubernetes Cluster.",
	"content": "Patch an existing Oracle OIG image, upgrade the Oracle WebLogic Kubernetes Operator release, or upgrade the underlying Kubernetes cluster to a new release.\n a. Patch an image  Instructions on how to update your OIG Kubernetes cluster with a new OIG docker image.\n b. Upgrade an operator release  Instructions on how to update the Oracle WebLogic Kubernetes Operator version.\n c. Upgrade a Kubernetes cluster  Instructions on how to upgrade a Kubernetes cluster.\n "
},
{
	"uri": "/fmw-kubernetes/oam/patch-and-upgrade/",
	"title": "Patch and Upgrade",
	"tags": [],
	"description": "This document provides steps to patch or upgrade an OAM image, Oracle WebLogic Server Kubernetes Operator or Kubernetes Cluster.",
	"content": "Patch an existing OAM image, upgrade the Oracle WebLogic Server Kubernetes Operator release, or upgrade the underlying Kubernetes cluster to a new release.\n a. Patch an image  Instructions on how to update your OAM Kubernetes cluster with a new OAM Docker image.\n b. Upgrade an operator release  Instructions on how to update the Oracle WebLogic Server Kubernetes Operator version.\n c. Upgrade a Kubernetes cluster  Instructions on how to upgrade a Kubernetes cluster.\n "
},
{
	"uri": "/fmw-kubernetes/oig/troubleshooting/",
	"title": "Troubleshooting",
	"tags": [],
	"description": "Sample for creating an OIG domain home on an existing PV or PVC, and the domain resource YAML file for deploying the generated OIG domain.",
	"content": "Domain creation failure If the OIG domain creation fails when running create-domain.sh, run the following to diagnose the issue:\n  Run the following command to diagnose the create domain job:\n$ kubectl logs \u0026lt;job_name\u0026gt; -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl logs oimcluster-create-fmw-infra-sample-domain-job-9wqzb -n oimcluster Also run:\n$ kubectl describe pod \u0026lt;job_domain\u0026gt; -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl describe pod oimcluster-create-fmw-infra-sample-domain-job-9wqzb -n oimcluster Using the output you should be able to diagnose the problem and resolve the issue.\nClean down the failed domain creation by following steps 1-4 in Delete the OIG domain home. Then recreate the PC and PVC then execute the OIG domain creation steps again.\n  If any of the above commands return the following error:\nFailed to start container \u0026#34;create-fmw-infra-sample-domain-job\u0026#34;: Error response from daemon: error while creating mount source path \u0026#39;/scratch/OIGDockerK8S/oimclusterdomainpv \u0026#39;: mkdir /scratch/OIGDockerK8S/oimclusterdomainpv : permission denied then there is a permissions error on the directory for the PV and PVC and the following should be checked:\na) The directory has 777 permissions: chmod -R 777 \u0026lt;work directory\u0026gt;/oimclusterdomainpv.\nb) If it does have the permissions, check if an oracle user exists and the uid and gid equal 1000, for example:\n$ uid=1000(oracle) gid=1000(spg) groups=1000(spg),59968(oinstall),8500(dba),100(users),1007(cgbudba) Create the oracle user if it doesn\u0026rsquo;t exist and set the uid and gid to 1000.\nc) Edit the \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-oim-domain-pv-pvc/create-pv-pvc-inputs.yaml and add a slash to the end of the directory for the weblogicDomainStoragePath parameter:\nweblogicDomainStoragePath: /scratch/OIGDockerK8S/oimclusterdomainpv/ Clean down the failed domain creation by following steps 1-4 in Delete the OIG domain home. Then recreate the PC and PVC and then execute the OIG domain creation steps again.\n  "
},
{
	"uri": "/fmw-kubernetes/oam/security-hardening/",
	"title": "Security Hardening",
	"tags": [],
	"description": "Review resources for the Docker and Kubernetes cluster hardening.",
	"content": "Securing a Kubernetes cluster involves hardening on multiple fronts - securing the API servers, etcd, nodes, container images, container run-time, and the cluster network. Apply principles of defense in depth, principle of least privilege, and minimize the attack surface. Use security tools such as Kube-Bench to verify the cluster’s security posture. Since Kubernetes is evolving rapidly, refer to Kubernetes Security Overview for the latest information on securing a Kubernetes cluster. Also ensure the deployed Docker containers follow the Docker Security guidance.\nThis section provides references on how to securely configure Docker and Kubernetes.\nReferences   Docker hardening:\n https://docs.docker.com/engine/security/security/ https://blog.aquasec.com/docker-security-best-practices    Kubernetes hardening:\n https://kubernetes.io/docs/concepts/security/overview/ https://kubernetes.io/docs/concepts/security/pod-security-standards/ https://blogs.oracle.com/developers/5-best-practices-for-kubernetes-security    "
},
{
	"uri": "/fmw-kubernetes/oig/security-hardening/",
	"title": "Security Hardening",
	"tags": [],
	"description": "Review resources for the Docker and Kubernetes cluster hardening.",
	"content": "Securing a Kubernetes cluster involves hardening on multiple fronts - securing the API servers, etcd, nodes, container images, container run-time, and the cluster network. Apply principles of defense in depth, principle of least privilege, and minimize the attack surface. Use security tools such as Kube-Bench to verify the cluster’s security posture. Since Kubernetes is evolving rapidly, refer to Kubernetes Security Overview for the latest information on securing a Kubernetes cluster. Also ensure the deployed Docker containers follow the Docker Security guidance.\nThis section provides references on how to securely configure Docker and Kubernetes.\nReferences   Docker hardening:\n https://docs.docker.com/engine/security/security/ https://blog.aquasec.com/docker-security-best-practices    Kubernetes hardening:\n https://kubernetes.io/docs/concepts/security/overview/ https://kubernetes.io/docs/concepts/security/pod-security-standards/ https://blogs.oracle.com/developers/5-best-practices-for-kubernetes-security    "
},
{
	"uri": "/fmw-kubernetes/oam/troubleshooting/",
	"title": "Troubleshooting",
	"tags": [],
	"description": "How to Troubleshoot domain creation failure.",
	"content": "Domain creation failure If the OAM domain creation fails when running create-domain.sh, run the following to diagnose the issue:\n  Run the following command to diagnose the create domain job:\n$ kubectl logs \u0026lt;domain_job\u0026gt; -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl logs accessinfra-create-fmw-infra-sample-domain-job-c6vfb -n accessns Also run:\n$ kubectl describe pod \u0026lt;domain_job\u0026gt; -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl describe pod accessinfra-create-fmw-infra-sample-domain-job-c6vfb -n accessns Using the output you should be able to diagnose the problem and resolve the issue.\nClean down the failed domain creation by following steps 1-4 in Delete the OAM domain home. Then recreate the PC and PVC then execute the OAM domain creation steps again.\n  If any of the above commands return the following error:\nFailed to start container \u0026#34;create-fmw-infra-sample-domain-job\u0026#34;: Error response from daemon: error while creating mount source path \u0026#39;/scratch/OAMDockerK8S/accessdomainpv \u0026#39;: mkdir /scratch/OAMDockerK8S/accessdomainpv : permission denied then there is a permissions error on the directory for the PV and PVC and the following should be checked:\na) The directory has 777 permissions: chmod -R 777 \u0026lt;work directory\u0026gt;/accessdomainpv.\nb) If it does have the permissions, check if an oracle user exists and the uid and gid equal 1000, for example:\n$ uid=1000(oracle) gid=1000(spg) groups=1000(spg),59968(oinstall),8500(dba),100(users),1007(cgbudba) Create the oracle user if it doesn\u0026rsquo;t exist and set the uid and gid to 1000.\nc) Edit the \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-access-domain-pv-pvc/create-pv-pvc-inputs.yaml and add a slash to the end of the directory for the weblogicDomainStoragePath parameter:\nweblogicDomainStoragePath: /scratch/OAMDockerK8S/accessdomainpv/ Clean down the failed domain creation by following steps 1-4 in Delete the OAM domain home. Then recreate the PC and PVC and then execute the OAM domain creation steps again.\n  "
},
{
	"uri": "/fmw-kubernetes/oud/create-oud-instances-helm/oud-ds-rs/",
	"title": "Helm Chart: oud-ds-rs: For deployment of replicated Oracle Unified Directory (DS+RS) instances",
	"tags": [],
	"description": "This document provides details of the oud-ds-rs Helm chart.",
	"content": " Introduction Deploy oud-ds-rs Helm Chart Verify the Replication Ingress Controller Setup  Ingress with NGINX Ingress with Voyager   Access to Interfaces through Ingress Configuration Parameters  Introduction This Helm chart provides for the deployment of replicated Oracle Unified Directory (DS+RS) instances on Kubernetes.\nThis chart can be used to deploy an Oracle Unified Directory instance as a base, with configured sample entries, and multiple replicated Oracle Unified Directory instances/pods/services based on the specified replicaCount.\nBased on the configuration, this chart deploys the following objects in the specified namespace of a Kubernetes cluster.\n Service Account Secret Persistent Volume and Persistent Volume Claim Pod(s)/Container(s) for Oracle Unified Directory Instances Services for interfaces exposed through Oracle Unified Directory Instances Ingress configuration  Create Kubernetes Namespace Create a Kubernetes namespace to provide a scope for other objects such as pods and services that you create in the environment. To create your namespace issue the following command:\n$ kubectl create ns myhelmns namespace/myhelmns created Deploy oud-ds-rs Helm Chart Create or Deploy a group of replicated Oracle Unified Directory instances along with Kubernetes objects in a specified namespace using the oud-ds-rs Helm Chart.\nThe deployment can be initiated by running the following Helm command with reference to the oud-ds-rs Helm Chart, along with configuration parameters according to your environment. Before deploying the Helm chart, the namespace should be created. Objects to be created by the Helm chart will be created inside the specified namespace.\n$ helm install --namespace \u0026lt;namespace\u0026gt; \\ \u0026lt;Configuration Parameters\u0026gt; \\ \u0026lt;deployment/release name\u0026gt; \\ \u0026lt;Helm Chart Path/Name\u0026gt; Configuration Parameters (override values in chart) can be passed on with --set arguments on the command line and/or with -f / --values arguments when referring to files.\nNote: Example files in the sections below provide values which allow the user to override the default values provided by the Helm chart.\nExamples Example where configuration parameters are passed with --set argument: $ helm install --namespace myhelmns \\ --set oudConfig.rootUserPassword=Oracle123,persistence.filesystem.hostPath.path=/scratch/shared/oud_user_projects \\ my-oud-ds-rs oud-ds-rs  For more details about the helm command and parameters, please execute helm --help and helm install --help. In this example, it is assumed that the command is executed from the directory containing the \u0026lsquo;oud-ds-rs\u0026rsquo; helm chart directory (OracleUnifiedDirectory/kubernetes/helm/).  Example where configuration parameters are passed with --values argument: $ helm install --namespace myhelmns \\ --values oud-ds-rs-values-override.yaml \\ my-oud-ds-rs oud-ds-rs  For more details about the helm command and parameters, please execute helm --help and helm install --help. In this example, it is assumed that the command is executed from the directory containing the \u0026lsquo;oud-ds-rs\u0026rsquo; helm chart directory (OracleUnifiedDirectory/kubernetes/helm/). The --values argument passes a file path/name which overrides values in the chart.  oud-ds-rs-values-override.yaml\noudConfig: rootUserPassword: Oracle123 persistence: type: filesystem filesystem: hostPath: path: /scratch/shared/oud_user_projects Example to scale-up through Helm Chart based deployment: In this example, we are setting replicaCount value to 3. If initially, the replicaCount value was 2, we will observe a new Oracle Unified Directory pod with assosiated services brought up by Kubernetes. So overall, 4 pods will be running now.\nWe have two ways to achieve our goal:\n$ helm upgrade --namespace myhelmns \\ --set replicaCount=3 \\ my-oud-ds-rs oud-ds-rs OR\n$ helm upgrade --namespace myhelmns \\ --values oud-ds-rs-values-override.yaml \\ my-oud-ds-rs oud-ds-rs oud-ds-rs-values-override.yaml\nreplicaCount: 3  For more details about the helm command and parameters, please execute helm --help and helm install --help. In this example, it is assumed that the command is executed from the directory containing the \u0026lsquo;oud-ds-rs\u0026rsquo; helm chart directory (OracleUnifiedDirectory/kubernetes/helm/).  Example to apply new Oracle Unified Directory patch through Helm Chart based deployment: In this example, we will apply PSU2020July-20200730 patch on earlier running Oracle Unified Directory version. If we describe pod we will observe that the container is up with new version.\nWe have two ways to achieve our goal:\n$ helm upgrade --namespace myhelmns \\ --set image.repository=oracle/oud,image.tag=12.2.1.4.0-PSU2020July-20200730 \\ my-oud-ds-rs oud-ds-rs OR\n$ helm upgrade --namespace myhelmns \\ --values oud-ds-rs-values-override.yaml \\ my-oud-ds-rs oud-ds-rs  For more details about the helm command and parameters, please execute helm --help and helm install --help. In this example, it is assumed that the command is executed from the directory containing the \u0026lsquo;oud-ds-rs\u0026rsquo; helm chart directory (OracleUnifiedDirectory/kubernetes/helm/).  oud-ds-rs-values-override.yaml\nimage: repository: oracle/oud tag: 12.2.1.4.0-PSU2020July-20200730 Example for using NFS as PV Storage: $ helm install --namespace myhelmns \\ --values oud-ds-rs-values-override-nfs.yaml \\ my-oud-ds-rs oud-ds-rs  For more details about the helm command and parameters, please execute helm --help and helm install --help. In this example, it is assumed that the command is executed from the directory containing the \u0026lsquo;oud-ds-rs\u0026rsquo; helm chart directory (OracleUnifiedDirectory/kubernetes/helm/). The --values argument passes a file path/name which overrides values in the chart.  oud-ds-rs-values-override-nfs.yaml\noudConfig: rootUserPassword: Oracle123 persistence: type: networkstorage networkstorage: nfs: path: /scratch/shared/oud_user_projects server: \u0026lt;NFS IP address \u0026gt; Example for using PV type of your choice: $ helm install --namespace myhelmns \\ --values oud-ds-rs-values-override-pv-custom.yaml \\ my-oud-ds-rs oud-ds-rs  For more details about the helm command and parameters, please execute helm --help and helm install --help. In this example, it is assumed that the command is executed from the directory containing the \u0026lsquo;oud-ds-rs\u0026rsquo; helm chart directory (OracleUnifiedDirectory/kubernetes/helm/). The --values argument passes a file path/name which overrides values in the chart.  oud-ds-rs-values-override-pv-custom.yaml\noudConfig: rootUserPassword: Oracle123 persistence: type: custom custom: nfs: # Path of NFS Share location path: /scratch/shared/oud_user_projects # IP of NFS Server server: \u0026lt;NFS IP address \u0026gt;  Under custom:, the configuration of your choice can be specified. This configuration will be used \u0026lsquo;as-is\u0026rsquo; for the PersistentVolume object.  Check Deployment Output for the helm install/upgrade command Ouput similar to the following is observed following successful execution of helm install/upgrade command.\nNAME: my-oud-ds-rs LAST DEPLOYED: Tue Mar 31 01:40:05 2020 NAMESPACE: myhelmns STATUS: deployed REVISION: 1 TEST SUITE: None  Check for the status of objects created through oud-ds-rs helm chart Command:\n$ kubectl --namespace myhelmns get nodes,pod,service,secret,pv,pvc,ingress -o wide Output is similar to the following:\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/my-oud-ds-rs-0 1/1 Running 0 8m44s 10.244.0.195 \u0026lt;Worker Node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/my-oud-ds-rs-1 1/1 Running 0 8m44s 10.244.0.194 \u0026lt;Worker Node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/my-oud-ds-rs-2 0/1 Running 0 8m44s 10.244.0.193 \u0026lt;Worker Node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service/my-oud-ds-rs-0 ClusterIP 10.99.232.83 \u0026lt;none\u0026gt; 1444/TCP,1888/TCP,1898/TCP 8m44s kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=my-oud-ds-rs-0 service/my-oud-ds-rs-1 ClusterIP 10.100.186.42 \u0026lt;none\u0026gt; 1444/TCP,1888/TCP,1898/TCP 8m45s app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=my-oud-ds-rs-1 service/my-oud-ds-rs-2 ClusterIP 10.104.55.53 \u0026lt;none\u0026gt; 1444/TCP,1888/TCP,1898/TCP 8m45s app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=my-oud-ds-rs-2 service/my-oud-ds-rs-http-0 ClusterIP 10.102.116.145 \u0026lt;none\u0026gt; 1080/TCP,1081/TCP 8m45s app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=my-oud-ds-rs-0 service/my-oud-ds-rs-http-1 ClusterIP 10.111.103.84 \u0026lt;none\u0026gt; 1080/TCP,1081/TCP 8m44s app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=my-oud-ds-rs-1 service/my-oud-ds-rs-http-2 ClusterIP 10.105.53.24 \u0026lt;none\u0026gt; 1080/TCP,1081/TCP 8m45s app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=my-oud-ds-rs-2 service/my-oud-ds-rs-lbr-admin ClusterIP 10.98.39.206 \u0026lt;none\u0026gt; 1888/TCP,1444/TCP 8m45s app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs service/my-oud-ds-rs-lbr-http ClusterIP 10.110.77.132 \u0026lt;none\u0026gt; 1080/TCP,1081/TCP 8m45s app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs service/my-oud-ds-rs-lbr-ldap ClusterIP 10.111.55.122 \u0026lt;none\u0026gt; 1389/TCP,1636/TCP 8m45s app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs service/my-oud-ds-rs-ldap-0 ClusterIP 10.108.155.81 \u0026lt;none\u0026gt; 1389/TCP,1636/TCP 8m44s app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=my-oud-ds-rs-0 service/my-oud-ds-rs-ldap-1 ClusterIP 10.104.88.44 \u0026lt;none\u0026gt; 1389/TCP,1636/TCP 8m45s app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=my-oud-ds-rs-1 service/my-oud-ds-rs-ldap-2 ClusterIP 10.105.253.120 \u0026lt;none\u0026gt; 1389/TCP,1636/TCP 8m45s app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=my-oud-ds-rs-2 NAME TYPE DATA AGE secret/default-token-tbjr5 kubernetes.io/service-account-token 3 25d secret/my-oud-ds-rs-creds opaque 8 8m48s secret/my-oud-ds-rs-token-cct26 kubernetes.io/service-account-token 3 8m50s secret/sh.helm.release.v1.my-oud-ds-rs.v1 helm.sh/release.v1 1 8m51s NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE persistentvolume/my-oud-ds-rs-pv 20Gi RWX Retain Bound myhelmns/my-oud-ds-rs-pvc manual 8m47s NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistentvolumeclaim/my-oud-ds-rs-pvc Bound my-oud-ds-rs-pv 20Gi RWX manual 8m48s NAME HOSTS ADDRESS PORTS AGE ingress.extensions/my-oud-ds-rs-admin-ingress-nginx my-oud-ds-rs-admin-0,my-oud-ds-rs-admin-1,my-oud-ds-rs-admin-2 + 2 more... 10.229.141.78 80 8m45s ingress.extensions/my-oud-ds-rs-http-ingress-nginx my-oud-ds-rs-http-0,my-oud-ds-rs-http-1,my-oud-ds-rs-http-2 + 3 more... 10.229.141.78 80 8m45s Kubernetes Objects Kubernetes objects created by the Helm chart are detailed in the table below:\n   Type Name Example Name Purpose     Service Account \u0026lt;deployment/release name\u0026gt; my-oud-ds-rs Kubernetes Service Account for the Helm Chart deployment   Secret \u0026lt;deployment/release name\u0026gt;-creds my-oud-ds-rs-creds Secret object for Oracle Unified Directory related critical values like passwords   Persistent Volume \u0026lt;deployment/release name\u0026gt;-pv my-oud-ds-rs-pv Persistent Volume for user_projects mount.   Persistent Volume Claim \u0026lt;deployment/release name\u0026gt;-pvc my-oud-ds-rs-pvc Persistent Volume Claim for user_projects mount.   Persistent Volume \u0026lt;deployment/release name\u0026gt;-pv-config my-oud-ds-rs-pv-config Persistent Volume for mounting volume in containers for configuration files like ldif, schema, jks, java.security, etc.   Persistent Volume Claim \u0026lt;deployment/release name\u0026gt;-pvc-config my-oud-ds-rs-pvc-config Persistent Volume Claim for mounting volume in containers for configuration files like ldif, schema, jks, java.security, etc.   Pod \u0026lt;deployment/release name\u0026gt;-0 my-oud-ds-rs-0 Pod/Container for base Oracle Unified Directory Instance which would be populated first with base configuration (like number of sample entries)   Pod \u0026lt;deployment/release name\u0026gt;-N my-oud-ds-rs-1, my-oud-ds-rs-2, \u0026hellip; Pod(s)/Container(s) for Oracle Unified Directory Instances - each would have replication enabled against base Oracle Unified Directory instance \u0026lt;deployment/release name\u0026gt;-0   Service \u0026lt;deployment/release name\u0026gt;-0 my-oud-ds-rs-0 Service for LDAPS Admin, REST Admin and Replication interfaces from base Oracle Unified Directory instance \u0026lt;deployment/release name\u0026gt;-0   Service \u0026lt;deployment/release name\u0026gt;-http-0 my-oud-ds-rs-http-0 Service for HTTP and HTTPS interfaces from base Oracle Unified Directory instance \u0026lt;deployment/release name\u0026gt;-0   Service \u0026lt;deployment/release name\u0026gt;-ldap-0 my-oud-ds-rs-ldap-0 Service for LDAP and LDAPS interfaces from base Oracle Unified Directory instance \u0026lt;deployment/release name\u0026gt;-0   Service \u0026lt;deployment/release name\u0026gt;-N my-oud-ds-rs-1, my-oud-ds-rs-2, \u0026hellip; Service(s) for LDAPS Admin, REST Admin and Replication interfaces from base Oracle Unified Directory instance \u0026lt;deployment/release name\u0026gt;-N   Service \u0026lt;deployment/release name\u0026gt;-http-N my-oud-ds-rs-http-1, my-oud-ds-rs-http-2, \u0026hellip; Service(s) for HTTP and HTTPS interfaces from base Oracle Unified Directory instance \u0026lt;deployment/release name\u0026gt;-N   Service \u0026lt;deployment/release name\u0026gt;-ldap-N my-oud-ds-rs-ldap-1, my-oud-ds-rs-ldap-2, \u0026hellip; Service(s) for LDAP and LDAPS interfaces from base Oracle Unified Directory instance \u0026lt;deployment/release name\u0026gt;-N   Service \u0026lt;deployment/release name\u0026gt;-lbr-admin my-oud-ds-rs-lbr-admin Service for LDAPS Admin, REST Admin and Replication interfaces from all Oracle Unified Directory instances   Service \u0026lt;deployment/release name\u0026gt;-lbr-http my-oud-ds-rs-lbr-http Service for HTTP and HTTPS interfaces from all Oracle Unified Directory instances   Service \u0026lt;deployment/release name\u0026gt;-lbr-ldap my-oud-ds-rs-lbr-ldap Service for LDAP and LDAPS interfaces from all Oracle Unified Directory instances   Ingress \u0026lt;deployment/release name\u0026gt;-admin-ingress-nginx my-oud-ds-rs-admin-ingress-nginx Ingress Rules for HTTP Admin interfaces.   Ingress \u0026lt;deployment/release name\u0026gt;-http-ingress-nginx my-oud-ds-rs-http-ingress-nginx Ingress Rules for HTTP (Data/REST) interfaces.     In the table above the \u0026lsquo;Example Name\u0026rsquo; for each Object is based on the value \u0026lsquo;my-oud-ds-rs\u0026rsquo; as deployment/release name for the Helm chart installation.  Verify the Replication Once all the PODs created are visible as READY (i.e. 1/1), you can verify your replication across multiple Oracle Unified Directory instances.\nTo verify the replication group, connect to the container and issue an Oracle Unified Directory Administration command to show details. You can get the name of the container by issuing the following:\n$ kubectl get pods -n \u0026lt;namespace\u0026gt; -o jsonpath='{.items[*].spec.containers[*].name}' For example:\n$ kubectl get pods -n myhelmns -o jsonpath='{.items[*].spec.containers[*].name}' oud-ds-rs With the container name you can then connect to the container:\n$ kubectl --namespace \u0026lt;namespace\u0026gt; exec -it -c \u0026lt;containername\u0026gt; \u0026lt;podname\u0026gt; /bin/bash For example:\n$ kubectl --namespace myhelmns exec -it -c oud-ds-rs my-oud-ds-rs-0 /bin/bash From the prompt, use the dsreplication command to check the status of your replication group:\n$ cd /u01/oracle/user_projects/my-oud-ds-rs-0/OUD/bin $ ./dsreplication status --trustAll \\ --hostname my-oud-ds-rs-0 --port 1444 --adminUID admin \\ --dataToDisplay compat-view --dataToDisplay rs-connections Output will be similar to the following (enter credentials where prompted):\n\u0026gt;\u0026gt;\u0026gt;\u0026gt; Specify Oracle Unified Directory LDAP connection parameters Password for user 'admin': Establishing connections and reading configuration ..... Done. dc=example,dc=com - Replication Enabled ======================================= Server : Entries : M.C. [1] : A.O.M.C. [2] : Port [3] : Encryption [4] : Trust [5] : U.C. [6] : Status [7] : ChangeLog [8] : Group ID [9] : Connected To [10] ---------------------:---------:----------:--------------:----------:----------------:-----------:----------:------------:---------------:--------------:------------------------------- my-oud-ds-rs-0:1444 : 1 : 0 : 0 : 1898 : Disabled : Trusted : -- : Normal : Enabled : 1 : my-oud-ds-rs-0:1898 : : : : : : : : : : : (GID=1) my-oud-ds-rs-1:1444 : 1 : 0 : 0 : 1898 : Disabled : Trusted : -- : Normal : Enabled : 1 : my-oud-ds-rs-1:1898 : : : : : : : : : : : (GID=1) my-oud-ds-rs-2:1444 : 1 : 0 : 0 : 1898 : Disabled : Trusted : -- : Normal : Enabled : 1 : my-oud-ds-rs-2:1898 : : : : : : : : : : : (GID=1) Replication Server [11] : RS #1 : RS #2 : RS #3 -------------------------------:-------:-------:------ my-oud-ds-rs-0:1898 : -- : Yes : Yes (#1) : : : my-oud-ds-rs-1:1898 : Yes : -- : Yes (#2) : : : my-oud-ds-rs-2:1898 : Yes : Yes : -- (#3) : : : [1] The number of changes that are still missing on this element (and that have been applied to at least one other server). [2] Age of oldest missing change: the age (in seconds) of the oldest change that has not yet arrived on this element. [3] The replication port used to communicate between the servers whose contents are being replicated. [4] Whether the replication communication initiated by this element is encrypted or not. [5] Whether the directory server is trusted or not. Updates coming from an untrusted server are discarded and not propagated. [6] The number of untrusted changes. These are changes generated on this server while it is untrusted. Those changes are not propagated to the rest of the topology but are effective on the untrusted server. [7] The status of the replication on this element. [8] Whether the external change log is enabled for the base DN on this server or not. [9] The ID of the replication group to which the server belongs. [10] The replication server this server is connected to with its group ID between brackets. [11] This table represents the connections between the replication servers. The headers of the columns use a number as identifier for each replication server. See the values of the first column to identify the corresponding replication server for each number. The dsreplication status command can be additionally invoked using the following syntax:\n$ kubectl --namespace \u0026lt;namespace\u0026gt; exec -it -c \u0026lt;containername\u0026gt; \u0026lt;podname\u0026gt; -- \\ /u01/oracle/user_projects/\u0026lt;OUD Instance/Pod Name\u0026gt;/OUD/bin/dsreplication status \\ --trustAll --hostname \u0026lt;OUD Instance/Pod Name\u0026gt; --port 1444 --adminUID admin \\ --dataToDisplay compat-view --dataToDisplay rs-connections For example:\n$ kubectl --namespace myhelmns exec -it -c oud-ds-rs my-oud-ds-rs-0 -- \\ /u01/oracle/user_projects/my-oud-ds-rs-0/OUD/bin/dsreplication status \\ --trustAll --hostname my-oud-ds-rs-0 --port 1444 --adminUID admin \\ --dataToDisplay compat-view --dataToDisplay rs-connections Ingress Controller Setup There are two types of Ingress controllers supported by this Helm chart. In the sub-sections below, configuration steps for each Controller are described.\nBy default Ingress configuration only supports HTTP and HTTPS Ports/Communication. To allow LDAP and LDAPS communication over TCP, configuration is required at Ingress Controller/Implementation level.\nIngress with NGINX Nginx-ingress controller implementation can be deployed/installed in a Kubernetes environment.\nCreate a Kubernetes Namespace Create a Kubernetes namespace to provide a scope for NGINX objects such as pods and services that you create in the environment. To create your namespace issue the following command:\n$ kubectl create ns mynginx namespace/mynginx created Add Repo reference to Helm for retrieving/installing Chart for nginx-ingress implementation. $ helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx Confirm the charts available by issuing the following command:\n$ helm search repo | grep nginx ingress-nginx/ingress-nginx 3.4.1 0.40.2 Ingress controller for Kubernetes using NGINX a... stable/ingress-nginx 3.4.1 0.40.2 Ingress controller for Kubernetes using NGINX a... Command helm install to install nginx-ingress related objects like pod, service, deployment, etc. To install and configure NGINX Ingress issue the following command:\n$ helm install --namespace mynginx \\ --values nginx-ingress-values-override.yaml \\ lbr-nginx ingress-nginx/ingress-nginx Where:\n lbr-nginx is your deployment name ingress-nginx/ingress-nginx is the chart reference  Output will be similar to the following:\n$ helm install --namespace mynginx --values samples/nginx-ingress-values-override.yaml lbr-nginx ingress-nginx/ingress-nginx NAME: lbr-nginx LAST DEPLOYED: Wed Oct 7 08:07:29 2020 NAMESPACE: mynginx STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: The ingress-nginx controller has been installed. It may take a few minutes for the LoadBalancer IP to be available. You can watch the status by running 'kubectl --namespace mynginx get services -o wide -w lbr-nginx-ingress-nginx-controller' An example Ingress that makes use of the controller: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx name: example namespace: foo spec: rules: - host: www.example.com http: paths: - backend: serviceName: exampleService servicePort: 80 path: / # This section is only required if TLS is to be enabled for the Ingress tls: - hosts: - www.example.com secretName: example-tls If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided: apiVersion: v1 kind: Secret metadata: name: example-tls namespace: foo data: tls.crt: \u0026lt;base64 encoded cert\u0026gt; tls.key: \u0026lt;base64 encoded key\u0026gt; type: kubernetes.io/tls  For more details about the helm command and parameters, please execute helm --help and helm install --help. The --values argument passes a file path/name which overrides values in the chart.  nginx-ingress-values-override.yaml\n# Configuration for additional TCP ports to be exposed through Ingress # Format for each port would be like: # \u0026lt;PortNumber\u0026gt;: \u0026lt;Namespace\u0026gt;/\u0026lt;Service\u0026gt; tcp: # Map 1389 TCP port to LBR LDAP service to get requests handled through any available POD/Endpoint serving LDAP Port 1389: myhelmns/my-oud-ds-rs-lbr-ldap:ldap # Map 1636 TCP port to LBR LDAP service to get requests handled through any available POD/Endpoint serving LDAPS Port 1636: myhelmns/my-oud-ds-rs-lbr-ldap:ldaps controller: admissionWebhooks: enabled: false extraArgs: # The secret referred to by this flag contains the default certificate to be used when accessing the catch-all server. # If this flag is not provided NGINX will use a self-signed certificate. # If the TLS Secret is in different namespace, name can be mentioned as \u0026lt;namespace\u0026gt;/\u0026lt;tlsSecretName\u0026gt; default-ssl-certificate: myhelmns/my-oud-ds-rs-tls-cert service: # controller service external IP addresses # externalIPs: # - \u0026lt; External IP Address \u0026gt; # To configure Ingress Controller Service as LoadBalancer type of Service # Based on the Kubernetes configuration, External LoadBalancer would be linked to the Ingress Controller Service type: LoadBalancer # Configuration for NodePort to be used for Ports exposed through Ingress # If NodePorts are not defied/configured, Node Port would be assigend automatically by Kubernetes # These NodePorts are helpful while accessing services directly through Ingress and without having External Load Balancer. nodePorts: # For HTTP Interface exposed through LoadBalancer/Ingress http: 30080 # For HTTPS Interface exposed through LoadBalancer/Ingress https: 30443 tcp: # For LDAP Interface 1389: 31389 # For LDAPS Interface 1636: 31636  The configuration above assumes that you have oud-ds-rs installed with value my-oud-ds-rs as a deployment/release name. Based on the deployment/release name in your environment, TCP port mapping may be required to be changed/updated.  Optional: Command helm upgrade to update nginx-ingress related objects like pod, service, deployment, etc. If required, an nginx-ingress deployment can be updated/upgraded with following command. In this example, nginx-ingress configuration is updated with an additional TCP port and Node Port for accessing the LDAP/LDAPS port of a specific POD.\n$ helm upgrade --namespace mynginx \\ --values nginx-ingress-values-override.yaml \\ lbr-nginx ingress-nginx/nginx-ingress  For more details about the helm command and parameters, please execute helm --help and helm install --help. The --values argument passes a file path/name which overrides values in the chart.  nginx-ingress-values-override.yaml\n# Configuration for additional TCP ports to be exposed through Ingress # Format for each port would be like: # \u0026lt;PortNumber\u0026gt;: \u0026lt;Namespace\u0026gt;/\u0026lt;Service\u0026gt; tcp: # Map 1389 TCP port to LBR LDAP service to get requests handled through any available POD/Endpoint serving LDAP Port 1389: myhelmns/my-oud-ds-rs-lbr-ldap:ldap # Map 1636 TCP port to LBR LDAP service to get requests handled through any available POD/Endpoint serving LDAPS Port 1636: myhelmns/my-oud-ds-rs-lbr-ldap:ldaps # Map specific ports for LDAP and LDAPS communication from individual Services/Pods # To redirect requests on 3890 port to myhelmns/my-oud-ds-rs-ldap-0:ldap 3890: myhelmns/my-oud-ds-rs-ldap-0:ldap # To redirect requests on 6360 port to myhelmns/my-oud-ds-rs-ldaps-0:ldap 6360: myhelmns/my-oud-ds-rs-ldap-0:ldaps # To redirect requests on 3891 port to myhelmns/my-oud-ds-rs-ldap-1:ldap 3891: myhelmns/my-oud-ds-rs-ldap-1:ldap # To redirect requests on 6361 port to myhelmns/my-oud-ds-rs-ldaps-1:ldap 6361: myhelmns/my-oud-ds-rs-ldap-1:ldaps # To redirect requests on 3892 port to myhelmns/my-oud-ds-rs-ldap-2:ldap 3892: myhelmns/my-oud-ds-rs-ldap-2:ldap # To redirect requests on 6362 port to myhelmns/my-oud-ds-rs-ldaps-2:ldap 6362: myhelmns/my-oud-ds-rs-ldap-2:ldaps # Map 1444 TCP port to LBR Admin service to get requests handled through any available POD/Endpoint serving Admin LDAPS Port 1444: myhelmns/my-oud-ds-rs-lbr-admin:adminldaps # To redirect requests on 4440 port to myhelmns/my-oud-ds-rs-0:adminldaps 4440: myhelmns/my-oud-ds-rs-0:adminldaps # To redirect requests on 4441 port to myhelmns/my-oud-ds-rs-1:adminldaps 4441: myhelmns/my-oud-ds-rs-1:adminldaps # To redirect requests on 4442 port to myhelmns/my-oud-ds-rs-2:adminldaps 4442: myhelmns/my-oud-ds-rs-2:adminldaps controller: admissionWebhooks: enabled: false extraArgs: # The secret referred to by this flag contains the default certificate to be used when accessing the catch-all server. # If this flag is not provided NGINX will use a self-signed certificate. # If the TLS Secret is in different namespace, name can be mentioned as \u0026lt;namespace\u0026gt;/\u0026lt;tlsSecretName\u0026gt; default-ssl-certificate: myhelmns/my-oud-ds-rs-tls-cert service: # controller service external IP addresses # externalIPs: # - \u0026lt; External IP Address \u0026gt; # To configure Ingress Controller Service as LoadBalancer type of Service # Based on the Kubernetes configuration, External LoadBalancer would be linked to the Ingress Controller Service type: LoadBalancer # Configuration for NodePort to be used for Ports exposed through Ingress # If NodePorts are not defied/configured, Node Port would be assigend automatically by Kubernetes # These NodePorts are helpful while accessing services directly through Ingress and without having External Load Balancer. nodePorts: # For HTTP Interface exposed through LoadBalancer/Ingress http: 30080 # For HTTPS Interface exposed through LoadBalancer/Ingress https: 30443 tcp: # For LDAP Interface referring to LBR LDAP services serving LDAP port 1389: 31389 # For LDAPS Interface referring to LBR LDAP services serving LDAPS port 1636: 31636 # For LDAP Interface from specific service oud-ds-rs-ldap-0 3890: 30890 # For LDAPS Interface from specific service oud-ds-rs-ldap-0 6360: 30360 # For LDAP Interface from specific service oud-ds-rs-ldap-1 3891: 30891 # For LDAPS Interface from specific service oud-ds-rs-ldap-1 6361: 30361 # For LDAP Interface from specific service oud-ds-rs-ldap-2 3892: 30892 # For LDAPS Interface from specific service oud-ds-rs-ldap-2 6362: 30362 # For LDAPS Interface referring to LBR Admin services serving adminldaps port 1444: 31444 # For Admin LDAPS Interface from specific service oud-ds-rs-0 4440: 30440 # For Admin LDAPS Interface from specific service oud-ds-rs-1 4441: 30441 # For Admin LDAPS Interface from specific service oud-ds-rs-2 4442: 30442  The configuration above assumes that you have oud-ds-rs installed with value my-oud-ds-rs as a deployment/release name. Based on the deployment/release name in your environment, TCP port mapping may be required to be changed/updated.  Ingress with Voyager Voyager ingress implementation can be deployed/installed in a Kubernetes environment.\nAdd Repo reference to helm for retriving/installing Chart for Voyager implementation. $ helm repo add appscode https://charts.appscode.com/stable Command helm install to install voyager related objects like pod, service, deployment, etc. $ helm install --namespace mynginx \\ --set cloudProvider=baremetal \\ voyager-operator appscode/voyager  For more details about the helm command and parameters, please execute helm --help and helm install --help.  Access to Interfaces through Ingress Using the Helm chart, Ingress objects are also created according to configuration. The following table details the rules configured in Ingress object(s) for access to Oracle Unified Directory Interfaces through Ingress.\n   Port NodePort Host Example Hostname Path Backend Service:Port Example Service Name:Port     http/https 30080/30443 \u0026lt;deployment/release name\u0026gt;-admin-0 my-oud-ds-rs-admin-0 * \u0026lt;deployment/release name\u0026gt;-0:adminhttps my-oud-ds-rs-0:adminhttps   http/https 30080/30443 \u0026lt;deployment/release name\u0026gt;-admin-N my-oud-ds-rs-admin-N * \u0026lt;deployment/release name\u0026gt;-N:adminhttps my-oud-ds-rs-1:adminhttps   http/https 30080/30443 \u0026lt;deployment/release name\u0026gt;-admin my-oud-ds-rs-admin * \u0026lt;deployment/release name\u0026gt;-lbr-admin:adminhttps my-oud-ds-rs-lbr-admin:adminhttps   http/https 30080/30443 * * /rest/v1/admin \u0026lt;deployment/release name\u0026gt;-lbr-admin:adminhttps my-oud-ds-rs-lbr-admin:adminhttps   http/https 30080/30443 \u0026lt;deployment/release name\u0026gt;-http-0 my-oud-ds-rs-http-0 * \u0026lt;deployment/release name\u0026gt;-http-0:http my-oud-ds-rs-http-0:http   http/https 30080/30443 \u0026lt;deployment/release name\u0026gt;-http-N my-oud-ds-rs-http-N * \u0026lt;deployment/release name\u0026gt;-http-N:http my-oud-ds-rs-http-N:http   http/https 30080/30443 \u0026lt;deployment/release name\u0026gt;-http my-oud-ds-rs-http * \u0026lt;deployment/release name\u0026gt;-lbr-http:http my-oud-ds-rs-lbr-http:http   http/https 30080/30443 * * /rest/v1/directory \u0026lt;deployment/release name\u0026gt;-lbr-http:http my-oud-ds-rs-lbr-http:http   http/https 30080/30443 * * /iam/directory \u0026lt;deployment/release name\u0026gt;-lbr-http:http my-oud-ds-rs-lbr-http:http     In the table above, example values are based on the value \u0026lsquo;my-oud-ds-rs\u0026rsquo; as the deployment/release name for Helm chart installation.The NodePorts mentioned in the table are according to Ingress configuration described in previous section.When External LoadBalancer is not available/configured, Interfaces can be accessed through NodePort on a Kubernetes Node.\n For LDAP/LDAPS access (based on the updated/upgraded configuration mentioned in previous section)\n   Port NodePort Backend Service:Port Example Service Name:Port     1389 31389 \u0026lt;deployment/release name\u0026gt;-lbr-ldap:ldap my-oud-ds-rs-lbr-ldap:ldap   1636 31636 \u0026lt;deployment/release name\u0026gt;-lbr-ldap:ldap my-oud-ds-rs-lbr-ldap:ldaps   1444 31444 \u0026lt;deployment/release name\u0026gt;-lbr-admin:adminldaps my-oud-ds-rs-lbr-admin:adminldaps   3890 30890 \u0026lt;deployment/release name\u0026gt;-ldap-0:ldap my-oud-ds-rs-ldap-0:ldap   6360 30360 \u0026lt;deployment/release name\u0026gt;-ldap-0:ldaps my-oud-ds-rs-ldap-0:ldaps   3891 30891 \u0026lt;deployment/release name\u0026gt;-ldap-1:ldap my-oud-ds-rs-ldap-1:ldap   6361 30361 \u0026lt;deployment/release name\u0026gt;-ldap-1:ldaps my-oud-ds-rs-ldap-1:ldaps   3892 30892 \u0026lt;deployment/release name\u0026gt;-ldap-2:ldap my-oud-ds-rs-ldap-2:ldap   6362 30362 \u0026lt;deployment/release name\u0026gt;-ldap-2:ldaps my-oud-ds-rs-ldap-2:ldaps   4440 30440 \u0026lt;deployment/release name\u0026gt;-0:adminldaps my-oud-ds-rs-ldap-0:adminldaps   4441 30441 \u0026lt;deployment/release name\u0026gt;-1:adminldaps my-oud-ds-rs-ldap-1:adminldaps   4442 30442 \u0026lt;deployment/release name\u0026gt;-2:adminldaps my-oud-ds-rs-ldap-2:adminldaps     In the table above, example values are based on value \u0026lsquo;my-oud-ds-rs\u0026rsquo; as the deployment/release name for helm chart installation. The NodePorts mentioned in the table are according to Ingress configuration described in previous section. When external LoadBalancer is not available/configured, Interfaces can be accessed through NodePort on a Kubernetes Node.  Changes in /etc/hosts to validate hostname based Ingress rules If it is not possible to have a LoadBalancer configuration updated to have host names added for Oracle Unified Directory Interfaces then the following entries can be added in /etc/hosts files on host from where Oracle Unified Directory interfaces will be accessed.\n\u0026lt;IP Address of External LBR or Kubernetes Node\u0026gt;\tmy-oud-ds-rs-http my-oud-ds-rs-http-0 my-oud-ds-rs-http-1 my-oud-ds-rs-http-2 my-oud-ds-rs-http-N \u0026lt;IP Address of External LBR or Kubernetes Node\u0026gt;\tmy-oud-ds-rs-admin my-oud-ds-rs-admin-0 my-oud-ds-rs-admin-1 my-oud-ds-rs-admin-2 my-oud-ds-rs-admin-N  In the table above, host names are based on the value \u0026lsquo;my-oud-ds-rs\u0026rsquo; as the deployment/release name for Helm chart installation. When External LoadBalancer is not available/configured, Interfaces can be accessed through NodePort on Kubernetes Node.  Validate access HTTPS/REST API against External LBR Host:Port Note: For commands mentioned in this section you need to have an external IP assigned at Ingress level.\na) Command to invoke Data REST API:\n$curl --noproxy \u0026quot;*\u0026quot; --location \\ --request GET 'https://\u0026lt;External LBR Host\u0026gt;/rest/v1/directory/uid=user.1,ou=People,dc=example,dc=com?scope=sub\u0026amp;attributes=*' \\ --header 'Authorization: Basic \u0026lt;Base64 of userDN:userPassword\u0026gt;' | json_pp  | json_pp is used to format output in readable json format on the client side. It can be ignored if you do not have the json_pp library. Base64 of userDN:userPassword can be generated using echo -n \u0026quot;userDN:userPassword\u0026quot; | base64  Output:\n{ \u0026#34;msgType\u0026#34; : \u0026#34;urn:ietf:params:rest:schemas:oracle:oud:1.0:SearchResponse\u0026#34;, \u0026#34;totalResults\u0026#34; : 1, \u0026#34;searchResultEntries\u0026#34; : [ { \u0026#34;dn\u0026#34; : \u0026#34;uid=user.1,ou=People,dc=example,dc=com\u0026#34;, \u0026#34;attributes\u0026#34; : { \u0026#34;st\u0026#34; : \u0026#34;OH\u0026#34;, \u0026#34;employeeNumber\u0026#34; : \u0026#34;1\u0026#34;, \u0026#34;postalCode\u0026#34; : \u0026#34;93694\u0026#34;, \u0026#34;description\u0026#34; : \u0026#34;This is the description for Aaren Atp.\u0026#34;, \u0026#34;telephoneNumber\u0026#34; : \u0026#34;+1 390 103 6917\u0026#34;, \u0026#34;homePhone\u0026#34; : \u0026#34;+1 280 375 4325\u0026#34;, \u0026#34;initials\u0026#34; : \u0026#34;ALA\u0026#34;, \u0026#34;objectClass\u0026#34; : [ \u0026#34;top\u0026#34;, \u0026#34;inetorgperson\u0026#34;, \u0026#34;organizationalperson\u0026#34;, \u0026#34;person\u0026#34; ], \u0026#34;uid\u0026#34; : \u0026#34;user.1\u0026#34;, \u0026#34;sn\u0026#34; : \u0026#34;Atp\u0026#34;, \u0026#34;street\u0026#34; : \u0026#34;70110 Fourth Street\u0026#34;, \u0026#34;mobile\u0026#34; : \u0026#34;+1 680 734 6300\u0026#34;, \u0026#34;givenName\u0026#34; : \u0026#34;Aaren\u0026#34;, \u0026#34;mail\u0026#34; : \u0026#34;user.1@maildomain.net\u0026#34;, \u0026#34;l\u0026#34; : \u0026#34;New Haven\u0026#34;, \u0026#34;postalAddress\u0026#34; : \u0026#34;Aaren Atp$70110 Fourth Street$New Haven, OH 93694\u0026#34;, \u0026#34;pager\u0026#34; : \u0026#34;+1 850 883 8888\u0026#34;, \u0026#34;cn\u0026#34; : \u0026#34;Aaren Atp\u0026#34; } } ] } b) Command to invoke Data REST API against specific Oracle Unified Directory Interface:\n$ curl --noproxy \u0026quot;*\u0026quot; --location \\ --request GET 'https://my-oud-ds-rs-http-0/rest/v1/directory/uid=user.1,ou=People,dc=example,dc=com?scope=sub\u0026amp;attributes=*' \\ --header 'Authorization: Basic \u0026lt;Base64 of userDN:userPassword\u0026gt;' | json_pp  | json_pp is used to format output in readable json format on the client side. It can be ignored if you do not have the json_pp library. Base64 of userDN:userPassword can be generated using echo -n \u0026quot;userDN:userPassword\u0026quot; | base64. For this example, it is assumed that the value \u0026lsquo;my-oud-ds-rs\u0026rsquo; is used as the deployment/release name for helm chart installation. It is assumed that \u0026lsquo;my-oud-ds-rs-http-0\u0026rsquo; points to an External LoadBalancer  HTTPS/REST API against Kubernetes NodePort for Ingress Controller Service a) Command to invoke Data SCIM API:\n$ curl --noproxy \u0026quot;*\u0026quot; --location \\ --request GET 'https://\u0026lt;Kubernetes Node\u0026gt;:30443/iam/directory/oud/scim/v1/Users' \\ --header 'Authorization: Basic \u0026lt;Base64 of userDN:userPassword\u0026gt;' | json_pp  | json_pp is used to format output in readable json format on the client side. It can be ignored if you do not have the json_pp library. Base64 of userDN:userPassword can be generated using echo -n \u0026quot;userDN:userPassword\u0026quot; | base64.  Output:\n{ \u0026#34;Resources\u0026#34; : [ { \u0026#34;id\u0026#34; : \u0026#34;ad55a34a-763f-358f-93f9-da86f9ecd9e4\u0026#34;, \u0026#34;userName\u0026#34; : [ { \u0026#34;value\u0026#34; : \u0026#34;user.0\u0026#34; } ], \u0026#34;schemas\u0026#34; : [ \u0026#34;urn:ietf:params:scim:schemas:core:2.0:User\u0026#34;, \u0026#34;urn:ietf:params:scim:schemas:extension:oracle:2.0:OUD:User\u0026#34;, \u0026#34;urn:ietf:params:scim:schemas:extension:enterprise:2.0:User\u0026#34; ], \u0026#34;meta\u0026#34; : { \u0026#34;location\u0026#34; : \u0026#34;http://idm-oke-lbr/iam/directory/oud/scim/v1/Users/ad55a34a-763f-358f-93f9-da86f9ecd9e4\u0026#34;, \u0026#34;resourceType\u0026#34; : \u0026#34;User\u0026#34; }, \u0026#34;addresses\u0026#34; : [ { \u0026#34;postalCode\u0026#34; : \u0026#34;50369\u0026#34;, \u0026#34;formatted\u0026#34; : \u0026#34;Aaccf Amar$01251 Chestnut Street$Panama City, DE 50369\u0026#34;, \u0026#34;streetAddress\u0026#34; : \u0026#34;01251 Chestnut Street\u0026#34;, \u0026#34;locality\u0026#34; : \u0026#34;Panama City\u0026#34;, \u0026#34;region\u0026#34; : \u0026#34;DE\u0026#34; } ], \u0026#34;urn:ietf:params:scim:schemas:extension:oracle:2.0:OUD:User\u0026#34; : { \u0026#34;description\u0026#34; : [ { \u0026#34;value\u0026#34; : \u0026#34;This is the description for Aaccf Amar.\u0026#34; } ], \u0026#34;mobile\u0026#34; : [ { \u0026#34;value\u0026#34; : \u0026#34;+1 010 154 3228\u0026#34; } ], \u0026#34;pager\u0026#34; : [ { \u0026#34;value\u0026#34; : \u0026#34;+1 779 041 6341\u0026#34; } ], \u0026#34;objectClass\u0026#34; : [ { \u0026#34;value\u0026#34; : \u0026#34;top\u0026#34; }, { \u0026#34;value\u0026#34; : \u0026#34;organizationalperson\u0026#34; }, { \u0026#34;value\u0026#34; : \u0026#34;person\u0026#34; }, { \u0026#34;value\u0026#34; : \u0026#34;inetorgperson\u0026#34; } ], \u0026#34;initials\u0026#34; : [ { \u0026#34;value\u0026#34; : \u0026#34;ASA\u0026#34; } ], \u0026#34;homePhone\u0026#34; : [ { \u0026#34;value\u0026#34; : \u0026#34;+1 225 216 5900\u0026#34; } ] }, \u0026#34;name\u0026#34; : [ { \u0026#34;givenName\u0026#34; : \u0026#34;Aaccf\u0026#34;, \u0026#34;familyName\u0026#34; : \u0026#34;Amar\u0026#34;, \u0026#34;formatted\u0026#34; : \u0026#34;Aaccf Amar\u0026#34; } ], \u0026#34;emails\u0026#34; : [ { \u0026#34;value\u0026#34; : \u0026#34;user.0@maildomain.net\u0026#34; } ], \u0026#34;phoneNumbers\u0026#34; : [ { \u0026#34;value\u0026#34; : \u0026#34;+1 685 622 6202\u0026#34; } ], \u0026#34;urn:ietf:params:scim:schemas:extension:enterprise:2.0:User\u0026#34; : { \u0026#34;employeeNumber\u0026#34; : [ { \u0026#34;value\u0026#34; : \u0026#34;0\u0026#34; } ] } } , . . . } b) Command to invoke Data SCIM API against specific Oracle Unified Directory Interface:\n$ curl --noproxy \u0026quot;*\u0026quot; --location \\ --request GET 'https://my-oud-ds-rs-http-0:30443/iam/directory/oud/scim/v1/Users' \\ --header 'Authorization: Basic \u0026lt;Base64 of userDN:userPassword\u0026gt;' | json_pp  | json_pp is used to format output in readable json format on the client side. It can be ignored if you do not have the json_pp library. Base64 of userDN:userPassword can be generated using echo -n \u0026quot;userDN:userPassword\u0026quot; | base64. For this example, it is assumed that the value \u0026lsquo;my-oud-ds-rs\u0026rsquo; is used as the deployment/release name for helm chart installation. It is assumed that \u0026lsquo;my-oud-ds-rs-http-0\u0026rsquo; points to an External LoadBalancer  HTTPS/REST Admin API a) Command to invoke Admin REST API against External LBR:\n$ curl --noproxy \u0026quot;*\u0026quot; --insecure --location \\ --request GET 'https://\u0026lt;External LBR Host\u0026gt;/rest/v1/admin/?scope=base\u0026amp;attributes=vendorName\u0026amp;attributes=vendorVersion\u0026amp;attributes=ds-private-naming-contexts\u0026amp;attributes=subschemaSubentry' \\ --header 'Content-Type: application/json' \\ --header 'Authorization: Basic \u0026lt;Base64 of userDN:userPassword\u0026gt;' | json_pp  | json_pp is used to format output in readable json format on the client side. It can be ignored if you do not have the json_pp library. Base64 of userDN:userPassword can be generated using echo -n \u0026quot;userDN:userPassword\u0026quot; | base64.  Output:\n{ \u0026#34;totalResults\u0026#34; : 1, \u0026#34;searchResultEntries\u0026#34; : [ { \u0026#34;dn\u0026#34; : \u0026#34;\u0026#34;, \u0026#34;attributes\u0026#34; : { \u0026#34;vendorVersion\u0026#34; : \u0026#34;Oracle Unified Directory 12.2.1.4.0\u0026#34;, \u0026#34;ds-private-naming-contexts\u0026#34; : [ \u0026#34;cn=admin data\u0026#34;, \u0026#34;cn=ads-truststore\u0026#34;, \u0026#34;cn=backups\u0026#34;, \u0026#34;cn=config\u0026#34;, \u0026#34;cn=monitor\u0026#34;, \u0026#34;cn=schema\u0026#34;, \u0026#34;cn=tasks\u0026#34;, \u0026#34;cn=virtual acis\u0026#34;, \u0026#34;dc=replicationchanges\u0026#34; ], \u0026#34;subschemaSubentry\u0026#34; : \u0026#34;cn=schema\u0026#34;, \u0026#34;vendorName\u0026#34; : \u0026#34;Oracle Corporation\u0026#34; } } ], \u0026#34;msgType\u0026#34; : \u0026#34;urn:ietf:params:rest:schemas:oracle:oud:1.0:SearchResponse\u0026#34; } b) Command to invoke Admin REST API against specific Oracle Unified Directory Admin Interface:\n$ curl --noproxy \u0026quot;*\u0026quot; --insecure --location \\ --request GET 'https://my-oud-ds-rs-admin-0/rest/v1/admin/?scope=base\u0026amp;attributes=vendorName\u0026amp;attributes=vendorVersion\u0026amp;attributes=ds-private-naming-contexts\u0026amp;attributes=subschemaSubentry' \\ --header 'Content-Type: application/json' \\ --header 'Authorization: Basic \u0026lt;Base64 of userDN:userPassword\u0026gt;' | json_pp  | json_pp is used to format output in readable json format on the client side. It can be ignored if you do not have the json_pp library. Base64 of userDN:userPassword can be generated using echo -n \u0026quot;userDN:userPassword\u0026quot; | base64.  c) Command to invoke Admin REST API against Kubernetes NodePort for Ingress Controller Service\n$ curl --noproxy \u0026quot;*\u0026quot; --insecure --location \\ --request GET 'https://my-oud-ds-rs-admin-0:30443/rest/v1/admin/?scope=base\u0026amp;attributes=vendorName\u0026amp;attributes=vendorVersion\u0026amp;attributes=ds-private-naming-contexts\u0026amp;attributes=subschemaSubentry' \\ --header 'Content-Type: application/json' \\ --header 'Authorization: Basic \u0026lt;Base64 of userDN:userPassword\u0026gt;' | json_pp  | json_pp is used to format output in readable json format on the client side. It can be ignored if you do not have the json_pp library. Base64 of userDN:userPassword can be generated using echo -n \u0026quot;userDN:userPassword\u0026quot; | base64.  LDAP against External Load Balancer a) Command to perform ldapsearch against External LBR and LDAP port\n$ \u0026lt;OUD Home\u0026gt;/bin/ldapsearch --hostname \u0026lt;External LBR\u0026gt; --port 1389 \\ -D \u0026quot;\u0026lt;Root User DN\u0026gt;\u0026quot; -w \u0026lt;Password for Root User DN\u0026gt; \\ -b \u0026quot;\u0026quot; -s base \u0026quot;(objectClass=*)\u0026quot; \u0026quot;*\u0026quot; Output:\ndn: objectClass: top objectClass: ds-root-dse lastChangeNumber: 0 firstChangeNumber: 0 changelog: cn=changelog entryDN: pwdPolicySubentry: cn=Default Password Policy,cn=Password Policies,cn=config subschemaSubentry: cn=schema supportedAuthPasswordSchemes: SHA256 supportedAuthPasswordSchemes: SHA1 supportedAuthPasswordSchemes: SHA384 supportedAuthPasswordSchemes: SHA512 supportedAuthPasswordSchemes: MD5 numSubordinates: 1 supportedFeatures: 1.3.6.1.1.14 supportedFeatures: 1.3.6.1.4.1.4203.1.5.1 supportedFeatures: 1.3.6.1.4.1.4203.1.5.2 supportedFeatures: 1.3.6.1.4.1.4203.1.5.3 lastExternalChangelogCookie: vendorName: Oracle Corporation vendorVersion: Oracle Unified Directory 12.2.1.4.0 componentVersion: 4 releaseVersion: 1 platformVersion: 0 supportedLDAPVersion: 2 supportedLDAPVersion: 3 supportedControl: 1.2.826.0.1.3344810.2.3 supportedControl: 1.2.840.113556.1.4.1413 supportedControl: 1.2.840.113556.1.4.319 supportedControl: 1.2.840.113556.1.4.473 supportedControl: 1.2.840.113556.1.4.805 supportedControl: 1.3.6.1.1.12 supportedControl: 1.3.6.1.1.13.1 supportedControl: 1.3.6.1.1.13.2 supportedControl: 1.3.6.1.4.1.26027.1.5.2 supportedControl: 1.3.6.1.4.1.26027.1.5.4 supportedControl: 1.3.6.1.4.1.26027.1.5.5 supportedControl: 1.3.6.1.4.1.26027.1.5.6 supportedControl: 1.3.6.1.4.1.26027.2.3.1 supportedControl: 1.3.6.1.4.1.26027.2.3.2 supportedControl: 1.3.6.1.4.1.26027.2.3.4 supportedControl: 1.3.6.1.4.1.42.2.27.8.5.1 supportedControl: 1.3.6.1.4.1.42.2.27.9.5.2 supportedControl: 1.3.6.1.4.1.42.2.27.9.5.8 supportedControl: 1.3.6.1.4.1.4203.1.10.1 supportedControl: 1.3.6.1.4.1.4203.1.10.2 supportedControl: 2.16.840.1.113730.3.4.12 supportedControl: 2.16.840.1.113730.3.4.16 supportedControl: 2.16.840.1.113730.3.4.17 supportedControl: 2.16.840.1.113730.3.4.18 supportedControl: 2.16.840.1.113730.3.4.19 supportedControl: 2.16.840.1.113730.3.4.2 supportedControl: 2.16.840.1.113730.3.4.3 supportedControl: 2.16.840.1.113730.3.4.4 supportedControl: 2.16.840.1.113730.3.4.5 supportedControl: 2.16.840.1.113730.3.4.9 supportedControl: 2.16.840.1.113894.1.8.21 supportedControl: 2.16.840.1.113894.1.8.31 supportedControl: 2.16.840.1.113894.1.8.36 maintenanceVersion: 2 supportedSASLMechanisms: PLAIN supportedSASLMechanisms: EXTERNAL supportedSASLMechanisms: CRAM-MD5 supportedSASLMechanisms: DIGEST-MD5 majorVersion: 12 orclGUID: D41D8CD98F003204A9800998ECF8427E entryUUID: d41d8cd9-8f00-3204-a980-0998ecf8427e ds-private-naming-contexts: cn=schema hasSubordinates: true nsUniqueId: d41d8cd9-8f003204-a9800998-ecf8427e structuralObjectClass: ds-root-dse supportedExtension: 1.3.6.1.4.1.4203.1.11.1 supportedExtension: 1.3.6.1.4.1.4203.1.11.3 supportedExtension: 1.3.6.1.1.8 supportedExtension: 1.3.6.1.4.1.26027.1.6.3 supportedExtension: 1.3.6.1.4.1.26027.1.6.2 supportedExtension: 1.3.6.1.4.1.26027.1.6.1 supportedExtension: 1.3.6.1.4.1.1466.20037 namingContexts: cn=changelog namingContexts: dc=example,dc=com b) Command to perform ldapsearch against External LBR and LDAP port for specific Oracle Unified Directory Interface\n$ \u0026lt;OUD Home\u0026gt;/bin/ldapsearch --hostname \u0026lt;External LBR\u0026gt; --port 3890 \\ -D \u0026quot;\u0026lt;Root User DN\u0026gt;\u0026quot; -w \u0026lt;Password for Root User DN\u0026gt; \\ -b \u0026quot;\u0026quot; -s base \u0026quot;(objectClass=*)\u0026quot; \u0026quot;*\u0026quot; LDAPS against Kubernetes NodePort for Ingress Controller Service a) Command to perform ldapsearch against External LBR and LDAP port\n$ \u0026lt;OUD Home\u0026gt;/bin/ldapsearch --hostname \u0026lt;Kubernetes Node\u0026gt; --port 31636 \\ --useSSL --trustAll \\ -D \u0026quot;\u0026lt;Root User DN\u0026gt;\u0026quot; -w \u0026lt;Password for Root User DN\u0026gt; \\ -b \u0026quot;\u0026quot; -s base \u0026quot;(objectClass=*)\u0026quot; \u0026quot;*\u0026quot; Output:\ndn: objectClass: top objectClass: ds-root-dse lastChangeNumber: 0 firstChangeNumber: 0 changelog: cn=changelog entryDN: pwdPolicySubentry: cn=Default Password Policy,cn=Password Policies,cn=config subschemaSubentry: cn=schema supportedAuthPasswordSchemes: SHA256 supportedAuthPasswordSchemes: SHA1 supportedAuthPasswordSchemes: SHA384 supportedAuthPasswordSchemes: SHA512 supportedAuthPasswordSchemes: MD5 numSubordinates: 1 supportedFeatures: 1.3.6.1.1.14 supportedFeatures: 1.3.6.1.4.1.4203.1.5.1 supportedFeatures: 1.3.6.1.4.1.4203.1.5.2 supportedFeatures: 1.3.6.1.4.1.4203.1.5.3 lastExternalChangelogCookie: vendorName: Oracle Corporation vendorVersion: Oracle Unified Directory 12.2.1.4.0 componentVersion: 4 releaseVersion: 1 platformVersion: 0 supportedLDAPVersion: 2 supportedLDAPVersion: 3 supportedControl: 1.2.826.0.1.3344810.2.3 supportedControl: 1.2.840.113556.1.4.1413 supportedControl: 1.2.840.113556.1.4.319 supportedControl: 1.2.840.113556.1.4.473 supportedControl: 1.2.840.113556.1.4.805 supportedControl: 1.3.6.1.1.12 supportedControl: 1.3.6.1.1.13.1 supportedControl: 1.3.6.1.1.13.2 supportedControl: 1.3.6.1.4.1.26027.1.5.2 supportedControl: 1.3.6.1.4.1.26027.1.5.4 supportedControl: 1.3.6.1.4.1.26027.1.5.5 supportedControl: 1.3.6.1.4.1.26027.1.5.6 supportedControl: 1.3.6.1.4.1.26027.2.3.1 supportedControl: 1.3.6.1.4.1.26027.2.3.2 supportedControl: 1.3.6.1.4.1.26027.2.3.4 supportedControl: 1.3.6.1.4.1.42.2.27.8.5.1 supportedControl: 1.3.6.1.4.1.42.2.27.9.5.2 supportedControl: 1.3.6.1.4.1.42.2.27.9.5.8 supportedControl: 1.3.6.1.4.1.4203.1.10.1 supportedControl: 1.3.6.1.4.1.4203.1.10.2 supportedControl: 2.16.840.1.113730.3.4.12 supportedControl: 2.16.840.1.113730.3.4.16 supportedControl: 2.16.840.1.113730.3.4.17 supportedControl: 2.16.840.1.113730.3.4.18 supportedControl: 2.16.840.1.113730.3.4.19 supportedControl: 2.16.840.1.113730.3.4.2 supportedControl: 2.16.840.1.113730.3.4.3 supportedControl: 2.16.840.1.113730.3.4.4 supportedControl: 2.16.840.1.113730.3.4.5 supportedControl: 2.16.840.1.113730.3.4.9 supportedControl: 2.16.840.1.113894.1.8.21 supportedControl: 2.16.840.1.113894.1.8.31 supportedControl: 2.16.840.1.113894.1.8.36 maintenanceVersion: 2 supportedSASLMechanisms: PLAIN supportedSASLMechanisms: EXTERNAL supportedSASLMechanisms: CRAM-MD5 supportedSASLMechanisms: DIGEST-MD5 majorVersion: 12 orclGUID: D41D8CD98F003204A9800998ECF8427E entryUUID: d41d8cd9-8f00-3204-a980-0998ecf8427e ds-private-naming-contexts: cn=schema hasSubordinates: true nsUniqueId: d41d8cd9-8f003204-a9800998-ecf8427e structuralObjectClass: ds-root-dse supportedExtension: 1.3.6.1.4.1.4203.1.11.1 supportedExtension: 1.3.6.1.4.1.4203.1.11.3 supportedExtension: 1.3.6.1.1.8 supportedExtension: 1.3.6.1.4.1.26027.1.6.3 supportedExtension: 1.3.6.1.4.1.26027.1.6.2 supportedExtension: 1.3.6.1.4.1.26027.1.6.1 supportedExtension: 1.3.6.1.4.1.1466.20037 namingContexts: cn=changelog namingContexts: dc=example,dc=com b) Command to perform ldapsearch against External LBR and LDAP port for specific Oracle Unified Directory Interface\n$ \u0026lt;OUD Home\u0026gt;/bin/ldapsearch --hostname \u0026lt;Kubernetes Node\u0026gt; --port 30360 \\ --useSSL --trustAll \\ -D \u0026quot;\u0026lt;Root User DN\u0026gt;\u0026quot; -w \u0026lt;Password for Root User DN\u0026gt; \\ -b \u0026quot;\u0026quot; -s base \u0026quot;(objectClass=*)\u0026quot; \u0026quot;*\u0026quot; Configuration Parameters The following table lists the configurable parameters of the oud-ds-rs chart and its default values.\n   Parameter Description Default Value     replicaCount Number of DS+RS instances/pods/services to be created with replication enabled against a base Oracle Unified Directory instance/pod. 3   restartPolicyName restartPolicy to be configured for each POD containing Oracle Unified Directory instance OnFailure   image.repository Oracle Unified Directory Image Registry/Repository and name. Based on this, image parameter would be configured for Oracle Unified Directory pods/containers oracle/oud   image.tag Oracle Unified Directory Image Tag. Based on this, image parameter would be configured for Oracle Unified Directory pods/containers 12.2.1.4.0   image.pullPolicy policy to pull the image IfnotPresent   imagePullSecrets.name name of Secret resource containing private registry credentials regcred   nameOverride override the fullname with this name    fullnameOverride Overrides the fullname with the provided string    serviceAccount.create Specifies whether a service account should be created true   serviceAccount.name If not set and create is true, a name is generated using the fullname template oud-ds-rs-\u0026lt; fullname \u0026gt;-token-\u0026lt; randomalphanum \u0026gt;   podSecurityContext Security context policies to add to the controller pod    securityContext Security context policies to add by default    service.type type of controller service to create ClusterIP   nodeSelector node labels for pod assignment    tolerations node taints to tolerate    affinity node/pod affinities    ingress.enabled  true   ingress.type Supported value: either nginx or voyager nginx   ingress.nginx.http.host Hostname to be used with Ingress Rules. If not set, hostname would be configured according to fullname. Hosts would be configured as \u0026lt; fullname \u0026gt;-http.\u0026lt; domain \u0026gt;, \u0026lt; fullname \u0026gt;-http-0.\u0026lt; domain \u0026gt;, \u0026lt; fullname \u0026gt;-http-1.\u0026lt; domain \u0026gt;, etc.    ingress.nginx.http.domain Domain name to be used with Ingress Rules. In ingress rules, hosts would be configured as \u0026lt; host \u0026gt;.\u0026lt; domain \u0026gt;, \u0026lt; host \u0026gt;-0.\u0026lt; domain \u0026gt;, \u0026lt; host \u0026gt;-1.\u0026lt; domain \u0026gt;, etc.    ingress.nginx.http.backendPort  http   ingress.nginx.http.nginxAnnotations  { kubernetes.io/ingress.class: \u0026ldquo;nginx\u0026quot;}   ingress.nginx.admin.host Hostname to be used with Ingress Rules. If not set, hostname would be configured according to fullname. Hosts would be configured as \u0026lt; fullname \u0026gt;-admin.\u0026lt; domain \u0026gt;, \u0026lt; fullname \u0026gt;-admin-0.\u0026lt; domain \u0026gt;, \u0026lt; fullname \u0026gt;-admin-1.\u0026lt; domain \u0026gt;, etc.    ingress.nginx.admin.domain Domain name to be used with Ingress Rules. In ingress rules, hosts would be configured as \u0026lt; host \u0026gt;.\u0026lt; domain \u0026gt;, \u0026lt; host \u0026gt;-0.\u0026lt; domain \u0026gt;, \u0026lt; host \u0026gt;-1.\u0026lt; domain \u0026gt;, etc.    ingress.nginx.admin.nginxAnnotations  { kubernetes.io/ingress.class: \u0026ldquo;nginx\u0026rdquo; nginx.ingress.kubernetes.io/backend-protocol: \u0026ldquo;https\u0026quot;}   ingress.voyagerAnnotations  { kubernetes.io/ingress.class: \u0026ldquo;voyager\u0026rdquo; ingress.appscode.com/type: \u0026ldquo;NodePort\u0026rdquo; }   ingress.voyagerNodePortHttp NodePort value for HTTP Port exposed through Voyager LoadBalancer Service 30080   ingress.voyagerNodePortHttps NodePort value for HTTPS Port exposed through Voyager LoadBalancer Service 30443   ingress.voyagerHttpPort Port value for HTTP Port exposed through Voyager LoadBalancer Service 80   ingress.voyagerHttpsPort Port value for HTTPS Port exposed through Voyager LoadBalancer Service 443   ingress.ingress.tlsSecret Secret name to use an already created TLS Secret. If such secret is not provided, one would be created with name \u0026lt; fullname \u0026gt;-tls-cert. If the TLS Secret is in different namespace, name can be mentioned as \u0026lt; namespace \u0026gt;/\u0026lt; tlsSecretName \u0026gt;    ingress.certCN Subject\u0026rsquo;s common name (cn) for SelfSigned Cert. \u0026lt; fullname \u0026gt;   ingress.certValidityDays Validity of Self-Signed Cert in days 365   secret.enabled If enabled it will use the secret created with base64 encoding. if value is false, secret would not be used and input values (through \u0026ndash;set, \u0026ndash;values, etc.) would be used while creation of pods. true   secret.name secret name to use an already created Secret oud-ds-rs-\u0026lt; fullname \u0026gt;-creds   secret.type Specifies the type of the secret Opaque   persistence.enabled If enabled, it will use the persistent volume. if value is false, PV and PVC would not be used and pods would be using the default emptyDir mount volume. true   persistence.pvname pvname to use an already created Persistent Volume , If blank will use the default name oud-ds-rs-\u0026lt; fullname \u0026gt;-pv   persistence.pvcname pvcname to use an already created Persistent Volume Claim , If blank will use default name oud-ds-rs-\u0026lt; fullname \u0026gt;-pvc   persistence.type supported values: either filesystem or networkstorage or custom filesystem   persistence.filesystem.hostPath.path The path location mentioned should be created and accessible from the local host provided with necessary privileges for the user. /scratch/shared/oud_user_projects   persistence.networkstorage.nfs.path Path of NFS Share location /scratch/shared/oud_user_projects   persistence.networkstorage.nfs.server IP or hostname of NFS Server 0.0.0.0   persistence.custom.* Based on values/data, YAML content would be included in PersistenceVolume Object    persistence.accessMode Specifies the access mode of the location provided ReadWriteMany   persistence.size Specifies the size of the storage 10Gi   persistence.storageClass Specifies the storageclass of the persistence volume. empty   persistence.annotations specifies any annotations that will be used { }   configVolume.enabled If enabled, it will use the persistent volume. If value is false, PV and PVC would not be used and pods would be using the default emptyDir mount volume. true   configVolume.mountPath If enabled, it will use the persistent volume. If value is false, PV and PVC would not be used and there would not be any mount point available for config false   configVolume.pvname pvname to use an already created Persistent Volume , If blank will use the default name oud-ds-rs-\u0026lt; fullname \u0026gt;-pv-config   configVolume.pvcname pvcname to use an already created Persistent Volume Claim , If blank will use default name oud-ds-rs-\u0026lt; fullname \u0026gt;-pvc-config   configVolume.type supported values: either filesystem or networkstorage or custom filesystem   configVolume.filesystem.hostPath.path The path location mentioned should be created and accessible from the local host provided with necessary privileges for the user. /scratch/shared/oud_user_projects   configVolume.networkstorage.nfs.path Path of NFS Share location /scratch/shared/oud_config   configVolume.networkstorage.nfs.server IP or hostname of NFS Server 0.0.0.0   configVolume.custom.* Based on values/data, YAML content would be included in PersistenceVolume Object    configVolume.accessMode Specifies the access mode of the location provided ReadWriteMany   configVolume.size Specifies the size of the storage 10Gi   configVolume.storageClass Specifies the storageclass of the persistence volume. empty   configVolume.annotations specifies any annotations that will be used { }   oudPorts.adminldaps Port on which Oracle Unified Directory Instance in the container should listen for Administration Communication over LDAPS Protocol 1444   oudPorts.adminhttps Port on which Oracle Unified Directory Instance in the container should listen for Administration Communication over HTTPS Protocol. 1888   oudPorts.ldap Port on which Oracle Unified Directory Instance in the container should listen for LDAP Communication. 1389   oudPorts.ldaps Port on which Oracle Unified Directory Instance in the container should listen for LDAPS Communication. 1636   oudPorts.http Port on which Oracle Unified Directory Instance in the container should listen for HTTP Communication. 1080   oudPorts.https Port on which Oracle Unified Directory Instance in the container should listen for HTTPS Communication. 1081   oudPorts.replication Port value to be used while setting up replication server. 1898   oudConfig.baseDN BaseDN for Oracle Unified Directory Instances dc=example,dc=com   oudConfig.rootUserDN Root User DN for Oracle Unified Directory Instances cn=Directory Manager   oudConfig.rootUserPassword Password for Root User DN RandomAlphanum   oudConfig.sampleData To specify that the database should be populated with the specified number of sample entries. 0   oudConfig.sleepBeforeConfig Based on the value for this parameter, initialization/configuration of each Oracle Unified Directory replica would be delayed. 120   oudConfig.adminUID AdminUID to be configured with each replicated Oracle Unified Directory instance admin   oudConfig.adminPassword Password for AdminUID. If the value is not passed, value of rootUserPassword would be used as password for AdminUID. rootUserPassword   baseOUD.envVarsConfigMap Reference to ConfigMap which can contain additional environment variables to be passed on to POD for Base Oracle Unified Directory Instance. Following are the environment variables which would not be honored from the ConfigMap. instanceType, sleepBeforeConfig, OUD_INSTANCE_NAME, hostname, baseDN, rootUserDN, rootUserPassword, adminConnectorPort, httpAdminConnectorPort, ldapPort, ldapsPort, httpPort, httpsPort, replicationPort, sampleData. -   baseOUD.envVars Environment variables in Yaml Map format. This is helpful when its requried to pass environment variables through \u0026ndash;values file. List of env variables which would not be honored from envVars map is same as list of env var names mentioned for envVarsConfigMap. -   replOUD.envVarsConfigMap Reference to ConfigMap which can contain additional environment variables to be passed on to PODs for Replicated Oracle Unified Directory Instances. Following are the environment variables which would not be honored from the ConfigMap. instanceType, sleepBeforeConfig, OUD_INSTANCE_NAME, hostname, baseDN, rootUserDN, rootUserPassword, adminConnectorPort, httpAdminConnectorPort, ldapPort, ldapsPort, httpPort, httpsPort, replicationPort, sampleData, sourceHost, sourceServerPorts, sourceAdminConnectorPort, sourceReplicationPort, dsreplication_1, dsreplication_2, dsreplication_3, dsreplication_4, post_dsreplication_dsconfig_1, post_dsreplication_dsconfig_2 -   replOUD.envVars Environment variables in Yaml Map format. This is helpful when its required to pass environment variables through \u0026ndash;values file. List of env variables which would not be honored from envVars map is same as list of env var names mentioned for envVarsConfigMap. -   replOUD.groupId Group ID to be used/configured with each Oracle Unified Directory instance in replicated topology. 1   elk.elasticsearch.enabled If enabled it will create the elastic search statefulset deployment false   elk.elasticsearch.image.repository Elastic Search Image name/Registry/Repository . Based on this elastic search instances will be created docker.elastic.co/elasticsearch/elasticsearch   elk.elasticsearch.image.tag Elastic Search Image tag .Based on this, image parameter would be configured for Elastic Search pods/instances 6.4.3   elk.elasticsearch.image.pullPolicy policy to pull the image IfnotPresent   elk.elasticsearch.esreplicas Number of Elastic search Instances will be created 3   elk.elasticsearch.minimumMasterNodes The value for discovery.zen.minimum_master_nodes. Should be set to (esreplicas / 2) + 1. 2   elk.elasticsearch.esJAVAOpts Java options for Elasticsearch. This is where you should configure the jvm heap size -Xms512m -Xmx512m   elk.elasticsearch.sysctlVmMaxMapCount Sets the sysctl vm.max_map_count needed for Elasticsearch 262144   elk.elasticsearch.resources.requests.cpu cpu resources requested for the elastic search 100m   elk.elasticsearch.resources.limits.cpu total cpu limits that are configures for the elastic search 1000m   elk.elasticsearch.esService.type Type of Service to be created for elastic search ClusterIP   elk.elasticsearch.esService.lbrtype Type of load balancer Service to be created for elastic search ClusterIP   elk.kibana.enabled If enabled it will create a kibana deployment false   elk.kibana.image.repository Kibana Image Registry/Repository and name. Based on this Kibana instance will be created docker.elastic.co/kibana/kibana   elk.kibana.image.tag Kibana Image tag. Based on this, Image parameter would be configured. 6.4.3   elk.kibana.image.pullPolicy policy to pull the image IfnotPresent   elk.kibana.kibanaReplicas Number of Kibana instances will be created 1   elk.kibana.service.tye Type of service to be created NodePort   elk.kibana.service.targetPort Port on which the kibana will be accessed 5601   elk.kibana.service.nodePort nodePort is the port on which kibana service will be accessed from outside 31119   elk.logstash.enabled If enabled it will create a logstash deployment false   elk.logstash.image.repository logstash Image Registry/Repository and name. Based on this logstash instance will be created logstash   elk.logstash.image.tag logstash Image tag. Based on this, Image parameter would be configured. 6.6.0   elk.logstash.image.pullPolicy policy to pull the image IfnotPresent   elk.logstash.containerPort Port on which the logstash container will be running 5044   elk.logstash.service.tye Type of service to be created NodePort   elk.logstash.service.targetPort Port on which the logstash will be accessed 9600   elk.logstash.service.nodePort nodePort is the port on which logstash service will be accessed from outside 32222   elk.logstash.logstashConfigMap Provide the configmap name which is already created with the logstash conf. if empty default logstash configmap will be created and used    elk.elkPorts.rest Port for REST 9200   elk.elkPorts.internode port used for communication between the nodes 9300   elk.busybox.image busy box image name. Used for initcontianers busybox   elk.elkVolume.enabled If enabled, it will use the persistent volume. if value is false, PV and pods would be using the default emptyDir mount volume. true   elk.elkVolume.pvname pvname to use an already created Persistent Volume , If blank will use the default name oud-ds-rs-\u0026lt; fullname \u0026gt;-espv   elk.elkVolume.type supported values: either filesystem or networkstorage or custom filesystem   elk.elkVolume.filesystem.hostPath.path The path location mentioned should be created and accessible from the local host provided with necessary privileges for the user. /scratch/shared/oud_elk/data   elk.elkVolume.networkstorage.nfs.path Path of NFS Share location /scratch/shared/oud_elk/data   elk.elkVolume.networkstorage.nfs.server IP or hostname of NFS Server 0.0.0.0   elk.elkVolume.custom.* Based on values/data, YAML content would be included in PersistenceVolume Object    elk.elkVolume.accessMode Specifies the access mode of the location provided ReadWriteMany   elk.elkVolume.size Specifies the size of the storage 20Gi   elk.elkVolume.storageClass Specifies the storageclass of the persistence volume. elk   elk.elkVolume.annotations specifies any annotations that will be used { }    "
},
{
	"uri": "/fmw-kubernetes/oudsm/create-oudsm-instances-helm/oudsm/",
	"title": "Helm Chart: oudsm: For deployment of Oracle Unified Directory Services Manager instances on Kubernetes",
	"tags": [],
	"description": "This document provides details of the oudsm Helm chart.",
	"content": " Introduction Create Kubernetes Namespace Deploy oudsm Helm Chart Verify the Installation Ingress Controller Setup  Ingress with NGINX Ingress with Voyager   Access to Interfaces through Ingress Configuration Parameters  Introduction This Helm chart provides for the deployment of replicated Oracle Unified Directory Services Manager instances on Kubernetes.\nBased on the configuration, this chart deploys the following objects in the specified namespace of a Kubernetes cluster.\n Service Account Secret Persistent Volume and Persistent Volume Claim Pod(s)/Container(s) for Oracle Unified Directory Services Manager Instances Services for interfaces exposed through Oracle Unified Directory Services Manager Instances Ingress configuration  Create Kubernetes Namespace Create a Kubernetes namespace to provide a scope for other objects such as pods and services that you create in the environment. To create your namespace issue the following command:\n$ kubectl create ns myhelmns namespace/myhelmns created Deploy oudsm Helm Chart Create/Deploy Oracle Unified Directory Services Manager instances along with Kubernetes objects in a specified namespace using the oudsm Helm Chart.\nThe deployment can be initiated by running the following Helm command with reference to the oudsm Helm Chart, along with configuration parameters according to your environment. Before deploying the Helm chart, the namespace should be created. Objects to be created by the Helm chart will be created inside the specified namespace.\n$ helm install --namespace \u0026lt;namespace\u0026gt; \\ \u0026lt;Configuration Parameters\u0026gt; \\ \u0026lt;deployment/release name\u0026gt; \\ \u0026lt;Helm Chart Path/Name\u0026gt; Configuration Parameters (override values in chart) can be passed on with --set arguments on the command line and/or with -f / --values arguments when referring to files.\nExamples Example where configuration parameters are passed with --set argument: $ helm install --namespace myhelmns \\ --set oudsm.adminUser=weblogic,oudsm.adminPass=Oracle123,persistence.filesystem.hostPath.path=/scratch/shared/oudsm_user_projects \\ my-oudsm oudsm  For more details about the helm command and parameters, please execute helm --help and helm install --help. In this example, it is assumed that the command is executed from the directory containing the \u0026lsquo;oudsm\u0026rsquo; helm chart directory (OracleUnifiedDirectorySM/kubernetes/helm/).  Example where configuration parameters are passed with --values argument: $ helm install --namespace myhelmns \\ --values oudsm-values-override.yaml \\ my-oudsm oudsm  For more details about the helm command and parameters, please execute helm --help and helm install --help. In this example, it is assumed that the command is executed from the directory containing the \u0026lsquo;oudsm\u0026rsquo; helm chart directory (OracleUnifiedDirectorySM/kubernetes/helm/). The --values argument passes a file path/name which overrides default values in the chart.  oudsm-values-override.yaml\noudsm: adminUser: weblogic adminPass: Oracle123 persistence: type: filesystem filesystem: hostPath: path: /scratch/shared/oudsm_user_projects Example to update/upgrade Helm Chart based deployment: $ helm upgrade --namespace myhelmns \\ --set oudsm.adminUser=weblogic,oudsm.adminPass=Oracle123,persistence.filesystem.hostPath.path=/scratch/shared/oudsm_user_projects,replicaCount=2 \\ my-oudsm oudsm  For more details about the helm command and parameters, please execute helm --help and helm install --help. In this example, it is assumed that the command is executed from the directory containing the \u0026lsquo;oudsm\u0026rsquo; helm chart directory (OracleUnifiedDirectorySM/kubernetes/helm/).  Example to apply new Oracle Unified Directory Services Manager patch through Helm Chart based deployment: In this example, we will apply PSU2020July-20200730 patch on earlier running Oracle Unified Directory Services Manager version. If we describe pod we will observe that the container is up with new version.\nWe have two ways to achieve our goal:\n$ helm upgrade --namespace myhelmns \\ --set image.repository=oracle/oudsm,image.tag=12.2.1.4.0-PSU2020July-20200730 \\ my-oudsm oudsm OR\n$ helm upgrade --namespace myhelmns \\ --values oudsm-values-override.yaml \\ my-oudsm oudsm  For more details about the helm command and parameters, please execute helm --help and helm install --help. In this example, it is assumed that the command is executed from the directory containing the \u0026lsquo;oudsm\u0026rsquo; helm chart directory (OracleUnifiedDirectorySM/kubernetes/helm/).  oudsm-values-override.yaml\nimage: repository: oracle/oudsm tag: 12.2.1.4.0-PSU2020July-20200730 Example for using NFS as PV Storage: $ helm install --namespace myhelmns \\ --values oudsm-values-override-nfs.yaml \\ my-oudsm oudsm  For more details about the helm command and parameters, please execute helm --help and helm install --help. In this example, it is assumed that the command is executed from the directory containing the \u0026lsquo;oudsm\u0026rsquo; helm chart directory (OracleUnifiedDirectorySM/kubernetes/helm/). The --values argument passes a file path/name which overrides values in the chart.  oudsm-values-override-nfs.yaml\noudsm: adminUser: weblogic adminPass: Oracle123 persistence: type: networkstorage networkstorage: nfs: path: /scratch/shared/oud_user_projects server: \u0026lt;NFS IP address\u0026gt; Example for using PV type of your choice: $ helm install --namespace myhelmns \\ --values oudsm-values-override-pv-custom.yaml \\ my-oudsm oudsm  For more details about the helm command and parameters, please execute helm --help and helm install --help. In this example, it is assumed that the command is executed from the directory containing the \u0026lsquo;oudsm\u0026rsquo; helm chart directory (OracleUnifiedDirectorySM/kubernetes/helm/). The --values argument passes a file path/name which overrides values in the chart.  oudsm-values-override-pv-custom.yaml\noudsm: adminUser: weblogic adminPass: Oracle123 persistence: type: custom custom: nfs: # Path of NFS Share location path: /scratch/shared/oudsm_user_projects # IP of NFS Server server: \u0026lt;NFS IP address\u0026gt;  Under custom:, the configuration of your choice can be specified. This configuration will be used \u0026lsquo;as-is\u0026rsquo; for the PersistentVolume object.\n Check Deployment Output for the helm install/upgrade command Ouput similar to the following is observed following successful execution of helm install/upgrade command.\nNAME: my-oudsm LAST DEPLOYED: Wed Oct 14 06:22:10 2020 NAMESPACE: myhelmns STATUS: deployed REVISION: 1 TEST SUITE: None Check for the status of objects created through oudsm helm chart Command:\n$ kubectl --namespace myhelmns get nodes,pod,service,secret,pv,pvc,ingress -o wide Output is similar to the following:\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/oudsm-1 1/1 Running 0 22h 10.244.0.19 100.102.51.238 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/oudsm-2 1/1 Running 0 22h 10.244.0.20 100.102.51.238 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service/oudsm-1 ClusterIP 10.96.108.200 \u0026lt;none\u0026gt; 7001/TCP,7002/TCP 22h app.kubernetes.io/instance=oudsm,app.kubernetes.io/name=oudsm,oudsm/instance=oudsm-1 service/oudsm-2 ClusterIP 10.96.96.12 \u0026lt;none\u0026gt; 7001/TCP,7002/TCP 22h app.kubernetes.io/instance=oudsm,app.kubernetes.io/name=oudsm,oudsm/instance=oudsm-2 service/oudsm-lbr ClusterIP 10.96.41.201 \u0026lt;none\u0026gt; 7001/TCP,7002/TCP 22h app.kubernetes.io/instance=oudsm,app.kubernetes.io/name=oudsm NAME TYPE DATA AGE secret/default-token-w4jft kubernetes.io/service-account-token 3 32d secret/oudsm-creds opaque 2 22h secret/oudsm-token-ksr4g kubernetes.io/service-account-token 3 22h secret/sh.helm.release.v1.oudsm.v1 helm.sh/release.v1 1 22h secret/sh.helm.release.v1.oudsm.v2 helm.sh/release.v1 1 21h secret/sh.helm.release.v1.oudsm.v3 helm.sh/release.v1 1 19h NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE VOLUMEMODE persistentvolume/oudsm-pv 30Gi RWX Retain Bound myoudsmns/oudsm-pvc manual 22h Filesystem NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE VOLUMEMODE persistentvolumeclaim/oudsm-pvc Bound oudsm-pv 30Gi RWX manual 22h Filesystem NAME HOSTS ADDRESS PORTS AGE ingress.extensions/oudsm-ingress-nginx oudsm-1,oudsm-2,oudsm + 1 more... 100.102.51.230 80 19h Kubernetes Objects Kubernetes objects created by the Helm chart are detailed in the table below:\n   Type Name Example Name Purpose     Service Account \u0026lt;deployment/release name\u0026gt; my-oudsm Kubernetes Service Account for the Helm Chart deployment   Secret \u0026lt;deployment/release name\u0026gt;-creds my-oudsm-creds Secret object for Oracle Unified Directory Services Manager related critical values like passwords   Persistent Volume \u0026lt;deployment/release name\u0026gt;-pv my-oudsm-pv Persistent Volume for user_projects mount.   Persistent Volume Claim \u0026lt;deployment/release name\u0026gt;-pvc my-oudsm-pvc Persistent Volume Claim for user_projects mount.   Pod \u0026lt;deployment/release name\u0026gt;-N my-oudsm-1, my-oudsm-2, \u0026hellip; Pod(s)/Container(s) for Oracle Unified Directory Services Manager Instances   Service \u0026lt;deployment/release name\u0026gt;-N my-oudsm-1, my-oudsm-2, \u0026hellip; Service(s) for HTTP and HTTPS interfaces from Oracle Unified Directory Services Manager instance \u0026lt;deployment/release name\u0026gt;-N   Ingress \u0026lt;deployment/release name\u0026gt;-ingress-nginx my-oudsm-ingress-nginx Ingress Rules for HTTP and HTTPS interfaces.     In the table above, the Example Name for each Object is based on the value \u0026lsquo;my-oudsm\u0026rsquo; as the deployment/release name for the Helm chart installation.  Verify the Installation Ingress Controller Setup There are two types of Ingress controllers supported by this Helm chart. In the sub-sections below, configuration steps for each Controller are described.\nBy default Ingress configuration only supports HTTP and HTTPS Ports/Communication. To allow LDAP and LDAPS communication over TCP, configuration is required at Ingress Controller/Implementation level.\nIngress with NGINX Nginx-ingress controller implementation can be deployed/installed in Kubernetes environment.\nAdd Repo reference to helm for retriving/installing Chart for nginx-ingress implementation. $ helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx Command helm install to install nginx-ingress related objects like pod, service, deployment, etc. $ helm install --namespace default \\ --values nginx-ingress-values-override.yaml \\ lbr-nginx ingress-nginx/ingress-nginx  For more details about the helm command and parameters, please execute helm --help and helm install --help. The --values argument passes a file path/name which overrides values in the chart.  nginx-ingress-values-override.yaml\ncontroller: admissionWebhooks: enabled: false extraArgs: # The secret referred to by this flag contains the default certificate to be used when accessing the catch-all server. # If this flag is not provided NGINX will use a self-signed certificate. # If the TLS Secret is in different namespace, name can be mentioned as \u0026lt;namespace\u0026gt;/\u0026lt;tlsSecretName\u0026gt; default-ssl-certificate=myhelmns/my-oudsm-tls-cert service: # controller service external IP addresses externalIPs: - \u0026lt; External IP Address \u0026gt; # To configure Ingress Controller Service as LoadBalancer type of Service # Based on the Kubernetes configuration, External LoadBalancer would be linked to the Ingress Controller Service type: LoadBalancer # Configuration for NodePort to be used for Ports exposed through Ingress # If NodePorts are not defined/configured, Node Port would be assigned automatically by Kubernetes # These NodePorts are helpful while accessing services directly through Ingress and without having External Load Balancer. nodePorts: # For HTTP Interface exposed through LoadBalancer/Ingress http: 30080 # For HTTPS Interface exposed through LoadBalancer/Ingress https: 30443 Ingress with Voyager Voyager ingress implementation can be deployed/installed in Kubernetes environment.\nAdd Repo reference to helm for retrieving/installing Chart for voyager implementation. $ helm repo add appscode https://charts.appscode.com/stable Command helm install to install Voyager related objects like pod, service, deployment, etc. $ helm install --namespace default \\ --set cloudProvider=baremetal \\ voyager-operator appscode/voyager  For more details about the helm command and parameters, please execute helm --help and helm install --help.  Access to Interfaces through Ingress With the helm chart, Ingress objects are also created according to configuration. Following are the rules configured in Ingress object(s) for access to Oracle Unified Directory Services Manager Interfaces through Ingress.\n   Port NodePort Host Example Hostname Path Backend Service:Port Example Service Name:Port     http/https 30080/30443 \u0026lt;deployment/release name\u0026gt;-N my-oudsm-N * \u0026lt;deployment/release name\u0026gt;-N:http my-oudsm-1:http   http/https 30080/30443 * * /oudsm/console \u0026lt;deployment/release name\u0026gt;-lbr:http my-oudsm-lbr:http     In the table above, the Example Name for each Object is based on the value \u0026lsquo;my-oudsm\u0026rsquo; as the deployment/release name for the Helm chart installation. NodePort mentioned in the table are according to Ingress configuration described in previous section. When an External LoadBalancer is not available/configured, Interfaces can be accessed through NodePort on the Kubernetes Node.  Changes in /etc/hosts to validate hostname based Ingress rules In case, its not possible for you to have LoadBalancer configuration updated to have host names added for Oracle Unified Directory Services Manager Interfaces, following kind of entries can be added in /etc/hosts files on host from where Oracle Unified Directory Services Manager interfaces would be accessed.\n\u0026lt;IP Address of External LBR or Kubernetes Node\u0026gt;\tmy-oudsm my-oudsm-1 my-oudsm-2 my-oudsm-N  In the table above, the Example Name for each Object is based on the value \u0026lsquo;my-oudsm\u0026rsquo; as the deployment/release name for the Helm chart installation. When an External LoadBalancer is not available/configured, Interfaces can be accessed through NodePort on the Kubernetes Node.  Configuration Parameters The following table lists the configurable parameters of the Oracle Unified Directory Services Manager chart and their default values.\n   Parameter Description Default Value     replicaCount Number of Oracle Unified Directory Services Manager instances/pods/services to be created 1   restartPolicyName restartPolicy to be configured for each POD containing Oracle Unified Directory Services Manager instance OnFailure   image.repository Oracle Unified Directory Services Manager Image Registry/Repository and name. Based on this, image parameter would be configured for Oracle Unified Directory Services Manager pods/containers oracle/oudsm   image.tag Oracle Unified Directory Services Manager Image Tag. Based on this, image parameter would be configured for Oracle Unified Directory Services Manager pods/containers 12.2.1.4.0   image.pullPolicy policy to pull the image IfnotPresent   imagePullSecrets.name name of Secret resource containing private registry credentials regcred   nameOverride override the fullname with this name    fullnameOverride Overrides the fullname with the provided string    serviceAccount.create Specifies whether a service account should be created true   serviceAccount.name If not set and create is true, a name is generated using the fullname template oudsm-\u0026lt; fullname \u0026gt;-token-\u0026lt; randomalphanum \u0026gt;   podSecurityContext Security context policies to add to the controller pod    securityContext Security context policies to add by default    service.type type of controller service to create ClusterIP   nodeSelector node labels for pod assignment    tolerations node taints to tolerate    affinity node/pod affinities    ingress.enabled  true   ingress.type Supported value: either nginx or voyager nginx   ingress.host Hostname to be used with Ingress Rules. If not set, hostname would be configured according to fullname. Hosts would be configured as \u0026lt; fullname \u0026gt;-http.\u0026lt; domain \u0026gt;, \u0026lt; fullname \u0026gt;-http-0.\u0026lt; domain \u0026gt;, \u0026lt; fullname \u0026gt;-http-1.\u0026lt; domain \u0026gt;, etc.    ingress.domain Domain name to be used with Ingress Rules. In ingress rules, hosts would be configured as \u0026lt; host \u0026gt;.\u0026lt; domain \u0026gt;, \u0026lt; host \u0026gt;-0.\u0026lt; domain \u0026gt;, \u0026lt; host \u0026gt;-1.\u0026lt; domain \u0026gt;, etc.    ingress.backendPort  http   ingress.nginxAnnotations  { kubernetes.io/ingress.class: \u0026ldquo;nginx\u0026quot;nginx.ingress.kubernetes.io/affinity-mode: \u0026ldquo;persistent\u0026rdquo; nginx.ingress.kubernetes.io/affinity: \u0026ldquo;cookie\u0026rdquo; }   ingress.voyagerAnnotations  { kubernetes.io/ingress.class: \u0026ldquo;voyager\u0026rdquo; ingress.appscode.com/affinity: \u0026ldquo;cookie\u0026rdquo; ingress.appscode.com/type: \u0026ldquo;NodePort\u0026rdquo; }   ingress.voyagerNodePortHttp NodePort value for HTTP Port exposed through Voyager LoadBalancer Service 30080   ingress.voyagerNodePortHttps NodePort value for HTTPS Port exposed through Voyager LoadBalancer Service 30443   ingress.voyagerHttpPort Port value for HTTP Port exposed through Voyager LoadBalancer Service 80   ingress.voyagerHttpsPort Port value for HTTPS Port exposed through Voyager LoadBalancer Service 443   ingress.ingress.tlsSecret Secret name to use an already created TLS Secret. If such secret is not provided, one would be created with name \u0026lt; fullname \u0026gt;-tls-cert. If the TLS Secret is in different namespace, name can be mentioned as \u0026lt; namespace \u0026gt;/\u0026lt; tlsSecretName \u0026gt;    ingress.certCN Subject\u0026rsquo;s common name (cn) for SelfSigned Cert. \u0026lt; fullname \u0026gt;   ingress.certValidityDays Validity of Self-Signed Cert in days 365   secret.enabled If enabled it will use the secret created with base64 encoding. if value is false, secret would not be used and input values (through \u0026ndash;set, \u0026ndash;values, etc.) would be used while creation of pods. true   secret.name secret name to use an already created Secret oudsm-\u0026lt; fullname \u0026gt;-creds   secret.type Specifies the type of the secret Opaque   persistence.enabled If enabled, it will use the persistent volume. if value is false, PV and PVC would not be used and pods would be using the default emptyDir mount volume. true   persistence.pvname pvname to use an already created Persistent Volume , If blank will use the default name oudsm-\u0026lt; fullname \u0026gt;-pv   persistence.pvcname pvcname to use an already created Persistent Volume Claim , If blank will use default name oudsm-\u0026lt; fullname \u0026gt;-pvc   persistence.type supported values: either filesystem or networkstorage or custom filesystem   persistence.filesystem.hostPath.path The path location mentioned should be created and accessible from the local host provided with necessary privileges for the user. /scratch/shared/oudsm_user_projects   persistence.networkstorage.nfs.path Path of NFS Share location /scratch/shared/oudsm_user_projects   persistence.networkstorage.nfs.server IP or hostname of NFS Server 0.0.0.0   persistence.custom.* Based on values/data, YAML content would be included in PersistenceVolume Object    persistence.accessMode Specifies the access mode of the location provided ReadWriteMany   persistence.size Specifies the size of the storage 10Gi   persistence.storageClass Specifies the storageclass of the persistence volume. empty   persistence.annotations specifies any annotations that will be used { }   ingress.voyagerNodePortHttp  31080   ingress.voyagerNodePortHttps  31443   ingress.voyagerHttpPort  80   ingress.voyagerHttpsPort  443   oudsm.adminUser Weblogic Administration User weblogic   oudsm.adminPass Password for Weblogic Administration User    oudsm.startupTime Expected startup time. After specified seconds readinessProbe would start 900   oudsm.livenessProbeInitialDelay Paramter to decide livenessProbe initialDelaySeconds 1200   elk.elasticsearch.enabled If enabled it will create the elastic search statefulset deployment false   elk.elasticsearch.image.repository Elastic Search Image name/Registry/Repository . Based on this elastic search instances will be created docker.elastic.co/elasticsearch/elasticsearch   elk.elasticsearch.image.tag Elastic Search Image tag .Based on this, image parameter would be configured for Elastic Search pods/instances 6.4.3   elk.elasticsearch.image.pullPolicy policy to pull the image IfnotPresent   elk.elasticsearch.esreplicas Number of Elastic search Instances will be created 3   elk.elasticsearch.minimumMasterNodes The value for discovery.zen.minimum_master_nodes. Should be set to (esreplicas / 2) + 1. 2   elk.elasticsearch.esJAVAOpts Java options for Elasticsearch. This is where you should configure the jvm heap size -Xms512m -Xmx512m   elk.elasticsearch.sysctlVmMaxMapCount Sets the sysctl vm.max_map_count needed for Elasticsearch 262144   elk.elasticsearch.resources.requests.cpu cpu resources requested for the elastic search 100m   elk.elasticsearch.resources.limits.cpu total cpu limits that are configures for the elastic search 1000m   elk.elasticsearch.esService.type Type of Service to be created for elastic search ClusterIP   elk.elasticsearch.esService.lbrtype Type of load balancer Service to be created for elastic search ClusterIP   elk.kibana.enabled If enabled it will create a kibana deployment false   elk.kibana.image.repository Kibana Image Registry/Repository and name. Based on this Kibana instance will be created docker.elastic.co/kibana/kibana   elk.kibana.image.tag Kibana Image tag. Based on this, Image parameter would be configured. 6.4.3   elk.kibana.image.pullPolicy policy to pull the image IfnotPresent   elk.kibana.kibanaReplicas Number of Kibana instances will be created 1   elk.kibana.service.tye Type of service to be created NodePort   elk.kibana.service.targetPort Port on which the kibana will be accessed 5601   elk.kibana.service.nodePort nodePort is the port on which kibana service will be accessed from outside 31119   elk.logstash.enabled If enabled it will create a logstash deployment false   elk.logstash.image.repository logstash Image Registry/Repository and name. Based on this logstash instance will be created logstash   elk.logstash.image.tag logstash Image tag. Based on this, Image parameter would be configured. 6.6.0   elk.logstash.image.pullPolicy policy to pull the image IfnotPresent   elk.logstash.containerPort Port on which the logstash container will be running 5044   elk.logstash.service.tye Type of service to be created NodePort   elk.logstash.service.targetPort Port on which the logstash will be accessed 9600   elk.logstash.service.nodePort nodePort is the port on which logstash service will be accessed from outside 32222   elk.logstash.logstashConfigMap Provide the configmap name which is already created with the logstash conf. if empty default logstash configmap will be created and used    elk.elkPorts.rest Port for REST 9200   elk.elkPorts.internode port used for communication between the nodes 9300   elk.busybox.image busy box image name. Used for initcontianers busybox   elk.elkVolume.enabled If enabled, it will use the persistent volume. if value is false, PV and pods would be using the default emptyDir mount volume. true   elk.elkVolume.pvname pvname to use an already created Persistent Volume , If blank will use the default name oudsm-\u0026lt; fullname \u0026gt;-espv   elk.elkVolume.type supported values: either filesystem or networkstorage or custom filesystem   elk.elkVolume.filesystem.hostPath.path The path location mentioned should be created and accessible from the local host provided with necessary privileges for the user. /scratch/shared/oud_elk/data   elk.elkVolume.networkstorage.nfs.path Path of NFS Share location /scratch/shared/oud_elk/data   elk.elkVolume.networkstorage.nfs.server IP or hostname of NFS Server 0.0.0.0   elk.elkVolume.custom.* Based on values/data, YAML content would be included in PersistenceVolume Object    elk.elkVolume.accessMode Specifies the access mode of the location provided ReadWriteMany   elk.elkVolume.size Specifies the size of the storage 20Gi   elk.elkVolume.storageClass Specifies the storageclass of the persistence volume. elk   elk.elkVolume.annotations specifies any annotations that will be used { }    "
},
{
	"uri": "/fmw-kubernetes/oud/manage-oud-containers/logging-and-visualization/",
	"title": "a) Logging and Visualization for Helm Chart oud-ds-rs Deployment",
	"tags": [],
	"description": "Describes the steps for logging and visualization with Elasticsearch and Kibana.",
	"content": " Introduction Installation  Enable Elasticsearch, Logstash, and Kibana Create Data Mount Points Configure Logstash Install or Upgrade Oracle Unified Directory Container with ELK Configuration Configure ElasticSearch   Verify Using the Kibana Application  Introduction This section describes how to install and configure logging and visualization for the oud-ds-rs Helm Chart deployment.\nThe ELK stack consists of Elasticsearch, Logstash, and Kibana. Using ELK we can gain insights in real-time from the log data from your applications.\n Elasticsearch is a distributed, RESTful search and analytics engine capable of solving a growing number of use cases. As the heart of the Elastic Stack, it centrally stores your data so you can discover the expected and uncover the unexpected. Logstash is an open source, server-side data processing pipeline that ingests data from a multitude of sources simultaneously, transforms it, and then sends it to your favorite “stash.” Kibana lets you visualize your Elasticsearch data and navigate the Elastic Stack. It gives you the freedom to select the way you give shape to your data. And you don’t always have to know what you\u0026rsquo;re looking for.  Installation ELK can be enabled for environments created using the Helm charts provided with this project. The example below will demonstrate installation and configuration of ELK for the oud-ds-rs chart.\nEnable Elasticsearch, Logstash, and Kibana Edit logging-override-values.yaml and set the enabled flag for each component to \u0026lsquo;true\u0026rsquo;.\nelk: elasticsearch: enabled: true ... kibana: enabled: true ... logstash: enabled: true ... elkVolume: # If enabled, it will use the persistent volume. # if value is false, PV and PVC would not be used and there would not be any mount point available for config enabled: true type: networkstorage networkstorage: nfs: server: myserver path: /scratch/oud_elk_data Note: if elkVolume.enabled is set to \u0026lsquo;true\u0026rsquo; you should supply a directory for the ELK log files. The userid for the directory can be anything but it must have uid:guid as 1000:1000, which is the same as the ‘oracle’ user running in the container. This ensures the ‘oracle’ user has access to the shared volume/directory.\nInstall or Upgrade Oracle Unified Directory Container with ELK Configuration If you have not installed the oud-ds-rs chart then you should install with the following command, picking up the ELK configuration from the previous steps:\n$ helm install --namespace \u0026lt;namespace\u0026gt; --values \u0026lt;valuesfile.yaml\u0026gt; \u0026lt;releasename\u0026gt; oud-ds-rs For example:\n$ helm install --namespace myhelmns --values logging-override-values.yaml my-oud-ds-rs oud-ds-rs If the oud-ds-rs chart is already installed then update the configuration with the ELK configuration from the previous steps:\n$ helm upgrade --namespace \u0026lt;namespace\u0026gt; --values \u0026lt;valuesfile.yaml\u0026gt; \u0026lt;releasename\u0026gt; oud-ds-rs For example:\n$ helm upgrade --namespace myhelmns --values logging-override-values.yaml my-oud-ds-rs oud-ds-rs Configure ElasticSearch List the PODs in your namespace:\n$ kubectl get pods -o wide -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl get pods -o wide -n myhelmns Output will be similar to the following:\n$ kubectl get pods -o wide -n myhelmns NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES my-oud-ds-rs-0 1/1 Running 0 39m 10.244.1.107 10.89.73.203 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; my-oud-ds-rs-1 1/1 Running 0 39m 10.244.1.108 10.89.73.203 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; my-oud-ds-rs-2 1/1 Running 0 39m 10.244.1.106 10.89.73.203 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; my-oud-ds-rs-es-cluster-0 1/1 Running 0 39m 10.244.1.109 10.89.73.203 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; my-oud-ds-rs-kibana-665f9d5fb-pmz4v 1/1 Running 0 39m 10.244.1.110 10.89.73.203 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; my-oud-ds-rs-logstash-756fd7c5f5-kvwrw 1/1 Running 0 39m 10.244.2.103 10.89.73.204 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; From this, identify the ElastiSearch POD, my-oud-ds-rs-es-cluster-0.\nRun the port-forward command to allow ElasticSearch to be listening on port 9200:\n$ kubectl port-forward oud-ds-rs-es-cluster-0 9200:9200 --namespace=\u0026lt;namespace\u0026gt; \u0026amp; For example:\n$ kubectl port-forward my-oud-ds-rs-es-cluster-0 9200:9200 --namespace=myhelmns \u0026amp; [1] 98458 bash-4.2$ Forwarding from 127.0.0.1:9200 -\u0026gt; 9200 Forwarding from [::1]:9200 -\u0026gt; 9200 Verify that ElasticSearch is running by interrogating port 9200:\n$ curl http://localhost:9200 Handling connection for 9200 { \u0026quot;name\u0026quot; : \u0026quot;mike-oud-ds-rs-es-cluster-0\u0026quot;, \u0026quot;cluster_name\u0026quot; : \u0026quot;OUD-elk\u0026quot;, \u0026quot;cluster_uuid\u0026quot; : \u0026quot;H2EBtAlJQUGpV6IkS46Yzw\u0026quot;, \u0026quot;version\u0026quot; : { \u0026quot;number\u0026quot; : \u0026quot;6.4.3\u0026quot;, \u0026quot;build_flavor\u0026quot; : \u0026quot;default\u0026quot;, \u0026quot;build_type\u0026quot; : \u0026quot;tar\u0026quot;, \u0026quot;build_hash\u0026quot; : \u0026quot;fe40335\u0026quot;, \u0026quot;build_date\u0026quot; : \u0026quot;2018-10-30T23:17:19.084789Z\u0026quot;, \u0026quot;build_snapshot\u0026quot; : false, \u0026quot;lucene_version\u0026quot; : \u0026quot;7.4.0\u0026quot;, \u0026quot;minimum_wire_compatibility_version\u0026quot; : \u0026quot;5.6.0\u0026quot;, \u0026quot;minimum_index_compatibility_version\u0026quot; : \u0026quot;5.0.0\u0026quot; }, \u0026quot;tagline\u0026quot; : \u0026quot;You Know, for Search\u0026quot; } Verify Using the Kibana Application List the Kibana application service using the following command:\n$ kubectl get svc -o wide -n \u0026lt;namespace\u0026gt; | grep kibana For example:\n$ kubectl get svc -o wide -n myhelmns | grep kibana Output will be similar to the following:\nmy-oud-ds-rs-kibana NodePort 10.103.169.218 \u0026lt;none\u0026gt; 5601:31199/TCP 67m app=kibana In this example, the port to access Kibana application via a Web browser will be 31199.\nEnter the following URL in a browser to access the Kibana application:\nhttp://\u0026lt;hostname\u0026gt;:\u0026lt;NodePort\u0026gt;/app/kibana\nFor example:\nhttp://myserver:31199/app/kibana\nFrom the Kibana Portal navigate to:\nManagement -\u0026gt; Index Patterns\nCreate an Index Pattern using the pattern \u0026lsquo;*\u0026rsquo;\nNavigate to Discover : from here you should be able to see logs from the Oracle Unified Directory environment.\n"
},
{
	"uri": "/fmw-kubernetes/oudsm/manage-oudsm-containers/logging-and-visualization/",
	"title": "a) Logging and Visualization for Helm Chart oudsm Deployment",
	"tags": [],
	"description": "Describes the steps for logging and visualization with Elasticsearch and Kibana.",
	"content": " Introduction Installation  Enable Elasticsearch, Logstash, and Kibana Create Data Mount Points Configure Logstash Install or Upgrade Oracle Unified Directory Services Manager Container with ELK Configuration Configure ElasticSearch   Verify Using the Kibana Application  Introduction This section describes how to install and configure logging and visualization for the oudsm Helm Chart deployment.\nThe ELK stack consists of Elasticsearch, Logstash, and Kibana. Using ELK we can gain insights in real-time from the log data from your applications.\n Elasticsearch is a distributed, RESTful search and analytics engine capable of solving a growing number of use cases. As the heart of the Elastic Stack, it centrally stores your data so you can discover the expected and uncover the unexpected. Logstash is an open source, server-side data processing pipeline that ingests data from a multitude of sources simultaneously, transforms it, and then sends it to your favorite “stash.” Kibana lets you visualize your Elasticsearch data and navigate the Elastic Stack. It gives you the freedom to select the way you give shape to your data. And you don’t always have to know what you\u0026rsquo;re looking for.  Installation ELK can be enabled for environments created using the Helm charts provided with this project. The example below will demonstrate installation and configuration of ELK for the oudsm chart.\nEdit logging-override-values.yaml and set the enabled flag for each component to \u0026lsquo;true\u0026rsquo;.\nelk: elasticsearch: enabled: true ... kibana: enabled: true ... logstash: enabled: true ... elkVolume: # If enabled, it will use the persistent volume. # if value is false, PV and PVC would not be used and there would not be any mount point available for config enabled: true type: networkstorage networkstorage: nfs: server: myserver path: /scratch/oud_elk_data Note: If elkVolume.enabled is set to \u0026lsquo;true\u0026rsquo; you should supply a directory for the ELK log files. The userid for the directory can be anything but it must have uid:guid as 1000:1000, which is the same as the ‘oracle’ user running in the container. This ensures the ‘oracle’ user has access to the shared volume/directory.\nInstall or Upgrade Oracle Unified Directory Services Manager Container with ELK Configuration If you have not installed the oudsm chart then you should install with the following command, picking up the ELK configuration from the previous steps:\n$ helm install --namespace \u0026lt;namespace\u0026gt; --values \u0026lt;valuesfile.yaml\u0026gt; \u0026lt;releasename\u0026gt; oudsm For example:\n$ helm install --namespace myhelmns --values logging-override-values.yaml my-oud-ds-rs oudsm If the oudsm chart is already installed then update the configuration with the ELK configuration from the previous steps:\n$ helm upgrade --namespace \u0026lt;namespace\u0026gt; --values \u0026lt;valuesfile.yaml\u0026gt; \u0026lt;releasename\u0026gt; oudsm For example:\n$ helm upgrade --namespace myhelmns --values logging-override-values.yaml my-oud-ds-rs oudsm Configure ElasticSearch List the PODs in your namespace:\n$ kubectl get pods -o wide -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl get pods -o wide -n myhelmns Output will be similar to the following:\n$ kubectl get pods -o wide -n myhelmns NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES my-oudsm-1 1/1 Running 0 19m 10.244.1.66 10.89.73.203 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; my-oudsm-es-cluster-0 1/1 Running 0 19m 10.244.1.69 10.89.73.203 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; my-oudsm-es-cluster-1 1/1 Running 0 18m 10.244.2.125 10.89.73.204 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; my-oudsm-es-cluster-2 1/1 Running 0 17m 10.244.1.70 10.89.73.203 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; my-oudsm-kibana-6bbd487d66-dr662 1/1 Running 0 19m 10.244.1.68 10.89.73.203 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; my-oudsm-logstash-56f4665997-vbx4q 1/1 Running 0 19m 10.244.1.67 10.89.73.203 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; From this, identify the ElastiSearch POD, my-oudsm-es-cluster-0.\nRun the port-forward command to allow ElasticSearch to be listening on port 9200:\n$ kubectl port-forward my-oudsm-es-cluster-0 9200:9200 --namespace=\u0026lt;namespace\u0026gt; \u0026amp; For example:\n$ kubectl port-forward my-oudsm-es-cluster-0 9200:9200 --namespace=myhelmns \u0026amp; [1] 98458 bash-4.2$ Forwarding from 127.0.0.1:9200 -\u0026gt; 9200 Forwarding from [::1]:9200 -\u0026gt; 9200 Verify that ElasticSearch is running by interrogating port 9200:\n$ curl http://localhost:9200 Handling connection for 9200 { \u0026quot;name\u0026quot; : \u0026quot;my-oudsm-es-cluster-0\u0026quot;, \u0026quot;cluster_name\u0026quot; : \u0026quot;OUD-elk\u0026quot;, \u0026quot;cluster_uuid\u0026quot; : \u0026quot;w5LKK98RRp-LMoCGA2AnsA\u0026quot;, \u0026quot;version\u0026quot; : { \u0026quot;number\u0026quot; : \u0026quot;6.4.3\u0026quot;, \u0026quot;build_flavor\u0026quot; : \u0026quot;default\u0026quot;, \u0026quot;build_type\u0026quot; : \u0026quot;tar\u0026quot;, \u0026quot;build_hash\u0026quot; : \u0026quot;fe40335\u0026quot;, \u0026quot;build_date\u0026quot; : \u0026quot;2018-10-30T23:17:19.084789Z\u0026quot;, \u0026quot;build_snapshot\u0026quot; : false, \u0026quot;lucene_version\u0026quot; : \u0026quot;7.4.0\u0026quot;, \u0026quot;minimum_wire_compatibility_version\u0026quot; : \u0026quot;5.6.0\u0026quot;, \u0026quot;minimum_index_compatibility_version\u0026quot; : \u0026quot;5.0.0\u0026quot; }, \u0026quot;tagline\u0026quot; : \u0026quot;You Know, for Search\u0026quot; } Verify Using the Kibana Application List the Kibana application service using the following command:\n$ kubectl get svc -o wide -n \u0026lt;namespace\u0026gt; | grep kibana For example:\n$ kubectl get svc -o wide -n myhelmns | grep kibana Output will be similar to the following:\nmy-oudsm-kibana NodePort 10.103.92.84 \u0026lt;none\u0026gt; 5601:31199/TCP 21m app=kibana In this example, the port to access Kibana application via a Web browser will be 31199.\nEnter the following URL in a browser to access the Kibana application:\nhttp://\u0026lt;hostname\u0026gt;:\u0026lt;NodePort\u0026gt;/app/kibana\nFor example:\nhttp://myserver:31199/app/kibana\nFrom the Kibana Portal navigate to:\nManagement -\u0026gt; Index Patterns\nCreate an Index Pattern using the pattern \u0026lsquo;*\u0026rsquo;\nNavigate to Discover : from here you should be able to see logs from the Oracle Unified Directory Services Manager environment.\n"
},
{
	"uri": "/fmw-kubernetes/oud/manage-oud-containers/monitoring-oud-instance/",
	"title": "b) Monitoring an Oracle Unified Directory Instance",
	"tags": [],
	"description": "Describes the steps for Monitoring the Oracle Unified Directory environment.",
	"content": " Introduction Install Prometheus and Grafana  Create a Kubernetes Namespace Add Prometheus and Grafana Helm Repositories Install the Prometheus Operator View Prometheus and Grafana Objects Created Add the NodePort   Verify Using Grafana GUI  Introduction After the Oracle Unified Directory instance is set up you can monitor it using Prometheus and Grafana.\nInstall Prometheus and Grafana Create a Kubernetes Namespace Create a Kubernetes namespace to provide a scope for Prometheus and Grafana objects such as pods and services that you create in the environment. To create your namespace issue the following command:\n$ kubectl create ns mypgns namespace/mypgns created Add Prometheus and Grafana Helm Repositories Add the Prometheus and Grafana Helm repositories by issuing the following commands:\n$ helm repo add prometheus https://prometheus-community.github.io/helm-charts \u0026quot;prometheus\u0026quot; has been added to your repositories $ helm repo add stable https://kubernetes-charts.storage.googleapis.com/ \u0026quot;stable\u0026quot; has been added to your repositories $ helm repo update Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \u0026quot;prometheus\u0026quot; chart repository ...Successfully got an update from the \u0026quot;stable\u0026quot; chart repository Update Complete. Happy Helming! $ Install the Prometheus Operator Install the Prometheus Operator using the helm command:\n$ helm install \u0026lt;release_name\u0026gt; prometheus/kube-prometheus-stack grafana.adminPassword=\u0026lt;password\u0026gt; -n \u0026lt;namespace\u0026gt; For example:\n$ helm install mypg prometheus/kube-prometheus-stack grafana.adminPassword=\u0026lt;password\u0026gt; -n mypgns Output should be similar to the following:\nNAME: mypg LAST DEPLOYED: Mon Oct 12 02:05:41 2020 NAMESPACE: mypgns STATUS: deployed REVISION: 1 NOTES: kube-prometheus-stack has been installed. Check its status by running: kubectl --namespace mypgns get pods -l \u0026quot;release=mypg\u0026quot; Visit https://github.com/prometheus-operator/kube-prometheus for instructions on how to create \u0026amp; configure Alertmanager and Prometheus instances using the Operator. View Prometheus and Grafana Objects Created View the objects created for Prometheus and Grafana by issuing the following command:\n$ kubectl get all,service,pod -o wide -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl get all,service,pod -o wide -n mypgns NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/alertmanager-mypg-kube-prometheus-stack-alertmanager-0 2/2 Running 0 25m 10.244.1.25 10.89.73.203 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/mypg-grafana-b7d4fbfb-jzccm 2/2 Running 0 25m 10.244.2.140 10.89.73.204 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/mypg-kube-prometheus-stack-operator-7fb485bbcd-lbh9d 2/2 Running 0 25m 10.244.2.139 10.89.73.204 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/mypg-kube-state-metrics-86dfdf9c75-nvbss 1/1 Running 0 25m 10.244.1.146 10.89.73.203 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/mypg-prometheus-node-exporter-29dzd 1/1 Running 0 25m 10.244.2.141 10.89.73.204 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/prometheus-mypg-kube-prometheus-stack-prometheus-0 3/3 Running 0 25m 10.244.2.140 10.89.73.203 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service/alertmanager-operated ClusterIP None \u0026lt;none\u0026gt; 9093/TCP,9094/TCP,9094/UDP 25m app=alertmanager service/mypg-grafana ClusterIP 10.111.28.76 \u0026lt;none\u0026gt; 80/TCP 25m app.kubernetes.io/instance=mypg,app.kubernetes.io/name=grafana service/mypg-kube-prometheus-stack-alertmanager ClusterIP 10.103.83.97 \u0026lt;none\u0026gt; 9093/TCP 25m alertmanager=mypg-kube-prometheus-stack-alertmanager,app=alertmanager service/mypg-kube-prometheus-stack-operator ClusterIP 10.110.216.204 \u0026lt;none\u0026gt; 8080/TCP,443/TCP 25m app=kube-prometheus-stack-operator,release=mypg service/mypg-kube-prometheus-stack-prometheus ClusterIP 10.104.11.9 \u0026lt;none\u0026gt; 9090/TCP 25m app=prometheus,prometheus=mypg-kube-prometheus-stack-prometheus service/mypg-kube-state-metrics ClusterIP 10.109.172.125 \u0026lt;none\u0026gt; 8080/TCP 25m app.kubernetes.io/instance=mypg,app.kubernetes.io/name=kube-state-metrics service/mypg-prometheus-node-exporter ClusterIP 10.110.249.92 \u0026lt;none\u0026gt; 9100/TCP 25m app=prometheus-node-exporter,release=mypg service/prometheus-operated ClusterIP None \u0026lt;none\u0026gt; 9090/TCP 25m app=prometheus NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE CONTAINERS IMAGES SELECTOR daemonset.apps/mypg-prometheus-node-exporter 3 3 0 3 0 \u0026lt;none\u0026gt; 25m node-exporter quay.io/prometheus/node-exporter:v1.0.1 app=prometheus-node-exporter,release=mypg NAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTOR deployment.apps/mypg-grafana 1/1 1 1 25m grafana-sc-dashboard,grafana kiwigrid/k8s-sidecar:0.1.209,grafana/grafana:7.2.0 app.kubernetes.io/instance=mypg,app.kubernetes.io/name=grafana deployment.apps/mypg-kube-prometheus-stack-operator 1/1 1 1 25m kube-prometheus-stack,tls-proxy quay.io/prometheus-operator/prometheus-operator:v0.42.1,squareup/ghostunnel:v1.5.2 app=kube-prometheus-stack-operator,release=mypg deployment.apps/mypg-kube-state-metrics 1/1 1 1 25m kube-state-metrics quay.io/coreos/kube-state-metrics:v1.9.7 app.kubernetes.io/name=kube-state-metrics NAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTOR replicaset.apps/mypg-grafana-b7d4fbfb 1 1 1 25m grafana-sc-dashboard,grafana kiwigrid/k8s-sidecar:0.1.209,grafana/grafana:7.2.0 app.kubernetes.io/instance=mypg,app.kubernetes.io/name=grafana,pod-template-hash=b7d4fbfb replicaset.apps/mypg-kube-prometheus-stack-operator-7fb485bbcd 1 1 1 25m kube-prometheus-stack,tls-proxy quay.io/prometheus-operator/prometheus-operator:v0.42.1,squareup/ghostunnel:v1.5.2 app=kube-prometheus-stack-operator,pod-template-hash=7fb485bbcd,release=mypg replicaset.apps/mypg-kube-state-metrics-86dfdf9c75 1 1 1 25m kube-state-metrics quay.io/coreos/kube-state-metrics:v1.9.7 app.kubernetes.io/name=kube-state-metrics,pod-template-hash=86dfdf9c75 NAME READY AGE CONTAINERS IMAGES statefulset.apps/alertmanager-mypg-kube-prometheus-stack-alertmanager 1/1 25m alertmanager,config-reloader quay.io/prometheus/alertmanager:v0.21.0,jimmidyson/configmap-reload:v0.4.0 statefulset.apps/prometheus-mypg-kube-prometheus-stack-prometheus 0/1 25m prometheus,prometheus-config-reloader,rules-configmap-reloader quay.io/prometheus/prometheus:v2.21.0,quay.io/prometheus-operator/prometheus-config-reloader:v0.42.1,docker.io/jimmidyson/configmap-reload:v0.4.0 Add the NodePort Edit the grafana service to add the NodePort in the service.nodeport=\u0026lt;nodeport\u0026gt; and type=NodePort and save:\n$ kubectl edit service/prometheus-grafana -n \u0026lt;namespace\u0026gt; ports: - name: service nodePort: 30091 port: 80 protocol: TCP targetPort: 3000 selector: app.kubernetes.io/instance: prometheus-operator app.kubernetes.io/name: grafana sessionAffinity: None type: NodePort Verify Using Grafana GUI Access the Grafana GUI using http://\u0026lt;HostIP\u0026gt;:\u0026lt;nodeport\u0026gt; with the default username=admin and password=grafana.adminPassword:\nCheck the Prometheus datasource from the DataSource pane:\nAdd the customized k8cluster view dashboard json to view the cluster monitoring dashboard, by importing the following json file.\nDownload the JSON file from monitoring a Kubernetes cluster using Prometheus from https://grafana.com/grafana/dashboards/10856. Import the downloaded json using the import option.\nVerify your installation by viewing some of the customized dashboard views.\n"
},
{
	"uri": "/fmw-kubernetes/oudsm/manage-oudsm-containers/monitoring-oudsm-instance/",
	"title": "b) Monitoring an Oracle Unified Directory Services Manager Instance",
	"tags": [],
	"description": "Describes the steps for Monitoring the Oracle Unified Directory Services Manager environment.",
	"content": " Introduction Install Prometheus and Grafana  Create a Kubernetes Namespace Add Prometheus and Grafana Helm Repositories Install the Prometheus Operator View Prometheus and Grafana Objects Created Add the NodePort   Verify Using Grafana GUI  Introduction After the Oracle Unified Directory Services Manager instance is set up you can monitor it using Prometheus and Grafana.\nInstall Prometheus and Grafana Create a Kubernetes Namespace Create a Kubernetes namespace to provide a scope for Prometheus and Grafana objects such as pods and services that you create in the environment. To create your namespace issue the following command:\n$ kubectl create ns mypgns namespace/mypgns created Add Prometheus and Grafana Helm Repositories Add the Prometheus and Grafana Helm repositories by issuing the following commands:\n$ helm repo add prometheus https://prometheus-community.github.io/helm-charts \u0026quot;prometheus\u0026quot; has been added to your repositories $ helm repo add stable https://kubernetes-charts.storage.googleapis.com/ \u0026quot;stable\u0026quot; has been added to your repositories $ helm repo update Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \u0026quot;prometheus\u0026quot; chart repository ...Successfully got an update from the \u0026quot;stable\u0026quot; chart repository Update Complete. Happy Helming! $ Install the Prometheus Operator Install the Prometheus Operator using the helm command:\n$ helm install \u0026lt;release_name\u0026gt; prometheus/kube-prometheus-stack grafana.adminPassword=\u0026lt;password\u0026gt; -n \u0026lt;namespace\u0026gt; For example:\n$ helm install mypg prometheus/kube-prometheus-stack grafana.adminPassword=\u0026lt;password\u0026gt; -n mypgns Output should be similar to the following:\nNAME: mypg LAST DEPLOYED: Mon Oct 12 02:05:41 2020 NAMESPACE: mypgns STATUS: deployed REVISION: 1 NOTES: kube-prometheus-stack has been installed. Check its status by running: kubectl --namespace mypgns get pods -l \u0026quot;release=mypg\u0026quot; Visit https://github.com/prometheus-operator/kube-prometheus for instructions on how to create \u0026amp; configure Alertmanager and Prometheus instances using the Operator. View Prometheus and Grafana Objects Created View the objects created for Prometheus and Grafana by issuing the following command:\n$ kubectl get all,service,pod -o wide -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl get all,service,pod -o wide -n mypgns NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/alertmanager-mypg-kube-prometheus-stack-alertmanager-0 2/2 Running 0 25m 10.244.1.25 10.89.73.203 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/mypg-grafana-b7d4fbfb-jzccm 2/2 Running 0 25m 10.244.2.140 10.89.73.204 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/mypg-kube-prometheus-stack-operator-7fb485bbcd-lbh9d 2/2 Running 0 25m 10.244.2.139 10.89.73.204 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/mypg-kube-state-metrics-86dfdf9c75-nvbss 1/1 Running 0 25m 10.244.1.146 10.89.73.203 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/mypg-prometheus-node-exporter-29dzd 1/1 Running 0 25m 10.244.2.141 10.89.73.204 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/prometheus-mypg-kube-prometheus-stack-prometheus-0 3/3 Running 0 25m 10.244.2.142 10.89.73.203 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service/alertmanager-operated ClusterIP None \u0026lt;none\u0026gt; 9093/TCP,9094/TCP,9094/UDP 25m app=alertmanager service/mypg-grafana ClusterIP 10.111.28.76 \u0026lt;none\u0026gt; 80/TCP 25m app.kubernetes.io/instance=mypg,app.kubernetes.io/name=grafana service/mypg-kube-prometheus-stack-alertmanager ClusterIP 10.103.83.97 \u0026lt;none\u0026gt; 9093/TCP 25m alertmanager=mypg-kube-prometheus-stack-alertmanager,app=alertmanager service/mypg-kube-prometheus-stack-operator ClusterIP 10.110.216.204 \u0026lt;none\u0026gt; 8080/TCP,443/TCP 25m app=kube-prometheus-stack-operator,release=mypg service/mypg-kube-prometheus-stack-prometheus ClusterIP 10.104.11.9 \u0026lt;none\u0026gt; 9090/TCP 25m app=prometheus,prometheus=mypg-kube-prometheus-stack-prometheus service/mypg-kube-state-metrics ClusterIP 10.109.172.125 \u0026lt;none\u0026gt; 8080/TCP 25m app.kubernetes.io/instance=mypg,app.kubernetes.io/name=kube-state-metrics service/mypg-prometheus-node-exporter ClusterIP 10.110.249.92 \u0026lt;none\u0026gt; 9100/TCP 25m app=prometheus-node-exporter,release=mypg service/prometheus-operated ClusterIP None \u0026lt;none\u0026gt; 9090/TCP 25m app=prometheus NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE CONTAINERS IMAGES SELECTOR daemonset.apps/mypg-prometheus-node-exporter 3 3 0 3 0 \u0026lt;none\u0026gt; 25m node-exporter quay.io/prometheus/node-exporter:v1.0.1 app=prometheus-node-exporter,release=mypg NAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTOR deployment.apps/mypg-grafana 1/1 1 1 25m grafana-sc-dashboard,grafana kiwigrid/k8s-sidecar:0.1.209,grafana/grafana:7.2.0 app.kubernetes.io/instance=mypg,app.kubernetes.io/name=grafana deployment.apps/mypg-kube-prometheus-stack-operator 1/1 1 1 25m kube-prometheus-stack,tls-proxy quay.io/prometheus-operator/prometheus-operator:v0.42.1,squareup/ghostunnel:v1.5.2 app=kube-prometheus-stack-operator,release=mypg deployment.apps/mypg-kube-state-metrics 1/1 1 1 25m kube-state-metrics quay.io/coreos/kube-state-metrics:v1.9.7 app.kubernetes.io/name=kube-state-metrics NAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTOR replicaset.apps/mypg-grafana-b7d4fbfb 1 1 1 25m grafana-sc-dashboard,grafana kiwigrid/k8s-sidecar:0.1.209,grafana/grafana:7.2.0 app.kubernetes.io/instance=mypg,app.kubernetes.io/name=grafana,pod-template-hash=b7d4fbfb replicaset.apps/mypg-kube-prometheus-stack-operator-7fb485bbcd 1 1 1 25m kube-prometheus-stack,tls-proxy quay.io/prometheus-operator/prometheus-operator:v0.42.1,squareup/ghostunnel:v1.5.2 app=kube-prometheus-stack-operator,pod-template-hash=7fb485bbcd,release=mypg replicaset.apps/mypg-kube-state-metrics-86dfdf9c75 1 1 1 25m kube-state-metrics quay.io/coreos/kube-state-metrics:v1.9.7 app.kubernetes.io/name=kube-state-metrics,pod-template-hash=86dfdf9c75 NAME READY AGE CONTAINERS IMAGES statefulset.apps/alertmanager-mypg-kube-prometheus-stack-alertmanager 1/1 25m alertmanager,config-reloader quay.io/prometheus/alertmanager:v0.21.0,jimmidyson/configmap-reload:v0.4.0 statefulset.apps/prometheus-mypg-kube-prometheus-stack-prometheus 0/1 25m prometheus,prometheus-config-reloader,rules-configmap-reloader quay.io/prometheus/prometheus:v2.21.0,quay.io/prometheus-operator/prometheus-config-reloader:v0.42.1,docker.io/jimmidyson/configmap-reload:v0.4.0 Add the NodePort Edit the grafana service to add the NodePort in the service.nodeport=\u0026lt;nodeport\u0026gt; and type=NodePort and save:\n$ kubectl edit service/prometheus-grafana -n \u0026lt;namespace\u0026gt; ports: - name: service nodePort: 30091 port: 80 protocol: TCP targetPort: 3000 selector: app.kubernetes.io/instance: prometheus-operator app.kubernetes.io/name: grafana sessionAffinity: None type: NodePort Verify Using Grafana GUI Access the Grafana GUI using http://\u0026lt;HostIP\u0026gt;:\u0026lt;nodeport\u0026gt; with the default username=admin and password=grafana.adminPassword:\nCheck the Prometheus datasource from the DataSource pane:\nAdd the customized k8cluster view dashboard json to view the cluster monitoring dashboard, by importing the following json file.\nDownload the JSON file from monitoring a Kubernetes cluster using Prometheus from https://grafana.com/grafana/dashboards/10856. Import the downloaded json using the import option.\nVerify your installation by viewing some of the customized dashboard views.\n"
},
{
	"uri": "/fmw-kubernetes/oud/patch-and-upgrade/upgrade_a_kubernetes_cluster/",
	"title": "a) Upgrade a Kubernetes cluster",
	"tags": [],
	"description": "Instructions on how to upgrade a Kubernetes cluster.",
	"content": "These instructions describe how to upgrade a Kubernetes cluster created using kubeadm on which an Oracle Unified Directory instance is deployed. A rolling upgrade approach is used to upgrade nodes (master and worker) of the Kubernetes cluster.\nIt is expected that there will be a down time during the upgrade of the Kubernetes cluster as the nodes need to be drained as part of the upgrade process.\n Prerequisites  Review Prerequisites and ensure that your Kubernetes cluster is ready for upgrade. Make sure your environment meets all prerequisites.  Upgrade the Kubernetes version An upgrade of Kubernetes is supported from one MINOR version to the next MINOR version, or between PATCH versions of the same MINOR. For example, you can upgrade from 1.x to 1.x+1, but not from 1.x to 1.x+2. To upgrade a Kubernetes version, first all the master nodes of the Kubernetes cluster must be upgraded sequentially, followed by the sequential upgrade of each worker node.\n See here for Kubernetes official documentation to upgrade from v1.16.x to v1.17.x. See here for Kubernetes official documentation to upgrade from v1.17.x to v1.18.x.  "
},
{
	"uri": "/fmw-kubernetes/oam/patch-and-upgrade/patch_an_image/",
	"title": "a. Patch an image",
	"tags": [],
	"description": "Instructions on how to update your OAM Kubernetes cluster with a new OAM Docker image.",
	"content": "To update your OAM Kubernetes cluster with a new OAM Docker image, first install the new Docker image on all nodes in your Kubernetes cluster.\nOnce the new image is installed, choose one of the following options to update your OAM kubernetes cluster to use the new image:\n Run the kubectl edit domain command Run the kubectl patch domain command  In all of the above cases, the Oracle WebLogic Server Kubernetes Operator will restart the Administration Server pod first and then perform a rolling restart on the OAM Managed Servers.\nRun the kubectl edit domain command   To update the domain with the kubectl edit domain command, run the following:\n$ kubectl edit domain \u0026lt;domainname\u0026gt; -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl edit domain accessinfra -n accessns   Update the image tag to point at the new image, for example:\ndomainHomeInImage: false image: oracle/oam:12.2.1.4.0-new imagePullPolicy: IfNotPresent   Save the file and exit (:wq!)\n  Run the kubectl patch command   To update the domain with the kubectl patch domain command, run the following:\n$ kubectl patch domain \u0026lt;domain\u0026gt; -n \u0026lt;namespace\u0026gt; --type merge -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;image\u0026#34;:\u0026#34;newimage:tag\u0026#34;}}\u0026#39; For example:\n$ kubectl patch domain accessinfra -n accessns --type merge -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;image\u0026#34;:\u0026#34;oracle/oam:12.2.1.4-new\u0026#34;}}\u0026#39; The output will look similar to the following:\ndomain.weblogic.oracle/oimcluster patched   "
},
{
	"uri": "/fmw-kubernetes/oig/patch-and-upgrade/patch_an_image/",
	"title": "a. Patch an image",
	"tags": [],
	"description": "Instructions on how to update your OIG Kubernetes cluster with a new OIG docker image.",
	"content": "To update your OIG Kubernetes cluster with a new OIG Docker image, first install the new Docker image on all nodes in your Kubernetes cluster.\nOnce the new image is installed choose one of the following options to update your OIG Kubernetes cluster to use the new image:\n Run the kubectl edit domain command Run the kubectl patch domain command  In all of the above cases, the Oracle WebLogic Kubernetes Operator will restart the Administration Server pod first and then perform a rolling restart on the OIG Managed Servers.\nRun the kubectl edit domain command   To update the domain with the kubectl edit domain command, run the following:\n$ kubectl edit domain \u0026lt;domainname\u0026gt; -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl edit domain oimcluster -n oimcluster   Update the image tag to point at the new image, for example:\ndomainHomeInImage: false image: oracle/oig:12.2.1.4.0-new imagePullPolicy: IfNotPresent   Save the file and exit (:wq!)\n  Run the kubectl patch command   To update the domain with the kubectl patch domain command, run the following:\n$ kubectl patch domain \u0026lt;domain\u0026gt; -n \u0026lt;namespace\u0026gt; --type merge -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;image\u0026#34;:\u0026#34;newimage:tag\u0026#34;}}\u0026#39; For example:\n$ kubectl patch domain oimcluster -n oimcluster --type merge -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;image\u0026#34;:\u0026#34;oracle/oig:12.2.1.4-new\u0026#34;}}\u0026#39; The output will look similar to the following:\ndomain.weblogic.oracle/oimcluster patched   "
},
{
	"uri": "/fmw-kubernetes/oig/post-install-config/set_oimfronendurl_using_mbeans/",
	"title": "a. Set OIMfrontendURL",
	"tags": [],
	"description": "Set the OIMfrontendURL in Oracle Enterprise Manager.",
	"content": "Set OIMFrontendURL using MBeans   Login to Oracle Enterprise Manager using the following URL:\nhttps://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/em\n  Click the Target Navigation icon in the top left of the screen and navigate to the following:\n Expand Identity and Access \u0026gt; Access \u0026gt; OIM \u0026gt; oim Right click the instance oim and select System MBean Browser Under Application Defined MBeans, navigate to oracle.iam, Server:oim_server1, Application:oim \u0026gt; XMLConfig \u0026gt; Config \u0026gt; XMLConfig.DiscoveryConfig \u0026gt; Discovery.    Enter a new value for the OimFrontEndURL attribute, in the format:\nhttp://\u0026lt;OIM-Cluster-Service-Name\u0026gt;:\u0026lt;Cluster-Service-Port\u0026gt;\nFor example:\nhttp://oimcluster-cluster-oim-cluster:14000\nThen click Apply.\nNote: To find the \u0026lt;OIM-Cluster-Service-Name\u0026gt; run the following command:\n$ kubectl -n oimcluster get svc Your output will look similar to this:\n$ kubectl -n oimcluster get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE oimcluster-adminserver ClusterIP None \u0026lt;none\u0026gt; 7001/TCP 6d23h oimcluster-cluster-oim-cluster ClusterIP 10.107.191.53 \u0026lt;none\u0026gt; 14000/TCP 6d23h oimcluster-cluster-soa-cluster ClusterIP 10.97.108.226 \u0026lt;none\u0026gt; 8001/TCP 6d23h oimcluster-oim-server1 ClusterIP None \u0026lt;none\u0026gt; 14000/TCP 6d23h oimcluster-oim-server2 ClusterIP 10.96.147.43 \u0026lt;none\u0026gt; 14000/TCP 6d23h oimcluster-oim-server3 ClusterIP 10.103.65.77 \u0026lt;none\u0026gt; 14000/TCP 6d23h oimcluster-oim-server4 ClusterIP 10.98.157.253 \u0026lt;none\u0026gt; 14000/TCP 6d23h oimcluster-oim-server5 ClusterIP 10.102.19.32 \u0026lt;none\u0026gt; 14000/TCP 6d23h oimcluster-soa-server1 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP 6d23h oimcluster-soa-server2 ClusterIP 10.96.73.62 \u0026lt;none\u0026gt; 8001/TCP 6d23h oimcluster-soa-server3 ClusterIP 10.105.198.83 \u0026lt;none\u0026gt; 8001/TCP 6d23h oimcluster-soa-server4 ClusterIP 10.98.171.18 \u0026lt;none\u0026gt; 8001/TCP 6d23h oimcluster-soa-server5 ClusterIP 10.105.196.107 \u0026lt;none\u0026gt; 8001/TCP 6d23h   "
},
{
	"uri": "/fmw-kubernetes/oam/configure-ingress/ingress-nginx-setup-for-oam-domain-setup-on-k8s/",
	"title": "a. Using an Ingress with NGINX",
	"tags": [],
	"description": "Steps to set up an Ingress for NGINX to direct traffic to the OAM domain.",
	"content": "Setting up an ingress for NGINX for the OAM Domain The instructions below explain how to set up NGINX as an ingress for the OAM domain with SSL termination.\nNote: All the steps below should be performed on the master node.\n Generate a SSL Certificate Install NGINX Create an Ingress for the Domain Verify that You can Access the Domain URL  Generate a SSL Certificate   Generate a private key and certificate signing request (CSR) using a tool of your choice. Send the CSR to your certificate authority (CA) to generate the certificate.\nIf you want to use a certificate for testing purposes you can generate a self signed certificate using openssl:\n$ mkdir \u0026lt;work directory\u0026gt;/ssl $ cd \u0026lt;work directory\u0026gt;/ssl $ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \u0026#34;/CN=\u0026lt;nginx-hostname\u0026gt;\u0026#34; For example:\n$ mkdir /scratch/OAMDockerK8S/ssl $ cd /scratch/OAMDockerK8S/ssl $ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \u0026#34;/CN=masternode.example.com\u0026#34; Note: The CN should match the host.domain of the master node in order to prevent hostname problems during certificate verification.\nThe output will look similar to the following:\nGenerating a 2048 bit RSA private key ..........................................+++ .......................................................................................................+++ writing new private key to \u0026#39;tls.key\u0026#39; -----   Create a secret for SSL by running the following command:\n$ kubectl -n accessns create secret tls \u0026lt;domain_uid\u0026gt;-tls-cert --key \u0026lt;work directory\u0026gt;/tls.key --cert \u0026lt;work directory\u0026gt;/tls.crt For example:\n$ kubectl -n accessns create secret tls accessinfra-tls-cert --key /scratch/OAMDockerK8S/ssl/tls.key --cert /scratch/OAMDockerK8S/ssl/tls.crt The output will look similar to the following:\nsecret/accessinfra-tls-cert created   Install NGINX Use helm to install NGINX.\n  Add the helm chart repository for NGINX using the following command:\n$ helm repo add stable https://kubernetes.github.io/ingress-nginx The output will look similar to the following:\n\u0026#34;stable\u0026#34; has been added to your repositories   Update the repository using the following command:\n$ helm repo update The output will look similar to the following:\nHang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \u0026#34;appscode\u0026#34; chart repository ...Successfully got an update from the \u0026#34;stable\u0026#34; chart repository Update Complete. ⎈ Happy Helming!⎈   Install NGINX using the following helm command:\n$ helm install nginx-ingress --set controller.extraArgs.default-ssl-certificate=\u0026lt;domain_namespace\u0026gt;/\u0026lt;ssl_secret\u0026gt; --set controller.service.type=NodePort --set controller.admissionWebhooks.enabled=false stable/ingress-nginx For example:\n$ helm install nginx-ingress --set controller.extraArgs.default-ssl-certificate=accessns/accessinfra-tls-cert --set controller.service.type=NodePort --set controller.admissionWebhooks.enabled=false stable/ingress-nginx The output will look similar to the following:\nNAME: nginx-ingress LAST DEPLOYED: Thu Sep 24 07:31:51 2020 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: The nginx-ingress controller has been installed. Get the application URL by running these commands: export HTTP_NODE_PORT=$(kubectl --namespace default get services -o jsonpath=\u0026#34;{.spec.ports[0].nodePort}\u0026#34; nginx-ingress-controller) export HTTPS_NODE_PORT=$(kubectl --namespace default get services -o jsonpath=\u0026#34;{.spec.ports[1].nodePort}\u0026#34; nginx-ingress-controller) export NODE_IP=$(kubectl --namespace default get nodes -o jsonpath=\u0026#34;{.items[0].status.addresses[1].address}\u0026#34;) echo \u0026#34;Visit http://$NODE_IP:$HTTP_NODE_PORTto access your application via HTTP.\u0026#34; echo \u0026#34;Visit https://$NODE_IP:$HTTPS_NODE_PORTto access your application via HTTPS.\u0026#34; An example Ingress that makes use of the controller: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx name: example namespace: foo spec: rules: - host: www.example.com http: paths: - backend: serviceName: exampleService servicePort: 80 path: / # This section is only required if TLS is to be enabled for the Ingress tls: - hosts: - www.example.com secretName: example-tls If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided: apiVersion: v1 kind: Secret metadata: name: example-tls namespace: foo data: tls.crt: \u0026lt;base64 encoded cert\u0026gt; tls.key: \u0026lt;base64 encoded key\u0026gt; type: kubernetes.io/tls   Create an Ingress for the Domain   Create an Ingress for the domain (access-ingress), in the domain namespace by using the sample Helm chart.\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain $ kubectl create -f ssl-nginx-ingress.yaml Note: The ssl-nginx-ingress.yaml has nginx.ingress.kubernetes.io/enable-access-log set to false. If you want to enable access logs then set this value to true before executing the command. Enabling access-logs can cause issues with disk space if not regularly maintained.\nFor example:\n$ cd /scratch/OAMDockerK8S/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain $ kubectl create -f ssl-nginx-ingress.yaml The output will look similar to the following:\ningress.extensions/access-ingress created   Run the following command to show the ingress is created successfully:\n$ kubectl get ing -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get ing -n accessns The output will look similar to the following:\nNAME CLASS HOSTS ADDRESS PORTS AGE access-ingress \u0026lt;none\u0026gt; * 80 2m53s   Find the node port of NGINX using the following command:\n$ kubectl --namespace default get services -o jsonpath=\u0026#34;{.spec.ports[1].nodePort}\u0026#34; nginx-ingress-ingress-nginx-controller The output will look similar to the following:\n30099   Run the following command to check the ingress:\n$ kubectl describe ing access-ingress -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl describe ing access-ingress -n accessns The output will look similar to the following:\nName: access-ingress Namespace: accessns Address: 10.107.181.157 Default backend: default-http-backend:80 (\u0026lt;error: endpoints \u0026#34;default-http-backend\u0026#34; not found\u0026gt;) Rules: Host Path Backends ---- ---- -------- * /console accessinfra-adminserver:7001 (10.244.1.7:7001) /rreg/rreg accessinfra-adminserver:7001 (10.244.1.7:7001) /em accessinfra-adminserver:7001 (10.244.1.7:7001) /oamconsole accessinfra-adminserver:7001 (10.244.1.7:7001) /dms accessinfra-adminserver:7001 (10.244.1.7:7001) /oam/services/rest accessinfra-adminserver:7001 (10.244.1.7:7001) /iam/admin/config accessinfra-adminserver:7001 (10.244.1.7:7001) /oam/admin/api accessinfra-adminserver:7001 (10.244.1.7:7001) /iam/admin/diag accessinfra-adminserver:7001 (10.244.1.7:7001) /iam/access accessinfra-cluster-oam-cluster:14100 (10.244.1.8:14100,10.244.2.3:14100) /oam/services/rest/access/api accessinfra-cluster-oam-cluster:14100 (10.244.1.8:14100,10.244.2.3:14100) /access accessinfra-cluster-policy-cluster:15100 (10.244.1.9:15100) / accessinfra-cluster-oam-cluster:14100 (10.244.1.8:14100,10.244.2.3:14100) Annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/configuration-snippet: more_set_input_headers \u0026#34;X-Forwarded-Proto: https\u0026#34;; more_set_input_headers \u0026#34;WL-Proxy-SSL: true\u0026#34;; nginx.ingress.kubernetes.io/ingress.allow-http: false nginx.ingress.kubernetes.io/proxy-buffer-size: 2000k Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal CREATE 85s nginx-ingress-controller Ingress accessns/access-ingress Normal UPDATE 40s nginx-ingress-controller Ingress accessns/access-ingress   To confirm that the new Ingress is successfully routing to the domain\u0026rsquo;s server pods, run the following command to send a request to the URL for the \u0026ldquo;WebLogic ReadyApp framework\u0026rdquo;:\n$ curl -v -k https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/weblogic/ready For example:\n$ curl -v -k https://masternode.example.com:30099/weblogic/ready The output will look similar to the following:\n* Trying 12.345.67.89... * Connected to 12.345.67.89 (12.345.67.89) port 30099 (#0) * Initializing NSS with certpath: sql:/etc/pki/nssdb * skipping SSL peer certificate verification * SSL connection using TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 * Server certificate: * subject: CN=masternode.example.com * start date: Sep 24 14:30:46 2020 GMT * expire date: Sep 24 14:30:46 2021 GMT * common name: masternode.example.com * issuer: CN=masternode.example.com \u0026gt; GET /weblogic/ready HTTP/1.1 \u0026gt; User-Agent: curl/7.29.0 \u0026gt; Host: masternode.example.com:30099 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; Server: nginx/1.19.2 \u0026lt; Date: Thu, 24 Sep 2020 14:51:06 GMT \u0026lt; Content-Length: 0 \u0026lt; Connection: keep-alive \u0026lt; Strict-Transport-Security: max-age=15724800; includeSubDomains \u0026lt; * Connection #0 to host 10.247.94.49 left intact   Verify that You can Access the Domain URL After setting up the NGINX ingress, verify that the domain applications are accessible through the NGINX ingress port (for example 30099) as per Validate Domain URLs \n"
},
{
	"uri": "/fmw-kubernetes/oig/configure-ingress/ingress-nginx-setup-for-oig-domain-setup-on-k8s/",
	"title": "a. Using an Ingress with NGINX (non-SSL)",
	"tags": [],
	"description": "Steps to set up an Ingress for NGINX to direct traffic to the OIG domain (non-SSL).",
	"content": "Setting Up an ingress for NGINX for the OIG Domain on Kubernetes (non-SSL) The instructions below explain how to set up NGINX as an ingress for the OIG domain with non-SSL termination.\nNote: All the steps below should be performed on the master node.\n Install NGINX  Configure the repository Create a Namespace Install NGINX using helm Setup Routing Rules for the Domain   Create an Ingress for the Domain Verify that You can Access the Domain URL Cleanup  Install NGINX Use helm to install NGINX.\nConfigure the repository   Add the Helm chart repository for NGINX using the following command:\n$ helm repo add stable https://kubernetes.github.io/ingress-nginx The output will look similar to the following:\n$ helm repo add stable https://kubernetes.github.io/ingress-nginx \u0026quot;stable\u0026quot; has been added to your repositories   Update the repository using the following command:\n$ helm repo update The output will look similar to the following:\nHang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \u0026#34;stable\u0026#34; chart repository Update Complete. Happy Helming!   Create a Namespace   Create a Kubernetes namespace for NGINX by running the following command:\n$ kubectl create namespace nginx The output will look similar to the following:\nnamespace/nginx created   Install NGINX using helm If you can connect directly to the master node IP address from a browser, then install NGINX with the --set controller.service.type=NodePort parameter.\nIf you are using a Managed Service for your Kubernetes cluster,for example Oracle Kubernetes Engine (OKE) on Oracle Cloud Infrastructure (OCI), and connect from a browser to the Load Balancer IP address, then use the --set controller.service.type=LoadBalancer parameter. This instructs the Managed Service to setup a Load Balancer to direct traffic to the NGINX ingress.\n  To install NGINX use the following helm command depending on if you are using NodePort or LoadBalancer:\na) Using NodePort\n$ helm install nginx-ingress -n nginx --set controller.service.type=NodePort --set controller.admissionWebhooks.enabled=false stable/ingress-nginx The output will look similar to the following:\nNAME: nginx-ingress LAST DEPLOYED: Tue Sep 29 08:07:03 2020 NAMESPACE: nginx STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: The nginx-ingress controller has been installed. Get the application URL by running these commands: export HTTP_NODE_PORT=$(kubectl --namespace nginx get services -o jsonpath=\u0026quot;{.spec.ports[0].nodePort}\u0026quot; nginx-ingress-controller) export HTTPS_NODE_PORT=$(kubectl --namespace nginx get services -o jsonpath=\u0026quot;{.spec.ports[1].nodePort}\u0026quot; nginx-ingress-controller) export NODE_IP=$(kubectl --namespace nginx get nodes -o jsonpath=\u0026quot;{.items[0].status.addresses[1].address}\u0026quot;) echo \u0026quot;Visit http://$NODE_IP:$HTTP_NODE_PORT to access your application via HTTP.\u0026quot; echo \u0026quot;Visit https://$NODE_IP:$HTTPS_NODE_PORT to access your application via HTTPS.\u0026quot; An example Ingress that makes use of the controller: apiVersion: extensions/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx name: example namespace: foo spec: rules: - host: www.example.com http: paths: - backend: serviceName: exampleService servicePort: 80 path: / # This section is only required if TLS is to be enabled for the Ingress tls: - hosts: - www.example.com secretName: example-tls If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided: apiVersion: v1 kind: Secret metadata: name: example-tls namespace: foo data: tls.crt: \u0026lt;base64 encoded cert\u0026gt; tls.key: \u0026lt;base64 encoded key\u0026gt; type: kubernetes.io/tls b) Using LoadBalancer\n$ helm install nginx-ingress -n nginx --set controller.service.type=LoadBalancer --set controller.admissionWebhooks.enabled=false stable/ingress-nginx The output will look similar to the following:\nNAME: nginx-ingress LAST DEPLOYED: Tue Sep 29 08:07:03 2020 NAMESPACE: nginx STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: The nginx-ingress controller has been installed. It may take a few minutes for the LoadBalancer IP to be available. You can watch the status by running 'kubectl --namespace nginx get services -o wide -w nginx-ingress-controller' An example Ingress that makes use of the controller: apiVersion: extensions/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx name: example namespace: foo spec: rules: - host: www.example.com http: paths: - backend: serviceName: exampleService servicePort: 80 path: / # This section is only required if TLS is to be enabled for the Ingress tls: - hosts: - www.example.com secretName: example-tls If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided: apiVersion: v1 kind: Secret metadata: name: example-tls namespace: foo data: tls.crt: \u0026lt;base64 encoded cert\u0026gt; tls.key: \u0026lt;base64 encoded key\u0026gt; type: kubernetes.io/tls   Setup Routing Rules for the Domain   Setup routing rules by running the following commands:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain $ cp values.yaml values.yaml.orig $ vi values.yaml Edit values.yaml and ensure that type=NGINX and tls=NONSSL are set, for example:\n$ cat values.yaml # Copyright 2020 Oracle Corporation and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # Default values for ingress-per-domain. # This is a YAML-formatted file. # Declare variables to be passed into your templates. # Load balancer type. Supported values are: VOYAGER, NGINX type: NGINX # Type of Configuration Supported Values are : NONSSL,SSL # tls: NONSSL tls: NONSSL # TLS secret name if the mode is SSL secretName: domain1-tls-cert # WLS domain as backend to the load balancer wlsDomain: domainUID: oimcluster oimClusterName: oim_cluster soaClusterName: soa_cluster soaManagedServerPort: 8001 oimManagedServerPort: 14000 adminServerName: adminserver adminServerPort: 7001 # Traefik specific values # traefik: # hostname used by host-routing # hostname: idmdemo.m8y.xyz # Voyager specific values voyager: # web port webPort: 30305 # stats port statsPort: 30315   Create an Ingress for the Domain   Create an Ingress for the domain (oimcluster-nginx), in the domain namespace by using the sample Helm chart:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator $ helm install oimcluster-nginx kubernetes/samples/charts/ingress-per-domain --namespace \u0026lt;namespace\u0026gt; --values kubernetes/samples/charts/ingress-per-domain/values.yaml Note: The \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain/templates/nginx-ingress.yaml has nginx.ingress.kubernetes.io/enable-access-log set to false. If you want to enable access logs then set this value to true before executing the command. Enabling access-logs can cause issues with disk space if not regularly maintained.\nFor example:\n$ cd /scratch/OIGDockerK8S/weblogic-kubernetes-operator $ helm install oimcluster-nginx kubernetes/samples/charts/ingress-per-domain --namespace oimcluster --values kubernetes/samples/charts/ingress-per-domain/values.yaml The output will look similar to the following:\n$ helm install oimcluster-nginx kubernetes/samples/charts/ingress-per-domain --namespace oimcluster --values kubernetes/samples/charts/ingress-per-domain/values.yaml NAME: oimcluster-nginx LAST DEPLOYED: Tue Sep 29 08:10:06 2020 NAMESPACE: oimcluster STATUS: deployed REVISION: 1 TEST SUITE: None   Run the following command to show the ingress is created successfully:\n$ kubectl get ing -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get ing -n oimcluster The output will look similar to the following:\nNAME CLASS HOSTS ADDRESS PORTS AGE oimcluster-nginx \u0026lt;none\u0026gt; * 80 47s   Find the NodePort of NGINX using the following command (only if you installed NGINX using NodePort):\n$ kubectl get services -n nginx -o jsonpath=”{.spec.ports[0].nodePort}” nginx-ingress-ingress-nginx-controller The output will look similar to the following:\n31578$   Run the following command to check the ingress:\n$ kubectl describe ing access-ingress -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl describe ing oimcluster-nginx -n oimcluster The output will look similar to the following:\nName: oimcluster-nginx Namespace: oimcluster Address: 10.97.68.171 Default backend: default-http-backend:80 (\u0026lt;error: endpoints \u0026quot;default-http-backend\u0026quot; not found\u0026gt;) Rules: Host Path Backends ---- ---- -------- * /console oimcluster-adminserver:7001 (10.244.1.42:7001) /em oimcluster-adminserver:7001 (10.244.1.42:7001) /soa oimcluster-cluster-soa-cluster:8001 (10.244.1.43:8001) /integration oimcluster-cluster-soa-cluster:8001 (10.244.1.43:8001) /soa-infra oimcluster-cluster-soa-cluster:8001 (10.244.1.43:8001) oimcluster-cluster-oim-cluster:14000 (10.244.1.44:14000) Annotations: meta.helm.sh/release-name: oimcluster-nginx meta.helm.sh/release-namespace: oimcluster Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal CREATE 53s nginx-ingress-controller Ingress oimcluster/oimcluster-nginx Normal UPDATE 42s nginx-ingress-controller Ingress oimcluster/oimcluster-nginx   To confirm that the new Ingress is successfully routing to the domain\u0026rsquo;s server pods, run the following command to send a request to the URL for the \u0026ldquo;WebLogic ReadyApp framework\u0026rdquo;:\n$ curl -v http://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/weblogic/ready For example:\na) For NodePort\n$ curl -v http://masternode.example.com:31578/weblogic/ready b) For LoadBalancer\n$ curl -v http://masternode.example.com:80/weblogic/ready The output will look similar to the following:\n$ curl -v -k http://masternode.example.com:31578/weblogic/ready * About to connect() to masternode.example.com port 31578 (#0) * Trying 12.345.67.890... * Connected to masternode.example.com (12.345.67.890) port 31578 (#0) \u0026gt; GET /weblogic/ready HTTP/1.1 \u0026gt; User-Agent: curl/7.29.0 \u0026gt; Host: masternode.example.com:31578 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; Server: nginx/1.19.2 \u0026lt; Date: Tue, 29 Sep 2020 15:16:20 GMT \u0026lt; Content-Length: 0 \u0026lt; Connection: keep-alive \u0026lt; * Connection #0 to host masternode.example.com left intact   Verify that You can Access the Domain URL After setting up the NGINX ingress, verify that the domain applications are accessible through the NGINX ingress port (for example 31578) as per Validate Domain URLs \nCleanup If you need to remove the NGINX Ingress (for example to setup NGINX with SSL) then remove the ingress with the following commands:\n$ helm delete oimcluster-nginx -n oimcluster $ helm delete nginx-ingress -n nginx $ kubectl delete namespace nginx The output will look similar to the following:\n$ helm delete oimcluster-nginx -n oimcluster release \u0026quot;oimcluster-nginx\u0026quot; uninstalled $ helm delete nginx-ingress -n nginx release \u0026quot;nginx-ingress\u0026quot; uninstalled $ kubectl delete namespace nginx namespace \u0026quot;nginx\u0026quot; deleted $ "
},
{
	"uri": "/fmw-kubernetes/oig/configure-design-console/using-the-design-console-with-nginx-non-ssl/",
	"title": "a. Using Design Console with NGINX(non-SSL)",
	"tags": [],
	"description": "Configure Design Console with NGINX(non-SSL).",
	"content": "Configure an NGINX ingress (non-SSL) to allow Design Console to connect to your Kubernetes cluster.\nDesign Console is not installed as part of the OAM Kubernetes cluster so must be installed on a seperate client before following the steps below.\n Add the NGINX ingress using helm Note: If already using NGINX with non-SSL for OIG you can skip this section:\n  Add the Helm chart repository for NGINX using the following command:\n$ helm repo add stable https://kubernetes.github.io/ingress-nginx The output will look similar to the following:\n\u0026#34;stable\u0026#34; has been added to your repositories   Update the repository using the following command:\n$ helm repo update The output will look similar to the following:\nHang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \u0026#34;stable\u0026#34; chart repository Update Complete. Happy Helming!   Create a Kubernetes namespace for NGINX by running the following command:\n$ kubectl create namespace nginx The output will look similar to the following:\nnamespace/nginx created   Install NGINX ingress using helm Install a NGINX ingress for the Design Console:\nIf you can connect directly to the master node IP address from a browser, then install NGINX with the --set controller.service.type=NodePort parameter.\nIf you are using a Managed Service for your Kubernetes cluster,for example Oracle Kubernetes Engine (OKE) on Oracle Cloud Infrastructure (OCI), and connect from a browser to the Load Balancer IP address, then use the --set controller.service.type=LoadBalancer parameter. This instructs the Managed Service to setup a Load Balancer to direct traffic to the NGINX ingress.\n  To install NGINX use the following helm command depending on if you are using NodePort or LoadBalancer:\na) Using NodePort\n$ helm install nginx-dc-operator stable/ingress-nginx -n nginx --set controller.service.type=NodePort --set controller.admissionWebhooks.enabled=false --set controller.service.nodePorts.http=30315 --set controller.ingressClass=nginx-designconsole The output will look similar to the following:\nNAME: nginx-dc-operator LAST DEPLOYED: Tue Oct 20 07:31:08 2020 NAMESPACE: nginx STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: The ingress-nginx controller has been installed. Get the application URL by running these commands: export HTTP_NODE_PORT=30315 export HTTPS_NODE_PORT=$(kubectl --namespace nginx get services -o jsonpath=\u0026quot;{.spec.ports[1].nodePort}\u0026quot; nginx-dc-operator-ingress-nginx-controller) export NODE_IP=$(kubectl --namespace nginx get nodes -o jsonpath=\u0026quot;{.items[0].status.addresses[1].address}\u0026quot;) echo \u0026quot;Visit http://$NODE_IP:$HTTP_NODE_PORT to access your application via HTTP.\u0026quot; echo \u0026quot;Visit https://$NODE_IP:$HTTPS_NODE_PORT to access your application via HTTPS.\u0026quot; An example Ingress that makes use of the controller: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx-designconsole name: example namespace: foo spec: rules: - host: www.example.com http: paths: - backend: serviceName: exampleService servicePort: 80 path: / # This section is only required if TLS is to be enabled for the Ingress tls: - hosts: - www.example.com secretName: example-tls If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided: apiVersion: v1 kind: Secret metadata: name: example-tls namespace: foo data: tls.crt: \u0026lt;base64 encoded cert\u0026gt; tls.key: \u0026lt;base64 encoded key\u0026gt; type: kubernetes.io/tls b) Using LoadBalancer\n$ helm install nginx-dc-operator stable/ingress-nginx -n nginx --set controller.service.type=LoadBalancer --set controller.admissionWebhooks.enabled=false The output will look similar to the following:\nLAST DEPLOYED: Tue Oct 20 07:39:27 2020 NAMESPACE: nginx STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: The ingress-nginx controller has been installed. It may take a few minutes for the LoadBalancer IP to be available. You can watch the status by running 'kubectl --namespace nginx get services -o wide -w nginx-dc-operator-ingress-nginx-controller' An example Ingress that makes use of the controller: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx name: example namespace: foo spec: rules: - host: www.example.com http: paths: - backend: serviceName: exampleService servicePort: 80 path: / # This section is only required if TLS is to be enabled for the Ingress tls: - hosts: - www.example.com secretName: example-tls If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided: apiVersion: v1 kind: Secret metadata: name: example-tls namespace: foo data: tls.crt: \u0026lt;base64 encoded cert\u0026gt; tls.key: \u0026lt;base64 encoded key\u0026gt; type: kubernetes.io/tls   Setup Routing Rules for the Design Console ingress   Setup routing rules by running the following commands:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/charts/design-console-ingress $ cp values.yaml values.yaml.orig $ vi values.yaml Edit values.yaml and ensure that type=NGINX and tls=NONSSL are set, for example:\n$ cat values.yaml # Copyright 2020 Oracle Corporation and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # Default values for design-console-ingress. # This is a YAML-formatted file. # Declare variables to be passed into your templates. # Load balancer type. Supported values are: VOYAGER, NGINX type: NGINX # Type of Configuration Supported Values are : NONSSL,SSL # tls: NONSSL tls: NONSSL # TLS secret name if the mode is SSL secretName: dc-tls-cert # WLS domain as backend to the load balancer wlsDomain: domainUID: oimcluster oimClusterName: oim_cluster oimServerT3Port: 14001 # Voyager specific values voyager: # web port webPort: 30320 # stats port statsPort: 30321   Create the ingress   Run the following command to create the ingress:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator $ helm install oimcluster-nginx-designconsole kubernetes/samples/charts/design-console-ingress --namespace oimcluster --values kubernetes/samples/charts/design-console-ingress/values.yaml For example:\n$ cd /scratch/OIGDockerK8S/weblogic-kubernetes-operator $ helm install oimcluster-nginx-designconsole kubernetes/samples/charts/design-console-ingress --namespace oimcluster --values kubernetes/samples/charts/design-console-ingress/values.yaml The output will look similar to the following:\nNAME: oimcluster-nginx-designconsole LAST DEPLOYED: Tue Oct 20 08:01:47 2020 NAMESPACE: oimcluster STATUS: deployed REVISION: 1 TEST SUITE: None   Run the following command to show the ingress is created successfully:\n$ kubectl describe ing oimcluster-nginx-designconsole -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl describe ing oimcluster-nginx-designconsole -n oimcluster The output will look similar to the following:\nName: oimcluster-nginx-designconsole Namespace: oimcluster Address: 10.99.240.21 Default backend: default-http-backend:80 (\u0026lt;error: endpoints \u0026quot;default-http-backend\u0026quot; not found\u0026gt;) Rules: Host Path Backends ---- ---- -------- * oimcluster-cluster-oim-cluster:14001 () Annotations: kubernetes.io/ingress.class: nginx-designconsole meta.helm.sh/release-name: oimcluster-nginx-designconsole meta.helm.sh/release-namespace: oimcluster nginx.ingress.kubernetes.io/affinity: cookie nginx.ingress.kubernetes.io/enable-access-log: false Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal CREATE 117s nginx-ingress-controller Ingress oimcluster/oimcluster-nginx-designconsole Normal UPDATE 64s nginx-ingress-controller Ingress oimcluster/oimcluster-nginx-designconsole   Login to the Design Console   Launch the Design Console and in the Oracle Identity Manager Design Console login page enter the following details:\nEnter the following details and click Login:\n Server URL: \u0026lt;url\u0026gt; User ID: xelsysadm Password: \u0026lt;password\u0026gt;.  where \u0026lt;url\u0026gt; is as per the following:\na) For NodePort: http://\u0026lt;masternode.example.com\u0026gt;:\u0026lt;NodePort\u0026gt;\nwhere \u0026lt;NodePort\u0026gt; is the value passed in the command earlier, for example: --set controller.service.nodePorts.http=30315\nb) For LoadBalancer: http://\u0026lt;loadbalancer.example.com\u0026gt;:\u0026lt;LBRPort\u0026gt;\n  If successful the Design Console will be displayed. If the VNC session disappears then the connection failed so double check the connection details and try again.\n  "
},
{
	"uri": "/fmw-kubernetes/oig/configure-design-console/using-the-design-console-with-nginx-ssl/",
	"title": "a. Using Design Console with NGINX(SSL)",
	"tags": [],
	"description": "Configure Design Console with NGINX(SSL).",
	"content": "Configure an NGINX ingress (SSL) to allow Design Console to connect to your Kubernetes cluster.\nDesign Console is not installed as part of the OAM Kubernetes cluster so must be installed on a seperate client before following the steps below.\n Generate SSL Certificate Note: If already using NGINX with SSL for OIG you can skip this section:\n  Generate a private key and certificate signing request (CSR) using a tool of your choice. Send the CSR to your certificate authority (CA) to generate the certificate.\nIf you want to use a certificate for testing purposes you can generate a self signed certificate using openssl:\n$ mkdir \u0026lt;work directory\u0026gt;/ssl $ cd \u0026lt;work directory\u0026gt;/ssl $ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \u0026quot;/CN=\u0026lt;nginx-hostname\u0026gt;\u0026quot; For example:\n$ mkdir /scratch/OIGDockerK8S/ssl $ cd /scratch/OIGDockerK8S/ssl $ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \u0026quot;/CN=masternode.example.com\u0026quot; Note: The CN should match the host.domain of the master node in order to prevent hostname problems during certificate verification.\nThe output will look similar to the following:\n$ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \u0026quot;/CN=masternode.example.com\u0026quot; Generating a 2048 bit RSA private key ..........................................+++ .......................................................................................................+++ writing new private key to 'tls.key' -----   Create a secret for SSL containing the SSL certificate by running the following command:\n$ kubectl -n oimcluster create secret tls \u0026lt;domain_id\u0026gt;-tls-cert --key \u0026lt;work directory\u0026gt;/tls.key --cert \u0026lt;work directory\u0026gt;/tls.crt For example:\n$ kubectl -n oimcluster create secret tls oimcluster-tls-cert --key /scratch/OIGDockerK8S/ssl/tls.key --cert /scratch/OIGDockerK8S/ssl/tls.crt The output will look similar to the following:\n$ kubectl -n oimcluster create secret tls oimcluster-tls-cert --key /scratch/OIGDockerK8S/ssl/tls.key --cert /scratch/OIGDockerK8S/ssl/tls.crt secret/oimcluster-tls-cert created $   Confirm that the secret is created by running the following command:\n$ kubectl get secret oimcluster-tls-cert -o yaml -n oimcluster The output will look similar to the following:\napiVersion: v1 data: tls.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURGVENDQWYyZ0F3SUJBZ0lKQUl3ZjVRMWVxZnljTUEwR0NTcUdTSWIzRFFFQkN3VUFNQ0V4SHpBZEJnTlYKQkFNTUZtUmxiakF4WlhadkxuVnpMbTl5WVdOc1pTNWpiMjB3SGhjTk1qQXdPREV3TVRReE9UUXpXaGNOTWpFdwpPREV3TVRReE9UUXpXakFoTVI4d0hRWURWUVFEREJaa1pXNHdNV1YyYnk1MWN5NXZjbUZqYkdVdVkyOXRNSUlCCklqQU5CZ2txaGtpRzl3MEJBUUVGQUFPQ0FROEFNSUlCQ2dLQ0FRRUEyY0lpVUhwcTRVZzBhaGR6aXkycHY2cHQKSVIza2s5REd2eVRNY0syaWZQQ2dtUU5CdHV6VXNFN0l4c294eldITmU5RFpXRXJTSjVON3FYYm1lTzJkMVd2NQp1aFhzbkFTbnkwY1NLUE9xVDNQSlpDVk1MK0llZVFKdnhaVjZaWWU4V2FFL1NQSGJzczRjYy9wcG1mc3pxCnErUi83cXEyMm9ueHNHaE9vQ1h1TlQvMFF2WXVzMnNucGtueWRKRHUxelhGbDREYkFIZGMvamNVK0NPWWROeS8KT3Iza2JIV0FaTkR4OWxaZUREOTRmNXZLcUF2V0FkSVJZa2UrSmpNTHg0VHo2ZlM0VXoxbzdBSTVuSApPQ1ZMblV5U0JkaGVuWTNGNEdFU0wwbnorVlhFWjRWVjRucWNjRmo5cnJ0Q29pT1BBNlgvNGdxMEZJbi9Qd0lECkFRQUJvMUF3VGpBZEJnTlZIUTRFRmdRVWw1VnVpVDBDT0xGTzcxMFBlcHRxSC9DRWZyY3dId1lEVlIwakJCZ3cKRm9BVWw1VnVpVDBDT0xGTzcxMFBlcHRxSC9DRWZyY3dEQVlEVlIwVEJBVXdBd0VCL3pBTkJna3Foa2lHOXcwQgpBUXNGQUFPQ0FRRUFXdEN4b2ZmNGgrWXZEcVVpTFFtUnpqQkVBMHJCOUMwL1FWOG9JQzJ3d1hzYi9KaVNuMHdOCjNMdHppejc0aStEbk1yQytoNFQ3enRaSkc3NVluSGRKcmxQajgzVWdDLzhYTlFCSUNDbTFUa3RlVU1jWG0reG4KTEZEMHpReFhpVzV0N1FHcWtvK2FjeTlhUnUvN3JRMXlNSE9HdVVkTTZETzErNXF4cTdFNXFMamhyNEdKejV5OAoraW8zK25UcUVKMHFQOVRocG96RXhBMW80OEY0ZHJybWdqd3ROUldEQVpBYmYyV1JNMXFKWXhxTTJqdU1FQWNsCnFMek1TdEZUQ2o1UGFTQ0NUV1VEK3ZlSWtsRWRpaFdpRm02dzk3Y1diZ0lGMlhlNGk4L2szMmF1N2xUTDEvd28KU3Q2dHpsa20yV25uUFlVMzBnRURnVTQ4OU02Z1dybklpZz09Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K tls.key: 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 kind: Secret metadata: creationTimestamp: \u0026quot;2020-09-29T15:51:22Z\u0026quot; managedFields: - apiVersion: v1 fieldsType: FieldsV1 fieldsV1: f:data: .: {} f:tls.crt: {} f:tls.key: {} f:type: {} manager: kubectl operation: Update time: \u0026quot;2020-09-29T15:51:22Z\u0026quot; name: oimcluster-tls-cert namespace: oimcluster resourceVersion: \u0026quot;1291036\u0026quot; selfLink: /api/v1/namespaces/oimcluster/secrets/oimcluster-tls-cert uid: a127e5fd-705b-43e1-ab56-590834efda5e type: kubernetes.io/tls   Add the NGINX ingress using helm Note: If already using NGINX with SSL for OIG you can skip this section:\n  Add the Helm chart repository for NGINX using the following command:\n$ helm repo add stable https://kubernetes.github.io/ingress-nginx The output will look similar to the following:\n\u0026#34;stable\u0026#34; has been added to your repositories   Update the repository using the following command:\n$ helm repo update The output will look similar to the following:\nHang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \u0026#34;stable\u0026#34; chart repository Update Complete. Happy Helming!   Create a Kubernetes namespace for NGINX:\n$ kubectl create namespace nginxssl The output will look similar to the following:\nnamespace/nginxssl created   Install NGINX ingress using helm Install a NGINX ingress for the Design Console:\nIf you can connect directly to the master node IP address from a browser, then install NGINX with the --set controller.service.type=NodePort parameter.\nIf you are using a Managed Service for your Kubernetes cluster,for example Oracle Kubernetes Engine (OKE) on Oracle Cloud Infrastructure (OCI), and connect from a browser to the Load Balancer IP address, then use the --set controller.service.type=LoadBalancer parameter. This instructs the Managed Service to setup a Load Balancer to direct traffic to the NGINX ingress.\n  To install NGINX use the following helm command depending on if you are using NodePort or LoadBalancer:\na) Using NodePort\n$ helm install nginx-dc-operator-ssl -n nginxssl --set controller.extraArgs.default-ssl-certificate=oimcluster/oimcluster-tls-cert --set controller.service.type=NodePort --set controller.admissionWebhooks.enabled=false --set controller.service.nodePorts.https=30321 --set controller.ingressClass=nginx-designconsole stable/ingress-nginx The output will look similar to the following:\nLAST DEPLOYED: Wed Oct 21 03:52:25 2020 NAMESPACE: nginxssl STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: The ingress-nginx controller has been installed. Get the application URL by running these commands: export HTTP_NODE_PORT=$(kubectl --namespace nginxssl get services -o jsonpath=\u0026quot;{.spec.ports[0].nodePort}\u0026quot; nginx-dc-operator-ssl-ingress-nginx-controller) export HTTPS_NODE_PORT=30321 export NODE_IP=$(kubectl --namespace nginxssl get nodes -o jsonpath=\u0026quot;{.items[0].status.addresses[1].address}\u0026quot;) echo \u0026quot;Visit http://$NODE_IP:$HTTP_NODE_PORT to access your application via HTTP.\u0026quot; echo \u0026quot;Visit https://$NODE_IP:$HTTPS_NODE_PORT to access your application via HTTPS.\u0026quot; An example Ingress that makes use of the controller: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx-designconsole name: example namespace: foo spec: rules: - host: www.example.com http: paths: - backend: serviceName: exampleService servicePort: 80 path: / # This section is only required if TLS is to be enabled for the Ingress tls: - hosts: - www.example.com secretName: example-tls If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided: apiVersion: v1 kind: Secret metadata: name: example-tls namespace: foo data: tls.crt: \u0026lt;base64 encoded cert\u0026gt; tls.key: \u0026lt;base64 encoded key\u0026gt; type: kubernetes.io/tls b) Using LoadBalancer\n$ helm install nginx-dc-operator-ssl -n nginxssl --set controller.extraArgs.default-ssl-certificate=oimcluster/oimcluster-tls-cert --set controller.service.type=LoadBalancer --set controller.admissionWebhooks.enabled=false stable/ingress-nginx The output will look similar to the following:\nNAME: nginx-dc-operator-ssl-lbr LAST DEPLOYED: Wed Oct 21 04:02:35 2020 NAMESPACE: nginxssl STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: The ingress-nginx controller has been installed. It may take a few minutes for the LoadBalancer IP to be available. You can watch the status by running 'kubectl --namespace nginxssl get services -o wide -w nginx-dc-operator-ssl-lbr-ingress-nginx-controller' An example Ingress that makes use of the controller: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx name: example namespace: foo spec: rules: - host: www.example.com http: paths: - backend: serviceName: exampleService servicePort: 80 path: / # This section is only required if TLS is to be enabled for the Ingress tls: - hosts: - www.example.com secretName: example-tls If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided: apiVersion: v1 kind: Secret metadata: name: example-tls namespace: foo data: tls.crt: \u0026lt;base64 encoded cert\u0026gt; tls.key: \u0026lt;base64 encoded key\u0026gt; type: kubernetes.io/tls   Setup Routing Rules for the Design Console ingress   Setup routing rules by running the following commands:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/charts/design-console-ingress $ cp values.yaml values.yaml.orig $ vi values.yaml Edit values.yaml and ensure that type=NGINX, tls=SSL and secretName:oimcluster-tls-cert are set, for example:\n$ cat values.yaml # Copyright 2020 Oracle Corporation and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # Default values for design-console-ingress. # This is a YAML-formatted file. # Declare variables to be passed into your templates. # Load balancer type. Supported values are: VOYAGER, NGINX type: NGINX # Type of Configuration Supported Values are : NONSSL,SSL # tls: NONSSL tls: SSL # TLS secret name if the mode is SSL secretName: oimcluster-tls-cert # WLS domain as backend to the load balancer wlsDomain: domainUID: oimcluster oimClusterName: oim_cluster oimServerT3Port: 14001 # Voyager specific values voyager: # web port webPort: 30320 # stats port statsPort: 30321   Create the ingress   Run the following command to create the ingress:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator $ helm install oimcluster-nginx-designconsole kubernetes/samples/charts/design-console-ingress --namespace oimcluster --values kubernetes/samples/charts/design-console-ingress/values.yaml For example:\n$ cd /scratch/OIGDockerK8S/weblogic-kubernetes-operator $ helm install oimcluster-nginx-designconsole kubernetes/samples/charts/design-console-ingress --namespace oimcluster --values kubernetes/samples/charts/design-console-ingress/values.yaml The output will look similar to the following:\nNAME: oimcluster-nginx-designconsole LAST DEPLOYED: Wed Oct 21 04:12:00 2020 NAMESPACE: oimcluster STATUS: deployed REVISION: 1 TEST SUITE: None   Run the following command to show the ingress is created successfully:\n$ kubectl describe ing oimcluster-nginx-designconsole -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl describe ing oimcluster-nginx-designconsole -n oimcluster The output will look similar to the following:\nName: oimcluster-nginx-designconsole Namespace: oimcluster Address: 10.106.181.99 Default backend: default-http-backend:80 (\u0026lt;error: endpoints \u0026quot;default-http-backend\u0026quot; not found\u0026gt;) Rules: Host Path Backends ---- ---- -------- * oimcluster-cluster-oim-cluster:14001 () Annotations: kubernetes.io/ingress.class: nginx-designconsole meta.helm.sh/release-name: oimcluster-nginx-designconsole meta.helm.sh/release-namespace: oimcluster nginx.ingress.kubernetes.io/affinity: cookie nginx.ingress.kubernetes.io/configuration-snippet: more_set_input_headers \u0026quot;X-Forwarded-Proto: https\u0026quot;; more_set_input_headers \u0026quot;WL-Proxy-SSL: true\u0026quot;; nginx.ingress.kubernetes.io/enable-access-log: false nginx.ingress.kubernetes.io/ingress.allow-http: false nginx.ingress.kubernetes.io/proxy-buffer-size: 2000k Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal CREATE 38s nginx-ingress-controller Ingress oimcluster/oimcluster-nginx-designconsole Normal UPDATE 10s nginx-ingress-controller Ingress oimcluster/oimcluster-nginx-designconsole   Design Console Client The instructions below should be performed on the client where Design Console is installed.\nImport the CA certificate into the java keystore If in Generate a SSL Certificate you requested a certificate from a Certificate Authority (CA), then you must import the CA certificate (e.g cacert.crt) that signed your certificate, into the java truststore used by Design Console.\nIf in Generate a SSL Certificate you generated a self-signed certicate (e.g tls.crt), you must import the self-signed certificate into the java truststore used by Design Console.\nImport the certificate using the following command:\n$ keytool -import -trustcacerts -alias dc -file \u0026lt;certificate\u0026gt; -keystore $JAVA_HOME/jre/lib/security/cacerts where \u0026lt;certificate\u0026gt; is the CA certificate, or self-signed certicate.\nLogin to the Design Console   Launch the Design Console and in the Oracle Identity Manager Design Console login page enter the following details:\nEnter the following details and click Login:\n Server URL: \u0026lt;url\u0026gt; User ID: xelsysadm Password: \u0026lt;password\u0026gt;.  where \u0026lt;url\u0026gt; is as per the following:\na) For NodePort: https://\u0026lt;masternode.example.com\u0026gt;:\u0026lt;NodePort\u0026gt;\nwhere \u0026lt;NodePort\u0026gt; is the value passed in the command earlier, for example: --set controller.service.nodePorts.http=30321\nb) For LoadBalancer: https://\u0026lt;loadbalancer.example.com\u0026gt;:\u0026lt;LBRPort\u0026gt;\n  If successful the Design Console will be displayed. If the VNC session disappears then the connection failed so double check the connection details and try again.\n  "
},
{
	"uri": "/fmw-kubernetes/oig/configure-design-console/using-the-design-console-with-voyager-non-ssl/",
	"title": "a. Using Design Console with Voyager(non-SSL)",
	"tags": [],
	"description": "Configure Design Console with Voyager(non-SSL).",
	"content": "Configure a Voyager ingress (non-SSL) to allow Design Console to connect to your Kubernetes cluster.\nDesign Console is not installed as part of the OAM Kubernetes cluster so must be installed on a seperate client before following the steps below.\n Add the Voyager ingress using helm Note: If already using Voyager with non-SSL for OIG you can skip this section:\n  Add the Helm chart repository for Voyager using the following command:\n$ helm repo add appscode https://charts.appscode.com/stable The output will look similar to the following:\n\u0026#34;appscode\u0026#34; has been added to your repositories   Update the repository using the following command:\n$ helm repo update The output will look similar to the following:\nHang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \u0026#34;appscode\u0026#34; chart repository Update Complete. Happy Helming!   Create a namespace for Voyager:\n$ kubectl create namespace voyager The output will look similar to the following:\nnamespace/voyager created   Install Voyager ingress using helm  $ helm install voyager-designconsole-operator appscode/voyager --version v12.0.0-rc.1 --namespace voyager --set cloudProvider=baremetal --set apiserver.enableValidatingWebhook=false Note: For bare metal Kubernetes use --set cloudProvider=baremetal. If using a managed Kubernetes service then the value should be set for your specific service as per the Voyager install guide.\nThe output will look similar to the following:\nNAME: voyager-designconsole-operator LAST DEPLOYED: Wed Oct 21 08:31:32 2020 NAMESPACE: voyager STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: Set cloudProvider for installing Voyager To verify that Voyager has started, run: kubectl --namespace=voyager get deployments -l \u0026quot;release=voyager-designconsole-operator, app=voyager\u0026quot; Setup Routing Rules for the Design Console ingress   Setup routing rules by running the following commands:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/charts/design-console-ingress $ cp values.yaml values.yaml.orig $ vi values.yaml Edit values.yaml and ensure that type=VOYAGER and tls=NONSSL are set, and that webPort and statsPort are set to free ports, for example:\n$ cat values.yaml # Copyright 2020 Oracle Corporation and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # Default values for design-console-ingress. # This is a YAML-formatted file. # Declare variables to be passed into your templates. # Load balancer type. Supported values are: VOYAGER, NGINX type: VOYAGER # Type of Configuration Supported Values are : NONSSL,SSL # tls: NONSSL tls: NONSSL # TLS secret name if the mode is SSL secretName: dc-tls-cert # WLS domain as backend to the load balancer wlsDomain: domainUID: oimcluster oimClusterName: oim_cluster oimServerT3Port: 14001 # Voyager specific values voyager: # web port webPort: 30325 # stats port statsPort: 30326   Create the ingress   Run the following command to create the ingress:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator $ helm install oimcluster-voyager-designconsole kubernetes/samples/charts/design-console-ingress --namespace oimcluster --values kubernetes/samples/charts/design-console-ingress/values.yaml For example:\n$ cd /scratch/OIGDockerK8S/weblogic-kubernetes-operator $ helm install oimcluster-voyager-designconsole kubernetes/samples/charts/design-console-ingress --namespace oimcluster --values kubernetes/samples/charts/design-console-ingress/values.yaml The output will look similar to the following:\nNAME: oimcluster-voyager-designconsole LAST DEPLOYED: Wed Oct 21 08:36:03 2020 NAMESPACE: oimcluster STATUS: deployed REVISION: 1 TEST SUITE: None   Run the following command to show the ingress is created successfully:\n$ kubectl get ingress.voyager.appscode.com -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get ingress.voyager.appscode.com -n oimcluster The output will look similar to the following:\nNAME HOSTS LOAD_BALANCER_IP AGE oimcluster-voyager-designconsole * 10s   Return details of the ingress using the following command:\n$ kubectl describe ingress.voyager.appscode.com oimcluster-voyager-designconsole -n oimcluster The output will look similar to the following:\nName: oimcluster-voyager-designconsole Namespace: oimcluster Labels: app.kubernetes.io/managed-by=Helm weblogic.resourceVersion=domain-v2 Annotations: ingress.appscode.com/affinity: cookie ingress.appscode.com/stats: true ingress.appscode.com/type: NodePort meta.helm.sh/release-name: oimcluster-voyager-designconsole meta.helm.sh/release-namespace: oimcluster API Version: voyager.appscode.com/v1beta1 Kind: Ingress Metadata: Creation Timestamp: 2020-10-21T15:46:29Z Generation: 1 Managed Fields: API Version: voyager.appscode.com/v1beta1 Fields Type: FieldsV1 fieldsV1: f:metadata: f:annotations: .: f:ingress.appscode.com/affinity: f:ingress.appscode.com/stats: f:ingress.appscode.com/type: f:meta.helm.sh/release-name: f:meta.helm.sh/release-namespace: f:labels: .: f:app.kubernetes.io/managed-by: f:weblogic.resourceVersion: f:spec: .: f:frontendRules: f:rules: f:tls: Manager: Go-http-client Operation: Update Time: 2020-10-21T15:46:29Z Resource Version: 6082128 Self Link: /apis/voyager.appscode.com/v1beta1/namespaces/oimcluster/ingresses/oimcluster-voyager-designconsole UID: a4968c01-28eb-4e4a-ac31-d60cfcd8705f Spec: Frontend Rules: Port: 443 Rules: http-request set-header WL-Proxy-SSL true Rules: Host: * Http: Node Port: 30325 Paths: Backend: Service Name: oimcluster-cluster-oim-cluster Service Port: 14001 Path: / Tls: Hosts: * Secret Name: oimcluster-tls-cert Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal DeploymentReconcileSuccessful 55m voyager-operator Successfully patched HAProxy Deployment voyager-oimcluster-voyager-designconsole Normal DeploymentReconcileSuccessful 45m voyager-operator Successfully patched HAProxy Deployment voyager-oimcluster-voyager-designconsole   Login to the Design Console   Launch the Design Console and in the Oracle Identity Manager Design Console login page enter the following details:\nEnter the following details and click Login:\n Server URL: \u0026lt;url\u0026gt; User ID: xelsysadm Password: \u0026lt;password\u0026gt;.  where \u0026lt;url\u0026gt; is http://\u0026lt;masternode.example.com\u0026gt;:\u0026lt;NodePort\u0026gt;\n\u0026lt;NodePort\u0026gt; is the value passed for webPort in the `values.yaml earlier, for example: 30325\n  If successful the Design Console will be displayed. If the VNC session disappears then the connection failed so double check the connection details and try again.\n  "
},
{
	"uri": "/fmw-kubernetes/oig/configure-design-console/using-the-design-console-with-voyager-ssl/",
	"title": "a. Using Design Console with Voyager(SSL)",
	"tags": [],
	"description": "Configure Design Console with Voyager(SSL).",
	"content": "Configure a Voyager ingress (SSL) to allow Design Console to connect to your Kubernetes cluster.\nDesign Console is not installed as part of the OAM Kubernetes cluster so must be installed on a seperate client before following the steps below.\n Generate SSL Certificate Note: If already using Voyager with SSL for OIG you can skip this section:\n  Generate a private key and certificate signing request (CSR) using a tool of your choice. Send the CSR to your certificate authority (CA) to generate the certificate.\nIf you want to use a certificate for testing purposes you can generate a self signed certificate using openssl:\n$ mkdir \u0026lt;work directory\u0026gt;/ssl $ cd \u0026lt;work directory\u0026gt;/ssl $ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \u0026quot;/CN=\u0026lt;nginx-hostname\u0026gt;\u0026quot; For example:\n$ mkdir /scratch/OIGDockerK8S/ssl $ cd /scratch/OIGDockerK8S/ssl $ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \u0026quot;/CN=masternode.example.com\u0026quot; Note: The CN should match the host.domain of the master node in order to prevent hostname problems during certificate verification.\nThe output will look similar to the following:\n$ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \u0026quot;/CN=masternode.example.com\u0026quot; Generating a 2048 bit RSA private key ..........................................+++ .......................................................................................................+++ writing new private key to 'tls.key' ----- $   Create a Kubernetes Secret for SSL Note: If already using Voyager with SSL for OIG you can skip this section:\n  Create a secret for SSL containing the SSL certificate by running the following command:\n$ kubectl -n oimcluster create secret tls \u0026lt;domain_id\u0026gt;-tls-cert --key \u0026lt;work directory\u0026gt;/tls.key --cert \u0026lt;work directory\u0026gt;/tls.crt For example:\n$ kubectl -n oimcluster create secret tls oimcluster-tls-cert --key /scratch/OIGDockerK8S/ssl/tls.key --cert /scratch/OIGDockerK8S/ssl/tls.crt The output will look similar to the following:\nsecret/oimcluster-tls-cert created Confirm that the secret is created by running the following command:\n$ kubectl get secret oimcluster-tls-cert -o yaml -n oimcluster The output will look similar to the following:\napiVersion: v1 data: tls.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURGVENDQWYyZ0F3SUJBZ0lKQUl3ZjVRMWVxZnljTUEwR0NTcUdTSWIzRFFFQkN3VUFNQ0V4SHpBZEJnTlYKQkFNTUZtUmxiakF4WlhadkxuVnpMbTl5WVdOc1pTNWpiMjB3SGhjTk1qQXdPREV3TVRReE9UUXpXaGNOTWpFdwpPREV3TVRReE9UUXpXakFoTVI4d0hRWURWUVFEREJaa1pXNHdNV1YyYnk1MWN5NXZjbUZqYkdVdVkyOXRNSUlCCklqQU5CZ2txaGtpRzl3MEJBUUVGQUFPQ0FROEFNSUlCQ2dLQ0FRRUEyY0lpVUhwcTRVZzBhaGR6aXkycHY2cHQKSVIza2s5REd2eVRNY0syaWZQQ2dtUU5CdHV6VXNFN0l4c294eldITmU5RFpXRXJTSjVON3FYYm1lTzJkMVd2NQp1aFhzbkFTbnkwY1NLUE9xOUNZVDNQSlpDVk1MK0llZVFKdnhaVjZaWWU4V2FFL1NQSGJzczRjYy9wcG1mc3pxCnErUi83cXEyMm9ueHNHaE9vQ1h1TlQvMFF2WXVzMnNucGtueWRKRHUxelhGbDREYkFIZGMvamNVK0NPWWROeS8KT3Iza2JIV0FaTkR4OWxaZUREOTRmNXZLbGJwMy9rcUF2V0FkSVJZa2UrSmpNTHg0VHo2ZlM0VXoxbzdBSTVuSApPQ1ZMblV5U0JkaGVuWTNGNEdFU0wwbnorVlhFWjRWVjRucWNjRmo5cnJ0Q29pT1BBNlgvNGdxMEZJbi9Qd0lECkFRQUJvMUF3VGpBZEJnTlZIUTRFRmdRVWw1VnVpVDBDT0xGTzcxMFBlcHRxSC9DRWZyY3dId1lEVlIwakJCZ3cKRm9BVWw1VnVpVDBDT0xGTzcxMFBlcHRxSC9DRWZyY3dEQVlEVlIwVEJBVXdBd0VCL3pBTkJna3Foa2lHOXcwQgpBUXNGQUFPQ0FRRUFXdEN4b2ZmNGgrWXZEcVVpTFFtUnpqQkVBMHJCOUMwL1FWOG9JQzJ3d1hzYi9KaVNuMHdOCjNMdHppejc0aStEbk1yQytoNFQ3enRaSkc3NVluSGRKcmxQajgzVWdDLzhYTlFCSUNDbTFUa3RlVU1jWG0reG4KTEZEMHpReFhpVzV0N1FHcWtvK2FjeN3JRMXlNSE9HdVVkTTZETzErNXF4cTdFNXFMamhyNEdKejV5OAoraW8zK25UcUVKMHFQOVRocG96RXhBMW80OEY0ZHJybWdqd3ROUldEQVpBYmYyV1JNMXFKWXhxTTJqdU1FQWNsCnFMek1TdEZUQ2o1UGFTQ0NUV1VEK3ZlSWtsRWRpaFdpRm02dzk3Y1diZ0lGMlhlNGk4L2szMmF1N2xUTDEvd28KU3Q2dHpsa20yV25uUFlVMzBnRURnVTQ4OU02Z1dybklpZz09Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K tls.key: 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 kind: Secret metadata: creationTimestamp: \u0026quot;2020-08-10T14:22:52Z\u0026quot; managedFields: - apiVersion: v1 fieldsType: FieldsV1 fieldsV1: f:data: .: {} f:tls.crt: {} f:tls.key: {} f:type: {} manager: kubectl operation: Update time: \u0026quot;2020-08-10T14:22:52Z\u0026quot; name: oimcluster-tls-cert namespace: oimcluster resourceVersion: \u0026quot;3722477\u0026quot; selfLink: /api/v1/namespaces/oimcluster/secrets/oimcluster-tls-cert uid: 596fe0fe-effd-4eb9-974d-691da3a3b15a type: kubernetes.io/tls   Add the Voyager ingress using helm Note: If already using Voyager with SSL for OIG you can skip this section:\n  Add the Helm chart repository for Voyager using the following command:\n$ helm repo add appscode https://charts.appscode.com/stable The output will look similar to the following:\n\u0026#34;appscode\u0026#34; has been added to your repositories   Update the repository using the following command:\n$ helm repo update The output will look similar to the following:\nHang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \u0026#34;appscode\u0026#34; chart repository Update Complete. Happy Helming!   Create a namespace for Voyager:\n$ kubectl create namespace voyagerssl The output will look similar to the following:\nnamespace/voyagerssl created   Install Voyager ingress using helm   Run the following command to install the ingress:\n$ helm install voyager-designconsole-operator appscode/voyager --version v12.0.0-rc.1 --namespace voyagerssl --set cloudProvider=baremetal --set apiserver.enableValidatingWebhook=false Note: For bare metal Kubernetes use --set cloudProvider=baremetal. If using a managed Kubernetes service then the value should be set for your specific service as per the Voyager install guide.\nThe output will look similar to the following:\nNAME: voyager-designconsole-operator LAST DEPLOYED: Wed Oct 21 09:24:55 2020 NAMESPACE: voyagerssl STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: Set cloudProvider for installing Voyager To verify that Voyager has started, run: kubectl --namespace=voyagerssl get deployments -l \u0026quot;release=voyager-designconsole-operator, app=voyager\u0026quot;   Setup Routing Rules for the Design Console ingress   Setup routing rules by running the following commands:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/charts/design-console-ingress $ cp values.yaml values.yaml.orig $ vi values.yaml Edit values.yaml and ensure that type=VOYAGER, tls=SSL and secretName:oimcluster-tls-cert are set, and that webPort and statsPort are set to free ports, for example:\n$ cat values.yaml # Copyright 2020 Oracle Corporation and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # Default values for design-console-ingress. # This is a YAML-formatted file. # Declare variables to be passed into your templates. # Load balancer type. Supported values are: VOYAGER, NGINX type: VOYAGER # Type of Configuration Supported Values are : NONSSL,SSL # tls: NONSSL tls: SSL # TLS secret name if the mode is SSL secretName: oimcluster-tls-cert # WLS domain as backend to the load balancer wlsDomain: domainUID: oimcluster oimClusterName: oim_cluster oimServerT3Port: 14001 # Voyager specific values voyager: # web port webPort: 30330 # stats port statsPort: 30331   Create the ingress   Run the following command to create the ingress:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator $ helm install oimcluster-voyager-designconsole kubernetes/samples/charts/design-console-ingress --namespace oimcluster --values kubernetes/samples/charts/design-console-ingress/values.yaml For example:\n$ cd /scratch/OIGDockerK8S/weblogic-kubernetes-operator $ helm install oimcluster-voyager-designconsole kubernetes/samples/charts/design-console-ingress --namespace oimcluster --values kubernetes/samples/charts/design-console-ingress/values.yaml The output will look similar to the following:\nNAME: oimcluster-voyager-designconsole LAST DEPLOYED: Wed Oct 21 09:59:43 2020 NAMESPACE: oimcluster STATUS: deployed REVISION: 1 TEST SUITE: None   Run the following command to show the ingress is created successfully:\n$ kubectl get ingress.voyager.appscode.com -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get ingress.voyager.appscode.com -n oimcluster The output will look similar to the following:\nNAME HOSTS LOAD_BALANCER_IP AGE oimcluster-voyager-designconsole * 10s   Return details of the ingress using the following command:\n$ kubectl describe ingress.voyager.appscode.com oimcluster-voyager-designconsole -n oimcluster The output will look similar to the following:\nName: oimcluster-voyager-designconsole Namespace: oimcluster Labels: app.kubernetes.io/managed-by=Helm weblogic.resourceVersion=domain-v2 Annotations: ingress.appscode.com/affinity: cookie ingress.appscode.com/stats: true ingress.appscode.com/type: NodePort meta.helm.sh/release-name: oimcluster-voyager-designconsole meta.helm.sh/release-namespace: oimcluster API Version: voyager.appscode.com/v1beta1 Kind: Ingress Metadata: Creation Timestamp: 2020-10-21T09:26:48Z Generation: 1 Resource Version: 15430914 Self Link: /apis/voyager.appscode.com/v1beta1/namespaces/oimcluster/ingresses/oimcluster-voyager-designconsole UID: 89f42060-c8e6-470f-b661-14b9969fe1aa Spec: Frontend Rules: Port: 443 Rules: http-request set-header WL-Proxy-SSL true Rules: Host: * Http: Node Port: 30330 Paths: Backend: Service Name: oimcluster-cluster-oim-cluster Service Port: 14001 Path: / Tls: Hosts: * Secret Name: dc-tls-cert Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ServiceReconcileSuccessful 54m voyager-operator Successfully patched NodePort Service voyager-oimcluster-voyager-designconsole Normal DeploymentReconcileSuccessful 54m voyager-operator Successfully patched HAProxy Deployment voyager-oimcluster-voyager-designconsole Normal DeploymentReconcileSuccessful 44m voyager-operator Successfully patched HAProxy Deployment voyager-oimcluster-voyager-designconsole   Design Console Client The instructions below should be performed on the client where Design Console is installed.\nImport the CA certificate into the java keystore If in Generate a SSL Certificate you requested a certificate from a Certificate Authority (CA), then you must import the CA certificate (e.g cacert.crt) that signed your certificate, into the java truststore used by Design Console.\nIf in Generate a SSL Certificate you generated a self-signed certicate (e.g tls.crt), you must import the self-signed certificate into the java truststore used by Design Console.\nImport the certificate using the following command:\n$ keytool -import -trustcacerts -alias dc -file \u0026lt;certificate\u0026gt; -keystore $JAVA_HOME/jre/lib/security/cacerts where \u0026lt;certificate\u0026gt; is the CA certificate, or self-signed certicate.\nLogin to the Design Console   Launch the Design Console and in the Oracle Identity Manager Design Console login page enter the following details:\nEnter the following details and click Login:\n Server URL: \u0026lt;url\u0026gt; User ID: xelsysadm Password: \u0026lt;password\u0026gt;.  where \u0026lt;url\u0026gt; is http://\u0026lt;masternode.example.com\u0026gt;:\u0026lt;NodePort\u0026gt;\n\u0026lt;NodePort\u0026gt; is the value passed for webPort in the `values.yaml earlier, for example: 30330\n  If successful the Design Console will be displayed. If the VNC session disappears then the connection failed so double check the connection details and try again.\n  "
},
{
	"uri": "/fmw-kubernetes/wcsites-domains/manage-wcsites-domains/loadbalancer-traefik-setup-for-wcsites-domain-setup-on-k8s/",
	"title": "a. Using Traefik Loadbalancer",
	"tags": [],
	"description": "Steps to set up Traefik as a loadbalancer for the WebCenter Sites domain.",
	"content": "Setting Up Loadbalancer Traefik for the WebCenter Sites Domain on K8S The Oracle WebLogic Server Kubernetes Operator supports three load balancers: Traefik, Voyager, and Apache. Follow these steps to set up Traefik as a loadbalancer for the WebCenter Sites domain:\n Install the Traefik Load Balancer Configure Traefik to Manage Ingresses Create an Ingress for the Domain Verify that You can Access the Domain URL  Install the Traefik Load Balancer   Use helm to install the Traefik load balancer. For detailed information, see this document. Use the values.yaml file in the sample but set kubernetes.namespaces specifically.\n Install Traefik\n $ cd weblogic-kubernetes-operator $ helm install stable/traefik --name traefik-operator \\  --namespace traefik --values kubernetes/samples/charts/traefik/values.yaml \\  --set \u0026#34;dashboard.domain=$(hostname -f),kubernetes.namespaces={traefik}\u0026#34;  Output\n $ cd weblogic-kubernetes-operator $ helm install stable/traefik --name traefik-operator --namespace traefik --values kubernetes/samples/charts/traefik/values.yaml --set \u0026#34;dashboard.domain=$(hostname -f),kubernetes.namespaces={traefik}\u0026#34; --wait NAME: traefik-operator LAST DEPLOYED: Sat Mar 14 13:53:16 2020 NAMESPACE: traefik STATUS: DEPLOYED RESOURCES: ==\u0026gt; v1/Service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE traefik-operator-dashboard ClusterIP 10.108.89.215 \u0026lt;none\u0026gt; 80/TCP 0s traefik-operator NodePort 10.99.75.162 \u0026lt;none\u0026gt; 443:30443/TCP,80:30305/TCP 0s ==\u0026gt; v1/Secret NAME TYPE DATA AGE traefik-operator-default-cert Opaque 2 0s ==\u0026gt; v1/ServiceAccount NAME SECRETS AGE traefik-operator 1 0s ==\u0026gt; v1/RoleBinding NAME AGE traefik-operator 0s ==\u0026gt; v1beta1/Ingress NAME HOSTS ADDRESS PORTS AGE traefik-operator-dashboard abc.def.com 80 0s ==\u0026gt; v1/Pod(related) NAME READY STATUS RESTARTS AGE traefik-operator-844859fdd6-prh55 0/1 ContainerCreating 0 0s ==\u0026gt; v1/ConfigMap NAME DATA AGE traefik-operator 1 0s ==\u0026gt; v1/Role NAME AGE traefik-operator 0s ==\u0026gt; v1/Deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE traefik-operator 1 1 1 0 0s NOTES: 1. Traefik is listening on the following ports on the host machine: http - 30305 https - 30443 2. Configure DNS records corresponding to Kubernetes ingress resources to point to the NODE_IP/NODE_HOST   Access the Traefik dashboard through the URL http://$(hostname -f):30305, with the HTTP host traefik.example.com. NOTE: Make sure you specify full qualified node name for $(hostname -f).\n$ curl -H \u0026#39;host: $(hostname -f)\u0026#39; http://$(hostname -f):30305/ \u0026lt;a href=\u0026#34;/dashboard/\u0026#34;\u0026gt;Found\u0026lt;/a\u0026gt;. $   Configure Traefik to Manage Ingresses Configure Traefik to manage Ingresses created in this namespace: Note: Here traefik is the Traefik namespace, wcsites-ns is the namespace of the domain.\n helm upgrade for traefik\n $ helm upgrade --reuse-values --set \u0026#34;kubernetes.namespaces={traefik,wcsites-ns}\u0026#34; --wait traefik-operator stable/traefik Release \u0026#34;traefik-operator\u0026#34; has been upgraded. Happy Helming! LAST DEPLOYED: Sat Mar 14 13:58:03 2020 NAMESPACE: traefik STATUS: DEPLOYED RESOURCES: ==\u0026gt; v1/ConfigMap NAME DATA AGE traefik-operator 1 5m2s ==\u0026gt; v1/ClusterRole NAME AGE traefik-operator 14s ==\u0026gt; v1/Service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE traefik-operator-dashboard ClusterIP 10.108.89.215 \u0026lt;none\u0026gt; 80/TCP 5m2s traefik-operator NodePort 10.99.75.162 \u0026lt;none\u0026gt; 443:30443/TCP,80:30305/TCP 5m2s ==\u0026gt; v1/Deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE traefik-operator 1 1 1 1 5m2s ==\u0026gt; v1/Secret NAME TYPE DATA AGE traefik-operator-default-cert Opaque 2 5m2s ==\u0026gt; v1/ServiceAccount NAME SECRETS AGE traefik-operator 1 5m2s ==\u0026gt; v1/ClusterRoleBinding NAME AGE traefik-operator 14s ==\u0026gt; v1beta1/Ingress NAME HOSTS ADDRESS PORTS AGE traefik-operator-dashboard abc.def.com 80 5m2s ==\u0026gt; v1/Pod(related) NAME READY STATUS RESTARTS AGE traefik-operator-844699994b-ttfb6 1/1 Running 0 14s traefik-operator-844859fdd6-prh55 0/1 Terminating 0 5m2s NOTES: 1. Traefik is listening on the following ports on the host machine: http - 30305 https - 30443 2. Configure DNS records corresponding to Kubernetes ingress resources to point to the NODE_IP/NODE_HOST Create an Ingress for the Domain   Create an Ingress for the domain (ingress-per-domain-wcsites), in the domain namespace by using the sample Helm chart. Here we are using the path-based routing for ingress. For detailed instructions about ingress, see this page).\nFor now, you can update the kubernetes/samples/scripts/create-wcsites-domain/ingress-per-domain/values.yaml with appropriate values. Sample values are shown below:\n$ cat kubernetes/samples/scripts/create-wcsites-domain/ingress-per-domain/values.yaml # Copyright 2020, Oracle Corporation and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # Default values for ingress-per-domain. # This is a YAML-formatted file. # Declare variables to be passed into your templates. # Load balancer type. Supported values are: TRAEFIK, VOYAGER type: TRAEFIK #type: VOYAGER # WLS domain as backend to the load balancer wlsDomain: domainUID: wcsitesinfra adminServerName: adminserver adminServerPort: 7001 wcsitesClusterName: wcsites_cluster wcsitesManagedServerPort: 8001 # Voyager specific values voyager: # web port webPort: 30305 # stats port statsPort: 30317   Update the kubernetes/samples/scripts/create-wcsites-domain/ingress-per-domain/templates/traefik-ingress.yaml with the url routes to be load balanced.\nBelow are the defined ingress rules:\nNOTE: This is not an exhaustive list of rules. You can enhance it based on the application urls that need to be accessed externally. These rules hold good for domain type WCSITES.\n$ vi kubernetes/samples/scripts/create-wcsites-domain/ingress-per-domain/templates/traefik-ingress.yaml # Copyright 2020, Oracle Corporation and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. {{- if eq .Values.type \u0026#34;TRAEFIK\u0026#34; }} --- apiVersion: extensions/v1beta1 kind: Ingress metadata: name: {{ .Values.wlsDomain.domainUID }}-traefik namespace: {{ .Release.Namespace }} labels: weblogic.resourceVersion: domain-v2 spec: annotations: kubernetes.io/ingress.class: traefik rules: - host: \u0026#39;{{ .Values.traefik.hostname }}\u0026#39; http: paths: - path: /console backend: serviceName: \u0026#39;{{ .Values.wlsDomain.domainUID }}-{{ .Values.wlsDomain.adminServerName | lower | replace \u0026#34;_\u0026#34; \u0026#34;-\u0026#34; }}\u0026#39; servicePort: {{ .Values.wlsDomain.adminServerPort }} - path: /em backend: serviceName: \u0026#39;{{ .Values.wlsDomain.domainUID }}-{{ .Values.wlsDomain.adminServerName | lower | replace \u0026#34;_\u0026#34; \u0026#34;-\u0026#34; }}\u0026#39; servicePort: {{ .Values.wlsDomain.adminServerPort }} - path: /wls-exporter backend: serviceName: \u0026#39;{{ .Values.wlsDomain.domainUID }}-{{ .Values.wlsDomain.adminServerName | lower | replace \u0026#34;_\u0026#34; \u0026#34;-\u0026#34; }}\u0026#39; servicePort: {{ .Values.wlsDomain.adminServerPort }} - path: /weblogic backend: serviceName: \u0026#39;{{ .Values.wlsDomain.domainUID }}-{{ .Values.wlsDomain.adminServerName | lower | replace \u0026#34;_\u0026#34; \u0026#34;-\u0026#34; }}\u0026#39; servicePort: {{ .Values.wlsDomain.adminServerPort }} - path: /sbconsole backend: serviceName: \u0026#39;{{ .Values.wlsDomain.domainUID }}-{{ .Values.wlsDomain.adminServerName | lower | replace \u0026#34;_\u0026#34; \u0026#34;-\u0026#34; }}\u0026#39; servicePort: {{ .Values.wlsDomain.adminServerPort }} - path: /sites backend: serviceName: \u0026#39;{{ .Values.wlsDomain.domainUID }}-cluster-{{ .Values.wlsDomain.wcsitesClusterName | lower | replace \u0026#34;_\u0026#34; \u0026#34;-\u0026#34; }}\u0026#39; servicePort: {{ .Values.wlsDomain.wcsitesManagedServerPort }} - path: /cas backend: serviceName: \u0026#39;{{ .Values.wlsDomain.domainUID }}-cluster-{{ .Values.wlsDomain.wcsitesClusterName | lower | replace \u0026#34;_\u0026#34; \u0026#34;-\u0026#34; }}\u0026#39; servicePort: {{ .Values.wlsDomain.wcsitesManagedServerPort }} - path: /wls-exporter backend: serviceName: \u0026#39;{{ .Values.wlsDomain.domainUID }}-cluster-{{ .Values.wlsDomain.wcsitesClusterName | lower | replace \u0026#34;_\u0026#34; \u0026#34;-\u0026#34; }}\u0026#39; servicePort: {{ .Values.wlsDomain.wcsitesManagedServerPort }} # - path: /wls-cat # backend: # serviceName: \u0026#39;{{ .Values.wlsDomain.domainUID }}-cluster-{{ .Values.wlsDomain.wcsitesClusterName | lower | replace \u0026#34;_\u0026#34; \u0026#34;-\u0026#34; }}\u0026#39; # servicePort: {{ .Values.wlsDomain.wcsitesManagedServerPort }} # - path: # backend: # serviceName: \u0026#39;{{ .Values.wlsDomain.domainUID }}-cluster-{{ .Values.wlsDomain.wcsitesClusterName | lower | replace \u0026#34;_\u0026#34; \u0026#34;-\u0026#34; }}\u0026#39; # servicePort: {{ .Values.wlsDomain.wcsitesManagedServerPort }} {{- end }}   Install \u0026ldquo;ingress-per-domain\u0026rdquo; using helm.\nbash-4.2$ cd weblogic-kubernetes-operator bash-4.2$ helm install kubernetes/samples/scripts/create-wcsites-domain/ingress-per-domain \\  --name wcsitesinfra-ingress --namespace wcsites-ns \\  --values kubernetes/samples/scripts/create-wcsites-domain/ingress-per-domain/values.yaml \\  --set \u0026#34;traefik.hostname=$(hostname -f)\u0026#34; NAME: wcsitesinfra-ingress LAST DEPLOYED: Sat Mar 14 14:03:22 2020 NAMESPACE: wcsites-ns STATUS: DEPLOYED RESOURCES: ==\u0026gt; v1beta1/Ingress NAME HOSTS ADDRESS PORTS AGE wcsitesinfra-traefik abc.def.com 80 0s   To confirm that the load balancer noticed the new Ingress and is successfully routing to the domain\u0026rsquo;s server pods, you can send a request to the URL for the \u0026ldquo;WebLogic ReadyApp framework\u0026rdquo; which should return a HTTP 200 status code, as shown in the example below:\n  -bash-4.2$ curl -v http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-PORT}/weblogic/ready * Trying 149.87.129.203... \u0026gt; GET http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-PORT}/weblogic/ready HTTP/1.1 \u0026gt; User-Agent: curl/7.29.0 \u0026gt; Accept: */* \u0026gt; Proxy-Connection: Keep-Alive \u0026gt; host: $(hostname -f) \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; Date: Sat, 14 Mar 2020 08:35:03 GMT \u0026lt; Vary: Accept-Encoding \u0026lt; Content-Length: 0 \u0026lt; Proxy-Connection: Keep-Alive \u0026lt; * Connection #0 to host localhost left intact Verify that You can Access the Domain URL After setting up the Traefik loadbalancer, verify that the domain applications are accessible through the loadbalancer port 30305. Through load balancer (Traefik port 30305), the following URLs are available for setting up domains of WebCenter Sites domain types:\nhttp://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-PORT}/weblogic/ready http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-PORT}/console http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-PORT}/em http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-PORT}/sites/version.jsp "
},
{
	"uri": "/fmw-kubernetes/oudsm/patch-and-upgrade/upgrade_a_kubernetes_cluster/",
	"title": "b) Upgrade a Kubernetes cluster",
	"tags": [],
	"description": "Instructions on how to upgrade a Kubernetes cluster.",
	"content": "These instructions describe how to upgrade a Kubernetes cluster created using kubeadm on which an Oracle Unified Directory Services Manager instance is deployed. A rolling upgrade approach is used to upgrade nodes (master and worker) of the Kubernetes cluster.\nIt is expected that there will be a down time during the upgrade of the Kubernetes cluster as the nodes need to be drained as part of the upgrade process.\n Prerequisites  Review Prerequisites and ensure that your Kubernetes cluster is ready for upgrade. Make sure your environment meets all prerequisites.  Upgrade the Kubernetes version An upgrade of Kubernetes is supported from one MINOR version to the next MINOR version, or between PATCH versions of the same MINOR. For example, you can upgrade from 1.x to 1.x+1, but not from 1.x to 1.x+2. To upgrade a Kubernetes version, first all the master nodes of the Kubernetes cluster must be upgraded sequentially, followed by the sequential upgrade of each worker node.\n See here for Kubernetes official documentation to upgrade from v1.16.x to v1.17.x. See here for Kubernetes official documentation to upgrade from v1.17.x to v1.18.x.  "
},
{
	"uri": "/fmw-kubernetes/oig/post-install-config/install_and_configure_connectors/",
	"title": "b. Install and Configure Connectors",
	"tags": [],
	"description": "Install and Configure Connectors.",
	"content": "Download the Connector   Download the Connector you are interested in from Oracle Identity Manager Connector Downloads.\n  Copy the connector zip file to a staging directory on the master node e.g. /scratch/OIGDocker/stage and unzip it:\n$ cp $HOME/Downloads/\u0026lt;connector\u0026gt;.zip \u0026lt;work directory\u0026gt;/\u0026lt;stage\u0026gt;/ $ cd \u0026lt;work directory\u0026gt;/\u0026lt;stage\u0026gt; $ unzip \u0026lt;connector\u0026gt;.zip For example:\n$ cp $HOME/Downloads/Exchange-12.2.1.3.0.zip /scratch/OIGDocker/stage/ $ cd /scratch/OIGDockerK8S/stage/ $ unzip Exchange-12.2.1.3.0.zip   Create a directory in the persistent volume   On the master node run the following command to create a ConnectorDefaultDirectory:\n$ kubectl exec -ti oimcluster-oim-server1 -n \u0026lt;domain_namespace\u0026gt; -- mkdir -p /u01/oracle/user_projects/domains/ConnectorDefaultDirectory For example:\n$ kubectl exec -ti oimcluster-oim-server1 -n oimcluster -- mkdir -p /u01/oracle/user_projects/domains/ConnectorDefaultDirectory Note: This will create a directory in the persistent volume e:g /scratch/OIGDockerK8S/oimclusterdomainpv/ConnectorDefaultDirectory,\n  Copy OIG Connectors There are two options to copy OIG Connectors to your Kubernetes cluster:\n a) Copy the connector directly to the Persistent Volume b) Use the kubectl cp command to copy the connector to the Persistent Volume  It is recommended to use option a), however there may be cases, for example when using a Managed Service such as Oracle Kubernetes Engine on Oracle Cloud Infrastructure, where it may not be feasible to directly mount the domain directory. In such cases option b) should be used.\na) Copy the connector directly to the Persistent Volume   Copy the connector zip file to the persistent volume. For example:\n$ cp -R \u0026lt;path_to\u0026gt;/\u0026lt;connector\u0026gt; \u0026lt;work directory\u0026gt;/oimclusterdomainpv/ConnectorDefaultDirectory/ For example:\n$ cp -R /scratch/OIGDockerK8S/stage/Exchange-12.2.1.3.0 /scratch/OIGDockerK8S/oimclusterdomainpv/ConnectorDefaultDirectory/   b) Use the kubectl cp command to copy the connector to the Persistent Volume   Run the following command to copy over the connector:\n$ kubectl -n \u0026lt;domain_namespace\u0026gt; cp \u0026lt;path_to\u0026gt;/\u0026lt;connector\u0026gt; \u0026lt;cluster_name\u0026gt;:/u01/oracle/idm/server/ConnectorDefaultDirectory/ For example:\n$ kubectl -n oimcluster cp /scratch/OIGDockerK8S/stage/Exchange-12.2.1.3.0 oimcluster-oim-server1:/u01/oracle/idm/server/ConnectorDefaultDirectory/   Install the Connector The connectors are installed as they are on a standard on-premises setup, via Application On Boarding or via Connector Installer.\nRefer to your Connector specific documentation for instructions.\n"
},
{
	"uri": "/fmw-kubernetes/oam/patch-and-upgrade/upgrade_an_operator_release/",
	"title": "b. Upgrade an operator release",
	"tags": [],
	"description": "Instructions on how to update the Oracle WebLogic Server Kubernetes Operator version.",
	"content": "These instructions apply to upgrading the operator within the 3.x release family as additional versions are released.\nThe new Oracle WebLogic Server Kubernetes Operator Docker image must be installed on the master node AND each of the worker nodes in your Kubernetes cluster. Alternatively you can place the image in a Docker registry that your cluster can access.\n   Pull the Oracle WebLogic Server Kubernetes Operator 3.X.X image by running the following command on the master node:\n$ docker pull oracle/weblogic-kubernetes-operator:3.X.X where 3.X.X is the version of the operator you require.\n  Run the docker tag command as follows:\n$ docker tag oracle/weblogic-kubernetes-operator:3.X.X weblogic-kubernetes-operator:3.X.X where 3.X.X is the version of the operator downloaded.\nAfter installing the new Oracle WebLogic Server Kubernetes Operator Docker image, repeat the above on the worker nodes.\n  On the master node, download the new Oracle WebLogic Server Kubernetes Operator source code from the operator github project:\n$ mkdir \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator-3.X.X $ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator-3.X.X $ git clone https://github.com/oracle/weblogic-kubernetes-operator.git --branch release/3.X.X For example:\n$ cd /scratch/OAMDockerK8S/weblogic-kubernetes-operator-3.X.X $ git clone https://github.com/oracle/weblogic-kubernetes-operator.git --branch release/3.X.X This will create the directory \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator-3.X.X/weblogic-kubernetes-operator\n  Run the following helm command to upgrade the operator:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator-3.X.X/weblogic-kubernetes-operator $ helm upgrade --reuse-values --set image=oracle/weblogic-kubernetes-operator:3.X.X --namespace \u0026lt;sample-kubernetes-operator-ns\u0026gt; --wait weblogic-kubernetes-operator kubernetes/charts/weblogic-operator For example:\n$ cd /scratch/OAMDockerK8S/weblogic-kubernetes-operator-3.X.X/weblogic-kubernetes-operator $ helm upgrade --reuse-values --set image=oracle/weblogic-kubernetes-operator:3.X.X --namespace opns --wait weblogic-kubernetes-operator kubernetes/charts/weblogic-operator The output will look similar to the following:\nRelease \u0026#34;weblogic-kubernetes-operator\u0026#34; has been upgraded. Happy Helming! NAME: weblogic-kubernetes-operator LAST DEPLOYED: Mon Sep 28 02:50:07 2020 NAMESPACE: opns STATUS: deployed REVISION: 3 TEST SUITE: None   "
},
{
	"uri": "/fmw-kubernetes/oig/patch-and-upgrade/upgrade_an_operator_release/",
	"title": "b. Upgrade an operator release",
	"tags": [],
	"description": "Instructions on how to update the Oracle WebLogic Kubernetes Operator version.",
	"content": "These instructions apply to upgrading operators within the 3.x release family as additional versions are released.\nThe new Oracle WebLogic Kubernetes Operator Docker image must be installed on the master node AND each of the worker nodes in your Kubernetes cluster. Alternatively you can place the image in a Docker registry that your cluster can access.\n   Pull the Oracle WebLogic Kubernetes Operator 3.X.X image by running the following command on the master node:\n$ docker pull oracle/weblogic-kubernetes-operator:3.X.X where 3.X.X is the version of the operator you require.\n  Run the docker tag command as follows:\n$ docker tag oracle/weblogic-kubernetes-operator:3.X.X weblogic-kubernetes-operator:3.X.X where 3.X.X is the version of the operator downloaded.\nAfter installing the new Oracle WebLogic Kubernetes Operator Docker image, repeat the above on the worker nodes.\n  On the master node, download the new Oracle WebLogic Kubernetes Operator source code from the operator github project:\n$ mkdir \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator-3.X.X $ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator-3.X.X $ git clone https://github.com/oracle/weblogic-kubernetes-operator.git --branch release/3.X.X For example:\n$ mkdir /scratch/OIGDockerK8S/weblogic-kubernetes-operator-3.X.X $ cd /scratch/OIGDockerK8S/weblogic-kubernetes-operator-3.X.X $ git clone https://github.com/oracle/weblogic-kubernetes-operator.git --branch release/3.X.X This will create the directory \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator-3.X.X/weblogic-kubernetes-operator\n  Run the following helm command to upgrade the operator:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator-3.X.X/weblogic-kubernetes-operator $ helm upgrade --reuse-values --set image=oracle/weblogic-kubernetes-operator:3.X.X --namespace \u0026lt;sample-kubernetes-operator-ns\u0026gt; --wait weblogic-kubernetes-operator kubernetes/charts/weblogic-operator For example:\n$ cd /scratch/OIGDockerK8S/weblogic-kubernetes-operator-3.X.X/weblogic-kubernetes-operator $ helm upgrade --reuse-values --set image=oracle/weblogic-kubernetes-operator:3.X.X --namespace operator --wait weblogic-kubernetes-operator kubernetes/charts/weblogic-operator The output will look similar to the following:\nRelease \u0026#34;weblogic-kubernetes-operator\u0026#34; has been upgraded. Happy Helming! NAME: weblogic-kubernetes-operator LAST DEPLOYED: Thu Oct 01 02:50:07 2020 NAMESPACE: operator STATUS: deployed REVISION: 3 TEST SUITE: None   "
},
{
	"uri": "/fmw-kubernetes/oig/configure-ingress/ingress-nginx-setup-for-oig-domain-setup-on-k8s-ssl/",
	"title": "b. Using an Ingress with NGINX (SSL)",
	"tags": [],
	"description": "Steps to set up an Ingress for NGINX to direct traffic to the OIG domain (SSL).",
	"content": "Setting Up an Ingress for NGINX for the OIG Domain on Kubernetes The instructions below explain how to set up NGINX as an ingress for the OIG domain with SSL termination.\nNote: All the steps below should be performed on the master node.\n Create a SSL Certificate  Generate SSL Certificate Create a Kubernetes Secret for SSL   Install NGINX  Configure the repository Create a Namespace Install NGINX using helm   Create an Ingress for the Domain Verify that You can Access the Domain URL Cleanup  Create a SSL Certificate Generate SSL Certificate   Generate a private key and certificate signing request (CSR) using a tool of your choice. Send the CSR to your certificate authority (CA) to generate the certificate.\nIf you want to use a certificate for testing purposes you can generate a self signed certificate using openssl:\n$ mkdir \u0026lt;work directory\u0026gt;/ssl $ cd \u0026lt;work directory\u0026gt;/ssl $ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \u0026quot;/CN=\u0026lt;nginx-hostname\u0026gt;\u0026quot; For example:\n$ mkdir /scratch/OIGDockerK8S/ssl $ cd /scratch/OIGDockerK8S/ssl $ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \u0026quot;/CN=masternode.example.com\u0026quot; Note: The CN should match the host.domain of the master node in order to prevent hostname problems during certificate verification.\nThe output will look similar to the following:\n$ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \u0026quot;/CN=masternode.example.com\u0026quot; Generating a 2048 bit RSA private key ..........................................+++ .......................................................................................................+++ writing new private key to 'tls.key' -----   Create a Kubernetes Secret for SSL   Create a secret for SSL containing the SSL certificate by running the following command:\n$ kubectl -n oimcluster create secret tls \u0026lt;domain_id\u0026gt;-tls-cert --key \u0026lt;work directory\u0026gt;/tls.key --cert \u0026lt;work directory\u0026gt;/tls.crt For example:\n$ kubectl -n oimcluster create secret tls oimcluster-tls-cert --key /scratch/OIGDockerK8S/ssl/tls.key --cert /scratch/OIGDockerK8S/ssl/tls.crt The output will look similar to the following:\n$ kubectl -n oimcluster create secret tls oimcluster-tls-cert --key /scratch/OIGDockerK8S/ssl/tls.key --cert /scratch/OIGDockerK8S/ssl/tls.crt secret/oimcluster-tls-cert created $   Confirm that the secret is created by running the following command:\n$ kubectl get secret oimcluster-tls-cert -o yaml -n oimcluster The output will look similar to the following:\napiVersion: v1 data: tls.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURGVENDQWYyZ0F3SUJBZ0lKQUl3ZjVRMWVxZnljTUEwR0NTcUdTSWIzRFFFQkN3VUFNQ0V4SHpBZEJnTlYKQkFNTUZtUmxiakF4WlhadkxuVnpMbTl5WVdOc1pTNWpiMjB3SGhjTk1qQXdPREV3TVRReE9UUXpXaGNOTWpFdwpPREV3TVRReE9UUXpXakFoTVI4d0hRWURWUVFEREJaa1pXNHdNV1YyYnk1MWN5NXZjbUZqYkdVdVkyOXRNSUlCCklqQU5CZ2txaGtpRzl3MEJBUUVGQUFPQ0FROEFNSUlCQ2dLQ0FRRUEyY0lpVUhwcTRVZzBhaGR6aXkycHY2cHQKSVIza2s5REd2eVRNY0syaWZQQ2dtUU5CdHV6VXNFN0l4c294eldITmU5RFpXRXJTSjVON3FYYm1lTzJkMVd2NQp1aFhzbkFTbnkwY1NLUE9xVDNQSlpDVk1MK0llZVFKdnhaVjZaWWU4V2FFL1NQSGJzczRjYy9wcG1mc3pxCnErUi83cXEyMm9ueHNHaE9vQ1h1TlQvMFF2WXVzMnNucGtueWRKRHUxelhGbDREYkFIZGMvamNVK0NPWWROeS8KT3Iza2JIV0FaTkR4OWxaZUREOTRmNXZLcUF2V0FkSVJZa2UrSmpNTHg0VHo2ZlM0VXoxbzdBSTVuSApPQ1ZMblV5U0JkaGVuWTNGNEdFU0wwbnorVlhFWjRWVjRucWNjRmo5cnJ0Q29pT1BBNlgvNGdxMEZJbi9Qd0lECkFRQUJvMUF3VGpBZEJnTlZIUTRFRmdRVWw1VnVpVDBDT0xGTzcxMFBlcHRxSC9DRWZyY3dId1lEVlIwakJCZ3cKRm9BVWw1VnVpVDBDT0xGTzcxMFBlcHRxSC9DRWZyY3dEQVlEVlIwVEJBVXdBd0VCL3pBTkJna3Foa2lHOXcwQgpBUXNGQUFPQ0FRRUFXdEN4b2ZmNGgrWXZEcVVpTFFtUnpqQkVBMHJCOUMwL1FWOG9JQzJ3d1hzYi9KaVNuMHdOCjNMdHppejc0aStEbk1yQytoNFQ3enRaSkc3NVluSGRKcmxQajgzVWdDLzhYTlFCSUNDbTFUa3RlVU1jWG0reG4KTEZEMHpReFhpVzV0N1FHcWtvK2FjeTlhUnUvN3JRMXlNSE9HdVVkTTZETzErNXF4cTdFNXFMamhyNEdKejV5OAoraW8zK25UcUVKMHFQOVRocG96RXhBMW80OEY0ZHJybWdqd3ROUldEQVpBYmYyV1JNMXFKWXhxTTJqdU1FQWNsCnFMek1TdEZUQ2o1UGFTQ0NUV1VEK3ZlSWtsRWRpaFdpRm02dzk3Y1diZ0lGMlhlNGk4L2szMmF1N2xUTDEvd28KU3Q2dHpsa20yV25uUFlVMzBnRURnVTQ4OU02Z1dybklpZz09Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K tls.key: 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 kind: Secret metadata: creationTimestamp: \u0026quot;2020-09-29T15:51:22Z\u0026quot; managedFields: - apiVersion: v1 fieldsType: FieldsV1 fieldsV1: f:data: .: {} f:tls.crt: {} f:tls.key: {} f:type: {} manager: kubectl operation: Update time: \u0026quot;2020-09-29T15:51:22Z\u0026quot; name: oimcluster-tls-cert namespace: oimcluster resourceVersion: \u0026quot;1291036\u0026quot; selfLink: /api/v1/namespaces/oimcluster/secrets/oimcluster-tls-cert uid: a127e5fd-705b-43e1-ab56-590834efda5e type: kubernetes.io/tls   Install NGINX Use helm to install NGINX.\nConfigure the repository   Add the Helm chart repository for installing NGINX using the following command:\n$ helm repo add stable https://kubernetes.github.io/ingress-nginx The output will look similar to the following:\n\u0026quot;stable\u0026quot; has been added to your repositories   Update the repository using the following command:\n$ helm repo update The output will look similar to the following:\nHang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \u0026quot;stable\u0026quot; chart repository Update Complete. Happy Helming!   Create a Namespace   Create a Kubernetes namespace for NGINX:\n$ kubectl create namespace nginxssl The output will look similar to the following:\nnamespace/nginxssl created   Install NGINX using helm If you can connect directly to the master node IP address from a browser, then install NGINX with the --set controller.service.type=NodePort parameter.\nIf you are using a Managed Service for your Kubernetes cluster, for example Oracle Kubernetes Engine (OKE) on Oracle Cloud Infrastructure (OCI), and connect from a browser to the Load Balancer IP address, then use the --set controller.service.type=LoadBalancer parameter. This instructs the Managed Service to setup a Load Balancer to direct traffic to the NGINX ingress.\n  To install NGINX use the following helm command depending on if you are using NodePort or LoadBalancer:\na) Using NodePort\n$ helm install nginx-ingress -n nginxssl --set controller.extraArgs.default-ssl-certificate=oimcluster/oimcluster-tls-cert --set controller.service.type=NodePort --set controller.admissionWebhooks.enabled=false stable/ingress-nginx The output will look similar to the following:\n$ helm install nginx-ingress -n nginxssl --set controller.extraArgs.default-ssl-certificate=oimcluster/oimcluster-tls-cert --set controller.service.type=NodePort --set controller.admissionWebhooks.enabled=false stable/ingress-nginx NAME: nginx-ingress LAST DEPLOYED: Tue Sep 29 08:53:30 2020 NAMESPACE: nginxssl STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: The nginx-ingress controller has been installed. Get the application URL by running these commands: export HTTP_NODE_PORT=$(kubectl --namespace nginxssl get services -o jsonpath=\u0026quot;{.spec.ports[0].nodePort}\u0026quot; nginx-ingress-controller) export HTTPS_NODE_PORT=$(kubectl --namespace nginxssl get services -o jsonpath=\u0026quot;{.spec.ports[1].nodePort}\u0026quot; nginx-ingress-controller) export NODE_IP=$(kubectl --namespace nginxssl get nodes -o jsonpath=\u0026quot;{.items[0].status.addresses[1].address}\u0026quot;) echo \u0026quot;Visit http://$NODE_IP:$HTTP_NODE_PORT to access your application via HTTP.\u0026quot; echo \u0026quot;Visit https://$NODE_IP:$HTTPS_NODE_PORT to access your application via HTTPS.\u0026quot; An example Ingress that makes use of the controller: apiVersion: extensions/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx name: example namespace: foo spec: rules: - host: www.example.com http: paths: - backend: serviceName: exampleService servicePort: 80 path: / # This section is only required if TLS is to be enabled for the Ingress tls: - hosts: - www.example.com secretName: example-tls If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided: apiVersion: v1 kind: Secret metadata: name: example-tls namespace: foo data: tls.crt: \u0026lt;base64 encoded cert\u0026gt; tls.key: \u0026lt;base64 encoded key\u0026gt; type: kubernetes.io/tls b) Using LoadBalancer\n$ helm install nginx-ingress -n nginxssl --set controller.extraArgs.default-ssl-certificate=oimcluster/oimcluster-tls-cert --set controller.service.type=LoadBalancer --set controller.admissionWebhooks.enabled=false stable/ingress-nginx The output will look similar to the following:\n$ helm install nginx-ingress -n nginxssl --set controller.extraArgs.default-ssl-certificate=oimcluster/oimcluster-tls-cert --set controller.service.type=LoadBalancer --set controller.admissionWebhooks.enabled=false stable/ingress-nginx NAME: nginx-ingress LAST DEPLOYED: Tue Sep 29 08:53:30 2020 NAMESPACE: nginxssl STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: The ingress-nginx controller has been installed. It may take a few minutes for the LoadBalancer IP to be available. You can watch the status by running 'kubectl --namespace nginxssl get services -o wide -w nginx-ingress-ingress-nginx-controller' An example Ingress that makes use of the controller: apiVersion: extensions/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx name: example namespace: foo spec: rules: - host: www.example.com http: paths: - backend: serviceName: exampleService servicePort: 80 path: / # This section is only required if TLS is to be enabled for the Ingress tls: - hosts: - www.example.com secretName: example-tls If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided: apiVersion: v1 kind: Secret metadata: name: example-tls namespace: foo data: tls.crt: \u0026lt;base64 encoded cert\u0026gt; tls.key: \u0026lt;base64 encoded key\u0026gt; type: kubernetes.io/tls   Setup Routing Rules for the Domain   Setup routing rules by running the following commands:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain $ cp values.yaml values.yaml.orig $ vi values.yaml   Edit values.yaml and ensure that the values type=NGINX, tls=SSL and secretName=oimcluster-tls-cert are set, for example:\n$ cat values.yaml # Copyright 2020 Oracle Corporation and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # Default values for ingress-per-domain. # This is a YAML-formatted file. # Declare variables to be passed into your templates. # Load balancer type. Supported values are: VOYAGER, NGINX type: NGINX # Type of Configuration Supported Values are : NONSSL,SSL # tls: NONSSL tls: SSL # TLS secret name if the mode is SSL secretName: oimcluster-tls-cert # WLS domain as backend to the load balancer wlsDomain: domainUID: oimcluster oimClusterName: oim_cluster soaClusterName: soa_cluster soaManagedServerPort: 8001 oimManagedServerPort: 14000 adminServerName: adminserver adminServerPort: 7001 # Traefik specific values # traefik: # hostname used by host-routing # hostname: idmdemo.m8y.xyz # Voyager specific values voyager: # web port webPort: 30305 # stats port statsPort: 30315   Create an Ingress for the Domain   Create an Ingress for the domain (oimcluster-nginx), in the domain namespace by using the sample Helm chart:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator $ helm install oimcluster-nginx kubernetes/samples/charts/ingress-per-domain --namespace oimcluster --values kubernetes/samples/charts/ingress-per-domain/values.yaml Note: The \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain/templates/nginx-ingress.yaml has nginx.ingress.kubernetes.io/enable-access-log set to false. If you want to enable access logs then set this value to true before executing the command. Enabling access-logs can cause issues with disk space if not regularly maintained.\nFor example:\n$ cd /scratch/OIGDockerK8S/weblogic-kubernetes-operator $ helm install oimcluster-nginx kubernetes/samples/charts/ingress-per-domain --namespace oimcluster --values kubernetes/samples/charts/ingress-per-domain/values.yaml The output will look similar to the following:\nNAME: oimcluster-nginx LAST DEPLOYED: Tue Sep 29 08:56:38 2020 NAMESPACE: oimcluster STATUS: deployed REVISION: 1 TEST SUITE: None   Run the following command to show the ingress is created successfully:\n$ kubectl get ing -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl get ing -n oimcluster The output will look similar to the following:\nNAME CLASS HOSTS ADDRESS PORTS AGE oimcluster-nginx \u0026lt;none\u0026gt; * 80 49s   Find the node port of NGINX using the following command:\n$ kubectl get services -n nginxssl -o jsonpath=\u0026quot;{.spec.ports[1].nodePort}\u0026quot; nginx-ingress-ingress-nginx-controller The output will look similar to the following:\n32033$   Run the following command to check the ingress:\n$ kubectl describe ing oimcluster-nginx -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl describe ing oimcluster-nginx -n oimcluster The output will look similar to the following:\nName: oimcluster-nginx Namespace: oimcluster Address: 10.103.131.225 Default backend: default-http-backend:80 (\u0026lt;error: endpoints \u0026quot;default-http-backend\u0026quot; not found\u0026gt;) Rules: Host Path Backends ---- ---- -------- * /console oimcluster-adminserver:7001 (10.244.1.42:7001) /em oimcluster-adminserver:7001 (10.244.1.42:7001) /soa oimcluster-cluster-soa-cluster:8001 (10.244.1.43:8001) /integration oimcluster-cluster-soa-cluster:8001 (10.244.1.43:8001) /soa-infra oimcluster-cluster-soa-cluster:8001 (10.244.1.43:8001) oimcluster-cluster-oim-cluster:14000 (10.244.1.44:14000) Annotations: kubernetes.io/ingress.class: nginx meta.helm.sh/release-name: oimcluster-nginx meta.helm.sh/release-namespace: oimcluster nginx.ingress.kubernetes.io/configuration-snippet: more_set_input_headers \u0026quot;X-Forwarded-Proto: https\u0026quot;; more_set_input_headers \u0026quot;WL-Proxy-SSL: true\u0026quot;; nginx.ingress.kubernetes.io/ingress.allow-http: false nginx.ingress.kubernetes.io/proxy-buffer-size: 2000k Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal CREATE 5m4s nginx-ingress-controller Ingress oimcluster/oimcluster-nginx Normal UPDATE 4m9s nginx-ingress-controller Ingress oimcluster/oimcluster-nginx   To confirm that the new Ingress is successfully routing to the domain\u0026rsquo;s server pods, run the following command to send a request to the URL for the \u0026ldquo;WebLogic ReadyApp framework\u0026rdquo;:\n$ curl -v -k https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/weblogic/ready For example:\n$ curl -v -k https://masternode.example.com:32033/weblogic/ready The output will look similar to the following:\n$ curl -v https://masternode.example.com:32033/weblogic/ready * About to connect() to 12.345.678.9 port 32033 (#0) * Trying 12.345.678.9... * Connected to 12.345.678.9 (12.345.678.9) port 32033 (#0) * Initializing NSS with certpath: sql:/etc/pki/nssdb * skipping SSL peer certificate verification * SSL connection using TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 * Server certificate: * subject: CN=masternode.example.com * start date: Sep 29 14:52:35 2020 GMT * expire date: Sep 29 14:52:35 2021 GMT * common name: masternode.example.com * issuer: CN=masternode.example.com \u0026gt; GET /weblogic/ready HTTP/1.1 \u0026gt; User-Agent: curl/7.29.0 \u0026gt; Host: 12.345.678.9:32033 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; Server: nginx/1.19.1 \u0026lt; Date: Tue, 29 Sep 2020 16:10:10 GMT \u0026lt; Content-Length: 0 \u0026lt; Connection: keep-alive \u0026lt; Strict-Transport-Security: max-age=15724800; includeSubDomains \u0026lt; * Connection #0 to host 12.345.678.9 left intact   Verify that You can Access the Domain URL After setting up the NGINX ingress, verify that the domain applications are accessible through the NGINX ingress port (for example 32033) as per Validate Domain URLs \nCleanup If you need to remove the NGINX Ingress then remove the ingress with the following commands:\n$ helm delete oimcluster-nginx -n oimcluster $ helm delete nginx-ingress -n nginxssl $ kubectl delete namespace nginxssl The output will look similar to the following:\n$ helm delete oimcluster-nginx -n oimcluster release \u0026quot;oimcluster-nginx\u0026quot; uninstalled $ helm delete nginx-ingress -n nginxssl release \u0026quot;nginx-ingress\u0026quot; uninstalled $ kubectl delete namespace nginxssl namespace \u0026quot;nginxssl\u0026quot; deleted "
},
{
	"uri": "/fmw-kubernetes/oam/configure-ingress/ingress-voyager-setup-for-oam-domain-setup-on-k8s/",
	"title": "b. Using an Ingress with Voyager",
	"tags": [],
	"description": "Steps to set up an Ingress for Voyager to direct traffic to the OAM domain.",
	"content": "Setting Up an Ingress for Voyager for the OAM Domain on K8S The instructions below explain how to set up Voyager as an Ingress for the OAM domain with SSL termination.\nNote: All the steps below should be performed on the master node.\n Generate a SSL Certificate Install Voyager Create an Ingress for the Domain Verify that You can Access the Domain URL  Generate a SSL Certificate   Generate a private key and certificate signing request (CSR) using a tool of your choice. Send the CSR to your certificate authority (CA) to generate the certificate.\nIf you want to use a certificate for testing purposes you can generate a self signed certificate using openssl:\n$ mkdir \u0026lt;work directory\u0026gt;/ssl $ cd \u0026lt;work directory\u0026gt;/ssl $ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \u0026#34;/CN=\u0026lt;nginx-hostname\u0026gt;\u0026#34; For example:\n$ mkdir /scratch/OAMDockerK8S/ssl $ cd /scratch/OAMDockerK8S/ssl $ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \u0026#34;/CN=masternode.example.com\u0026#34; Note: The CN should match the host.domain of the master node in order to prevent hostname problems during certificate verification.\nThe output will look similar to the following:\nGenerating a 2048 bit RSA private key ..........................................+++ .......................................................................................................+++ writing new private key to \u0026#39;tls.key\u0026#39; -----   Create a secret for SSL by running the following command:\n$ kubectl -n accessns create secret tls \u0026lt;domain_uid\u0026gt;-tls-cert --key \u0026lt;work directory\u0026gt;/tls.key --cert \u0026lt;work directory\u0026gt;/tls.crt For example:\n$ kubectl -n accessns create secret tls accessinfra-tls-cert --key /scratch/OAMDockerK8S/ssl/tls.key --cert /scratch/OAMDockerK8S/ssl/tls.crt The output will look similar to the following:\nsecret/accessinfra-tls-cert created   Install Voyager Use helm to install Voyager.\n  Add the helm chart repository for Voyager using the following command:\n$ helm repo add googleapis https://kubernetes-charts.storage.googleapis.com/ The output will look similar to the following:\n\u0026#34;googleapis\u0026#34; has been added to your repositories   Add the appscode chart repository using the following command:\n$ helm repo add appscode https://charts.appscode.com/stable/ The output will look similar to the following:\n\u0026#34;appscode\u0026#34; has been added to your repositories   Update the repository using the following command:\n$ helm repo update The output will look similar to the following:\nHang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \u0026#34;appscode\u0026#34; chart repository ...Successfully got an update from the \u0026#34;stable\u0026#34; chart repository Update Complete. ⎈ Happy Helming!⎈   Run the following command to show the voyager chart was added successfully.\n$ helm search repo appscode/voyager The output will look similar to the following:\nNAME CHART VERSION APP VERSION DESCRIPTION appscode/voyager v12.0.0 v12.0.0 Voyager by AppsCode - Secure HAProxy Ingress Co...   Create a namespace for the voyager:\n$ kubectl create namespace voyager The output will look similar to the following:\nnamespace/voyager created   Install Voyager using the following helm command:\n$ helm install voyager-operator appscode/voyager --version 12.0.0 --namespace voyager --set cloudProvider=baremetal --set apiserver.enableValidatingWebhook=false Note: For bare metal Kubernetes use --set cloudProvider=baremetal. If using a managed Kubernetes service then the value should be set for your specific service as per the Voyager install guide.\nThe output will look similar to the following:\nNAME: voyager-operator LAST DEPLOYED: Fri Sep 25 01:15:31 2020 NAMESPACE: voyager STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: Set cloudProvider for installing Voyager To verify that Voyager has started, run: kubectl get deployment --namespace voyager -l \u0026#34;app.kubernetes.io/name=voyager,app.kubernetes.io/instance=voyager-operator\u0026#34;   Create an Ingress for the Domain   Edit the values.yaml and change domainUID to the domainUID you created previously:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain For example:\n$ cd /scratch/OAMDockerK8S/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain Edit values.yaml and change to domainUID: \u0026lt;domain_UID\u0026gt;, for example domainUID: accessinfra.\n  Navigate to the following directory:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain/templates For example:\n$ cd /scratch/OAMDockerK8S/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain/templates Edit the voyager-ingress.yaml and change the secretName to the value created earlier, for example:\n# Copyright (c) 2020, Oracle Corporation and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. {{- if eq .Values.type \u0026#34;VOYAGER\u0026#34; }} --- apiVersion: voyager.appscode.com/v1beta1 kind: Ingress metadata: name: {{ .Values.wlsDomain.domainUID }}-voyager namespace: {{ .Release.Namespace }} annotations: ingress.appscode.com/type: \u0026#39;NodePort\u0026#39; kubernetes.io/ingress.class: \u0026#39;voyager\u0026#39; ingress.appscode.com/stats: \u0026#39;true\u0026#39; ingress.appscode.com/default-timeout: \u0026#39;{\u0026#34;connect\u0026#34;: \u0026#34;1800s\u0026#34;, \u0026#34;server\u0026#34;: \u0026#34;1800s\u0026#34;}\u0026#39; ingress.appscode.com/proxy-body-size: \u0026#34;2000000\u0026#34; labels: weblogic.resourceVersion: domain-v2 spec: {{- if eq .Values.tls \u0026#34;SSL\u0026#34; }} frontendRules: - port: 443 rules: - http-request set-header WL-Proxy-SSL true tls: - secretName: accessinfra-tls-cert hosts: - \u0026#39;*\u0026#39; {{- end }} ...   Create an Ingress for the domain (oam-voyager-ingress), in the domain namespace by using the sample Helm chart.\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator $ helm install oam-voyager-ingress kubernetes/samples/charts/ingress-per-domain --namespace \u0026lt;domain_namespace\u0026gt; --values kubernetes/samples/charts/ingress-per-domain/values.yaml For example:\n$ cd /scratch/OAMDockerK8S/weblogic-kubernetes-operator $ helm install oam-voyager-ingress kubernetes/samples/charts/ingress-per-domain --namespace accessns --values kubernetes/samples/charts/ingress-per-domain/values.yaml The output will look similar to the following:\nNAME: oam-voyager-ingress Fri Sep 25 01:18:01 2020 NAMESPACE: accessns STATUS: deployed REVISION: 1 TEST SUITE: None   Run the following command to show the ingress is created successfully:\n$ kubectl get ingress.voyager.appscode.com --all-namespaces The output will look similar to the following:\nNAMESPACE NAME HOSTS LOAD_BALANCER_IP AGE accessns accessinfra-voyager * 80s   Find the node port of the ingress using the following command:\n$ kubectl describe svc voyager-accessinfra-voyager -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl describe svc voyager-accessinfra-voyager -n accessns The output will look similar to the following:\nName: voyager-accessinfra-voyager Namespace: accessns Labels: app.kubernetes.io/managed-by=Helm origin=voyager origin-api-group=voyager.appscode.com origin-name=accessinfra-voyager weblogic.resourceVersion=domain-v2 Annotations: ingress.appscode.com/last-applied-annotation-keys: ingress.appscode.com/origin-api-schema: voyager.appscode.com/v1beta1 ingress.appscode.com/origin-name: accessinfra-voyager Selector: origin-api-group=voyager.appscode.com,origin-name=accessinfra-voyager,origin=voyager Type: NodePort IP: 10.105.242.191 Port: tcp-443 443/TCP TargetPort: 443/TCP NodePort: tcp-443 30305/TCP Endpoints: 10.244.2.4:443 Port: tcp-80 80/TCP TargetPort: 80/TCP NodePort: tcp-80 32064/TCP Endpoints: 10.244.2.4:80 Session Affinity: None External Traffic Policy: Cluster Events: \u0026lt;none\u0026gt; In the above example the NodePort for tcp-443 is 30305.\n  To confirm that the new Ingress is successfully routing to the domain\u0026rsquo;s server pods, run the following command to send a request to the URL for the \u0026ldquo;WebLogic ReadyApp framework\u0026rdquo;:\n$ curl -v https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/weblogic/ready For example:\n$ curl -v -k https://masternode.example.com:30305/weblogic/ready The output will look similar to the following:\n* Trying 12.345.67.89... * Connected to 12.345.67.89 (12.345.67.89) port 30305 (#0) * Initializing NSS with certpath: sql:/etc/pki/nssdb * skipping SSL peer certificate verification * SSL connection using TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 * Server certificate: * subject: CN=masternode.example.com * start date: Sep 24 14:30:46 2020 GMT * expire date: Sep 24 14:30:46 2021 GMT * common name: masternode.example.com * issuer: CN=masternode.example.com \u0026gt; GET /weblogic/ready HTTP/1.1 \u0026gt; User-Agent: curl/7.29.0 \u0026gt; Host: masternode.example.com:30305 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; Date: 25 Sep 2020 08:22:11 GMT \u0026lt; Content-Length: 0 \u0026lt; Strict-Transport-Security: max-age=15768000 \u0026lt; * Connection #0 to host 12.345.67.89 left intact   Verify that You can Access the Domain URL After setting up the Voyager ingress, verify that the domain applications are accessible through the Voyager ingress port (for example 30305) as per Validate Domain URLs \n"
},
{
	"uri": "/fmw-kubernetes/wcsites-domains/manage-wcsites-domains/loadbalancer-voyager-setup-for-wcsites-domain-setup-on-k8s/",
	"title": "b. Using Voyager Loadbalancer",
	"tags": [],
	"description": "Steps to set up Voyager as a loadbalancer for the WebCenter Sites domain.",
	"content": "Setting Up Loadbalancer Voyager for the WebCenter Sites Domain on K8S The Oracle WebLogic Server Kubernetes Operator supports three load balancers: Traefik, Voyager, and Apache. Follow these steps to set up Voyager as a loadbalancer for the WebCenter Sites domain:\n Install the Voyager Load Balancer Configure Voyager to Manage Ingresses Verify that You can Access the Domain URL  Install the Voyager Load Balancer See the official installation document and follow step 1 and 2 here.\nConfigure Voyager to Manage Ingresses   Create an Ingress for the domain (ingress-per-domain) in the domain namespace, by using the sample Helm chart.\nHere we are using the path based routing for ingress. For detailed instructions about ingress, refer this page.\nFor this update the kubernetes/samples/scripts/create-wcsites-domain/ingress-per-domain/values.yaml with appropriate values, sample values are shown below:\n-bash-4.2$ cat kubernetes/samples/scripts/create-wcsites-domain/ingress-per-domain/values.yaml type: VOYAGER # Copyright 2020, Oracle Corporation and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # Default values for ingress-per-domain. # This is a YAML-formatted file. # Declare variables to be passed into your templates. # Load balancer type. Supported values are: TRAEFIK, VOYAGER #type: TRAEFIK type: VOYAGER # WLS domain as backend to the load balancer wlsDomain: domainUID: wcsitesinfra adminServerName: adminserver adminServerPort: 7001 wcsitesClusterName: wcsites_cluster wcsitesManagedServerPort: 8001 # Voyager specific values voyager: # web port webPort: 30305 # stats port statsPort: 30317   Update the kubernetes/samples/scripts/create-wcsites-domain/ingress-per-domain/templates/voyager-ingress.yaml with the url routes to be load balanced.\nBelow are the ingress rules defined:\nNOTE: These are not the exhausted list of rules. These can be enhanced based on the application urls that needs to be accessed externally.\nBelow rules hold good for domain type WCSITES.\n# Copyright 2020, Oracle Corporation and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. {{- if eq .Values.type \u0026#34;VOYAGER\u0026#34; }} --- apiVersion: voyager.appscode.com/v1beta1 kind: Ingress metadata: name: {{ .Values.wlsDomain.domainUID }}-voyager namespace: {{ .Release.Namespace }} annotations: ingress.appscode.com/type: \u0026#39;NodePort\u0026#39; ingress.appscode.com/stats: \u0026#39;true\u0026#39; ingress.appscode.com/affinity: \u0026#39;cookie\u0026#39; ingress.appscode.com/default-timeout: \u0026#39;{\u0026#34;connect\u0026#34;: \u0026#34;1800s\u0026#34;, \u0026#34;server\u0026#34;: \u0026#34;1800s\u0026#34;}\u0026#39; spec: rules: - host: \u0026#39;*\u0026#39; http: nodePort: {{ .Values.voyager.webPort }} paths: - path: /console backend: serviceName: {{ .Values.wlsDomain.domainUID }}-{{ .Values.wlsDomain.adminServerName | lower | replace \u0026#34;_\u0026#34; \u0026#34;-\u0026#34; }} servicePort: {{ .Values.wlsDomain.adminServerPort }} - path: /em backend: serviceName: {{ .Values.wlsDomain.domainUID }}-{{ .Values.wlsDomain.adminServerName | lower | replace \u0026#34;_\u0026#34; \u0026#34;-\u0026#34; }} servicePort: {{ .Values.wlsDomain.adminServerPort }} # - path: /wls-exporter # backend: # serviceName: {{ .Values.wlsDomain.domainUID }}-{{ .Values.wlsDomain.adminServerName | lower | replace \u0026#34;_\u0026#34; \u0026#34;-\u0026#34; }} # servicePort: {{ .Values.wlsDomain.adminServerPort }} - path: /weblogic backend: serviceName: {{ .Values.wlsDomain.domainUID }}-{{ .Values.wlsDomain.adminServerName | lower | replace \u0026#34;_\u0026#34; \u0026#34;-\u0026#34; }} servicePort: {{ .Values.wlsDomain.adminServerPort }} - path: /sbconsole backend: serviceName: {{ .Values.wlsDomain.domainUID }}-{{ .Values.wlsDomain.adminServerName | lower | replace \u0026#34;_\u0026#34; \u0026#34;-\u0026#34; }} servicePort: {{ .Values.wlsDomain.adminServerPort }} - path: /sites backend: serviceName: {{ .Values.wlsDomain.domainUID }}-cluster-{{ .Values.wlsDomain.wcsitesClusterName | lower | replace \u0026#34;_\u0026#34; \u0026#34;-\u0026#34; }} servicePort: {{ .Values.wlsDomain.wcsitesManagedServerPort }} - path: /cas backend: serviceName: {{ .Values.wlsDomain.domainUID }}-cluster-{{ .Values.wlsDomain.wcsitesClusterName | lower | replace \u0026#34;_\u0026#34; \u0026#34;-\u0026#34; }} servicePort: {{ .Values.wlsDomain.wcsitesManagedServerPort }} # - path: /wls-exporter # backend: # serviceName: {{ .Values.wlsDomain.domainUID }}-cluster-{{ .Values.wlsDomain.wcsitesClusterName | lower | replace \u0026#34;_\u0026#34; \u0026#34;-\u0026#34; }} # servicePort: {{ .Values.wlsDomain.wcsitesManagedServerPort }} --- apiVersion: v1 kind: Service metadata: name: {{ .Values.wlsDomain.domainUID }}-voyager-stats namespace: {{ .Release.Namespace }} spec: type: NodePort ports: - name: client protocol: TCP port: 56789 targetPort: 56789 nodePort: {{ .Values.voyager.statsPort }} selector: origin: voyager origin-name: {{ .Values.wlsDomain.domainUID }}-voyager {{- end }}   Install ingress-per-domain using helm.\nbash-4.2$ cd weblogic-kubernetes-operator -bash-4.2$ helm install kubernetes/samples/scripts/create-wcsites-domain/ingress-per-domain \\  --name wcsitesinfra-voyager-ingress --namespace wcsites-ns \\  --values kubernetes/samples/scripts/create-wcsites-domain/ingress-per-domain/values.yaml NAME: wcsitesinfra-voyager-ingress LAST DEPLOYED: Fri Feb 14 13:20:17 2020 NAMESPACE: wcsites-ns STATUS: DEPLOYED RESOURCES: ==\u0026gt; v1/Service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE wcsitesinfra-voyager-stats NodePort 10.101.94.249 \u0026lt;none\u0026gt; 56789:30317/TCP 0s ==\u0026gt; v1beta1/Ingress NAME HOSTS ADDRESS PORTS AGE wcsitesinfra-ingress yourcompany-loadbalancer.com 80 0s NAME AGE wcsitesinfra-voyager 0s   To confirm that the load balancer noticed the new Ingress and is successfully routing to the domain\u0026rsquo;s server pods, you can send a request to the URL for the \u0026ldquo;WebLogic ReadyApp framework\u0026rdquo; which should return a HTTP 200 status code, as shown in the example below:\n  -bash-4.2$ curl -v http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-PORT}/weblogic/ready * About to connect() to localhost port 30305 (#0) * Trying 127.0.0.1... * Connected to localhost (127.0.0.1) port 30305 (#0) \u0026gt; GET /weblogic/ready HTTP/1.1 \u0026gt; User-Agent: curl/7.29.0 \u0026gt; Accept: */* \u0026gt; host: *****.com \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; Content-Length: 0 \u0026lt; Date: Thu, 12 Mar 2020 10:16:43 GMT \u0026lt; Vary: Accept-Encoding \u0026lt; * Connection #0 to host localhost left intact Verify that You can Access the Domain URL After setting up the Traefik loadbalancer, verify that the domain applications are accessible through the loadbalancer port 30305. Through load balancer (Traefik port 30305), the following URLs are available for setting up domains of WebCenter Sites domain types:\nhttp://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-PORT}/weblogic/ready http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-PORT}/console http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-PORT}/em http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-PORT}/sites/version.jsp "
},
{
	"uri": "/fmw-kubernetes/wcsites-domains/manage-wcsites-domains/elasticsearch-integration-with-wls-operator-and-wls-server-logs/",
	"title": "c. ELK Integration for Logs",
	"tags": [],
	"description": "",
	"content": "1. Integrate Elasticsearch to WebLogic Kubernetes Operator For reference information, see Elasticsearch integration for the WebLogic Kubernetes Operator.\nTo enable elasticsearch integration, you must edit file kubernetes/charts/weblogic-operator/values.yaml before deploying the WebLogic Kubernetes Operator.\n# elkIntegrationEnabled specifies whether or not ELK integration is enabled. elkIntegrationEnabled: true # logStashImage specifies the docker image containing logstash. # This parameter is ignored if 'elkIntegrationEnabled' is false. logStashImage: \u0026quot;logstash:6.6.0\u0026quot; # elasticSearchHost specifies the hostname of where Elasticsearch is running. # This parameter is ignored if 'elkIntegrationEnabled' is false. elasticSearchHost: \u0026quot;elasticsearch.default.svc.cluster.local\u0026quot; # elasticSearchPort specifies the port number of where Elasticsearch is running. # This parameter is ignored if 'elkIntegrationEnabled' is false. elasticSearchPort: 9200 After you\u0026rsquo;ve deployed WebLogic Kubernetes Operator and made the above changes, the weblogic-operator pod will have additional Logstash container. The Logstash container will push the weblogic-operator logs to the configured Elasticsearch server.\n2. Publish WebLogic Server and WebCenter Sites Logs using Logstash Pod You can publish the WebLogic Server logs to Elasticsearch Server using Logstash pod. This Logstash pod must have access to the shared domain home. For the WebCenter Sites wcsitesinfra, you can use the persistent volume of the domain home in the Logstash pod. The steps to create the Logstash pod are as follows:\nSample Logstash configuration file is located at kubernetes/samples/scripts/create-wcsites-domain/utils/logstash/logstash.conf\n$ vi kubernetes/samples/scripts/create-wcsites-domain/utils/logstash/logstash.conf input { file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/logs/wcsitesinfra/AdminServer.log\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/logs/wcsitesinfra/wcsites_server*.log\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/logs/wcsitesinfra/AdminServer.out\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/logs/wcsitesinfra/wcsites_server*.out\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/wcsitesinfra/servers/**/logs/sites.log\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/wcsitesinfra/servers/**/logs/cas.log\u0026quot; start_position =\u0026gt; beginning } } filter { grok { match =\u0026gt; [ \u0026quot;message\u0026quot;, \u0026quot;\u0026lt;%{DATA:log_timestamp}\u0026gt; \u0026lt;%{WORD:log_level}\u0026gt; \u0026lt;%{WORD:thread}\u0026gt; \u0026lt;%{HOSTNAME:hostname}\u0026gt; \u0026lt;%{HOSTNAME:servername}\u0026gt; \u0026lt;%{DATA:timer}\u0026gt; \u0026lt;\u0026lt;%{DATA:kernel}\u0026gt;\u0026gt; \u0026lt;\u0026gt; \u0026lt;%{DATA:uuid}\u0026gt; \u0026lt;%{NUMBER:timestamp}\u0026gt; \u0026lt;%{DATA:misc}\u0026gt; \u0026lt;%{DATA:log_number}\u0026gt; \u0026lt;%{DATA:log_message}\u0026gt;\u0026quot; ] } } output { elasticsearch { hosts =\u0026gt; [\u0026quot;elasticsearch.default.svc.cluster.local:9200\u0026quot;] } } Here ** means that all sites.log and cas.log from any servers under wcsitesinfra will be pushed to Logstash.\n$ kubectl cp kubernetes/samples/scripts/create-wcsites-domain/utils/logstash/logstash.conf wcsites-ns/wcsitesinfra-adminserver:/u01/oracle/user_projects/domains/logs/logstash.conf Get the persistent volume details of the domain home of the WebLogic Server(s). The following command will list the persistent volume details in the namespace - \u0026ldquo;wcsites-ns\u0026rdquo;:\n$ kubectl get pv -n wcsites-ns NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE wcsitesinfra-domain-pv 10Gi RWX Retain Bound wcsites-ns/wcsitesinfra-domain-pvc wcsitesinfra-domain-storage-class 5d21h Sample Logstash deployment is located at kubernetes/samples/scripts/create-wcsites-domain/utils/logstash/logstash.yaml for Logstash pod. The mounted persistent volume of the domain home will provide access to the WebLogic Server logs to Logstash pod.\napiVersion: apps/v1beta1 kind: Deployment metadata: name: logstash-wls namespace: wcsites-ns spec: template: # create pods using pod definition in this template metadata: labels: k8s-app: logstash-wls spec: volumes: - name: weblogic-domain-storage-volume persistentVolumeClaim: claimName: wcsitesinfra-domain-pvc - name: shared-logs emptyDir: {} containers: - name: logstash image: logstash:6.6.0 command: [\u0026quot;/bin/sh\u0026quot;] args: [\u0026quot;/usr/share/logstash/bin/logstash\u0026quot;, \u0026quot;-f\u0026quot;, \u0026quot;/u01/oracle/user_projects/domains/logs/logstash.conf\u0026quot;] imagePullPolicy: IfNotPresent volumeMounts: - mountPath: /u01/oracle/user_projects/domains name: weblogic-domain-storage-volume - name: shared-logs mountPath: /shared-logs ports: - containerPort: 5044 name: logstash After you have created the Logstash deployment yaml and Logstash configuration file, deploy Logstash using following command:\n$ kubectl create -f kubernetes/samples/scripts/create-wcsites-domain/utils/logstash/logstash.yaml 3. Test the Deployment of Elasticsearch and Kibana The WebLogic Operator also provides a sample deployment of Elasticsearch and Kibana for testing purpose. You can deploy Elasticsearch and Kibana on the Kubernetes cluster as shown below:\n$ kubectl create -f kubernetes/samples/scripts/elasticsearch-and-kibana/elasticsearch_and_kibana.yaml Get the Kibana dashboard port information as shown below: Wait for pods to start:\n-bash-4.2$ kubectl get pods -w NAME READY STATUS RESTARTS AGE elasticsearch-8bdb7cf54-mjs6s 1/1 Running 0 4m3s kibana-dbf8964b6-n8rcj 1/1 Running 0 4m3s -bash-4.2$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE elasticsearch ClusterIP 10.100.11.154 \u0026lt;none\u0026gt; 9200/TCP,9300/TCP 4m32s kibana NodePort 10.97.205.0 \u0026lt;none\u0026gt; 5601:31884/TCP 4m32s kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 71d You can access the Kibana dashboard at http://mycompany.com:kibana-nodeport/. In our example, the node port would be 31884.\nCreate an Index Pattern in Kibana Create an index pattern logstash* in Kibana \u0026gt; Management. After the servers are started, you will see the log data in the Kibana dashboard:\n"
},
{
	"uri": "/fmw-kubernetes/oam/patch-and-upgrade/upgrade_a_kubernetes_cluster/",
	"title": "c. Upgrade a Kubernetes cluster",
	"tags": [],
	"description": "Instructions on how to upgrade a Kubernetes cluster.",
	"content": "These instructions describe how to upgrade a Kubernetes cluster created using kubeadm on which an OAM domain is deployed. A rolling upgrade approach is used to upgrade nodes (master and worker) of the Kubernetes cluster.\nIt is expected that there will be a down time during the upgrade of the Kubernetes cluster as the nodes need to be drained as part of the upgrade process.\n Prerequisites  Review Prerequisites and ensure that your Kubernetes cluster is ready for upgrade. Make sure your environment meets all prerequisites. Make sure the database used for the OAM domain deployment is up and running during the upgrade process.  Upgrade the Kubernetes version An upgrade of Kubernetes is supported from one MINOR version to the next MINOR version, or between PATCH versions of the same MINOR. For example, you can upgrade from 1.x to 1.x+1, but not from 1.x to 1.x+2. To upgrade a Kubernetes version, first all the master nodes of the Kubernetes cluster must be upgraded sequentially, followed by the sequential upgrade of each worker node.\n See here for Kubernetes official documentation to upgrade from v1.16.x to v1.17.x. See here for Kubernetes official documentation to upgrade from v1.17.x to v1.18.x.  "
},
{
	"uri": "/fmw-kubernetes/oig/patch-and-upgrade/upgrade_a_kubernetes_cluster/",
	"title": "c. Upgrade a Kubernetes cluster",
	"tags": [],
	"description": "Instructions on how to upgrade a Kubernetes cluster.",
	"content": "These instructions describe how to upgrade a Kubernetes cluster created using kubeadm on which an OIG domain is deployed. A rolling upgrade approach is used to upgrade nodes (master and worker) of the Kubernetes cluster.\nIt is expected that there will be a down time during the upgrade of the Kubernetes cluster as the nodes need to be drained as part of the upgrade process.\n Prerequisites  Review Prerequisites and ensure that your Kubernetes cluster is ready for upgrade. Make sure your environment meets all prerequisites. Make sure the database used for the OIG domain deployment is up and running during the upgrade process.  Upgrade the Kubernetes version An upgrade of Kubernetes is supported from one MINOR version to the next MINOR version, or between PATCH versions of the same MINOR. For example, you can upgrade from 1.x to 1.x+1, but not from 1.x to 1.x+2. To upgrade a Kubernetes version, first all the master nodes of the Kubernetes cluster must be upgraded sequentially, followed by the sequential upgrade of each worker node.\n See here for Kubernetes official documentation to upgrade from v1.16.x to v1.17.x. See here for Kubernetes official documentation to upgrade from v1.17.x to v1.18.x.  "
},
{
	"uri": "/fmw-kubernetes/oig/configure-ingress/ingress-voyager-setup-for-oig-domain-setup-on-k8s/",
	"title": "c. Using an Ingress with Voyager (non-SSL)",
	"tags": [],
	"description": "Steps to set up an Ingress for Voyager to direct traffic to the OIG domain (non-SSL).",
	"content": "Setting Up an Ingress for Voyager for the OIG Domain on Kubernetes The instructions below explain how to set up Voyager as an Ingress for the OIG domain with non-SSL termination.\nNote: All the steps below should be performed on the master node.\n Install Voyager  Configure the repository Create Namespace and Install Voyager Setup Routing Rules for the Domain   Create an Ingress for the Domain Verify that You can Access the Domain URL Cleanup  Install Voyager Use Helm to install Voyager. For detailed information, see this document.\nConfigure the repository   Add the Helm chart repository for installing Voyager using the following command:\n$ helm repo add appscode https://charts.appscode.com/stable The output will look similar to the following:\n\u0026quot;appscode\u0026quot; has been added to your repositories   Update the repository using the following command:\n$ helm repo update The output will look similar to the following:\nHang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \u0026quot;appscode\u0026quot; chart repository Update Complete. Happy Helming!   Run the following command to show the Voyager chart was added successfully.\n$ helm search repo appscode/voyager The output will look similar to the following:\nNAME CHART VERSION APP VERSION DESCRIPTION appscode/voyager v12.0.0 v12.0.0 Voyager by AppsCode - Secure HAProxy Ingress Co...   Create Namespace and Install Voyager   Create a namespace for Voyager:\n$ kubectl create namespace voyager The output will look similar to the following:\nnamespace/voyager created   Install Voyager using the following Helm command:\n$ helm install voyager-ingress appscode/voyager --version 12.0.0 --namespace voyager --set cloudProvider=baremetal --set apiserver.enableValidatingWebhook=false Note: For bare metal Kubernetes use --set cloudProvider=baremetal. If using a managed Kubernetes service then the value should be set for your specific service as per the Voyager install guide.\nThe output will look similar to the following:\nNAME: voyager-ingress LAST DEPLOYED: Tue Sep 29 09:23:22 2020 NAMESPACE: voyager STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: Set cloudProvider for installing Voyager To verify that Voyager has started, run: $ kubectl get deployment --namespace voyager -l \u0026quot;app.kubernetes.io/name=voyager,app.kubernetes.io/instance=voyager-ingress\u0026quot;   Verify the ingress has started by running the following command:\n$ kubectl get deployment --namespace voyager -l \u0026quot;app.kubernetes.io/name=voyager,app.kubernetes.io/instance=voyager-ingress\u0026quot; The output will look similar to the following:\nNAME READY UP-TO-DATE AVAILABLE AGE voyager-ingress 1/1 1 1 89s   Setup Routing Rules for the Domain   Setup routing rules using the following commands:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain $ cp values.yaml values.yaml.orig $ vi values.yaml Edit values.yaml and ensure that the values type=VOYAGER and tls=NONSSL are set, for example:\n$ cat values.yaml # Copyright 2020 Oracle Corporation and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # Default values for ingress-per-domain. # This is a YAML-formatted file. # Declare variables to be passed into your templates. # Load balancer type. Supported values are: VOYAGER, NGINX type: VOYAGER # Type of Configuration Supported Values are : NONSSL,SSL # tls: NONSSL tls: NONSSL # TLS secret name if the mode is SSL secretName: domain1-tls-cert # WLS domain as backend to the load balancer wlsDomain: domainUID: oimcluster oimClusterName: oim_cluster soaClusterName: soa_cluster soaManagedServerPort: 8001 oimManagedServerPort: 14000 adminServerName: adminserver adminServerPort: 7001 # Traefik specific values # traefik: # hostname used by host-routing # hostname: idmdemo.m8y.xyz # Voyager specific values voyager: # web port webPort: 30305 # stats port statsPort: 30315   Create an Ingress for the Domain   Create an Ingress for the domain (oimcluster-voyager), in the domain namespace by using the sample Helm chart:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator $ helm install oimcluster-voyager kubernetes/samples/charts/ingress-per-domain --namespace \u0026lt;namespace\u0026gt; --values kubernetes/samples/charts/ingress-per-domain/values.yaml For example:\n$ cd /scratch/OIGDockerK8S/weblogic-kubernetes-operator $ helm install oimcluster-voyager kubernetes/samples/charts/ingress-per-domain --namespace oimcluster --values kubernetes/samples/charts/ingress-per-domain/values.yaml The output will look similar to the following:\nNAME: oimcluster-voyager LAST DEPLOYED: Tue Sep 29 09:28:12 2020 NAMESPACE: oimcluster STATUS: deployed REVISION: 1 TEST SUITE: None   Run the following command to show the ingress is created successfully:\n$ kubectl get ingress.voyager.appscode.com -n oimcluster The output will look similar to the following:\nNAME HOSTS LOAD_BALANCER_IP AGE oimcluster-voyager * 78s   Return details of the ingress using the following command:\n$ kubectl describe ingress.voyager.appscode.com oimcluster-voyager -n oimcluster The output will look similar to the following:\nName: oimcluster-voyager Namespace: oimcluster Labels: app.kubernetes.io/managed-by=Helm weblogic.resourceVersion=domain-v2 Annotations: ingress.appscode.com/affinity: cookie ingress.appscode.com/stats: true ingress.appscode.com/type: NodePort meta.helm.sh/release-name: oimcluster-voyager meta.helm.sh/release-namespace: oimcluster API Version: voyager.appscode.com/v1beta1 Kind: Ingress Metadata: Creation Timestamp: 2020-09-29T09:28:12Z Generation: 1 Managed Fields: API Version: voyager.appscode.com/v1beta1 Fields Type: FieldsV1 fieldsV1: f:metadata: f:annotations: .: f:ingress.appscode.com/affinity: f:ingress.appscode.com/stats: f:ingress.appscode.com/type: f:meta.helm.sh/release-name: f:meta.helm.sh/release-namespace: f:labels: .: f:app.kubernetes.io/managed-by: f:weblogic.resourceVersion: f:spec: .: f:rules: Manager: Go-http-client Operation: Update Time: 2020-09-29T09:28:12Z Resource Version: 4168835 Self Link: /apis/voyager.appscode.com/v1beta1/namespaces/oimcluster/ingresses/oimcluster-voyager UID: 2ea71f79-6836-4df2-8200-8418abf6ad9f Spec: Rules: Host: * Http: Node Port: 30305 Paths: Backend: Service Name: oimcluster-adminserver Service Port: 7001 Path: /console Backend: Service Name: oimcluster-adminserver Service Port: 7001 Path: /em Backend: Service Name: oimcluster-cluster-soa-cluster Service Port: 8001 Path: /soa-infra Backend: Service Name: oimcluster-cluster-oim-cluster Service Port: 14000 Path: / Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ServiceReconcileSuccessful 5m22s voyager-operator Successfully created NodePort Service voyager-oimcluster-voyager Normal ConfigMapReconcileSuccessful 5m22s voyager-operator Successfully created ConfigMap voyager-oimcluster-voyager Normal RBACSuccessful 5m22s voyager-operator Successfully created ServiceAccount voyager-oimcluster-voyager Normal RBACSuccessful 5m22s voyager-operator Successfully created Role voyager-oimcluster-voyager Normal RBACSuccessful 5m22s voyager-operator Successfully created RoleBinding voyager-oimcluster-voyager Normal DeploymentReconcileSuccessful 5m22s voyager-operator Successfully created HAProxy Deployment voyager-oimcluster-voyager Normal StatsServiceReconcileSuccessful 5m22s voyager-operator Successfully created stats Service voyager-oimcluster-voyager-stats   Find the NodePort of Voyager using the following command:\n$ kubectl get svc -n oimcluster The output will look similar to the following:\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE oimcluster-adminserver ClusterIP None \u0026lt;none\u0026gt; 7001/TCP 19h oimcluster-cluster-oim-cluster ClusterIP 10.97.121.159 \u0026lt;none\u0026gt; 14000/TCP 19h oimcluster-cluster-soa-cluster ClusterIP 10.111.231.242 \u0026lt;none\u0026gt; 8001/TCP 19h oimcluster-oim-server1 ClusterIP None \u0026lt;none\u0026gt; 14000/TCP 19h oimcluster-oim-server2 ClusterIP 10.108.139.30 \u0026lt;none\u0026gt; 14000/TCP 19h oimcluster-oim-server3 ClusterIP 10.97.170.104 \u0026lt;none\u0026gt; 14000/TCP 19h oimcluster-oim-server4 ClusterIP 10.99.82.214 \u0026lt;none\u0026gt; 14000/TCP 19h oimcluster-oim-server5 ClusterIP 10.98.75.228 \u0026lt;none\u0026gt; 14000/TCP 19h oimcluster-soa-server1 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP 19h oimcluster-soa-server2 ClusterIP 10.107.232.220 \u0026lt;none\u0026gt; 8001/TCP 19h oimcluster-soa-server3 ClusterIP 10.108.203.6 \u0026lt;none\u0026gt; 8001/TCP 19h oimcluster-soa-server4 ClusterIP 10.96.178.0 \u0026lt;none\u0026gt; 8001/TCP 19h oimcluster-soa-server5 ClusterIP 10.107.83.62 \u0026lt;none\u0026gt; 8001/TCP 19h oimcluster-voyager-stats NodePort 10.99.34.145 \u0026lt;none\u0026gt; 56789:30315/TCP 3m36s voyager-oimcluster-voyager NodePort 10.106.40.20 \u0026lt;none\u0026gt; 80:30305/TCP 3m36s voyager-oimcluster-voyager-stats ClusterIP 10.100.89.234 \u0026lt;none\u0026gt; 56789/TCP 3m30s Identify the service voyager-oimcluster-voyager in the above output and get the NodePort which corresponds to port 80. In this example it will be 30305.\n  To confirm that the new Ingress is successfully routing to the domain\u0026rsquo;s server pods, run the following command to send a request to the URL for the \u0026ldquo;WebLogic ReadyApp framework\u0026rdquo;:\n$ curl -v http://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/weblogic/ready For example:\n$ curl -v http://masternode.example.com:30305/weblogic/ready The output will look similar to the following:\n$ curl -v -k http://masternode.example.com:30305/weblogic/ready * About to connect() to masternode.example.com port 30305 (#0) * Trying 12.345.67.890... * Connected to masternode.example.com (12.345.67.890) port 30305 (#0) \u0026gt; GET /weblogic/ready HTTP/1.1 \u0026gt; User-Agent: curl/7.29.0 \u0026gt; Host: masternode.example.com:30305 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; Date: Wed, 29 Sep 2020 09:30:56 GMT \u0026lt; Content-Length: 0 \u0026lt; Set-Cookie: SERVERID=pod-oimcluster-oim-server1; path=/ \u0026lt; Cache-control: private \u0026lt; * Connection #0 to host masternode.example.com left intact   Verify that You can Access the Domain URL After setting up the Voyager ingress, verify that the domain applications are accessible through the Voyager ingress port (for example 30305) as per Validate Domain URLs \nCleanup If you need to remove the Voyager Ingress (for example to setup Voyager with SSL) then remove the ingress with the following commands:\n$ helm delete oimcluster-voyager -n oimcluster $ helm delete voyager-ingress -n voyager $ kubectl delete namespace voyager The output will look similar to the following:\n$ helm delete oimcluster-voyager -n oimcluster release \u0026quot;oimcluster-voyager\u0026quot; uninstalled $ helm delete voyager-ingress -n voyager release \u0026quot;voyager-ingress\u0026quot; uninstalled $ kubectl delete namespace voyager namespace \u0026quot;voyager\u0026quot; deleted "
},
{
	"uri": "/fmw-kubernetes/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/fmw-kubernetes/wcsites-domains/manage-wcsites-domains/weblogic-logging-exporter-setup/",
	"title": "d. Publish logs to Elasticsearch",
	"tags": [],
	"description": "Use the WebLogic Logging Exporter to publish the WebLogic Server logs to Elasticsearch.",
	"content": "The WebLogic Logging Exporter adds a log event handler to WebLogic Server. WebLogic Server logs can be pushed to Elasticsearch in Kubernetes directly by using the Elasticsearch REST API. For more details, see to the WebLogic Logging Exporter project.\nThis sample shows you how to publish WebLogic Server logs to Elasticsearch and view them in Kibana. For publishing operator logs, see this sample.\nPrerequisites This document assumes that you have already set up Elasticsearch and Kibana for logs collection. If you have not, please see this document.\n Download the WebLogic Logging Exporter binaries The pre-built binaries are available on the WebLogic Logging Exporter Releases page.\nDownload:\n weblogic-logging-exporter-1.0.0.jar from the Releases page. snakeyaml-1.25.jar from Maven Central.  These identifiers are used in the sample commands in this document.\n wcsites-ns: WebCenter Sites domain namespace wcsitesinfra: domainUID wcsitesinfra-adminserver: Administration Server pod name   Copy the JAR Files to the WebLogic Domain Home Copy the weblogic-logging-exporter-1.0.0.jar and snakeyaml-1.25.jar files to the domain home directory in the Administration Server pod.\n$ kubectl cp \u0026lt;file-to-copy\u0026gt; \u0026lt;namespace\u0026gt;/\u0026lt;Administration-Server-pod\u0026gt;:\u0026lt;domainhome\u0026gt; $ kubectl cp snakeyaml-1.25.jar wcsites-ns/wcsitesinfra-adminserver:/u01/oracle/user_projects/domains/wcsitesinfra/ $ kubectl cp weblogic-logging-exporter-1.0.0.jar wcsites-ns/wcsitesinfra-adminserver:/u01/oracle/user_projects/domains/wcsitesinfra/ Add a Startup Class to the Domain Configuration   In the WebLogic Server Administration Console, in the left navigation pane, expand Environment, and then select Startup and Shutdown Classes.\n  Add a new startup class. You may choose any descriptive name, however, the class name must be weblogic.logging.exporter.Startup.\n  Target the startup class to each server from which you want to export logs.\n  In your /u01/oracle/user_projects/domains/wcsitesinfra/config/config.xml file, this update should look similar to the following example:\n$ kubectl exec -it wcsitesinfra-adminserver -n wcsites-ns cat /u01/oracle/user_projects/domains/wcsitesinfra/config/config.xml \u0026lt;startup-class\u0026gt; \u0026lt;name\u0026gt;weblogic-logging-exporter\u0026lt;/name\u0026gt; \u0026lt;target\u0026gt;AdminServer,wcsites_cluster\u0026lt;/target\u0026gt; \u0026lt;class-name\u0026gt;weblogic.logging.exporter.Startup\u0026lt;/class-name\u0026gt; \u0026lt;/startup-class\u0026gt;   Update the WebLogic Server CLASSPATH   Copy the setDomainEnv.sh file from the pod to a local folder:\n$ kubectl cp wcsites-ns/wcsitesinfra-adminserver:/u01/oracle/user_projects/domains/wcsitesinfra/bin/setDomainEnv.sh $PWD/setDomainEnv.sh tar: Removing leading `/' from member names Ignore exception: tar: Removing leading '/' from member names\n  Update the server class path in setDomainEnv.sh:\nCLASSPATH=/u01/oracle/user_projects/domains/wcsitesinfra/weblogic-logging-exporter-1.0.0.jar:/u01/oracle/user_projects/domains/wcsitesinfra/snakeyaml-1.25.jar:${CLASSPATH} export CLASSPATH   Copy back the modified setDomainEnv.sh file to the pod:\n$ kubectl cp setDomainEnv.sh wcsites-ns/wcsitesinfra-adminserver:/u01/oracle/user_projects/domains/wcsitesinfra/bin/setDomainEnv.sh ``\n  Create a Configuration File for the WebLogic Logging Exporter   Specify the Elasticsearch server host and port number in file kubernetes/samples/scripts/create-wcsites-domain/utils/weblogic-logging-exporter/WebLogicLoggingExporter.yaml:\nExample:\nweblogicLoggingIndexName: wls publishHost: elasticsearch.default.svc.cluster.local publishPort: 9200 domainUID: wcsitesinfra weblogicLoggingExporterEnabled: true weblogicLoggingExporterSeverity: TRACE weblogicLoggingExporterBulkSize: 1   Copy the WebLogicLoggingExporter.yaml file to the domain home directory in the WebLogic Administration Server pod:\n$ kubectl cp kubernetes/samples/scripts/create-wcsites-domain/utils/weblogic-logging-exporter/WebLogicLoggingExporter.yaml wcsites-ns/wcsitesinfra-adminserver:/u01/oracle/user_projects/domains/wcsitesinfra/config/   Restart All the Servers in the Domain To restart the servers, stop and then start them using the following commands:\nTo stop the servers:\n$ kubectl patch domain wcsitesinfra -n wcsites-ns --type='json' -p='[{\u0026quot;op\u0026quot;: \u0026quot;replace\u0026quot;, \u0026quot;path\u0026quot;: \u0026quot;/spec/serverStartPolicy\u0026quot;, \u0026quot;value\u0026quot;: \u0026quot;NEVER\u0026quot; }]' To start the servers:\n$ kubectl patch domain wcsitesinfra -n wcsites-ns --type='json' -p='[{\u0026quot;op\u0026quot;: \u0026quot;replace\u0026quot;, \u0026quot;path\u0026quot;: \u0026quot;/spec/serverStartPolicy\u0026quot;, \u0026quot;value\u0026quot;: \u0026quot;IF_NEEDED\u0026quot; }]' After all the servers are restarted, see their server logs to check that the weblogic-logging-exporter class is called, as shown below:\n======================= WebLogic Logging Exporter Startup class called Reading configuration from file name: /u01/oracle/user_projects/domains/wcsitesinfra/config/WebLogicLoggingExporter.yaml Config{weblogicLoggingIndexName='wls', publishHost='domain.host.com', publishPort=9200, weblogicLoggingExporterSeverity='Notice', weblogicLoggingExporterBulkSize='2', enabled=true, weblogicLoggingExporterFilters=FilterConfig{expression='NOT(MSGID = 'BEA-000449')', servers=[]}], domainUID='wcsitesinfra'} Create an Index Pattern in Kibana Create an index pattern wls* in Kibana \u0026gt; Management. After the servers are started, you will see the log data in the Kibana dashboard:\n"
},
{
	"uri": "/fmw-kubernetes/oig/configure-ingress/ingress-voyager-setup-for-oig-domain-setup-on-k8s-ssl/",
	"title": "d. Using an Ingress with Voyager (SSL)",
	"tags": [],
	"description": "Steps to set up an Ingress for Voyager to direct traffic to the OIG domain (SSL).",
	"content": "Setting Up an Ingress for Voyager for the OIG Domain on Kubernetes The instructions below explain how to set up Voyager as an Ingress for the OIG domain with SSL termination.\nNote: All the steps below should be performed on the master node.\n Create a SSL Certificate  Generate SSL Certificate Create a Kubernetes Secret for SSL   Install Voyager  Configure the repository Create Namespace and Install Voyager Setup Routing Rules for the Domain   Create an Ingress for the Domain Verify that You can Access the Domain URL Cleanup  Create a SSL Certificate Generate SSL Certificate   Generate a private key and certificate signing request (CSR) using a tool of your choice. Send the CSR to your certificate authority (CA) to generate the certificate.\nIf you want to use a certificate for testing purposes you can generate a self signed certificate using openssl:\n$ mkdir \u0026lt;work directory\u0026gt;/ssl $ cd \u0026lt;work directory\u0026gt;/ssl $ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \u0026quot;/CN=\u0026lt;nginx-hostname\u0026gt;\u0026quot; For example:\n$ mkdir /scratch/OIGDockerK8S/ssl $ cd /scratch/OIGDockerK8S/ssl $ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \u0026quot;/CN=masternode.example.com\u0026quot; Note: The CN should match the host.domain of the master node in order to prevent hostname problems during certificate verification.\nThe output will look similar to the following:\n$ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \u0026quot;/CN=masternode.example.com\u0026quot; Generating a 2048 bit RSA private key ..........................................+++ .......................................................................................................+++ writing new private key to 'tls.key' ----- $   Create a Kubernetes Secret for SSL   Create a secret for SSL containing the SSL certificate by running the following command:\n$ kubectl -n oimcluster create secret tls \u0026lt;domain_id\u0026gt;-tls-cert --key \u0026lt;work directory\u0026gt;/tls.key --cert \u0026lt;work directory\u0026gt;/tls.crt For example:\n$ kubectl -n oimcluster create secret tls oimcluster-tls-cert --key /scratch/OIGDockerK8S/ssl/tls.key --cert /scratch/OIGDockerK8S/ssl/tls.crt The output will look similar to the following:\nsecret/oimcluster-tls-cert created Confirm that the secret is created by running the following command:\n$ kubectl get secret oimcluster-tls-cert -o yaml -n oimcluster The output will look similar to the following:\napiVersion: v1 data: tls.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURGVENDQWYyZ0F3SUJBZ0lKQUl3ZjVRMWVxZnljTUEwR0NTcUdTSWIzRFFFQkN3VUFNQ0V4SHpBZEJnTlYKQkFNTUZtUmxiakF4WlhadkxuVnpMbTl5WVdOc1pTNWpiMjB3SGhjTk1qQXdPREV3TVRReE9UUXpXaGNOTWpFdwpPREV3TVRReE9UUXpXakFoTVI4d0hRWURWUVFEREJaa1pXNHdNV1YyYnk1MWN5NXZjbUZqYkdVdVkyOXRNSUlCCklqQU5CZ2txaGtpRzl3MEJBUUVGQUFPQ0FROEFNSUlCQ2dLQ0FRRUEyY0lpVUhwcTRVZzBhaGR6aXkycHY2cHQKSVIza2s5REd2eVRNY0syaWZQQ2dtUU5CdHV6VXNFN0l4c294eldITmU5RFpXRXJTSjVON3FYYm1lTzJkMVd2NQp1aFhzbkFTbnkwY1NLUE9xOUNZVDNQSlpDVk1MK0llZVFKdnhaVjZaWWU4V2FFL1NQSGJzczRjYy9wcG1mc3pxCnErUi83cXEyMm9ueHNHaE9vQ1h1TlQvMFF2WXVzMnNucGtueWRKRHUxelhGbDREYkFIZGMvamNVK0NPWWROeS8KT3Iza2JIV0FaTkR4OWxaZUREOTRmNXZLbGJwMy9rcUF2V0FkSVJZa2UrSmpNTHg0VHo2ZlM0VXoxbzdBSTVuSApPQ1ZMblV5U0JkaGVuWTNGNEdFU0wwbnorVlhFWjRWVjRucWNjRmo5cnJ0Q29pT1BBNlgvNGdxMEZJbi9Qd0lECkFRQUJvMUF3VGpBZEJnTlZIUTRFRmdRVWw1VnVpVDBDT0xGTzcxMFBlcHRxSC9DRWZyY3dId1lEVlIwakJCZ3cKRm9BVWw1VnVpVDBDT0xGTzcxMFBlcHRxSC9DRWZyY3dEQVlEVlIwVEJBVXdBd0VCL3pBTkJna3Foa2lHOXcwQgpBUXNGQUFPQ0FRRUFXdEN4b2ZmNGgrWXZEcVVpTFFtUnpqQkVBMHJCOUMwL1FWOG9JQzJ3d1hzYi9KaVNuMHdOCjNMdHppejc0aStEbk1yQytoNFQ3enRaSkc3NVluSGRKcmxQajgzVWdDLzhYTlFCSUNDbTFUa3RlVU1jWG0reG4KTEZEMHpReFhpVzV0N1FHcWtvK2FjeN3JRMXlNSE9HdVVkTTZETzErNXF4cTdFNXFMamhyNEdKejV5OAoraW8zK25UcUVKMHFQOVRocG96RXhBMW80OEY0ZHJybWdqd3ROUldEQVpBYmYyV1JNMXFKWXhxTTJqdU1FQWNsCnFMek1TdEZUQ2o1UGFTQ0NUV1VEK3ZlSWtsRWRpaFdpRm02dzk3Y1diZ0lGMlhlNGk4L2szMmF1N2xUTDEvd28KU3Q2dHpsa20yV25uUFlVMzBnRURnVTQ4OU02Z1dybklpZz09Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K tls.key: LS0tLS1CRUdJQVRFIEtFWS0tLS0tCk1JSUV1d0lCQURBTkJna3Foa2lHOXcwQkFRRUZBQVNDQktVd2dnU2hBZ0VBQW9JQkFRRFp3aUpRZW1yaFNEUnEKRjNPTExhbS9xbTBoSGVTVDBNYS9KTXh3cmFKODhLQ1pBMEcyN05Td1Rzakd5akhOWWMxNzBObFlTdEluazN1cApkdVo0N1ozVmEvbTZGZXljQktmTFJ4SW84NnIwSmhQYzhsa0pVd3Y0aDU1QW0vRmxYcGxoN3hab1Q5SThkdXl6Cmh4eittbVorek9xcjVIL3VxcmJhaWZHd2FFNmdKZTQxUC9SQzlpNnpheWVtU2ZKMGtPN1hOY1dYZ05zQWQxeisKTnhUNEk1aDAzTDg2dmVSc2RZQmswUEgyVmw0TVAzaC9tOHFWdW5mK1NvQzlZQjBoRmlSNzRtTXd2SGhQUHA5TApoVFBXanNBam1jYzRKVXVkVEpJRjJGNmRqY1hnWVJJdlNmUDVWY1JuaFZYaWVweHdXUDJ1dTBLaUk0OERwZi9pCkNyUVVpZjgvQWdNQkFBRUNnZjl6cnE2TUVueTFNYWFtdGM2c0laWU1QSDI5R2lSVVlwVXk5bG1sZ3BqUHh3V0sKUkRDay9Td0FmZG9yd1Q2ejNVRk1oYWJ4UU01a04vVjZFYkJlamQxTGhCRW15bjdvWTVEQWJRRTR3RG9SZWlrVApONndWU0FrVC92Z1RXc1RqRlY1bXFKMCt6U2ppOWtySkZQNVNRN1F2cUswQ3BHRlNhVjY2dW8ycktiNmJWSkJYCkxPZmZPMytlS0tVazBaTnE1Q1NVQk9mbnFoNVFJSGdpaDNiMTRlNjBDdGhYcEh6bndrNWhaMHBHZE9BQm9aTkoKZ21lanUyTEdzVWxXTjBLOVdsUy9lcUllQzVzQm9jaWlocmxMVUpGWnpPRUV6LzErT2cyemhmT29yTE9rMTIrTgpjQnV0cTJWQ2I4ZFJDaFg1ZzJ0WnBrdzgzcXN5RSt3M09zYlQxa0VDZ1lFQTdxUnRLWGFONUx1SENvWlM1VWhNCm9Hak1WcnYxTEg0eGNhaDJITmZnMksrMHJqQkJONGpkZkFDMmF3R3ZzU1EyR0lYRzVGYmYyK0pwL1kxbktKOEgKZU80MzNLWVgwTDE4NlNNLzFVay9HSEdTek1CWS9KdGR6WkRrbTA4UnBwaTl4bExTeDBWUWtFNVJVcnJJcTRJVwplZzBOM2RVTHZhTVl1UTBrR2dncUFETUNnWUVBNlpqWCtjU2VMZ1BVajJENWRpUGJ1TmVFd2RMeFNPZDFZMUFjCkUzQ01YTWozK2JxQ3BGUVIrTldYWWVuVmM1QiszajlSdHVnQ0YyTkNSdVdkZWowalBpL243UExIRHdCZVY0bVIKM3VQVHJmamRJbFovSFgzQ2NjVE94TmlaajU4VitFdkRHNHNHOGxtRTRieStYRExIYTJyMWxmUk9sUVRMSyswVgpyTU93eU1VQ2dZRUF1dm14WGM4NWxZRW9hU0tkU0cvQk9kMWlYSUtmc2VDZHRNT2M1elJ0UXRsSDQwS0RscE54CmxYcXBjbVc3MWpyYzk1RzVKNmE1ZG5xTE9OSFZoWW8wUEpmSXhPU052RXI2MTE5NjRBMm5sZXRHYlk0M0twUkEKaHBPRHlmdkZoSllmK29kaUJpZFUyL3ZBMCtUczNSUHJzRzBSOUVDOEZqVDNaZVhaNTF1R0xPa0NnWUFpTmU0NwplQjRxWXdrNFRsMTZmZG5xQWpaQkpLR05xY2c1V1R3alpMSkp6R3owdCtuMkl4SFd2WUZFSjdqSkNmcHFsaDlqCmlDcjJQZVV3K09QTlNUTG1JcUgydzc5L1pQQnNKWXVsZHZ4RFdGVWFlRXg1aHpkNDdmZlNRRjZNK0NHQmthYnIKVzdzU3R5V000ZFdITHpDaGZMS20yWGJBd0VqNUQrbkN1WTRrZVFLQmdFSkRHb0puM1NCRXcra2xXTE85N09aOApnc3lYQm9mUW1lRktIS2NHNzFZUFhJbTRlV1kyUi9KOCt5anc5b1FJQ3o5NlRidkdSZEN5QlJhbWhoTmFGUzVyCk9MZUc0ejVENE4zdThUc0dNem9QcU13KzBGSXJiQ3FzTnpGWTg3ekZweEdVaXZvRWZLNE82YkdERTZjNHFqNGEKNmlmK0RSRSt1TWRMWTQyYTA3ekoKLS0tLS1FTkQgUFJJVkFURSBLRVktLS0tLQo= kind: Secret metadata: creationTimestamp: \u0026quot;2020-08-10T14:22:52Z\u0026quot; managedFields: - apiVersion: v1 fieldsType: FieldsV1 fieldsV1: f:data: .: {} f:tls.crt: {} f:tls.key: {} f:type: {} manager: kubectl operation: Update time: \u0026quot;2020-08-10T14:22:52Z\u0026quot; name: oimcluster-tls-cert namespace: oimcluster resourceVersion: \u0026quot;3722477\u0026quot; selfLink: /api/v1/namespaces/oimcluster/secrets/oimcluster-tls-cert uid: 596fe0fe-effd-4eb9-974d-691da3a3b15a type: kubernetes.io/tls   Install Voyager Use Helm to install Voyager. For detailed information, see this document.\nConfigure the repository   Add the Helm chart repository for installing Voyager using the following command:\n$ helm repo add appscode https://charts.appscode.com/stable The output will look similar to the following:\n$ helm repo add appscode https://charts.appscode.com/stable \u0026quot;appscode\u0026quot; has been added to your repositories   Update the repository using the following command:\n$ helm repo update The output will look similar to the following:\nHang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \u0026quot;appscode\u0026quot; chart repository Update Complete. Happy Helming!   Run the following command to show the Voyager chart was added successfully.\n$ helm search repo appscode/voyager The output will look similar to the following:\nNAME CHART VERSION APP VERSION DESCRIPTION appscode/voyager v12.0.0 v12.0.0 Voyager by AppsCode - Secure HAProxy Ingress Co...   Create Namespace and Install Voyager   Create a namespace for Voyager:\n$ kubectl create namespace voyagerssl The output will look similar to the following:\nnamespace/voyagerssl created   Install Voyager using the following Helm command:\n$ helm install voyager-ingress appscode/voyager --version 12.0.0 --namespace voyagerssl --set cloudProvider=baremetal --set apiserver.enableValidatingWebhook=false Note: For bare metal Kubernetes use --set cloudProvider=baremetal. If using a managed Kubernetes service then the value should be set for your specific service as per the Voyager install guide.\nThe output will look similar to the following:\nNAME: voyager-ingress LAST DEPLOYED: Wed Aug 12 09:00:58 2020 NAMESPACE: voyagerssl STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: Set cloudProvider for installing Voyager To verify that Voyager has started, run: kubectl get deployment --namespace voyagerssl -l \u0026quot;app.kubernetes.io/name=voyager,app.kubernetes.io/instance=voyager-ingress\u0026quot;   Verify that the ingress has started by running the following command:\n$ kubectl get deployment --namespace voyagerssl -l \u0026quot;app.kubernetes.io/name=voyager,app.kubernetes.io/instance=voyager-ingress\u0026quot; The output will look similar to the following:\nNAME READY UP-TO-DATE AVAILABLE AGE voyager-ingress 1/1 1 1 89s   Setup Routing Rules for the Domain   Setup routing rules using the following commands:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain $ cp values.yaml values.yaml.orig $ vi values.yaml   Edit values.yaml and ensure that type=VOYAGER,tls=SSL, and secretName: \u0026lt;SSL Secret\u0026gt; , for example:\n$ cat values.yaml # Copyright 2020 Oracle Corporation and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # Default values for ingress-per-domain. # This is a YAML-formatted file. # Declare variables to be passed into your templates. # Load balancer type. Supported values are: VOYAGER, NGINX type: VOYAGER # Type of Configuration Supported Values are : NONSSL,SSL # tls: NONSSL tls: SSL # TLS secret name if the mode is SSL secretName: oimcluster-tls-cert # WLS domain as backend to the load balancer wlsDomain: domainUID: oimcluster oimClusterName: oim_cluster soaClusterName: soa_cluster soaManagedServerPort: 8001 oimManagedServerPort: 14000 adminServerName: adminserver adminServerPort: 7001 # Traefik specific values # traefik: # hostname used by host-routing # hostname: idmdemo.m8y.xyz # Voyager specific values voyager: # web port webPort: 30305 # stats port statsPort: 30315 $   Create an Ingress for the Domain   Create an Ingress for the domain (oimcluster-voyager), in the domain namespace by using the sample Helm chart.\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator $ helm install oimcluster-voyager kubernetes/samples/charts/ingress-per-domain --namespace \u0026lt;namespace\u0026gt; --values kubernetes/samples/charts/ingress-per-domain/values.yaml For example:\n$ cd /scratch/OIGDockerK8S/weblogic-kubernetes-operator $ helm install oimcluster-voyager kubernetes/samples/charts/ingress-per-domain --namespace oimcluster --values kubernetes/samples/charts/ingress-per-domain/values.yaml The output will look similar to the following:\nNAME: oimcluster-voyager LAST DEPLOYED: Wed Sep 30 01:51:05 2020 NAMESPACE: oimcluster STATUS: deployed REVISION: 1 TEST SUITE: None   Run the following command to show the ingress is created successfully:\n$ kubectl get ingress.voyager.appscode.com -n oimcluster The output will look similar to the following:\nNAME HOSTS LOAD_BALANCER_IP AGE oimcluster-voyager * 3m44s   Return details of the ingress using the following command:\n$ kubectl describe ingress.voyager.appscode.com oimcluster-voyager -n oimcluster The output will look similar to the following:\nName: oimcluster-voyager Namespace: oimcluster Labels: app.kubernetes.io/managed-by=Helm weblogic.resourceVersion=domain-v2 Annotations: ingress.appscode.com/affinity: cookie ingress.appscode.com/stats: true ingress.appscode.com/type: NodePort meta.helm.sh/release-name: oimcluster-voyager meta.helm.sh/release-namespace: oimcluster API Version: voyager.appscode.com/v1beta1 Kind: Ingress Metadata: Creation Timestamp: 2020-09-30T08:51:05Z Generation: 1 Managed Fields: API Version: voyager.appscode.com/v1beta1 Fields Type: FieldsV1 fieldsV1: f:metadata: f:annotations: .: f:ingress.appscode.com/affinity: f:ingress.appscode.com/stats: f:ingress.appscode.com/type: f:meta.helm.sh/release-name: f:meta.helm.sh/release-namespace: f:labels: .: f:app.kubernetes.io/managed-by: f:weblogic.resourceVersion: f:spec: .: f:frontendRules: f:rules: f:tls: Manager: Go-http-client Operation: Update Time: 2020-09-30T08:51:05Z Resource Version: 1440614 Self Link: /apis/voyager.appscode.com/v1beta1/namespaces/oimcluster/ingresses/oimcluster-voyager UID: 875e7d90-b166-40ff-b792-c764d514c0c3 Spec: Frontend Rules: Port: 443 Rules: http-request set-header WL-Proxy-SSL true Rules: Host: * Http: Node Port: 30305 Paths: Backend: Service Name: oimcluster-adminserver Service Port: 7001 Path: /console Backend: Service Name: oimcluster-adminserver Service Port: 7001 Path: /em Backend: Service Name: oimcluster-cluster-soa-cluster Service Port: 8001 Path: /soa-infra Backend: Service Name: oimcluster-cluster-soa-cluster Service Port: 8001 Path: /soa Backend: Service Name: oimcluster-cluster-soa-cluster Service Port: 8001 Path: /integration Backend: Service Name: oimcluster-cluster-oim-cluster Service Port: 14000 Path: / Tls: Hosts: * Secret Name: oimcluster-tls-cert Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ServiceReconcileSuccessful 65s voyager-operator Successfully created NodePort Service voyager-oimcluster-voyager Normal ConfigMapReconcileSuccessful 64s voyager-operator Successfully created ConfigMap voyager-oimcluster-voyager Normal RBACSuccessful 64s voyager-operator Successfully created ServiceAccount voyager-oimcluster-voyager Normal RBACSuccessful 64s voyager-operator Successfully created Role voyager-oimcluster-voyager Normal RBACSuccessful 64s voyager-operator Successfully created RoleBinding voyager-oimcluster-voyager Normal DeploymentReconcileSuccessful 64s voyager-operator Successfully created HAProxy Deployment voyager-oimcluster-voyager Normal StatsServiceReconcileSuccessful 64s voyager-operator Successfully created stats Service voyager-oimcluster-voyager-stats Normal DeploymentReconcileSuccessful 64s voyager-operator Successfully patched HAProxy Deployment voyager-oimcluster-voyager   Find the NodePort of Voyager using the following command:\n$ kubectl get svc -n oimcluster The output will look similar to the following:\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE oimcluster-adminserver ClusterIP None \u0026lt;none\u0026gt; 7001/TCP 18h oimcluster-cluster-oim-cluster ClusterIP 10.97.121.159 \u0026lt;none\u0026gt; 14000/TCP 18h oimcluster-cluster-soa-cluster ClusterIP 10.111.231.242 \u0026lt;none\u0026gt; 8001/TCP 18h oimcluster-oim-server1 ClusterIP None \u0026lt;none\u0026gt; 14000/TCP 18h oimcluster-oim-server2 ClusterIP 10.108.139.30 \u0026lt;none\u0026gt; 14000/TCP 18h oimcluster-oim-server3 ClusterIP 10.97.170.104 \u0026lt;none\u0026gt; 14000/TCP 18h oimcluster-oim-server4 ClusterIP 10.99.82.214 \u0026lt;none\u0026gt; 14000/TCP 18h oimcluster-oim-server5 ClusterIP 10.98.75.228 \u0026lt;none\u0026gt; 14000/TCP 18h oimcluster-soa-server1 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP 18h oimcluster-soa-server2 ClusterIP 10.107.232.220 \u0026lt;none\u0026gt; 8001/TCP 18h oimcluster-soa-server3 ClusterIP 10.108.203.6 \u0026lt;none\u0026gt; 8001/TCP 18h oimcluster-soa-server4 ClusterIP 10.96.178.0 \u0026lt;none\u0026gt; 8001/TCP 18h oimcluster-soa-server5 ClusterIP 10.107.83.62 \u0026lt;none\u0026gt; 8001/TCP 18h oimcluster-voyager-stats NodePort 10.96.62.0 \u0026lt;none\u0026gt; 56789:30315/TCP 3m19s voyager-oimcluster-voyager NodePort 10.97.231.109 \u0026lt;none\u0026gt; 443:30305/TCP,80:30419/TCP 3m12s voyager-oimcluster-voyager-stats ClusterIP 10.99.185.46 \u0026lt;none\u0026gt; 56789/TCP 3m6s Identify the service voyager-oimcluster-voyager in the above output and get the NodePort which corresponds to port 443. In this example it will be 30305.\n  To confirm that the new Ingress is successfully routing to the domain\u0026rsquo;s server pods, run the following command to send a request to the URL for the \u0026ldquo;WebLogic ReadyApp framework\u0026rdquo;:\n$ curl -v -k https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/weblogic/ready For example:\n$ curl -v -k https://masternode.example.com:30305/weblogic/ready The output will look similar to the following:\n* About to connect() to masternode.example.com port 30305 (#0) * Trying 12.345.678.9... * Connected to masternode.example.com (12.345.678.9) port 30305 (#0) * Initializing NSS with certpath: sql:/etc/pki/nssdb * skipping SSL peer certificate verification * SSL connection using TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 * Server certificate: * subject: CN=masternode.example.com * start date: Sep 29 14:52:35 2020 GMT * expire date: Sep 29 14:52:35 2021 GMT * common name: masternode.example.com * issuer: CN=masternode.example.com \u0026gt; GET /weblogic/ready HTTP/1.1 \u0026gt; User-Agent: curl/7.29.0 \u0026gt; Host: masternode.example.com:30305 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; Date: Wed, 30 Sep 2020 08:56:08 GMT \u0026lt; Content-Length: 0 \u0026lt; Strict-Transport-Security: max-age=15768000 \u0026lt; Set-Cookie: SERVERID=pod-oimcluster-oim-server1; path=/ \u0026lt; Cache-control: private \u0026lt; * Connection #0 to host masternode.example.com left intact   Verify that You can Access the Domain URL After setting up the Voyager ingress, verify that the domain applications are accessible through the Voyager ingress port (for example 30305) as per Validate Domain URLs \nCleanup If you need to remove the Voyager Ingress then remove the ingress with the following commands:\n$ helm delete oimcluster-voyager -n oimcluster $ helm delete voyager-ingress -n voyagerssl $ kubectl delete namespace voyagerssl The output will look similar to the following:\n$ helm delete oimcluster-voyager -n oimcluster release \u0026quot;oimcluster-voyager\u0026quot; uninstalled $ helm delete voyager-ingress -n voyagerssl release \u0026quot;voyager-ingress\u0026quot; uninstalled $ kubectl delete namespace voyagerssl namespace \u0026quot;voyagerssl\u0026quot; deleted "
},
{
	"uri": "/fmw-kubernetes/wcsites-domains/manage-wcsites-domains/weblogic-monitoring-exporter-setup/",
	"title": "e. Monitor a WebCenter Sites domain",
	"tags": [],
	"description": "Use the WebLogic Monitoring Exporter to monitor a WebCenter Sites instance using Prometheus and Grafana.",
	"content": "You can monitor a WebCenter Sites domain using Prometheus and Grafana by exporting the metrics from the domain instance using the WebLogic Monitoring Exporter. This sample shows you how to set up the WebLogic Monitoring Exporter to push the data to Prometheus.\nPrerequisites This document assumes that the Prometheus Operator is deployed on the Kubernetes cluster. If it is not already deployed, follow the steps below for deploying the Prometheus Operator.\nClone the kube-prometheus project $ git clone https://github.com/coreos/kube-prometheus.git Label the nodes Kube-Prometheus requires all the exporter nodes to be labelled with kubernetes.io/os=linux. If a node is not labelled, then you must label it using the following command:\n$ kubectl label nodes --all kubernetes.io/os=linux Create Prometheus and Grafana resources Change to the kube-prometheus directory and execute the following commands to create the namespace and CRDs:\nNOTE: Wait for a minute for each command to process.\n$ cd kube-prometheus $ kubectl create -f manifests/setup $ until kubectl get servicemonitors --all-namespaces ; do date; sleep 1; echo \u0026#34;\u0026#34;; done $ kubectl create -f manifests/ Provide external access To provide external access for Grafana, Prometheus, and Alertmanager, execute the commands below:\n$ kubectl patch svc grafana -n monitoring --type=json -p \u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/type\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;NodePort\u0026#34; },{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/ports/0/nodePort\u0026#34;, \u0026#34;value\u0026#34;: 32100 }]\u0026#39; $ kubectl patch svc prometheus-k8s -n monitoring --type=json -p \u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/type\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;NodePort\u0026#34; },{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/ports/0/nodePort\u0026#34;, \u0026#34;value\u0026#34;: 32101 }]\u0026#39; $ kubectl patch svc alertmanager-main -n monitoring --type=json -p \u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/type\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;NodePort\u0026#34; },{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/ports/0/nodePort\u0026#34;, \u0026#34;value\u0026#34;: 32102 }]\u0026#39; NOTE:\n 32100 is the external port for Grafana 32101 is the external port for Prometheus 32102 is the external port for Alertmanager   Set Up the WebLogic Monitoring Exporter Set up the WebLogic Monitoring Exporter that will collect WebLogic Server metrics and monitor your WebCenter Sites domain.\nGenerate the WebLogic Monitoring Exporter Deployment Package Two packages are required as the listening ports are different for the Administration Server and Managed Servers. One binary required for the Admin Server (wls-exporter-as.war) and one for Managed Cluster (wls-exporter-ms.war). Set the required proxies and then run the script getX.X.X.sh to generate two binaries:\n$ cd kubernetes/samples/scripts/create-wcsites-domain/utils/weblogic-monitoring-exporter $ sh get1.1.0.sh Output:\n % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 607 0 607 0 0 357 0 --:--:-- 0:00:01 --:--:-- 357 100 2016k 100 2016k 0 0 398k 0 0:00:05 0:00:05 --:--:-- 797k -------------------wls-exporter-ms start------------------- created /tmp/ci-GNysQzP1kv Copying completed /tmp/ci-GNysQzP1kv /kubernetes/2.4.0/scripts/create-wcsites-domain/utils/weblogic-monitoring-exporter in temp dir adding: WEB-INF/weblogic.xml (deflated 66%) adding: config.yml (deflated 63%) wls-exporter-ms.war is ready -------------------wls-exporter-ms end------------------- -------------------wls-exporter-as start------------------- Copying completed in temp dir adding: WEB-INF/weblogic.xml (deflated 66%) adding: config.yml (deflated 52%) wls-exporter-as.war is ready -------------------wls-exporter-as end------------------- zip completed kubernetes/2.4.0/scripts/create-wcsites-domain/utils/weblogic-monitoring-exporter Copy the WAR Files to the WebLogic Domain Home Copy the wls-exporter-as.war and wls-exporter-ms.war files to the domain home directory in the Administration Server pod.\n$ kubectl cp wls-exporter-as.war wcsites-ns/wcsitesinfra-adminserver:/u01/oracle/user_projects/domains/wcsitesinfra/ $ kubectl cp wls-exporter-ms.war wcsites-ns/wcsitesinfra-adminserver:/u01/oracle/user_projects/domains/wcsitesinfra/ Deploy the WebLogic Monitoring Exporter Follow these steps to deploy the package in the WebLogic Server instances:\n  In the Administration Server and Managed Servers, deploy the WebLogic Monitoring Exporter (wls-exporter-ms.war) separately using the Oracle Enterprise Manager.\n  Select the servers to which the Exporter WAR should be deployed:\n  Set the application name. The application name must be different if it is deployed separately in the Administration Server and Managed Servers. Make sure the context-root for both the deployments is wls-exporter:\n  Click Install and start application.\n  Then deploy the WebLogic Monitoring Exporter application (wls-exporter-ms.war).\n  Activate the changes to start the application. If the application is started and the port is exposed, then you can access the WebLogic Monitoring Exporter console using this URL: http://\u0026lt;server:port\u0026gt;/wls-exporter.\n  Repeat same steps for wls-exporter-as.war.\n  Configure Prometheus Operator Prometheus enables you to collect metrics from the WebLogic Monitoring Exporter. The Prometheus Operator identifies the targets using service discovery. To get the WebLogic Monitoring Exporter end point discovered as a target, you must create a service monitor pointing to the service.\nSee the following sample service monitor deployment YAML configuration file located at\nkubernetes/samples/scripts/create-wcsites-domain/utils/weblogic-monitoring-exporter/wls-exporter.yaml.\nServiceMonitor for wls-exporter:\napiVersion: v1 kind: Secret metadata: name: basic-auth namespace: monitoring data: password: V2VsY29tZTE= # Welcome1 i.e.'WebLogic password' user: d2VibG9naWM= # weblogic i.e. 'WebLogic username' type: Opaque --- apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: wls-exporter-wcsitesinfra namespace: monitoring labels: k8s-app: wls-exporter spec: namespaceSelector: matchNames: - wcsites-ns selector: matchLabels: weblogic.domainName: wcsitesinfra endpoints: - basicAuth: password: name: basic-auth key: password username: name: basic-auth key: user port: default relabelings: - action: labelmap regex: __meta_kubernetes_service_label_(.+) interval: 10s honorLabels: true path: /wls-exporter/metrics The exporting of metrics from wls-exporter requires basicAuth so a Kubernetes Secret is created with the user name and password that are base64 encoded. This Secret will be used in the ServiceMonitor deployment.\nWhen generating the base64 encoded strings for the user name and password, observe if a new line character is appended in the encoded string. This line character causes an authentication failure. To avoid a new line string, use the following example:\n$ echo -n \u0026quot;Welcome1\u0026quot; | base64 V2VsY29tZTE= In the deployment YAML configuration for wls-exporter shown above, weblogic.domainName: wcsitesinfra is used as a label under spec.selector.matchLabels, so all the services will be selected for the service monitor. If you don\u0026rsquo;t use this label, you should create separate service monitors for each server\u0026ndash;if the server name is used as matching labels in spec.selector.matchLabels. Doing so will require you to relabel the configuration because Prometheus, by default, ignores the labels provided in the wls-exporter.\nBy default, Prometheus does not store all the labels provided by the target. In the service monitor deployment YAML configuration, you must mention the relabeling configuration (spec.endpoints.relabelings) so that certain labels provided by weblogic-monitoring-exporter (required for the Grafana dashboard) are stored in Prometheus. Do not delete the following section from the configuration YAML file:\nrelabelings: - action: labelmap regex: __meta_kubernetes_service_label_(.+) Add RoleBinding and Role for the WebLogic Domain Namespace The RoleBinding is required for Prometheus to access the endpoints provided by the WebLogic Monitoring Exporter. You need to add RoleBinding for the namespace under which the WebLogic Servers pods are running in the Kubernetes cluster. Edit the kube-prometheus/manifests/prometheus-roleBindingSpecificNamespaces.yaml file in the Prometheus Operator deployment manifests and add the RoleBinding for the namespace (wcsites-ns) similar to the following example:\n- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: prometheus-k8s namespace: wcsites-ns roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: prometheus-k8s subjects: - kind: ServiceAccount name: prometheus-k8s namespace: monitoring Similarly, add the Role for the namespace under which the WebLogic Servers pods are running in the Kubernetes cluster. Edit kube-prometheus/manifests/prometheus-roleSpecificNamespaces.yaml in the Prometheus Operator deployment manifests and add the Role for the namespace (wcsites-ns) similar to the following example:\n- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: prometheus-k8s namespace: wcsites-ns rules: - apiGroups: - \u0026quot;\u0026quot; resources: - services - endpoints - pods verbs: - get - list - watch Then apply prometheus-roleBindingSpecificNamespaces.yaml and prometheus-roleSpecificNamespaces.yaml for the RoleBinding and Role to take effect in the cluster.\n$ kubectl apply -f kube-prometheus/manifests/prometheus-roleBindingSpecificNamespaces.yaml $ kubectl apply -f kube-prometheus/manifests/prometheus-roleSpecificNamespaces.yaml Deploy the Service Monitor To deploy the service monitor, use the above wls-exporter.yaml deployment YAML and run the following command:\n$ kubectl create -f kubernetes/samples/scripts/create-wcsites-domain/utils/weblogic-monitoring-exporter/wls-exporter.yaml Additional Setup For Voyager Load Balancer In step 2 of Configure Voyager to Manage Ingresses, for wcsites-cluster, enable the last rule for path ‘wls-exporter’ and then re-deploy Voyager Load Balancer.\nEnable Prometheus to Discover the Service After the deployment of the service monitor, Prometheus should be able to discover wls-exporter and export metrics.\nYou can access the Prometheus dashboard at http://mycompany.com:32101/.\nDeploy Grafana Dashboard To view the domain metrics, deploy the Grafana dashboard provided in the WebLogic Monitoring Exporter.\nYou can access the Grafana dashboard at http://mycompany.com:32100/.\n  Log in to Grafana dashboard with admin/admin.\n  Go to Settings, then select DataSources, and then Add Data Source.\nHTTP URL: Prometheus URL http://mycompany.com:32101/\nAuth: Enable Basic Auth\nBasic Auth Details: Weblogic credentials provided in step Configure Prometheus Operator\n  Download the weblogic_dashboard.json file from here.\n  Click Add and then Import. Paste the modified JSON in the Paste JSON block, and then load it.\nThis displays the WebLogic Server Dashboard.\n  "
},
{
	"uri": "/fmw-kubernetes/wcsites-domains/release-notes/",
	"title": "Release Notes",
	"tags": [],
	"description": "",
	"content": "Recent changes    Date Version Introduces backward incompatibilities Change     2020-03-18 2.4.0 No Starting with Oracle WebCenter Sites 12.2.1.4.0 support with WebLogic Kubernetes Operator 2.4.0    "
},
{
	"uri": "/fmw-kubernetes/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]