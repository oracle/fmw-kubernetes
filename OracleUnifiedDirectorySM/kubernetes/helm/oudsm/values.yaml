#
# Copyright (c) 2020, 2022, Oracle and/or its affiliates.
#
# Licensed under the Universal Permissive License v 1.0 as shown at 
# https://oss.oracle.com/licenses/upl
#
# Default values for oudsm.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 1

image:
  repository: oracle/oudsm
  tag: 12.2.1.4.0
  pullPolicy: IfNotPresent

imagePullSecrets:
  - name: regcred

nameOverride: ""

fullnameOverride: ""

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name:

podSecurityContext: {}
  # fsGroup: 2000

securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

service:
  type: ClusterIP
  port: 7001
  sslPort: 7002

ingress:
  enabled: true
  type: nginx
  # Hostname to be used with Ingress Rules.
  # If not set, hostname would be configured according to fullname
  # Hosts would be configured as <fullname>-admin.<domain>, <fullname>-admin-0.<domain>, <fullname>-admin-1.<domain>, etc.
  host:
  # domain name to be used with Ingress Rules.
  # In ingress rules, hosts would be configured as <host>.<domain>, <host>-0.<domain>, <host>-1.<domain>, etc.
  domain:
  # Backend Port to be used. Either http or https
  backendPort: http
  # SSL/TLS configuration for the Ingress
  # Flag to decide whether to force TLS/HTTPS or not
  tlsEnabled: true
  # Secret name to use an already created TLS Secret. If such secret is not provided, one would be created with name <fullname>-tls-cert
  # If the TLS Secret is in different namespace, name can be mentioned as <namespace>/<tlsSecretName>
  tlsSecret:
  # Parameters for generation of SelfSigned Cert
  # Subject's common name (cn) for SelfSigned Cert. Default Value <fullname>
  certCN:
  # Validity of Self-Signed Cert in days
  certValidityDays: 365
  nginxAnnotations:
    kubernetes.io/ingress.class: nginx
    # Following two annotations are for session stickyness
    nginx.ingress.kubernetes.io/affinity-mode: "persistent"
    nginx.ingress.kubernetes.io/affinity: "cookie"
    # Server-side HTTPS enforcement through redirect - for specific ingress resources
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
    # Required to be set if backendPort is configured as https
    # nginx.ingress.kubernetes.io/backend-protocol: "https"
  nginxAnnotationsTLS:
    kubernetes.io/ingress.class: nginx
    # Following two annotations are for session stickyness
    nginx.ingress.kubernetes.io/affinity-mode: "persistent"
    nginx.ingress.kubernetes.io/affinity: "cookie"
    # Required to be set if backendPort is configured as https
    # nginx.ingress.kubernetes.io/backend-protocol: "https"
    # For Forcing TLS/HTTPS
    nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
    nginx.ingress.kubernetes.io/ingress.allow-http: "false"
    nginx.ingress.kubernetes.io/configuration-snippet: |
      more_set_input_headers "X-Forwarded-Proto: https";
      more_set_input_headers "WL-Proxy-SSL: true";
    # nginx.ingress.kubernetes.io/x-forwarded-proto: "https"
    # nginx.ingress.kubernetes.io/x-forwarded-port: "https"

nodeSelector: {}

tolerations: []

affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          topologyKey: "kubernetes.io/hostname"
          labelSelector:
            matchExpressions:
            - key: app.kubernetes.io/name
              operator: In
              values:
              - oudsm
secret:
  # If enabled it will use the secret created with base64 encoding. if value is false, secret would not be used and input values (through --set, --values, etc.) would be used while creation of pods.
  enabled: true
  # provided the secret name  to use an already created Secret
  name:
  type: opaque

persistence:
  # If enabled, it will use the persistent volume.
  # if value is false, PV and PVC would not be used and pods would be using the default emptyDir mount volume
  enabled: true
  # provided the pvname to use an already created Persistent Volume. If blank, will use the default name from Chart
  pvname: 
  # provided the pvname to use an already created Persistent Volume Claim. If blank, will use default name from Chart
  pvcname: 
  accessMode: ReadWriteMany
  size: 20Gi
  reclaimPolicy: "Delete"
  storageClass: manual
# default supported values: either filesystem or networkstorage or custom
  type: filesystem
  networkstorage:
    nfs:
      path: /scratch/shared/oudsm_user_projects
      server: 0.0.0.0
  filesystem:
    hostPath:
      # The path location mentioned should be created and made accessible with necessary privileges for access from pods/containers. 
      path: /scratch/shared/oudsm_user_projects
  custom:
    # YAML content to be included in PersistenceVolume Object
  annotations: {}

# Parameters/Configurations for OUDSM Deployment
oudsm:
  # Weblogic Administration User
  adminUser:
  # Password for Weblogic Administration User
  adminPass:
  # Expected startup time. After specified seconds readinessProbe would start
  startupTime: 900
  # Paramter to decide livenessProbe initialDelaySeconds
  livenessProbeInitialDelay: 1200
  # Whether to invoke setWeblogicPluginEnabled or not
  weblogicPluginEnabled: "true"


elk:
  # Enabled flag to enable the integrated ELK stack for OUDSM
  enabled: false 
  elasticsearch:
    image:
      repository: docker.elastic.co/elasticsearch/elasticsearch
      tag: 6.8.0
      pullPolicy: IfNotPresent
    # esreplicas is the number of replicas of ElasticSearch deployment to be created
    esreplicas: 1
    #minimumMasterNodes is the the minimum number of master nodes that needs to configured. This will use the number of replicas (esreplicas / 2) + 1
    minimumMasterNodes: 1
    # Java options for Elasticsearch. This is where you should configure the jvm heap size
    esJAVAOpts: "-Xms512m -Xmx512m"
    #sysctl vm.max_map_count needed for Elasticsearch
    sysctlVmMaxMapCount: 262144
    ## cpu resources for elastic search
    resources:
      requests:
        cpu: "100m"
      limits:
        cpu: "1000m"
    
    esService:
  # Type of Service to be created for elastic search interfaces
      type: ClusterIP
  # Service Type for loadbalancer services exposing elastic search.
      lbrtype: ClusterIP
    
  # Kibana configuration parameters
  kibana:
    image:
      repository: docker.elastic.co/kibana/kibana
      tag: 6.8.0
      pullPolicy: IfNotPresent
    #Number of Kibana instances will be created
    kibanaReplicas : 1
    #Type of kibana service to be created
    #Port on which the kibana will be accessed inside the cluster
    #nodePort is the port on which kibana service will be accessed from outside
    service:
      type: NodePort
      targetPort: 5601
      nodePort: 31195
    
  # Logstash Configuration parameters
  logstash:
    image:
      repository: logstash
      tag: 6.6.0
      pullPolicy: IfNotPresent
    ##Port on which the logstash container will be running
    containerPort : 5044
    #targetPort Port on which the logstash will be accessed with in the cluster
    #nodePort Port on which the logstash will be accessed outside the cluster
    service:
      type: NodePort
      targetPort: 9600
      nodePort: 32222
  # If logstashConfigMap is empty, Then default logstashConfigMap will be created and used.
    logstashConfigMap:
 
  # ELK Ports on which elastic search will be listening across the nodes and outside the nodes
  elkPorts:
    rest: 9200
    internode: 9300
  #Image used for initiContainers
  busybox:
    image: busybox 

  imagePullSecrets:
    - name: dockercred


elkVolume:
  # If enabled, it will use the persistent volume.
  # if value is false, PV and PVC would not be used and there would not be any mount point available for config
  enabled: false
  # Path at which the volume would be mounted
  mountPath: /usr/share/elasticsearch/data
  # provided the pvname to use an already created Persistent Volume. If blank, will use the default name from Chart
  pvname: 
  # provided the pvname to use an already created Persistent Volume Claim. If blank, will use default name from Chart
  #pvcname: 
  accessMode: ReadWriteMany
  size: 20Gi
  storageClass: elk-oudsm 
# default supported values: either filesystem or networkstorage or custom
  type: filesystem
  networkstorage:
    nfs:
      path: /scratch/shared/oudsm_elk/data
      server: 0.0.0.0
  filesystem:
    hostPath:
      # The path location mentioned should be created and made accessible with necessary privileges for access from pods/containers. 
      path: /scratch/shared/oudsm_user_projects/data
  custom:
    # YAML content to be included in PersistenceVolume Object
  annotations: {}
