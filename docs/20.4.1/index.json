[
{
	"uri": "/fmw-kubernetes/20.4.1/oud/prerequisites/",
	"title": "Prerequisites",
	"tags": [],
	"description": "Oracle Unified Directory Prerequisites.",
	"content": "Introduction This document provides information about the system requirements for deploying and running Oracle Unified Directory 12c PS4 (12.2.1.4.0) in a Kubernetes environment.\nSystem Requirements for Oracle Unified Directory on Kubernetes  Kubernetes 1.16.0 or higher (check with kubectl version). Docker 18.03 or higher (check with docker version) Helm 3.0.2+ or higher (check with helm version)  "
},
{
	"uri": "/fmw-kubernetes/20.4.1/oudsm/prerequisites/",
	"title": "Prerequisites",
	"tags": [],
	"description": "Oracle Unified Directory Services Manager Prerequisites.",
	"content": "Introduction This document provides information about the system requirements for deploying and running Oracle Unified Directory Services Manager 12c PS4 (12.2.1.4.0) in a Kubernetes environment.\nSystem Requirements for Oracle Unified Directory Services Manager on Kubernetes  Kubernetes 1.16.0 or higher (check with kubectl version). Docker 18.03 or higher (check with docker version) Helm 3.0.2+ or higher (check with helm version)  "
},
{
	"uri": "/fmw-kubernetes/20.4.1/",
	"title": "Oracle Fusion Middleware on Kubernetes",
	"tags": [],
	"description": "This document lists all the Oracle Fusion Middleware products deployment supported on Kubernetes.",
	"content": "Oracle Fusion Middleware on Kubernetes Oracle supports the deployment of the following Oracle Fusion Middleware products on Kubernetes. Click on the appropriate document link below to get started on setting up the product.\n Oracle Access Management  The Oracle WebLogic Server Kubernetes Operator supports deployment of Oracle Access Management (OAM). Follow the instructions in this guide to set up these Oracle Access Management domains on Kubernetes.\n Oracle Identity Governance  The Oracle WebLogic Kubernetes Operator supports deployment of Oracle Identity Governance. Follow the instructions in this guide to set up Oracle Identity Governance domains on Kubernetes.\n Oracle Unified Directory  Oracle Unified Directory provides a comprehensive Directory Solution for robust Identity Management\n Oracle Unified Directory Services Manager  Oracle Unified Directory Services Manager provides an interface for managing instances of Oracle Unified Directory\n "
},
{
	"uri": "/fmw-kubernetes/20.4.1/oam/manage-oam-domains/domain-lifecycle/",
	"title": "Domain Life Cycle",
	"tags": [],
	"description": "Learn about the domain life cyle of an OAM domain.",
	"content": "As OAM domains use the Oracle WebLogic Server Kubernetes Operator, domain lifecyle operations are managed using the Oracle WebLogic Server Kubernetes Operator itself.\nThis document shows the basic operations for starting, stopping and scaling servers in the OAM domain.\nFor more detailed information refer to Domain Life Cycle in the Oracle WebLogic Server Kubernetes Operator documentation.\nDo not use the WebLogic Server Administration Console or Oracle Enterprise Manager Console to start or stop servers.\n View existing OAM servers The default OAM deployment starts the AdminServer (AdminServer), two OAM Managed Servers (oam_server1 and oam_server2) and one OAM Policy Manager server (oam_policy_mgr1).\nThe deployment also creates, but doesn\u0026rsquo;t start, three extra OAM Managed Servers (oam-server3 to oam-server5) and four more OAM Policy Manager servers (oam_policy_mgr2 to oam_policy_mgr5).\nAll these servers are visible in the WebLogic Server Console https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/console by navigating to Domain Structure \u0026gt; oamcluster \u0026gt; Environment \u0026gt; Servers.\nTo view the running servers using kubectl, run the following command:\n$ kubectl get pods -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get pods -n accessns The output should look similar to the following:\nNAME READY STATUS RESTARTS AGE accessinfra-adminserver 1/1 Running 0 18h accessinfra-create-oam-infra-domain-job-vj69h 0/1 Completed 0 23h accessinfra-oam-policy-mgr1 1/1 Running 0 18h accessinfra-oam-server1 1/1 Running 0 18h accessinfra-oam-server2 1/1 Running 0 18h helper 1/1 Running 0 40h voyager-accessinfra-voyager-698764d6d-w8pbt 1/1 Running 0 8m47s bash-4.2$ Starting/Scaling up OAM Managed Servers The number of OAM Managed Servers running is dependent on the replicas parameter configured for the cluster. To start more OAM Managed Servers perform the following steps:\n  Run the following kubectl command to edit the domain:\n$ kubectl edit domain \u0026lt;domain_uid\u0026gt; -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl edit domain accessinfra -n accessns Note: This opens an edit session for the domain where parameters can be changed using standard vi commands.\n  In the edit session search for \u0026ldquo;clusterName: oam_cluster\u0026rdquo; and look for the replicas parameter. By default the replicas parameter is set to \u0026ldquo;2\u0026rdquo; hence two OAM Managed Servers are started (oam_server1 and oam_server2):\n clusters: - clusterName: oam_cluster replicas: 2 serverPod: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - podAffinityTerm: labelSelector: matchExpressions: - key: weblogic.clusterName operator: In values: - $(CLUSTER_NAME)   To start more OAM Managed Servers, increase the replicas value as desired. In the example below, two more managed servers will be started by setting replicas to \u0026ldquo;4\u0026rdquo;:\n clusters: - clusterName: oam_cluster replicas: 4 serverPod: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - podAffinityTerm: labelSelector: matchExpressions: - key: weblogic.clusterName operator: In values: - $(CLUSTER_NAME)   Save the file and exit (:wq!)\nThe output will look similar to the following:\ndomain.weblogic.oracle/accessinfra edited   Run the following kubectl command to view the pods:\n$ kubectl get pods -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get pods -n accessns The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE accessinfra-adminserver 1/1 Running 0 18h accessinfra-create-oam-infra-domain-job-vj69h 0/1 Completed 0 23h accessinfra-oam-policy-mgr1 1/1 Running 0 18h accessinfra-oam-server1 1/1 Running 0 18h accessinfra-oam-server2 1/1 Running 0 18h accessinfra-oam-server3 0/1 Running 0 6s accessinfra-oam-server4 0/1 Running 0 6s helper 1/1 Running 0 40h voyager-accessinfra-voyager-698764d6d-w8pbt 1/1 Running 0 10m Two new pods (accessinfra-oam-server3 and accessinfra-oam-server4) are started, but currently have a READY status of 0/1. This means oam_server3 and oam_server4 are not currently running but are in the process of starting. The servers will take several minutes to start so keep executing the command until READY shows 1/1:\nNAME READY STATUS RESTARTS AGE accessinfra-adminserver 1/1 Running 0 18h accessinfra-create-oam-infra-domain-job-vj69h 0/1 Completed 0 24h accessinfra-oam-policy-mgr1 1/1 Running 0 18h accessinfra-oam-server1 1/1 Running 0 18h accessinfra-oam-server2 1/1 Running 0 18h accessinfra-oam-server3 1/1 Running 0 5m5s accessinfra-oam-server4 1/1 Running 0 5m5s helper 1/1 Running 0 41h voyager-accessinfra-voyager-698764d6d-w8pbt 1/1 Running 0 15m Note: To check what is happening during server startup when READY is 0/1, run the following command to view the log of the pod that is starting:\n$ kubectl logs \u0026lt;pod\u0026gt; -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl logs accessinfra-oam-server3 -n accessns   To start more OAM Policy Manager servers, repeat the previous commands but change the replicas parameter for the policy_cluster. In the example below replicas has been increased to \u0026ldquo;2\u0026rdquo;:\n- clusterName: policy_cluster replicas: 2 serverPod: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - podAffinityTerm: labelSelector: matchExpressions: - key: weblogic.clusterName operator: In values: - $(CLUSTER_NAME) After saving the changes a new pod will be started. After a few minutes it will have a READY status of 1/1. In the example below accessinfra-oam-policy-mgr2 is started:\nNAME READY STATUS RESTARTS AGE accessinfra-adminserver 1/1 Running 0 18h accessinfra-create-oam-infra-domain-job-vj69h 0/1 Completed 0 24h accessinfra-oam-policy-mgr1 1/1 Running 0 18h accessinfra-oam-policy-mgr2 1/1 Running 0 4m3s accessinfra-oam-server1 1/1 Running 0 18h accessinfra-oam-server2 1/1 Running 0 18h accessinfra-oam-server3 1/1 Running 0 10m accessinfra-oam-server4 1/1 Running 0 10m helper 1/1 Running 0 41h voyager-accessinfra-voyager-698764d6d-w8pbt 1/1 Running 0 21m   Stopping/Scaling down OAM Managed Servers As mentioned in the previous section, the number of OAM Managed Servers running is dependent on the replicas parameter configured for the cluster. To stop one or more OAM Managed Servers, perform the following:\n  Run the following kubectl command to edit the domain:\n$ kubectl edit domain \u0026lt;domain_uid\u0026gt; -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl edit domain accessinfra -n accessns   In the edit session search for \u0026ldquo;clusterName: oam_cluster\u0026rdquo; and look for the replicas parameter. In the example below replicas is set to \u0026ldquo;4\u0026rdquo;, hence four OAM Managed Servers are started (oam_server1 - oam_server4):\nclusters: - clusterName: oam_cluster replicas: 4 serverPod: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - podAffinityTerm: labelSelector: matchExpressions: - key: weblogic.clusterName operator: In values: - $(CLUSTER_NAME)   To stop OAM Managed Servers, decrease the replicas value as desired. In the example below, we will stop two managed servers by setting replicas to \u0026ldquo;2\u0026rdquo;:\n clusters: - clusterName: oam_cluster replicas: 2 serverPod: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - podAffinityTerm: labelSelector: matchExpressions: - key: weblogic.clusterName operator: In values: - $(CLUSTER_NAME)   Save the file and exit (:wq!)\n  Run the following kubectl command to view the pods:\n$ kubectl get pods -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get pods -n accessns The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE accessinfra-adminserver 1/1 Running 0 18h accessinfra-create-oam-infra-domain-job-vj69h 0/1 Completed 0 24h accessinfra-oam-policy-mgr1 1/1 Running 0 18h accessinfra-oam-policy-mgr2 1/1 Running 0 5m21s accessinfra-oam-server1 1/1 Running 0 18h accessinfra-oam-server2 1/1 Running 0 18h accessinfra-oam-server3 1/1 Terminating 0 12m accessinfra-oam-server4 1/1 Terminating 0 12m helper 1/1 Running 0 41h voyager-accessinfra-voyager-698764d6d-w8pbt 1/1 Running 0 23m Two pods now have a STATUS of Terminating (accessinfra-oam-server3 and accessinfra-oam-server4). The servers will take a minute or two to stop, so keep executing the command until the pods have disappeared:\nNAME READY STATUS RESTARTS AGE accessinfra-adminserver 1/1 Running 0 18h accessinfra-create-oam-infra-domain-job-vj69h 0/1 Completed 0 24h accessinfra-oam-policy-mgr1 1/1 Running 0 18h accessinfra-oam-policy-mgr2 1/1 Running 0 6m3s accessinfra-oam-server1 1/1 Running 0 18h accessinfra-oam-server2 1/1 Running 0 18h helper 1/1 Running 0 41h voyager-accessinfra-voyager-698764d6d-w8pbt 1/1 Running 0 23m   To stop OAM Policy Manager servers, repeat the previous commands but change the replicas parameter for the policy_cluster. In the example below replicas has been decreased from \u0026ldquo;2\u0026rdquo; to \u0026ldquo;1\u0026rdquo;:\n- clusterName: policy_cluster replicas: 1 serverPod: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - podAffinityTerm: labelSelector: matchExpressions: - key: weblogic.clusterName operator: In values: - $(CLUSTER_NAME) After saving the changes one pod will move to a STATUS of Terminating (accessinfra-oam-policy-mgr2).\nNAME READY STATUS RESTARTS AGE accessinfra-adminserver 1/1 Running 0 18h accessinfra-create-oam-infra-domain-job-vj69h 0/1 Completed 0 24h accessinfra-oam-policy-mgr1 1/1 Running 0 18h accessinfra-oam-policy-mgr2 1/1 Terminating 0 7m12s accessinfra-oam-server1 1/1 Running 0 18h accessinfra-oam-server2 1/1 Running 0 18h helper 1/1 Running 0 41h voyager-accessinfra-voyager-698764d6d-w8pbt 1/1 Running 0 24m The server will take a minute or two to stop, so keep executing the command until the pod has disappeared:\nNAME READY STATUS RESTARTS AGE accessinfra-adminserver 1/1 Running 0 18h accessinfra-create-oam-infra-domain-job-vj69h 0/1 Completed 0 24h accessinfra-oam-policy-mgr1 1/1 Running 0 18h accessinfra-oam-server1 1/1 Running 0 18h accessinfra-oam-server2 1/1 Running 0 18h helper 1/1 Running 0 41h voyager-accessinfra-voyager-698764d6d-w8pbt 1/1 Running 0 25m   Stopping and Starting the AdminServer and Managed Servers To stop all the OAM Managed Servers and the AdminServer in one operation:\n  Run the following kubectl command to edit the domain:\n$ kubectl edit domain \u0026lt;domain_uid\u0026gt; -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl edit domain accessinfra -n accessns   In the edit session search for serverStartPolicy: IF_NEEDED:\n volumes: - name: weblogic-domain-storage-volume persistentVolumeClaim: claimName: accessinfra-domain-pvc serverStartPolicy: IF_NEEDED webLogicCredentialsSecret: name: accessinfra-domain-credentials   Change serverStartPolicy: IF_NEEDED to NEVER as follows:\n volumes: - name: weblogic-domain-storage-volume persistentVolumeClaim: claimName: accessinfra-domain-pvc serverStartPolicy: NEVER webLogicCredentialsSecret: name: accessinfra-domain-credentials   Save the file and exit (:wq!).\n  Run the following kubectl command to view the pods:\n$ kubectl get pods -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get pods -n accessns The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE accessinfra-adminserver 1/1 Terminating 0 18h accessinfra-create-oam-infra-domain-job-vj69h 0/1 Completed 0 24h accessinfra-oam-policy-mgr1 1/1 Terminating 0 18h accessinfra-oam-server1 1/1 Terminating 0 18h accessinfra-oam-server2 1/1 Terminating 0 18h helper 1/1 Running 0 41h voyager-accessinfra-voyager-698764d6d-w8pbt 1/1 Running 0 27m The AdminServer pods and Managed Server pods will move to a STATUS of Terminating. After a few minutes, run the command again and the pods should have disappeared:\nNAME READY STATUS RESTARTS AGE accessinfra-create-oam-infra-domain-job-vj69h 0/1 Completed 0 24h helper 1/1 Running 0 41h voyager-accessinfra-voyager-698764d6d-w8pbt 1/1 Running 0 28m   To start the AdminServer and Managed Servers up again, repeat the previous steps but change serverStartPolicy: NEVER to IF_NEEDED as follows:\n volumes: - name: weblogic-domain-storage-volume persistentVolumeClaim: claimName: accessinfra-domain-pvc serverStartPolicy: IF_NEEDED webLogicCredentialsSecret: name: accessinfra-domain-credentials   Run the following kubectl command to view the pods:\n$ kubectl get pods -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get pods -n accessns The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE accessinfra-create-oam-infra-domain-job-vj69h 0/1 Completed 0 24h accessinfra-introspect-domain-job-7qx29 1/1 Running 0 8s helper 1/1 Running 0 41h voyager-accessinfra-voyager-698764d6d-w8pbt 1/1 Running 0 29m The AdminServer pod will start followed by the OAM Managed Servers pods. This process will take several minutes, so keep executing the command until all the pods are running with READY status 1/1 :\nNAME READY STATUS RESTARTS AGE accessinfra-adminserver 1/1 Running 0 6m4s accessinfra-create-oam-infra-domain-job-vj69h 0/1 Completed 0 24h accessinfra-oam-policy-mgr1 1/1 Running 0 3m5s accessinfra-oam-server1 1/1 Running 0 3m5s accessinfra-oam-server2 1/1 Running 0 3m5s helper 1/1 Running 0 41h voyager-accessinfra-voyager-698764d6d-w8pbt 1/1 Running 0 36m   "
},
{
	"uri": "/fmw-kubernetes/20.4.1/oig/manage-oig-domains/domain-lifecycle/",
	"title": "Domain Life Cycle",
	"tags": [],
	"description": "Learn about the domain life cyle of an OIG domain.",
	"content": " View Existing OIG Servers Starting/Scaling up OIG Managed Servers Stopping/Scaling down OIG Managed Servers Stopping and Starting the AdminServer and Managed Servers  As OIG domains use the Oracle WebLogic Kubernetes Operator, domain lifecyle operations are managed using the Oracle WebLogic Kubernetes Operator itself.\nThis document shows the basic operations for starting, stopping and scaling servers in the OIG domain.\nFor more detailed information refer to Domain Life Cycle in the Oracle WebLogic Kubernetes Operator documentation.\nDo not use the WebLogic Server Administration Console or Oracle Enterprise Manager Console to start or stop servers.\n View Existing OIG Servers The default OIG deployment starts the AdminServer (AdminServer), one OIG Managed Server (oim_server1) and one SOA Managed Server (soa_server1).\nThe deployment also creates, but doesn\u0026rsquo;t start, four extra OIG Managed Servers (oim-server2 to oim-server5) and four more SOA Managed Servers (soa_server2 to soa_server5).\nAll these servers are visible in the WebLogic Server Administration Console https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/console by navigating to Domain Structure \u0026gt; oimcluster \u0026gt; Environment \u0026gt; Servers.\nTo view the running servers using kubectl, run the following command:\n$ kubectl get pods -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get pods -n oimcluster The output should look similar to the following:\n$ kubectl get pods -n oimcluster oimcluster-adminserver 1/1 Running 0 23h oimcluster-create-fmw-infra-sample-domain-job-dktkk 0/1 Completed 0 24h oimcluster-oim-server1 1/1 Running 0 23h oimcluster-soa-server1 1/1 Running 0 23h Starting/Scaling up OIG Managed Servers The number of OIG Managed Servers running is dependent on the replicas parameter configured for the cluster. To start more OIG Managed Servers perform the following steps:\n  Run the following kubectl command to edit the domain:\n$ kubectl edit domain \u0026lt;domain_uid\u0026gt; -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl edit domain oimcluster -n oimcluster Note: This opens an edit session for the domain where parameters can be changed using standard vi commands.\n  In the edit session search for \u0026ldquo;clusterName: oim_cluster\u0026rdquo; and look for the replicas parameter. By default the replicas parameter is set to \u0026ldquo;1\u0026rdquo; hence a single OIG Managed Server is started (oim_server1):\n - clusterName: oim_cluster clusterService: annotations: traefik.ingress.kubernetes.io/affinity: \u0026quot;true\u0026quot; replicas: 1 serverPod: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - podAffinityTerm: labelSelector: matchExpressions: - key: weblogic.clusterName operator: In values: - $(CLUSTER_NAME)   To start more OIG Managed Servers, increase the replicas value as desired. In the example below, one more Managed Server will be started by setting replicas to \u0026ldquo;2\u0026rdquo;:\n - clusterName: oim_cluster clusterService: annotations: traefik.ingress.kubernetes.io/affinity: \u0026quot;true\u0026quot; replicas: 2 serverPod: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - podAffinityTerm: labelSelector: matchExpressions: - key: weblogic.clusterName operator: In values: - $(CLUSTER_NAME)   Save the file and exit (:wq)\nThe output will look similar to the following:\ndomain.weblogic.oracle/oimcluster edited   Run the following kubectl command to view the pods:\n$ kubectl get pods -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get pods -n oimcluster The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE oimcluster-adminserver 1/1 Running 0 23h oimcluster-create-fmw-infra-sample-domain-job-dktkk 0/1 Completed 0 24h oimcluster-oim-server1 1/1 Running 0 23h oimcluster-oim-server2 0/1 Running 0 7s oimcluster-soa-server1 1/1 Running 0 23h One new pod (oimcluster-oim-server2) is started, but currently has a READY status of 0/1. This means oim_server2 is not currently running but is in the process of starting. The server will take several minutes to start so keep executing the command until READY shows 1/1:\nNAME READY STATUS RESTARTS AGE oimcluster-adminserver 1/1 Running 0 23h oimcluster-create-fmw-infra-sample-domain-job-dktkk 0/1 Completed 0 24h oimcluster-oim-server1 1/1 Running 0 23h oimcluster-oim-server2 1/1 Running 0 5m27s oimcluster-soa-server1 1/1 Running 0 23h Note: To check what is happening during server startup when READY is 0/1, run the following command to view the log of the pod that is starting:\n$ kubectl logs \u0026lt;pod\u0026gt; -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl logs oimcluster-oim-server2 -n oimcluster   Stopping/Scaling down OIG Managed Servers As mentioned in the previous section, the number of OIG Managed Servers running is dependent on the replicas parameter configured for the cluster. To stop one or more OIG Managed Servers, perform the following:\n  Run the following kubectl command to edit the domain:\n$ kubectl edit domain \u0026lt;domain_uid\u0026gt; -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl edit domain oimcluster -n oimcluster   In the edit session search for \u0026ldquo;clusterName: oim_cluster\u0026rdquo; and look for the replicas parameter. In the example below replicas is set to \u0026ldquo;2\u0026rdquo; hence two OIG Managed Servers are started (oim_server1 and oim_server2):\n - clusterName: oim_cluster clusterService: annotations: traefik.ingress.kubernetes.io/affinity: \u0026quot;true\u0026quot; replicas: 2 serverPod: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - podAffinityTerm: labelSelector: matchExpressions: - key: weblogic.clusterName operator: In values: - $(CLUSTER_NAME)   To stop OIG Managed Servers, decrease the replicas value as desired. In the example below, we will stop one Managed Server by setting replicas to \u0026ldquo;1\u0026rdquo;:\n - clusterName: oim_cluster clusterService: annotations: traefik.ingress.kubernetes.io/affinity: \u0026quot;true\u0026quot; replicas: 1 serverPod: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - podAffinityTerm: labelSelector: matchExpressions: - key: weblogic.clusterName operator: In values: - $(CLUSTER_NAME)   Save the file and exit (:wq)\n  Run the following kubectl command to view the pods:\n$ kubectl get pods -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get pods -n oimcluster The output will look similar to the following:\n$ kubectl get pods -n oimcluster NAME READY STATUS RESTARTS AGE oimcluster-adminserver 1/1 Running 0 23h oimcluster-create-fmw-infra-sample-domain-job-dktkk 0/1 Completed 0 24h oimcluster-oim-server1 1/1 Running 0 23h oimcluster-oim-server2 1/1 Terminating 0 7m30s oimcluster-soa-server1 1/1 Running 0 23h The exiting pod shows a STATUS of Terminating (oimcluster-oim-server2). The server may take a minute or two to stop, so keep executing the command until the pod has disappeared:\n$ kubectl get pods -n oimcluster NAME READY STATUS RESTARTS AGE oimcluster-adminserver 1/1 Running 0 23h oimcluster-create-fmw-infra-sample-domain-job-dktkk 0/1 Completed 0 24h oimcluster-oim-server1 1/1 Running 0 23h oimcluster-soa-server1 1/1 Running 0 23h   Stopping and Starting the AdminServer and Managed Servers To stop all the OIG Managed Servers and the AdminServer in one operation:\n  Run the following kubectl command to edit the domain:\n$ kubectl edit domain \u0026lt;domain_uid\u0026gt; -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl edit domain oimcluster -n oimcluster   In the edit session search for serverStartPolicy: IF_NEEDED:\n volumeMounts: - mountPath: /u01/oracle/user_projects/domains name: weblogic-domain-storage-volume volumes: - name: weblogic-domain-storage-volume persistentVolumeClaim: claimName: oimcluster-oim-pvc serverStartPolicy: IF_NEEDED   Change serverStartPolicy: IF_NEEDED to NEVER as follows:\n volumeMounts: - mountPath: /u01/oracle/user_projects/domains name: weblogic-domain-storage-volume volumes: - name: weblogic-domain-storage-volume persistentVolumeClaim: claimName: oimcluster-oim-pvc serverStartPolicy: NEVER   Save the file and exit (:wq).\n  Run the following kubectl command to view the pods:\n$ kubectl get pods -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get pods -n oimcluster The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE oimcluster-adminserver 1/1 Terminating 0 23h oimcluster-create-fmw-infra-sample-domain-job-dktkk 0/1 Completed 0 24h oimcluster-oim-server1 1/1 Terminating 0 23h oimcluster-soa-server1 1/1 Terminating 0 23h The AdminServer pod and Managed Server pods will move to a STATUS of Terminating. After a few minutes, run the command again and the pods should have disappeared:\nNAME READY STATUS RESTARTS AGE oimcluster-create-fmw-infra-sample-domain-job-dktkk 0/1 Completed 0 24h $   To start the AdminServer and Managed Servers up again, repeat the previous steps but change serverStartPolicy: NEVER to IF_NEEDED as follows:\n volumeMounts: - mountPath: /u01/oracle/user_projects/domains name: weblogic-domain-storage-volume volumes: - name: weblogic-domain-storage-volume persistentVolumeClaim: claimName: oimcluster-oim-pvc serverStartPolicy: IF_NEEDED   Run the following kubectl command to view the pods:\n$ kubectl get pods -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get pods -n oimcluster The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE oimcluster-adminserver 0/1 Running 0 22s oimcluster-create-fmw-infra-sample-domain-job-dktkk 0/1 Completed 0 24h The AdminServer pod will start followed by the OIG Managed Servers pods. This process will take several minutes, so keep executing the command until all the pods are running with READY status 1/1 :\nNAME READY STATUS RESTARTS AGE oimcluster-adminserver 1/1 Running 0 6m57s oimcluster-create-fmw-infra-sample-domain-job-dktkk 0/1 Completed 0 24h oimcluster-oim-server1 1/1 Running 0 4m33s oimcluster-soa-server1 1/1 Running 0 4m33s   "
},
{
	"uri": "/fmw-kubernetes/20.4.1/oam/",
	"title": "Oracle Access Management",
	"tags": [],
	"description": "The Oracle WebLogic Server Kubernetes Operator supports deployment of Oracle Access Management (OAM). Follow the instructions in this guide to set up these Oracle Access Management domains on Kubernetes.",
	"content": "The Oracle WebLogic Server Kubernetes Operator supports deployment of Oracle Access Management (OAM).\nIn this release, OAM domains are supported using the “domain on a persistent volume” model only, where the domain home is located in a persistent volume (PV).\nThe Oracle WebLogic Server Kubernetes Operator has several key features to assist you with deploying and managing Oracle Access Management domains in a Kubernetes environment. You can:\n Create OAM instances in a Kubernetes persistent volume. This persistent volume can reside in an NFS file system or other Kubernetes volume types. Start servers based on declarative startup parameters and desired states. Expose the OAM Services through external access. Scale OAM domains by starting and stopping Managed Servers on demand. Publish operator and WebLogic Server logs into Elasticsearch and interact with them in Kibana. Monitor the OAM instance using Prometheus and Grafana.  Limitations See here for limitations in this release.\nGetting started For detailed information about deploying Oracle Access Management domains, start at Prerequisites and follow this documentation sequentially.\nCurrent release The current supported release of the Oracle WebLogic Server Kubernetes Operator, for Oracle Access Management domains deployment is 3.0.1.\n"
},
{
	"uri": "/fmw-kubernetes/20.4.1/oam/prerequisites/",
	"title": "Prerequisites",
	"tags": [],
	"description": "System requirements and limitations for deploying and running an OAM domain home",
	"content": "Introduction This document provides information about the system requirements and limitations for deploying and running OAM domains with the Oracle WebLogic Server Kubernetes Operator 3.0.1.\nIn this release, OAM domains are supported using the “domain on a persistent volume” model only, where the domain home is located in a persistent volume (PV).\nSystem requirements for oam domains  Kubernetes 1.16.0+, 1.17.0+, and 1.18.0+ (check with kubectl version). Flannel networking v0.9.1-amd64 or later (check with docker images | grep flannel). Docker 18.09.1+ or 19.03.1+ (check with docker version) Helm 3.1.3+ (check with helm version). You must have the cluster-admin role to install the operator. We do not currently support running OAM in non-Linux containers. A running Oracle Database 12.2.0.1 or later. The database must be a supported version for OAM as outlined in Oracle Fusion Middleware 12c certifications. It must meet the requirements as outlined in About Database Requirements for an Oracle Fusion Middleware Installation and in RCU Requirements for Oracle Databases.  Limitations Compared to running a WebLogic Server domain in Kubernetes using the operator, the following limitations currently exist for OAM domains:\n The \u0026ldquo;domain in image\u0026rdquo; model is not supported. Only configured clusters are supported. Dynamic clusters are not supported for OAM domains. Note that you can still use all of the scaling features, you just need to define the maximum size of your cluster at domain creation time. Deploying and running OAM domains is supported only with Oracle WebLogic Server Kubernetes Operator version 3.0.1 The WebLogic Monitoring Exporter currently supports the WebLogic MBean trees only. Support for JRF MBeans has not been added yet.  "
},
{
	"uri": "/fmw-kubernetes/20.4.1/oig/prerequisites/",
	"title": "Prerequisites",
	"tags": [],
	"description": "Sample for creating an OIG Suite domain home on an existing PV or PVC, and the domain resource YAML file for deploying the generated OIG domain.",
	"content": "Introduction This document provides information about the system requirements and limitations for deploying and running OIG domains with the Oracle WebLogic Kubernetes Operator 3.0.1.\nIn this release, OIG domains are supported using the “domain on a persistent volume” model only, where the domain home is located in a persistent volume (PV).\nSystem requirements for OIG domains  Kubernetes 1.14.8+, 1.15.7+, 1.16.0+, 1.17.0+, and 1.18.0+ (check with kubectl version). Flannel networking v0.9.1-amd64 or later (check with docker images | grep flannel). Docker 18.9.1 or 19.03.1 (check with docker version). Helm 3.1.3+ (check with helm version). You must have the cluster-admin role to install the operator. We do not currently support running OIG in non-Linux containers. A running Oracle Database 12.2.0.1 or later. The database must be a supported version for OIG as outlined in Oracle Fusion Middleware 12c certifications, and must meet the requirements as outlined in About Database Requirements for an Oracle Fusion Middleware Installation. Java Developer Kit (11.0.3 or later recommended)  Limitations Compared to running a WebLogic Server domain in Kubernetes using the operator, the following limitations currently exist for OIG domains:\n The \u0026ldquo;domain in image\u0026rdquo; model is not supported. Only configured clusters are supported. Dynamic clusters are not supported for OIG domains. Note that you can still use all of the scaling features, you just need to define the maximum size of your cluster at domain creation time. Deploying and running OIG domains is supported only with Oracle WebLogic Kubernetes Operator version 3.0.1 currently supports the WebLogic MBean trees only. The WebLogic Monitoring Exporter currently supports the WebLogic MBean trees only. Support for JRF MBeans has not been added yet.  "
},
{
	"uri": "/fmw-kubernetes/20.4.1/oud/prepare-your-environment/",
	"title": "Prepare Your Environment",
	"tags": [],
	"description": "Prepare your environment",
	"content": " Set up your Kubernetes Cluster Check the Kubernetes Cluster is Ready Install the Oracle Unified Directory Image Setup the Code Repository To Deploy Oracle Unified Directory  Set up your Kubernetes Cluster If you need help setting up a Kubernetes environment, check our cheat sheet.\nIt is recommended you have a master node and one or more worker nodes. The examples in this documentation assume one master and two worker nodes.\nAfter creating Kubernetes clusters, you can optionally:\n Configure an Ingress to direct traffic to backend instances.  Check the Kubernetes Cluster is Ready  Run the following command on the master node to check the cluster and worker nodes are running:  $ kubectl get nodes,pods -n kube-system The output will look similar to the following:\nNAME STATUS ROLES AGE VERSION node/10.89.73.203 Ready \u0026lt;none\u0026gt; 66d v1.18.4 node/10.89.73.204 Ready \u0026lt;none\u0026gt; 66d v1.18.4 node/10.89.73.42 Ready master 67d v1.18.4 NAME READY STATUS RESTARTS AGE pod/coredns-66bff467f8-slxdq 1/1 Running 1 67d pod/coredns-66bff467f8-v77qt 1/1 Running 1 67d pod/etcd-10.89.73.42 1/1 Running 1 67d pod/kube-apiserver-10.89.73.42 1/1 Running 1 67d pod/kube-controller-manager-10.89.73.42 1/1 Running 27 67d pod/kube-flannel-ds-amd64-r2m8r 1/1 Running 2 48d pod/kube-flannel-ds-amd64-rdhrf 1/1 Running 2 6d1h pod/kube-flannel-ds-amd64-vpcbj 1/1 Running 3 66d pod/kube-proxy-jtcxm 1/1 Running 1 67d pod/kube-proxy-swfmm 1/1 Running 1 66d pod/kube-proxy-w6x6t 1/1 Running 1 66d pod/kube-scheduler-10.89.73.42 1/1 Running 29 67d Install the Oracle Unified Directory Image You can deploy Oracle Unified Directory images in the following ways:\n Download a pre-built Oracle Unified Directory image from My Oracle Support. by referring to the document ID 2723908.1. This image is prebuilt by Oracle and includes Oracle Unified Directory 12.2.1.4.0 and the latest PSU. Build your own Oracle Unified Directory container image either by using the WebLogic Image Tool or by using the dockerfile, scripts and base image from Oracle Container Registry (OCR). You can also build your own image by using only the dockerfile and scripts. For more information about the various ways in which you can build your own container image, see Installing the Oracle Unified Directory Image.  Choose one of these options based on your requirements.\nThe Oracle Unified Directory image must be installed on the master node and each of the worker nodes in your Kubernetes cluster. Alternatively you can place the image in a docker registry that your cluster can access.\n After installing the Oracle Unified Directory image run the following command to make sure the image is installed correctly on the master and worker nodes:\n$ docker images The output will look similar to the following:\nREPOSITORY TAG IMAGE ID CREATED SIZE oracle/oud 12.2.1.4.0 8a937042bef3 3 weeks ago 992MB k8s.gcr.io/kube-proxy v1.18.4 718fa77019f2 3 months ago 117MB k8s.gcr.io/kube-scheduler v1.18.4 c663567f869e 3 months ago 95.3MB k8s.gcr.io/kube-controller-manager v1.18.4 e8f1690127c4 3 months ago 162MB k8s.gcr.io/kube-apiserver v1.18.4 408913fc18eb 3 months ago 173MB quay.io/coreos/flannel v0.12.0-amd64 4e9f801d2217 6 months ago 52.8MB k8s.gcr.io/pause 3.2 80d28bedfe5d 7 months ago 683kB k8s.gcr.io/coredns 1.6.7 67da37a9a360 8 months ago 43.8MB k8s.gcr.io/etcd 3.4.3-0 303ce5db0e90 11 months ago 288MB quay.io/prometheus/node-exporter v0.18.1 e5a616e4b9cf 16 months ago 22.9MB quay.io/coreos/kube-rbac-proxy v0.4.1 70eeaa7791f2 20 months ago 41.3MB ... Setup the Code Repository To Deploy Oracle Unified Directory Oracle Unified Directory deployment on Kubernetes leverages deployment scripts provided by Oracle for creating Oracle Unified Directory containers using samples or Helm charts provided. To deploy Oracle Unified Directory on Kubernetes you should set up the deployment scripts on the master node as below:\nCreate a working directory to setup the source code.\n$ mkdir \u0026lt;work directory\u0026gt; For example:\n$ mkdir /scratch/OUDContainer From the directory you created, download the Oracle Unified Directory deployment scripts from the Oracle Unified Directory repository.\n$ git clone https://github.com/oracle/fmw-kubernetes.git You can now use the deployment scripts from \u0026lt;work directory\u0026gt;/fmw-kubernetes/OracleUnifiedDirectory/kubernetes/samples/ to set up the Oracle Unified Directory environments as further described in this document.\n"
},
{
	"uri": "/fmw-kubernetes/20.4.1/oudsm/prepare-your-environment/",
	"title": "Prepare Your Environment",
	"tags": [],
	"description": "Prepare your environment",
	"content": " Set up your Kubernetes Cluster Check the Kubernetes Cluster is Ready Install the Oracle Unified Directory Services Manager Image Setup the Code Repository To Deploy Oracle Unified Directory Services Manager  Set up your Kubernetes Cluster If you need help setting up a Kubernetes environment, check our cheat sheet.\nIt is recommended you have a master node and one or more worker nodes. The examples in this documentation assume one master and two worker nodes.\nAfter creating Kubernetes clusters, you can optionally:\n Configure an Ingress to direct traffic to backend instances.  Check the Kubernetes Cluster is Ready  Run the following command on the master node to check the cluster and worker nodes are running:  $ kubectl get nodes,pods -n kube-system The output will look similar to the following:\nNAME STATUS ROLES AGE VERSION node/10.89.73.203 Ready \u0026lt;none\u0026gt; 66d v1.18.4 node/10.89.73.204 Ready \u0026lt;none\u0026gt; 66d v1.18.4 node/10.89.73.42 Ready master 67d v1.18.4 NAME READY STATUS RESTARTS AGE pod/coredns-66bff467f8-slxdq 1/1 Running 1 67d pod/coredns-66bff467f8-v77qt 1/1 Running 1 67d pod/etcd-10.89.73.42 1/1 Running 1 67d pod/kube-apiserver-10.89.73.42 1/1 Running 1 67d pod/kube-controller-manager-10.89.73.42 1/1 Running 27 67d pod/kube-flannel-ds-amd64-r2m8r 1/1 Running 2 48d pod/kube-flannel-ds-amd64-rdhrf 1/1 Running 2 6d1h pod/kube-flannel-ds-amd64-vpcbj 1/1 Running 3 66d pod/kube-proxy-jtcxm 1/1 Running 1 67d pod/kube-proxy-swfmm 1/1 Running 1 66d pod/kube-proxy-w6x6t 1/1 Running 1 66d pod/kube-scheduler-10.89.73.42 1/1 Running 29 67d Install the Oracle Unified Directory Services Manager Image You can deploy Oracle Unified Directory Services Manager images in the following ways:\n Download a pre-built Oracle Unified Directory Services Manager image from My Oracle Support. by referring to the document ID 2723908.1. This image is prebuilt by Oracle and includes Oracle Unified Directory 12.2.1.4.0 and the latest PSU. Build your own Oracle Unified Directory Services Manager container image either by using the WebLogic Image Tool or by using the dockerfile, scripts and base image from Oracle Container Registry (OCR). You can also build your own image by using only the dockerfile and scripts. For more information about the various ways in which you can build your own container image, see Installing the Oracle Unified Directory Services Manager Image.  Choose one of these options based on your requirements.\nThe Oracle Unified Directory Services Manager image must be installed on the master node and each of the worker nodes in your Kubernetes cluster. Alternatively you can place the image in a docker registry that your cluster can access.\n After installing the Oracle Unified Directory Services Manager image run the following command to make sure the image is installed correctly on the master and worker nodes:\n$ docker images The output will look similar to the following:\nREPOSITORY TAG IMAGE ID CREATED SIZE oracle/oudsm 12.2.1.4.0 7157885054a2 2 weeks ago 2.74GB k8s.gcr.io/kube-proxy v1.18.4 718fa77019f2 3 months ago 117MB k8s.gcr.io/kube-scheduler v1.18.4 c663567f869e 3 months ago 95.3MB k8s.gcr.io/kube-controller-manager v1.18.4 e8f1690127c4 3 months ago 162MB k8s.gcr.io/kube-apiserver v1.18.4 408913fc18eb 3 months ago 173MB quay.io/coreos/flannel v0.12.0-amd64 4e9f801d2217 6 months ago 52.8MB k8s.gcr.io/pause 3.2 80d28bedfe5d 7 months ago 683kB k8s.gcr.io/coredns 1.6.7 67da37a9a360 8 months ago 43.8MB k8s.gcr.io/etcd 3.4.3-0 303ce5db0e90 11 months ago 288MB quay.io/prometheus/node-exporter v0.18.1 e5a616e4b9cf 16 months ago 22.9MB quay.io/coreos/kube-rbac-proxy v0.4.1 70eeaa7791f2 20 months ago 41.3MB ... Setup the Code Repository To Deploy Oracle Unified Directory Services Manager Oracle Unified Directory Services Manager deployment on Kubernetes leverages deployment scripts provided by Oracle for creating Oracle Unified Directory Services Manager containers using samples or Helm charts provided. To deploy Oracle Unified Directory Services Manager on Kubernetes you should set up the deployment scripts on the master node as below:\nCreate a working directory to setup the source code.\n$ mkdir \u0026lt;work directory\u0026gt; For example:\n$ mkdir /scratch/OUDSMContainer From the directory you created, download the Oracle Unified Directory Services Manager deployment scripts from the Oracle Unified Directory Services Manager repository.\n$ git clone https://github.com/oracle/fmw-kubernetes.git You can now use the deployment scripts from \u0026lt;work directory\u0026gt;/fmw-kubernetes/OracleUnifiedDirectorySM/kubernetes/samples/scripts/ to set up the Oracle Unified Directory Services Manager environments as further described in this document.\n"
},
{
	"uri": "/fmw-kubernetes/20.4.1/oig/",
	"title": "Oracle Identity Governance",
	"tags": [],
	"description": "The Oracle WebLogic Kubernetes Operator supports deployment of Oracle Identity Governance. Follow the instructions in this guide to set up Oracle Identity Governance domains on Kubernetes.",
	"content": "The Oracle WebLogic Kubernetes Operator supports deployment of Oracle Identity Governance (OIG).\nIn this release, OIG domains are supported using the “domain on a persistent volume” model only, where the domain home is located in a persistent volume (PV).\nThe operator has several key features to assist you with deploying and managing OIG domains in a Kubernetes environment. You can:\n Create OIG instances in a Kubernetes persistent volume. This persistent volume can reside in an NFS file system or other Kubernetes volume types. Start servers based on declarative startup parameters and desired states. Expose the OIG Services for external access. Scale OIG domains by starting and stopping Managed Servers on demand. Publish operator and WebLogic Server logs into Elasticsearch and interact with them in Kibana. Monitor the OIG instance using Prometheus and Grafana.  Limitations See here for limitations in this release.\nGetting started For detailed information about deploying Oracle Identity Governance domains, start at Prerequisites and follow this documentation sequentially.\nCurrent release The current supported release of the Oracle WebLogic Kubernetes Operator, for Oracle Identity Governance domains deployment is 3.0.1.\n"
},
{
	"uri": "/fmw-kubernetes/20.4.1/oam/prepare-your-environment/",
	"title": "Prepare your environment",
	"tags": [],
	"description": "Sample for creating an OAM domain home on an existing PV or PVC, and the domain resource YAML file for deploying the generated OAM domain.",
	"content": " Set up your Kubernetes cluster Install Helm Check the Kubernetes cluster is ready Install the OAM Docker image Install the Oracle WebLogic Server Kubernetes Operator docker image Set up the code repository to deploy OAM domains Install the Oracle WebLogic Server Kubernetes Operator RCU schema creation Preparing the environment for domain creation  Configure the operator for the domain namespace Creating Kubernetes secrets for the domain and RCU Create a Kubernetes persistent volume and persistent volume claim    Set up your Kubernetes cluster If you need help setting up a Kubernetes environment, check our cheat sheet.\nIt is recommended you have a master node and one or more worker nodes. The examples in this documentation assume one master and two worker nodes.\nAfter creating Kubernetes clusters, you can optionally:\n Configure an Ingress to direct traffic to backend domains. Configure Kibana and Elasticsearch for your operator logs.  Install Helm As per the prerequisites an installation of Helm is required to create and deploy the necessary resources and then run the operator in a Kubernetes cluster. For Helm installation and usage information, refer to the README.\nCheck the Kubernetes cluster is ready   Run the following command on the master node to check the cluster and worker nodes are running:\n$ kubectl get nodes,pods -n kube-system The output will look similar to the following:\nNAME STATUS ROLES AGE VERSION node/worker-node1 Ready \u0026lt;none\u0026gt; 17h v1.18.4 node/worker-node2 Ready \u0026lt;none\u0026gt; 17h v1.18.4 node/master-node Ready master 23h v1.18.4 NAME READY STATUS RESTARTS AGE pod/coredns-66bff467f8-fnhbq 1/1 Running 0 23h pod/coredns-66bff467f8-xtc8k 1/1 Running 0 23h pod/etcd-master 1/1 Running 0 21h pod/kube-apiserver-master-node 1/1 Running 0 21h pod/kube-controller-manager-master-node 1/1 Running 0 21h pod/kube-flannel-ds-amd64-lxsfw 1/1 Running 0 17h pod/kube-flannel-ds-amd64-pqrqr 1/1 Running 0 17h pod/kube-flannel-ds-amd64-wj5nh 1/1 Running 0 17h pod/kube-proxy-2kxv2 1/1 Running 0 17h pod/kube-proxy-82vvj 1/1 Running 0 17h pod/kube-proxy-nrgw9 1/1 Running 0 23h pod/kube-scheduler-master 1/1 Running 0 21   Install the OAM Docker image You can deploy OAM Docker images in the following ways:\n  Download a prebuilt OAM Docker image from My Oracle Support by referring to the document ID 2723908.1. This image is prebuilt by Oracle and includes Oracle Access Management 12.2.1.4.0 and the latest PSU.\n  Build your own OAM image using the WebLogic Image Tool or by using the dockerfile, scripts and base images from Oracle Container Registry (OCR). You can also build your own image by using only the dockerfile and scripts. For more information about the various ways in which you can build your own container image, see Building the OAM Image.\n  Choose one of these options based on your requirements.\nIf building your own image for OAM, you must include the mandatory patch 30571576.\n The OAM Docker image must be installed on the master node and each of the worker nodes in your Kubernetes cluster. Alternatively you can place the image in a Docker registry that your cluster can access.\n After installing the OAM Docker image run the following command to make sure the image is installed correctly on the master and worker nodes:\n$ docker images The output will look similar to the following:\nREPOSITORY TAG IMAGE ID CREATED SIZE quay.io/coreos/flannel v0.13.0-rc2 79dd6d6368e2 7 days ago 57.2MB oracle/oam 12.2.1.4.0 720a172374e6 2 weeks ago 3.38GB k8s.gcr.io/kube-proxy v1.18.4 718fa77019f2 3 weeks ago 117MB k8s.gcr.io/kube-controller-manager v1.18.4 e8f1690127c4 3 weeks ago 162MB k8s.gcr.io/kube-apiserver v1.18.4 408913fc18eb 3 weeks ago 173MB k8s.gcr.io/kube-scheduler v1.18.4 c663567f869e 3 weeks ago 95.3MB k8s.gcr.io/pause 3.2 80d28bedfe5d 5 months ago 683kB k8s.gcr.io/coredns 1.6.7 67da37a9a360 5 months ago 43.8MB k8s.gcr.io/etcd 3.4.3-0 303ce5db0e90 8 months ago 288MB Install the Oracle WebLogic Server Kubernetes Operator Docker image In this release only Oracle WebLogic Server Kubernetes Operator 3.0.1 is supported.\nThe Oracle WebLogic Server Kubernetes Operator Docker image must be installed on the master node and each of the worker nodes in your Kubernetes cluster. Alternatively you can place the image in a Docker registry that your cluster can access.\n   Pull the Oracle WebLogic Server Kubernetes Operator 3.0.1 image by running the following command on the master node:\n$ docker pull ghcr.io/oracle/weblogic-kubernetes-operator:3.0.1 The output will look similar to the following:\nTrying to pull repository ghcr.io/oracle/weblogic-kubernetes-operator ... 3.0.1: Pulling from ghcr.io/oracle/weblogic-kubernetes-operator bce8f778fef0: Already exists de14ddc50a70: Pull complete 77401a861078: Pull complete 9c5ac1423af4: Pull complete 2b6f244f998f: Pull complete 625e05083092: Pull complete Digest: sha256:27047d032ac5a9077b39bec512b99d8ca54bf9bf71227f5fd1b7b26ac80c20d3 Status: Downloaded newer image for ghcr.io/oracle/weblogic-kubernetes-operator ghcr.io/oracle/weblogic-kubernetes-operator:3.0.1   Run the docker tag command as follows:\n$ docker tag ghcr.io/oracle/weblogic-kubernetes-operator:3.0.1 weblogic-kubernetes-operator:3.0.1 After installing the Oracle WebLogic Server Kubernetes Operator 3.0.1 Docker image, repeat the above on the worker nodes.\n  Set up the code repository to deploy OAM domains OAM domain deployment on Kubernetes leverages the Oracle WebLogic Server Kubernetes Operator infrastructure. For deploying the OAM domains, you need to set up the deployment scripts on the master node as below:\n  Create a working directory to setup the source code.\n$ mkdir \u0026lt;work directory\u0026gt; For example:\n$ mkdir /scratch/OAMDockerK8S   Download the supported version of the WebLogic Kubernetes operator source code from the operator github project. Currently the supported operator version is 3.0.1:\n$ cd \u0026lt;work directory\u0026gt; $ git clone https://github.com/oracle/weblogic-kubernetes-operator.git --branch release/3.0.1 For example:\n$ cd /scratch/OAMDockerK8S $ git clone https://github.com/oracle/weblogic-kubernetes-operator.git --branch release/3.0.1 This will create the directory \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator\n  Download the OAM deployment scripts from the OAM repository and copy them in to the Oracle WebLogic Server Kubernetes Operator samples location.\n$ git clone https://github.com/oracle/fmw-kubernetes.git $ cp -rf \u0026lt;work directory\u0026gt;/fmw-kubernetes/OracleAccessManagement/kubernetes/3.0.1/create-access-domain \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/scripts/ $ mv -f \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain_backup $ cp -rf \u0026lt;work directory\u0026gt;/fmw-kubernetes/OracleAccessManagement/kubernetes/3.0.1/ingress-per-domain \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain For example:\n$ git clone https://github.com/oracle/fmw-kubernetes.git $ cp -rf /scratch/OAMDockerK8S/fmw-kubernetes/OracleAccessManagement/kubernetes/3.0.1/create-access-domain /scratch/OAMDockerK8S/weblogic-kubernetes-operator/kubernetes/samples/scripts/ $ mv -f /scratch/OAMDockerK8S/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain /scratch/OAMDockerK8S/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain_backup $ cp -rf /scratch/OAMDockerK8S/fmw-kubernetes/OracleAccessManagement/kubernetes/3.0.1/ingress-per-domain /scratch/OAMDockerK8S/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain You can now use the deployment scripts from \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/scripts/ to set up the OAM domains as further described in this document.\n  Run the following command and see if the WebLogic custom resource definition name already exists:\n$ kubectl get crd In the output you should see:\nNo resources found in default namespace. If you see the following:\nNAME AGE domains.weblogic.oracle 5d then run the following command to delete the existing crd:\n$ kubectl delete crd domains.weblogic.oracle customresourcedefinition.apiextensions.k8s.io \u0026#34;domains.weblogic.oracle\u0026#34; deleted   Install the Oracle WebLogic Server Kubernetes Operator   On the master node run the following command to create a namespace for the operator:\n$ kubectl create namespace \u0026lt;sample-kubernetes-operator-ns\u0026gt; For example:\n$ kubectl create namespace opns The output will look similar to the following:\nnamespace/opns created   Create a service account for the operator in the operator\u0026rsquo;s namespace by running the following command:\n$ kubectl create serviceaccount -n \u0026lt;sample-kubernetes-operator-ns\u0026gt; \u0026lt;sample-kubernetes-operator-sa\u0026gt; For example:\n$ kubectl create serviceaccount -n opns op-sa The output will look similar to the following:\nserviceaccount/op-sa created   If you want to to setup logging and visualisation with Elasticsearch and Kibana (post domain creation) edit the \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/charts/weblogic-operator/values.yaml and set the parameter elkIntegrationEnabled to true and make sure the following parameters are set:\n# elkIntegrationEnabled specifies whether or not ELK integration is enabled. elkIntegrationEnabled: true # logStashImage specifies the docker image containing logstash. # This parameter is ignored if \u0026#39;elkIntegrationEnabled\u0026#39; is false. logStashImage: \u0026#34;logstash:6.6.0\u0026#34; # elasticSearchHost specifies the hostname of where elasticsearch is running. # This parameter is ignored if \u0026#39;elkIntegrationEnabled\u0026#39; is false. elasticSearchHost: \u0026#34;elasticsearch.default.svc.cluster.local\u0026#34; # elasticSearchPort specifies the port number of where elasticsearch is running. # This parameter is ignored if \u0026#39;elkIntegrationEnabled\u0026#39; is false. elasticSearchPort: 9200 After the domain creation see Logging and Visualization in order to complete the setup of Elasticsearch and Kibana.\n  Run the following helm command to install and start the operator:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator $ helm install kubernetes/charts/weblogic-operator \\ --namespace \u0026lt;sample-kubernetes-operator-ns\u0026gt; \\ --set image=weblogic-kubernetes-operator:3.0.1 \\ --set serviceAccount=\u0026lt;sample-kubernetes-operator-sa\u0026gt; --set \u0026#34;domainNamespaces={}\u0026#34; --set \u0026#34;javaLoggingLevel=FINE\u0026#34; --wait For example:\n$ cd /scratch/OAMDockerK8S/weblogic-kubernetes-operator $ helm install weblogic-kubernetes-operator kubernetes/charts/weblogic-operator \\ --namespace opns --set image=weblogic-kubernetes-operator:3.0.1 \\ --set serviceAccount=op-sa --set \u0026#34;domainNamespaces={}\u0026#34; --set \u0026#34;javaLoggingLevel=FINE\u0026#34; --wait The output will look similar to the following:\nNAME: weblogic-kubernetes-operator LAST DEPLOYED: Wed Sep 23 08:04:20 2020 NAMESPACE: opns STATUS: deployed REVISION: 1 TEST SUITE: None   Verify that the operator\u0026rsquo;s pod and services are running by executing the following command:\n$ kubectl get all -n \u0026lt;sample-kubernetes-operator-ns\u0026gt; For example:\n$ kubectl get all -n opns The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE pod/weblogic-operator-759b7c657-8gd7g 2/2 Running 0 107s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/internal-weblogic-operator-svc ClusterIP 10.102.11.143 \u0026lt;none\u0026gt; 8082/TCP 107s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/weblogic-operator 1/1 1 1 107s NAME DESIRED CURRENT READY AGE replicaset.apps/weblogic-operator-759b7c657 1 1 1 107s   Verify the operator pod\u0026rsquo;s log:\n$ kubectl logs -n \u0026lt;sample-kubernetes-operator-ns\u0026gt; -c weblogic-operator deployments/weblogic-operator For example:\n$ kubectl logs -n opns -c weblogic-operator deployments/weblogic-operator The output will look similar to the following:\n... {\u0026#34;timestamp\u0026#34;:\u0026#34;09-23-2020T15:04:30.485+0000\u0026#34;,\u0026#34;thread\u0026#34;:28,\u0026#34;fiber\u0026#34;:\u0026#34;fiber-1\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;opns\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.rest.RestServer\u0026#34;,ethod\u0026#34;:\u0026#34;start\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1600873470485,\u0026#34;message\u0026#34;:\u0026#34;Started the internal ssl REST server on https://0.0.0.0:8082/operator\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;} {\u0026#34;timestamp\u0026#34;:\u0026#34;09-23-2020T15:04:30.487+0000\u0026#34;,\u0026#34;thread\u0026#34;:28,\u0026#34;fiber\u0026#34;:\u0026#34;fiber-1\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;opns\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.Main\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;mkReadyAndStartLivenessThread\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1600873470487,\u0026#34;message\u0026#34;:\u0026#34;Starting Operator Liveness Thread\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;} {\u0026#34;timestamp\u0026#34;:\u0026#34;09-23-2020T15:06:27.528+0000\u0026#34;,\u0026#34;thread\u0026#34;:22,\u0026#34;fiber\u0026#34;:\u0026#34;engine-operator-thread-5-fiber-2\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;opns\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;FINE\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.orator.helpers.ConfigMapHelper$ScriptConfigMapContext\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;loadScriptsFromClasspath\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1600873587528,\u0026#34;message\u0026#34;:\u0026#34;Loading scripts into domain control config mapor namespace: opns\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;} {\u0026#34;timestamp\u0026#34;:\u0026#34;09-23-2020T15:06:27.529+0000\u0026#34;,\u0026#34;thread\u0026#34;:22,\u0026#34;fiber\u0026#34;:\u0026#34;engine-operator-thread-5-fiber-2\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;opns\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;FINE\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.orator.Main\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;readExistingDomains\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1600873587529,\u0026#34;message\u0026#34;:\u0026#34;Listing WebLogic Domains\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;} {\u0026#34;timestamp\u0026#34;:\u0026#34;09-23-2020T15:06:27.576+0000\u0026#34;,\u0026#34;thread\u0026#34;:20,\u0026#34;fiber\u0026#34;:\u0026#34;fiber-2-child-1\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;opns\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;FINE\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.helpers.CfigMapHelper$ConfigMapContext$ReadResponseStep\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;logConfigMapExists\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1600873587576,\u0026#34;message\u0026#34;:\u0026#34;Existing config map, ConfigMapHelper$ConfigMapContext$Readsponse, is correct for namespace: opns.\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;}   RCU schema creation In this section you create the RCU schemas in the Oracle Database.\nBefore following the steps in this section, make sure that the database and listener are up and running and you can connect to the database via SQL*Plus or other client tool.\n  Run the following command to create a namespace for the domain:\n$ kubectl create namespace \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl create namespace accessns The output will look similar to the following:\nnamespace/accessns created   Run the following command to create a helper pod to run RCU:\n$ kubectl run helper --image \u0026lt;image_name\u0026gt; -n \u0026lt;domain_namespace\u0026gt; -- sleep infinity For example:\n$ kubectl run helper --image oracle/oam:12.2.1.4.0 -n accessns -- sleep infinity The output will look similar to the following:\npod/helper created   Run the following command to check the pod is running:\n$ kubectl get pods -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get pods -n accessns The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE helper 1/1 Running 0 8s   Run the following command to start a bash shell in the helper pod:\n$ kubectl exec -it helper -n \u0026lt;domain_namespace\u0026gt; -- /bin/bash For example:\n$ kubectl exec -it helper -n accessns -- /bin/bash This will take you into a bash shell in the running helper pod:\n[oracle@helper ~]$   In the helper bash shell run the following commands to set the environment:\n[oracle@helper ~]$ export CONNECTION_STRING=\u0026lt;db_host.domain\u0026gt;:\u0026lt;db_port\u0026gt;/\u0026lt;service_name\u0026gt; [oracle@helper ~]$ export RCUPREFIX=\u0026lt;rcu_schema_prefix\u0026gt; [oracle@helper ~]$ echo -e \u0026lt;db_pwd\u0026gt;\u0026#34;\\n\u0026#34;\u0026lt;rcu_schema_pwd\u0026gt; \u0026gt; /tmp/pwd.txt [oracle@helper ~]$ cat /tmp/pwd.txt where:\n\u0026lt;db_host.domain\u0026gt;:\u0026lt;db_port\u0026gt;/\u0026lt;service_name\u0026gt;\tis your database connect string\n\u0026lt;rcu_schema_prefix\u0026gt; is the RCU schema prefix you want to set\n\u0026lt;db_pwd\u0026gt; is the SYS password for the database\n\u0026lt;rcu_schema_pwd\u0026gt; is the password you want to set for the \u0026lt;rcu_schema_prefix\u0026gt;\nFor example:\n[oracle@helper ~]$ export CONNECTION_STRING=mydatabasehost.example.com:1521/orcl.example.com [oracle@helper ~]$ export RCUPREFIX=OAMK8S [oracle@helper ~]$ echo -e \u0026lt;password\u0026gt;\u0026#34;\\n\u0026#34;\u0026lt;password\u0026gt; \u0026gt; /tmp/pwd.txt [oracle@helper ~]$ cat /tmp/pwd.txt \u0026lt;password\u0026gt; \u0026lt;password\u0026gt;   In the helper bash shell run the following command to create the RCU schemas in the database:\n$ [oracle@helper ~]$ /u01/oracle/oracle_common/bin/rcu -silent -createRepository -databaseType ORACLE -connectString \\ $CONNECTION_STRING -dbUser sys -dbRole sysdba -useSamePasswordForAllSchemaUsers true \\ -selectDependentsForComponents true -schemaPrefix $RCUPREFIX -component MDS -component IAU \\ -component IAU_APPEND -component IAU_VIEWER -component OPSS -component WLS -component STB -component OAM -f \u0026lt; /tmp/pwd.txt The output will look similar to the following:\nRCU Logfile: /tmp/RCU2020-09-23_15-36_1649016162/logs/rcu.log Processing command line .... Repository Creation Utility - Checking Prerequisites Checking Global Prerequisites Repository Creation Utility - Checking Prerequisites Checking Component Prerequisites Repository Creation Utility - Creating Tablespaces Validating and Creating Tablespaces Create tablespaces in the repository database Repository Creation Utility - Create Repository Create in progress. Executing pre create operations Percent Complete: 18 Percent Complete: 18 Percent Complete: 19 Percent Complete: 20 Percent Complete: 21 Percent Complete: 21 Percent Complete: 22 Percent Complete: 22 Creating Common Infrastructure Services(STB) Percent Complete: 30 Percent Complete: 30 Percent Complete: 39 Percent Complete: 39 Percent Complete: 39 Creating Audit Services Append(IAU_APPEND) Percent Complete: 46 Percent Complete: 46 Percent Complete: 55 Percent Complete: 55 Percent Complete: 55 Creating Audit Services Viewer(IAU_VIEWER) Percent Complete: 62 Percent Complete: 62 Percent Complete: 63 Percent Complete: 63 Percent Complete: 64 Percent Complete: 64 Creating Metadata Services(MDS) Percent Complete: 73 Percent Complete: 73 Percent Complete: 73 Percent Complete: 74 Percent Complete: 74 Percent Complete: 75 Percent Complete: 75 Percent Complete: 75 Creating Weblogic Services(WLS) Percent Complete: 80 Percent Complete: 80 Percent Complete: 83 Percent Complete: 83 Percent Complete: 91 Percent Complete: 98 Percent Complete: 98 Creating Audit Services(IAU) Percent Complete: 100 Creating Oracle Platform Security Services(OPSS) Creating Oracle Access Manager(OAM) Executing post create operations Repository Creation Utility: Create - Completion Summary Database details: ----------------------------- Host Name : mydatabasehost.example.com Port : 1521 Service Name : ORCL.EXAMPLE.COM Connected As : sys Prefix for (prefixable) Schema Owners : OAMK8S RCU Logfile : /tmp/RCU2020-09-23_15-36_1649016162/logs/rcu.log Component schemas created: ----------------------------- Component Status Logfile Common Infrastructure Services Success /tmp/RCU2020-09-23_15-36_1649016162/logs/stb.log Oracle Platform Security Services Success /tmp/RCU2020-09-23_15-36_1649016162/logs/opss.log Oracle Access Manager Success /tmp/RCU2020-09-23_15-36_1649016162/logs/oam.log Audit Services Success /tmp/RCU2020-09-23_15-36_1649016162/logs/iau.log Audit Services Append Success /tmp/RCU2020-09-23_15-36_1649016162/logs/iau_append.log Audit Services Viewer Success /tmp/RCU2020-09-23_15-36_1649016162/logs/iau_viewer.log Metadata Services Success /tmp/RCU2020-09-23_15-36_1649016162/logs/mds.log WebLogic Services Success /tmp/RCU2020-09-23_15-36_1649016162/logs/wls.log Repository Creation Utility - Create : Operation Completed [oracle@helper ~]$   Exit the helper bash shell by issuing the command exit.\n  Preparing the environment for domain creation In this section you prepare the environment for the OAM domain creation. This involves the following steps:\n Configure the operator for the domain namespace Create Kubernetes secrets for the domain and RCU Create a Kubernetes PV and PVC (Persistent Volume and Persistent Volume Claim)  Configure the operator for the domain namespace   Configure the Oracle WebLogic Server Kubernetes Operator to manage the domain in the domain namespace by running the following command:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator $ helm upgrade --reuse-values --namespace \u0026lt;operator_namespace\u0026gt; --set \u0026#34;domainNamespaces={\u0026lt;domain_namespace\u0026gt;}\u0026#34; --wait weblogic-kubernetes-operator kubernetes/charts/weblogic-operator For example:\n$ cd /scratch/OAMDockerK8S/weblogic-kubernetes-operator $ helm upgrade --reuse-values --namespace opns --set \u0026#34;domainNamespaces={accessns}\u0026#34; --wait weblogic-kubernetes-operator kubernetes/charts/weblogic-operator The output will look similar to the following:\nRelease \u0026#34;weblogic-kubernetes-operator\u0026#34; has been upgraded. Happy Helming! NAME: weblogic-kubernetes-operator LAST DEPLOYED: Wed Sep 23 08:44:48 2020 NAMESPACE: opns STATUS: deployed REVISION: 2 TEST SUITE: None   Creating Kubernetes secrets for the domain and RCU   Create a Kubernetes secret for the domain using the create-weblogic-credentials script in the same Kubernetes namespace as the domain:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-weblogic-domain-credentials $ ./create-weblogic-credentials.sh -u weblogic -p \u0026lt;pwd\u0026gt; -n \u0026lt;domain_namespace\u0026gt; -d \u0026lt;domain_uid\u0026gt; -s \u0026lt;kubernetes_domain_secret\u0026gt; where:\n-u weblogic is the WebLogic username\n-p \u0026lt;pwd\u0026gt; is the password for the weblogic user\n-n \u0026lt;domain_namespace\u0026gt; is the domain namespace\n-d \u0026lt;domain_uid\u0026gt; is the domain UID to be created. The default is domain1 if not specified\n-s \u0026lt;kubernetes_domain_secret\u0026gt; is the name you want to create for the secret for this namespace. The default is to use the domainUID if not specified\nFor example:\n$ cd /scratch/OAMDockerK8S/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-weblogic-domain-credentials $ ./create-weblogic-credentials.sh -u weblogic -p \u0026lt;password\u0026gt; -n accessns -d accessinfra -s accessinfra-domain-credentials The output will look similar to the following:\nsecret/accessinfra-domain-credentials created secret/accessinfra-domain-credentials labeled The secret accessinfra-domain-credentials has been successfully created in the accessns namespace.   Verify the secret is created using the following command:\n$ kubectl get secret \u0026lt;kubernetes_domain_secret\u0026gt; -o yaml -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get secret accessinfra-domain-credentials -o yaml -n accessns The output will look similar to the following:\napiVersion: v1 data: password: V2VsY29tZTE= username: d2VibG9naWM= kind: Secret metadata: creationTimestamp: \u0026#34;2020-09-23T15:46:25Z\u0026#34; labels: weblogic.domainName: accessinfra weblogic.domainUID: accessinfra managedFields: - apiVersion: v1 fieldsType: FieldsV1 fieldsV1: f:data: .: {} f:password: {} f:username: {} f:metadata: f:labels: .: {} f:weblogic.domainName: {} f:weblogic.domainUID: {} f:type: {} manager: kubectl operation: Update time: \u0026#34;2020-09-23T15:46:25Z\u0026#34; name: accessinfra-domain-credentials namespace: accessns resourceVersion: \u0026#34;50606\u0026#34; selfLink: /api/v1/namespaces/accessns/secrets/accessinfra-domain-credentials uid: 29f638f5-11d9-4b62-9cbb-03ff13ae3a90 type: Opaque   Create a Kubernetes secret for RCU using the create-weblogic-credentials script in the same Kubernetes namespace as the domain:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-rcu-credentials $ ./create-rcu-credentials.sh -u \u0026lt;rcu_prefix\u0026gt; -p \u0026lt;rcu_schema_pwd\u0026gt; -a sys -q \u0026lt;sys_db_pwd\u0026gt; -d \u0026lt;domain_uid\u0026gt; -n \u0026lt;domain_namespace\u0026gt; -s \u0026lt;kubernetes_rcu_secret\u0026gt; where:\n-u \u0026lt;rcu_prefix\u0026gt; is the name of the RCU schema prefix created previously\n-p \u0026lt;rcu_schema_pwd\u0026gt; is the password for the RCU schema prefix\n-q \u0026lt;sys_db_pwd\u0026gt; is the sys database password\n-d \u0026lt;domain_uid\u0026gt; is the domain_uid that you created earlier\n-n \u0026lt;domain_namespace\u0026gt; is the domain namespace\n-s \u0026lt;kubernetes_rcu_secret\u0026gt; is the name of the rcu secret to create\nFor example:\n$ cd /scratch/OAMDockerK8S/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-rcu-credentials $ ./create-rcu-credentials.sh -u OAMK8S -p \u0026lt;password\u0026gt; -a sys -q \u0026lt;password\u0026gt; -d accessinfra -n accessns -s accessinfra-rcu-credentials The output will look similar to the following:\nsecret/accessinfra-rcu-credentials created secret/accessinfra-rcu-credentials labeled The secret accessinfra-rcu-credentials has been successfully created in the accessns namespace.   Verify the secret is created using the following command:\n$ kubectl get secret \u0026lt;kubernetes_rcu_secret\u0026gt; -o yaml -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get secret accessinfra-rcu-credentials -o yaml -n accessns The output will look similar to the following:\napiVersion: v1 data: password: V2VsY29tZTE= sys_password: V2VsY29tZTE= sys_username: c3lz username: T0FNSzhT kind: Secret metadata: creationTimestamp: \u0026#34;2020-09-23T15:50:04Z\u0026#34; labels: weblogic.domainName: accessinfra weblogic.domainUID: accessinfra managedFields: - apiVersion: v1 fieldsType: FieldsV1 fieldsV1: f:data: .: {} f:password: {} f:sys_password: {} f:sys_username: {} f:username: {} f:metadata: f:labels: .: {} f:weblogic.domainName: {} f:weblogic.domainUID: {} f:type: {} manager: kubectl operation: Update time: \u0026#34;2020-09-23T15:50:04Z\u0026#34; name: accessinfra-rcu-credentials namespace: accessns resourceVersion: \u0026#34;51134\u0026#34; selfLink: /api/v1/namespaces/accessns/secrets/accessinfra-rcu-credentials uid: fce2499c-d8c8-4e9c-93e0-b15722bfc4d7 type: Opaque   Create a Kubernetes persistent volume and persistent volume claim In the Kubernetes namespace created above, create the persistent volume (PV) and persistent volume claim (PVC) by running the create-pv-pvc.sh script.\n  Make a backup copy of the create-pv-pvc-inputs.yaml file and create required directories:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-weblogic-domain-pv-pvc $ cp create-pv-pvc-inputs.yaml create-pv-pvc-inputs.yaml.orig $ mkdir output_access $ mkdir -p /\u0026lt;work directory\u0026gt;/accessdomainpv $ chmod -R 777 /\u0026lt;work directory\u0026gt;/accessdomainpv For example:\n$ cd /scratch/OAMDockerK8S/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-weblogic-domain-pv-pvc $ cp create-pv-pvc-inputs.yaml create-pv-pvc-inputs.yaml.orig $ mkdir output_access $ mkdir -p /scratch/OAMDockerK8S/accessdomainpv $ chmod -R 777 /scratch/OAMDockerK8S/accessdomainpv Note: The persistent volume directory needs to be accessible to both the master and worker node(s) via NFS. Make sure this path has full access permissions, and that the folder is empty. In this example /scratch/OAMDockerK8S/accessdomainpv is accessible from all nodes via NFS.\n  On the master node run the following command to ensure it is possible to read and write to the persistent volume:\ncd \u0026lt;work directory\u0026gt;/accessdomainpv touch filemaster.txt ls filemaster.txt For example:\ncd /scratch/OAMDockerK8S/accessdomainpv touch filemaster.txt ls filemaster.txt On the first worker node run the following to ensure it is possible to read and write to the persistent volume:\ncd /scratch/OAMDockerK8S/accessdomainpv ls filemaster.txt touch fileworker1.txt ls fileworker1.txt Repeat the above for any other worker nodes e.g fileworker2.txt etc. Once proven that it\u0026rsquo;s possible to read and write from each node to the persistent volume, delete the files created.\n  Edit the create-pv-pvc-inputs.yaml file and update the following parameters to reflect your settings. Save the file when complete:\nbaseName: \u0026lt;domain\u0026gt; domainUID: \u0026lt;domain_uid\u0026gt; namespace: \u0026lt;domain_namespace\u0026gt; weblogicDomainStorageType: NFS weblogicDomainStorageNFSServer: \u0026lt;nfs_server\u0026gt; weblogicDomainStoragePath: \u0026lt;physical_path_of_persistent_storage\u0026gt; For example:\n# The base name of the pv and pvc baseName: domain # Unique ID identifying a domain. # If left empty, the generated pv can be shared by multiple domains # This ID must not contain an underscope (\u0026#34;_\u0026#34;), and must be lowercase and unique across all domains in a Kubernetes cluster. domainUID: accessinfra # Name of the namespace for the persistent volume claim namespace: accessns ... # Persistent volume type for the persistent storage. # The value must be \u0026#39;HOST_PATH\u0026#39; or \u0026#39;NFS\u0026#39;. # If using \u0026#39;NFS\u0026#39;, weblogicDomainStorageNFSServer must be specified. weblogicDomainStorageType: NFS # The server name or ip address of the NFS server to use for the persistent storage. # The following line must be uncomment and customized if weblogicDomainStorateType is NFS: weblogicDomainStorageNFSServer: mynfsserver # Physical path of the persistent storage. # When weblogicDomainStorageType is set to HOST_PATH, this value should be set the to path to the # domain storage on the Kubernetes host. # When weblogicDomainStorageType is set to NFS, then weblogicDomainStorageNFSServer should be set # to the IP address or name of the DNS server, and this value should be set to the exported path # on that server. # Note that the path where the domain is mounted in the WebLogic containers is not affected by this # setting, that is determined when you create your domain. # The following line must be uncomment and customized: weblogicDomainStoragePath: /scratch/OAMDockerK8S/accessdomainpv   Execute the create-pv-pvc.sh script to create the PV and PVC configuration files:\n$ ./create-pv-pvc.sh -i create-pv-pvc-inputs.yaml -o output_access The output will be similar to the following:\nInput parameters being used export version=\u0026#34;create-weblogic-sample-domain-pv-pvc-inputs-v1\u0026#34; export baseName=\u0026#34;domain\u0026#34; export domainUID=\u0026#34;accessinfra\u0026#34; export namespace=\u0026#34;accessns\u0026#34; export weblogicDomainStorageType=\u0026#34;NFS\u0026#34; export weblogicDomainStorageNFSServer=\u0026#34;mynfsserver\u0026#34; export weblogicDomainStoragePath=\u0026#34;/scratch/OAMDockerK8S/accessdomainpv\u0026#34; export weblogicDomainStorageReclaimPolicy=\u0026#34;Retain\u0026#34; export weblogicDomainStorageSize=\u0026#34;10Gi\u0026#34; Generating output_access/pv-pvcs/accessinfra-weblogic-sample-pv.yaml Generating output_access/pv-pvcs/accessinfra-weblogic-sample-pvc.yaml The following files were generated: output_access/pv-pvcs/accessinfra-weblogic-sample-pv.yaml output_access/pv-pvcs/accessinfra-weblogic-sample-pvc.yaml   Run the following to show the files are created:\n$ ls output_access/pv-pvcs create-pv-pvc-inputs.yaml accessinfra-weblogic-sample-pv.yaml accessinfra-weblogic-sample-pvc.yaml   Run the following kubectl command to create the PV and PVC in the domain namespace:\n$ kubectl create -f output_access/pv-pvcs/accessinfra-domain-pv.yaml -n \u0026lt;domain_namespace\u0026gt; $ kubectl create -f output_access/pv-pvcs/accessinfra-domain-pvc.yaml -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl create -f output_access/pv-pvcs/accessinfra-domain-pv.yaml -n accessns $ kubectl create -f output_access/pv-pvcs/accessinfra-domain-pvc.yaml -n accessns The output will look similar to the following:\npersistentvolume/accessinfra-domain-pv created persistentvolumeclaim/accessinfra-domain-pvc created   Run the following commands to verify the PV and PVC were created successfully:\n$ kubectl describe pv \u0026lt;pv_name\u0026gt; $ kubectl describe pvc \u0026lt;pvc_name\u0026gt; -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl describe pv accessinfra-domain-pv $ kubectl describe pvc accessinfra-domain-pvc -n accessns The output will look similar to the following:\n$ kubectl describe pv accessinfra-domain-pv Name: accessinfra-domain-pv Labels: weblogic.domainUID=accessinfra Annotations: pv.kubernetes.io/bound-by-controller: yes Finalizers: [kubernetes.io/pv-protection] StorageClass: accessinfra-domain-storage-class Status: Bound Claim: accessns/accessinfra-domain-pvc Reclaim Policy: Retain Access Modes: RWX VolumeMode: Filesystem Capacity: 10Gi Node Affinity: \u0026lt;none\u0026gt; Message: Source: Type: NFS (an NFS mount that lasts the lifetime of a pod) Server: mynfsserver Path: /scratch/OAMDockerK8S/accessdomainpv ReadOnly: false Events: \u0026lt;none\u0026gt; $ kubectl describe pvc accessinfra-domain-pvc -n accessns Name: accessinfra-domain-pvc Namespace: accessns StorageClass: accessinfra-domain-storage-class Status: Bound Volume: accessinfra-domain-pv Labels: weblogic.domainUID=accessinfra Annotations: pv.kubernetes.io/bind-completed: yes pv.kubernetes.io/bound-by-controller: yes Finalizers: [kubernetes.io/pvc-protection] Capacity: 10Gi Access Modes: RWX VolumeMode: Filesystem Events: \u0026lt;none\u0026gt; Mounted By: \u0026lt;none\u0026gt; You are now ready to create the OAM domain as per Create OAM Domains\n  "
},
{
	"uri": "/fmw-kubernetes/20.4.1/oig/prepare-your-environment/",
	"title": "Prepare your environment",
	"tags": [],
	"description": "Preparation to deploy OIG on Kubernetes",
	"content": " Set up your Kubernetes cluster Install Helm Check the Kubernetes cluster is ready Install the OIG Docker image Install the Oracle WebLogic Server Kubernetes Operator Docker Image Setup the Code Repository to Deploy Oracle Identity Governance Domains Install the Oracle WebLogic Kubernetes Operator RCU schema creation Preparing the environment for domain creation  Configure the operator for the domain namespace Creating Kubernetes secrets for the domain and RCU Create a Kubernetes persistent volume and persistent volume claim    Set up your Kubernetes cluster If you need help setting up a Kubernetes environment, check our cheat sheet.\nIt is recommended you have a master node and one or more worker nodes. The examples in this documentation assume one master and two worker nodes.\nAfter creating Kubernetes clusters, you can optionally:\n Configure an Ingress to direct traffic to backend domains. Configure Kibana and Elasticsearch for your operator logs.  Install Helm As per the prerequisites an installation of Helm is required to create and deploy the necessary resources and then run the operator in a Kubernetes cluster. For Helm installation and usage information, refer to the README.\nCheck the Kubernetes cluster is ready Run the following command on the master node to check the cluster and worker nodes are running:\n$ kubectl get nodes,pods -n kube-system The output will look similar to the following:\n$ kubectl get nodes,pods -n kube-system NAME STATUS ROLES AGE VERSION node/worker-node1 Ready \u0026lt;none\u0026gt; 10d v1.18.4 node/worker-node2 Ready \u0026lt;none\u0026gt; 10d v1.18.4 node/master-node Ready master 11d v1.18.4 NAME READY STATUS RESTARTS AGE pod/coredns-66bff467f8-slxdq 1/1 Running 0 11d pod/coredns-66bff467f8-v77qt 1/1 Running 0 11d pod/etcd-master-node 1/1 Running 0 11d pod/kube-apiserver-master-node 1/1 Running 0 11d pod/kube-controller-manager-master-node 1/1 Running 0 11d pod/kube-flannel-ds-amd64-dcqjn 1/1 Running 0 10d pod/kube-flannel-ds-amd64-g4ztq 1/1 Running 0 11d pod/kube-flannel-ds-amd64-vpcbj 1/1 Running 1 10d pod/kube-proxy-jtcxm 1/1 Running 0 11d pod/kube-proxy-swfmm 1/1 Running 0 10d pod/kube-proxy-w6x6t 1/1 Running 0 10d pod/kube-scheduler-master-node 1/1 Running 0 11d $ Install the OIG Docker Image You can deploy OIG Docker images in the following ways:\n  Download a prebuilt OIG Docker image from My Oracle Support by referring to the document ID 2723908.1. This image is prebuilt by Oracle and includes Oracle Identity Governance 12.2.1.4.0 and the latest PSU.\n  Build your own OIG image using the WebLogic Image Tool or by using the dockerfile, scripts and base images from Oracle Container Registry (OCR). You can also build your own image by using only the dockerfile and scripts. For more information about the various ways in which you can build your own container image, see Building the OIG Docker Image.\n  Choose one of these options based on your requirements.\nThe OIG Docker image must be installed on the master node AND each of the worker nodes in your Kubernetes cluster. Alternatively you can place the image in a Docker registry that your cluster can access.\n After installing the OIG Docker image run the following command to make sure the image is installed correctly on the master and worker nodes:\n$ docker images The output will look similar to the following:\nREPOSITORY TAG IMAGE ID CREATED SIZE oracle/oig 12.2.1.4.0 59ffc14dddbb 3 days ago 4.96GB k8s.gcr.io/kube-proxy v1.18.4 718fa77019f2 6 weeks ago 117MB k8s.gcr.io/kube-scheduler v1.18.4 c663567f869e 6 weeks ago 95.3MB k8s.gcr.io/kube-controller-manager v1.18.4 e8f1690127c4 6 weeks ago 162MB k8s.gcr.io/kube-apiserver v1.18.4 408913fc18eb 6 weeks ago 173MB quay.io/coreos/flannel v0.12.0-amd64 4e9f801d2217 4 months ago 52.8MB k8s.gcr.io/pause 3.2 80d28bedfe5d 5 months ago 683kB k8s.gcr.io/coredns 1.6.7 67da37a9a360 6 months ago 43.8MB k8s.gcr.io/etcd 3.4.3-0 303ce5db0e90 9 months ago 288MB Install the Oracle WebLogic Server Kubernetes Operator Docker Image In this release only Oracle WebLogic Server Kubernetes Operator 3.0.1 is supported.\nThe Oracle WebLogic Server Kubernetes Operator Docker image must be installed on the master node and each of the worker nodes in your Kubernetes cluster. Alternatively you can place the image in a Docker registry that your cluster can access.\n   Pull the Oracle WebLogic Server Kubernetes Operator 3.0.1 image by running the following command on the master node:\n$ docker pull oracle/weblogic-kubernetes-operator:3.0.1 The output will look similar to the following:\nTrying to pull repository docker.io/oracle/weblogic-kubernetes-operator ... 3.0.1: Pulling from docker.io/oracle/weblogic-kubernetes-operator bce8f778fef0: Already exists de14ddc50a70: Pull complete 77401a861078: Pull complete 9c5ac1423af4: Pull complete 2b6f244f998f: Pull complete 625e05083092: Pull complete Digest: sha256:27047d032ac5a9077b39bec512b99d8ca54bf9bf71227f5fd1b7b26ac80c20d3 Status: Downloaded newer image for oracle/weblogic-kubernetes-operator:3.0.1 oracle/weblogic-kubernetes-operator:3.0.1   Run the docker tag command as follows:\n$ docker tag oracle/weblogic-kubernetes-operator:3.0.1 weblogic-kubernetes-operator:3.0.1 After installing the Oracle WebLogic Server Kubernetes Operator 3.0.1 Docker image, repeat the above on the worker nodes.\n  Setup the Code Repository to Deploy Oracle Identity Governance Domains Oracle Identity Governance domain deployment on Kubernetes leverages the Oracle WebLogic Kubernetes Operator infrastructure. For deploying the Oracle Identity Governance domains, you need to set up the deployment scripts on the master node as below:\n  Create a working directory to setup the source code.\n$ mkdir \u0026lt;work directory\u0026gt; For example:\n$ mkdir /scratch/OIGDockerK8S   Download the supported version of the Oracle WebLogic Kubernetes Operator source code from the operator github project. Currently the supported operator version is 3.0.1:\n$ cd \u0026lt;work directory\u0026gt; $ git clone https://github.com/oracle/weblogic-kubernetes-operator.git --branch release/3.0.1 For example:\n$ cd /scratch/OIGDockerK8S $ git clone https://github.com/oracle/weblogic-kubernetes-operator.git --branch release/3.0.1 This will create the directory \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator\n  Clone the Oracle Identity Governance deployment scripts from the OIG repository and copy them into the WebLogic operator samples location.\n$ git clone https://github.com/oracle/fmw-kubernetes.git $ cp -rf \u0026lt;work directory\u0026gt;/fmw-kubernetes/OracleIdentityGovernance/kubernetes/3.0.1/create-oim-domain \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/scripts/ $ mv -f \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain_backup $ cp -rf \u0026lt;work directory\u0026gt;/fmw-kubernetes/OracleIdentityGovernance/kubernetes/3.0.1/ingress-per-domain \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain $ cp -rf \u0026lt;work directory\u0026gt;/fmw-kubernetes/OracleIdentityGovernance/kubernetes/3.0.1/design-console-ingress \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/charts/design-console-ingress For example:\n$ git clone https://github.com/oracle/fmw-kubernetes.git $ cp -rf /scratch/OIGDockerK8S/fmw-kubernetes/OracleIdentityGovernance/kubernetes/3.0.1/create-oim-domain /scratch/OIGDockerK8S/weblogic-kubernetes-operator/kubernetes/samples/scripts/ $ mv -f /scratch/OIGDockerK8S/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain /scratch/OIGDockerK8S/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain_backup $ cp -rf /scratch/OIGDockerK8S/fmw-kubernetes/OracleIdentityGovernance/kubernetes/3.0.1/ingress-per-domain /scratch/OIGDockerK8S/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain cp -rf /scratch/OIGDockerK8S/fmw-kubernetes/OracleIdentityGovernance/kubernetes/3.0.1/design-console-ingress /scratch/OIGDockerK8S/weblogic-kubernetes-operator/kubernetes/samples/charts/design-console-ingress You can now use the deployment scripts from \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/scripts/ to set up the OIG domains as further described in this document.\n  Run the following command and see if the WebLogic custom resource definition name already exists:\n$ kubectl get crd In the output you should see:\nNo resources found in default namespace. If you see the following:\nNAME AGE domains.weblogic.oracle 5d then run the following command to delete the existing crd:\n$ kubectl delete crd domains.weblogic.oracle customresourcedefinition.apiextensions.k8s.io \u0026#34;domains.weblogic.oracle\u0026#34; deleted   Install the Oracle WebLogic Kubernetes Operator   On the master node run the following command to create a namespace for the operator:\n$ kubectl create namespace \u0026lt;sample-kubernetes-operator-ns\u0026gt; For example:\n$ kubectl create namespace operator The output will look similar to the following:\nnamespace/operator created   Create a service account for the operator in the operator\u0026rsquo;s namespace by running the following command:\n$ kubectl create serviceaccount -n \u0026lt;sample-kubernetes-operator-ns\u0026gt; \u0026lt;sample-kubernetes-operator-sa\u0026gt; For example:\n$ kubectl create serviceaccount -n operator operator-serviceaccount The output will look similar to the following:\nserviceaccount/operator-serviceaccount created   If you want to setup logging and visualisation with Elasticsearch and Kibana (post domain creation) edit the \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/charts/weblogic-operator/values.yaml and set the parameter elkIntegrationEnabled to true and make sure the following parameters are set:\n# elkIntegrationEnabled specifies whether or not ELK integration is enabled. elkIntegrationEnabled: true # logStashImage specifies the docker image containing logstash. # This parameter is ignored if \u0026#39;elkIntegrationEnabled\u0026#39; is false. logStashImage: \u0026#34;logstash:6.6.0\u0026#34; # elasticSearchHost specifies the hostname of where elasticsearch is running. # This parameter is ignored if \u0026#39;elkIntegrationEnabled\u0026#39; is false. elasticSearchHost: \u0026#34;elasticsearch.default.svc.cluster.local\u0026#34; # elasticSearchPort specifies the port number of where elasticsearch is running. # This parameter is ignored if \u0026#39;elkIntegrationEnabled\u0026#39; is false. elasticSearchPort: 9200 After the domain creation see Logging and Visualization in order to complete the setup of Elasticsearch and Kibana.\n  Run the following helm command to install and start the operator:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator $ helm install kubernetes/charts/weblogic-operator \\ --namespace \u0026lt;sample-kubernetes-operator-ns\u0026gt; \\ --set image=weblogic-kubernetes-operator:3.0.1 \\ --set serviceAccount=\u0026lt;sample-kubernetes-operator-sa\u0026gt; \\ --set \u0026#34;domainNamespaces={}\u0026#34; For example:\n$ cd /scratch/OIGDockerK8S/weblogic-kubernetes-operator $ helm install weblogic-kubernetes-operator kubernetes/charts/weblogic-operator \\ --namespace operator \\ --set image=weblogic-kubernetes-operator:3.0.1 \\ --set serviceAccount=operator-serviceaccount \\ --set \u0026#34;domainNamespaces={}\u0026#34; The output will look similar to the following:\nNAME: weblogic-kubernetes-operator LAST DEPLOYED: Tue Sep 29 02:33:06 2020 NAMESPACE: operator STATUS: deployed REVISION: 1 TEST SUITE: None   Verify that the operator\u0026rsquo;s pod is running by executing the following command to list the pods in the operator\u0026rsquo;s namespace:\n$ kubectl get all -n \u0026lt;sample-kubernetes-operator-ns\u0026gt; For example:\n$ kubectl get all -n operator The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE pod/weblogic-operator-5d5dfb74ff-t7ct5 2/2 Running 0 17m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/internal-weblogic-operator-svc ClusterIP 10.101.11.127 \u0026lt;none\u0026gt; 8082/TCP 17m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/weblogic-operator 1/1 1 1 17m NAME DESIRED CURRENT READY AGE replicaset.apps/weblogic-operator-5d5dfb74ff 1 1 1 17m   Verify that the operator is up and running by viewing the operator pod\u0026rsquo;s log:\n$ kubectl logs -n \u0026lt;sample-kubernetes-operator-ns\u0026gt; -c weblogic-operator deployments/weblogic-operator For example:\n$ kubectl logs -n operator -c weblogic-operator deployments/weblogic-operator The output will look similar to the following:\n{\u0026#34;timestamp\u0026#34;:\u0026#34;09-29-2020T09:33:26.284+0000\u0026#34;,\u0026#34;thread\u0026#34;:27,\u0026#34;fiber\u0026#34;:\u0026#34;fiber-1\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;operator\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;WARNING\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.utils.Certificates\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;getCertificate\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1601372006284,\u0026#34;message\u0026#34;:\u0026#34;No external certificate configured for REST endpoint. Endpoint will be disabled.\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;} {\u0026#34;timestamp\u0026#34;:\u0026#34;09-29-2020T09:33:28.611+0000\u0026#34;,\u0026#34;thread\u0026#34;:27,\u0026#34;fiber\u0026#34;:\u0026#34;fiber-1\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;operator\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.rest.RestServer\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;start\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1601372008611,\u0026#34;message\u0026#34;:\u0026#34;Started the internal ssl REST server on https://0.0.0.0:8082/operator\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;} {\u0026#34;timestamp\u0026#34;:\u0026#34;09-29-2020T09:33:28.613+0000\u0026#34;,\u0026#34;thread\u0026#34;:27,\u0026#34;fiber\u0026#34;:\u0026#34;fiber-1\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;operator\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.Main\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;markReadyAndStartLivenessThread\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1601372008613,\u0026#34;message\u0026#34;:\u0026#34;Starting Operator Liveness Thread\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;}   RCU schema creation In this section you create the RCU schemas in the Oracle Database.\nBefore following the steps in this section, make sure that the database and listener are up and running and you can connect to the database via SQL*Plus or other client tool.\n  Run the following command to create a namespace for the domain:\n$ kubectl create namespace \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl create namespace oimcluster The output will look similar to the following:\nnamespace/oimcluster created Run the following command to create a helper pod:\n$ kubectl run helper --image \u0026lt;image_name\u0026gt; -n \u0026lt;domain_namespace\u0026gt; -- sleep infinity For example:\n$ kubectl run helper --image oracle/oig:12.2.1.4.0 -n oimcluster -- sleep infinity The output will look similar to the following:\npod/helper created   Run the following command to start a bash shell in the helper pod:\n$ kubectl exec -it helper -n \u0026lt;domain_namespace\u0026gt; -- /bin/bash For example:\n$ kubectl exec -it helper -n oimcluster -- /bin/bash This will take you into a bash shell in the running rcu pod:\n[oracle@helper oracle]$   In the helper bash shell run the following commands to set the environment:\n[oracle@helper oracle]$ export DB_HOST=\u0026lt;db_host.domain\u0026gt; [oracle@helper oracle]$ export DB_PORT=\u0026lt;db_port\u0026gt; [oracle@helper oracle]$ export DB_SERVICE=\u0026lt;service_name\u0026gt; [oracle@helper oracle]$ export RCUPREFIX=\u0026lt;rcu_schema_prefix\u0026gt; [oracle@helper oracle]$ export RCU_SCHEMA_PWD=\u0026lt;rcu_schema_pwd\u0026gt; [oracle@helper oracle]$ echo -e \u0026lt;db_pwd\u0026gt;\u0026#34;\\n\u0026#34;\u0026lt;rcu_schema_pwd\u0026gt; \u0026gt; /tmp/pwd.txt [oracle@helper oracle]$ cat /tmp/pwd.txt where:\n\u0026lt;db_host.domain\u0026gt; is the database server hostname\n\u0026lt;db_port\u0026gt; is the database listener port\n\u0026lt;service_name\u0026gt; is the database service name\n\u0026lt;rcu_schema_prefix\u0026gt; is the RCU schema prefix you want to set\n\u0026lt;rcu_schema_pwd\u0026gt; is the password you want to set for the \u0026lt;rcu_schema_prefix\u0026gt;\n\u0026lt;db_pwd\u0026gt; is the SYS password for the database\nFor example:\n[oracle@helper oracle]$ export DB_HOST=mydatabasehost.example.com [oracle@helper oracle]$ export DB_PORT=1521 [oracle@helper oracle]$ export DB_SERVICE=orcl.example.com [oracle@helper oracle]$ export RCUPREFIX=OIGK8S [oracle@helper oracle]$ export RCU_SCHEMA_PWD=\u0026lt;password\u0026gt; [oracle@helper oracle]$ echo -e \u0026lt;password\u0026gt;\u0026#34;\\n\u0026#34;\u0026lt;password\u0026gt; \u0026gt; /tmp/pwd.txt [oracle@helper oracle]$ cat /tmp/pwd.txt \u0026lt;password\u0026gt; \u0026lt;password\u0026gt;   In the helper bash shell run the following commands to create the RCU schemas in the database:\n[oracle@helper oracle]$ /u01/oracle/oracle_common/bin/rcu -silent -createRepository -databaseType ORACLE -connectString \\ $DB_HOST:$DB_PORT/$DB_SERVICE -dbUser sys -dbRole sysdba -useSamePasswordForAllSchemaUsers true \\ -selectDependentsForComponents true -schemaPrefix $RCUPREFIX -component OIM -component MDS -component SOAINFRA -component OPSS \\ -f \u0026lt; /tmp/pwd.txt The output will look similar to the following:\nRCU Logfile: /tmp/RCU2020-09-29_10-51_508080961/logs/rcu.log Processing command line .... Repository Creation Utility - Checking Prerequisites Checking Global Prerequisites Repository Creation Utility - Checking Prerequisites Checking Component Prerequisites Repository Creation Utility - Creating Tablespaces Validating and Creating Tablespaces Create tablespaces in the repository database Repository Creation Utility - Create Repository Create in progress. Percent Complete: 10 Executing pre create operations Percent Complete: 25 Percent Complete: 25 Percent Complete: 26 Percent Complete: 27 Percent Complete: 28 Percent Complete: 28 Percent Complete: 29 Percent Complete: 29 Creating Common Infrastructure Services(STB) Percent Complete: 36 Percent Complete: 36 Percent Complete: 44 Percent Complete: 44 Percent Complete: 44 Creating Audit Services Append(IAU_APPEND) Percent Complete: 51 Percent Complete: 51 Percent Complete: 59 Percent Complete: 59 Percent Complete: 59 Creating Audit Services Viewer(IAU_VIEWER) Percent Complete: 66 Percent Complete: 66 Percent Complete: 67 Percent Complete: 67 Percent Complete: 68 Percent Complete: 68 Creating Metadata Services(MDS) Percent Complete: 76 Percent Complete: 76 Percent Complete: 76 Percent Complete: 77 Percent Complete: 77 Percent Complete: 78 Percent Complete: 78 Percent Complete: 78 Creating Weblogic Services(WLS) Percent Complete: 82 Percent Complete: 82 Percent Complete: 83 Percent Complete: 84 Percent Complete: 86 Percent Complete: 88 Percent Complete: 88 Percent Complete: 88 Creating User Messaging Service(UCSUMS) Percent Complete: 92 Percent Complete: 92 Percent Complete: 95 Percent Complete: 95 Percent Complete: 100 Creating Audit Services(IAU) Creating Oracle Platform Security Services(OPSS) Creating SOA Infrastructure(SOAINFRA) Creating Oracle Identity Manager(OIM) Executing post create operations Repository Creation Utility: Create - Completion Summary Database details: ----------------------------- Host Name : mydatabasehost.example.com Port : 1521 Service Name : ORCL.EXAMPLE.COM Connected As : sys Prefix for (prefixable) Schema Owners : OIGK8S RCU Logfile : /tmp/RCU2020-09-29_10-51_508080961/logs/rcu.log Component schemas created: ----------------------------- Component Status Logfile Common Infrastructure Services Success /tmp/RCU2020-09-29_10-51_508080961/logs/stb.log Oracle Platform Security Services Success /tmp/RCU2020-09-29_10-51_508080961/logs/opss.log SOA Infrastructure Success /tmp/RCU2020-09-29_10-51_508080961/logs/soainfra.log Oracle Identity Manager Success /tmp/RCU2020-09-29_10-51_508080961/logs/oim.log User Messaging Service Success /tmp/RCU2020-09-29_10-51_508080961/logs/ucsums.log Audit Services Success /tmp/RCU2020-09-29_10-51_508080961/logs/iau.log Audit Services Append Success /tmp/RCU2020-09-29_10-51_508080961/logs/iau_append.log Audit Services Viewer Success /tmp/RCU2020-09-29_10-51_508080961/logs/iau_viewer.log Metadata Services Success /tmp/RCU2020-09-29_10-51_508080961/logs/mds.log WebLogic Services Success /tmp/RCU2020-09-29_10-51_508080961/logs/wls.log Repository Creation Utility - Create : Operation Completed [oracle@helper oracle]$   Run the following command to patch schemas in the database:\n[oracle@helper oracle]$ /u01/oracle/oracle_common/modules/thirdparty/org.apache.ant/1.10.5.0.0/apache-ant-1.10.5/bin/ant \\ -f /u01/oracle/idm/server/setup/deploy-files/automation.xml \\ run-patched-sql-files \\ -logger org.apache.tools.ant.NoBannerLogger \\ -logfile /u01/oracle/idm/server/bin/patch_oim_wls.log \\ -DoperationsDB.host=$DB_HOST \\ -DoperationsDB.port=$DB_PORT \\ -DoperationsDB.serviceName=$DB_SERVICE \\ -DoperationsDB.user=${RCUPREFIX}_OIM \\ -DOIM.DBPassword=$RCU_SCHEMA_PWD \\ -Dojdbc=/u01/oracle/oracle_common/modules/oracle.jdbc/ojdbc8.jar The output will look similar to the following:\nBuildfile: /u01/oracle/idm/server/setup/deploy-files/automation.xml   Verify the database was patched successfully by viewing the patch_oim_wls.log:\n[oracle@helper oracle]$ cat /u01/oracle/idm/server/bin/patch_oim_wls.log The output should look similar to below:\nrun-patched-sql-files: [sql] Executing resource: /u01/oracle/idm/server/db/oim/oracle/StoredProcedures/API/oim_role_mgmt_pkg_body.sql [sql] Executing resource: /u01/oracle/idm/server/db/oim/oracle/Upgrade/oim12cps4/list/oim12cps4_dml_pty_insert_sysprop_ssointg_grprecon_matching_rolename.sql [sql] Executing resource: /u01/oracle/idm/server/db/oim/oracle/Upgrade/oim12cps4/list/oim12cps4_dml_pty_insert_sysprop_oimadpswdpolicy.sql [sql] 3 of 3 SQL statements executed successfully BUILD SUCCESSFUL Total time: 1 second   Exit the helper bash shell by issuing the command exit.\n  Preparing the environment for domain creation In this section you prepare the environment for the OIG domain creation. This involves the following steps:\n Configure the operator for the domain namespace Create Kubernetes secrets for the domain and RCU Create a Kubernetes PV and PVC (Persistent Volume and Persistent Volume Claim)  Configure the operator for the domain namespace   Configure the Oracle WebLogic Kubernetes Operator to manage the domain in the domain namespace by running the following command:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator $ helm upgrade --reuse-values --namespace \u0026lt;operator_namespace\u0026gt; --set \u0026#34;domainNamespaces={oimcluster}\u0026#34; --wait weblogic-kubernetes-operator kubernetes/charts/weblogic-operator For example:\n$ cd /scratch/OIGDockerK8S/weblogic-kubernetes-operator $ helm upgrade --reuse-values --namespace operator --set \u0026#34;domainNamespaces={oimcluster}\u0026#34; --wait weblogic-kubernetes-operator kubernetes/charts/weblogic-operator The output will look similar to the following:\nRelease \u0026#34;weblogic-kubernetes-operator\u0026#34; has been upgraded. Happy Helming! NAME: weblogic-kubernetes-operator LAST DEPLOYED: Tue Sep 29 04:01:43 2020 NAMESPACE: operator STATUS: deployed REVISION: 2 TEST SUITE: None   Creating Kubernetes secrets for the domain and RCU   Create a Kubernetes secret for the domain using the create-weblogic-credentials script in the same Kubernetes namespace as the domain:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-weblogic-domain-credentials $ ./create-weblogic-credentials.sh -u weblogic -p \u0026lt;pwd\u0026gt; -n \u0026lt;domain_namespace\u0026gt; -d \u0026lt;domain_uid\u0026gt; -s \u0026lt;kubernetes_domain_secret\u0026gt; where:\n-u weblogic is the WebLogic username\n-p \u0026lt;pwd\u0026gt; is the password for the WebLogic user\n-n \u0026lt;domain_namespace\u0026gt; is the domain namespace\n-d \u0026lt;domain_uid\u0026gt; is the domain UID to be created. The default is domain1 if not specified\n-s \u0026lt;kubernetes_domain_secret\u0026gt; is the name you want to create for the secret for this namespace. The default is to use the domainUID if not specified\nFor example:\n$ cd /scratch/OIGDockerK8S/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-weblogic-domain-credentials $ ./create-weblogic-credentials.sh -u weblogic -p \u0026lt;password\u0026gt; -n oimcluster -d oimcluster -s oimcluster-domain-credentials The output will look similar to the following:\nsecret/oimcluster-domain-credentials created secret/oimcluster-domain-credentials labeled The secret oimcluster-domain-credentials has been successfully created in the oimcluster namespace.   Verify the secret is created using the following command:\n$ kubectl get secret \u0026lt;kubernetes_domain_secret\u0026gt; -o yaml -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get secret oimcluster-domain-credentials -o yaml -n oimcluster The output will look similar to the following:\n$ kubectl get secret oimcluster-domain-credentials -o yaml -n oimcluster apiVersion: v1 data: password: V2VsY29tZTE= username: d2VibG9naWM= kind: Secret metadata: creationTimestamp: \u0026#34;2020-09-29T11:04:44Z\u0026#34; labels: weblogic.domainName: oimcluster weblogic.domainUID: oimcluster managedFields: - apiVersion: v1 fieldsType: FieldsV1 fieldsV1: f:data: .: {} f:password: {} f:username: {} f:metadata: f:labels: .: {} f:weblogic.domainName: {} f:weblogic.domainUID: {} f:type: {} manager: kubectl operation: Update time: \u0026#34;2020-09-29T11:04:44Z\u0026#34; name: oimcluster-domain-credentials namespace: oimcluster resourceVersion: \u0026#34;1249007\u0026#34; selfLink: /api/v1/namespaces/oimcluster/secrets/oimcluster-domain-credentials uid: 4ade08f3-7b11-4bb0-9340-7304a2ef9b64 type: Opaque   Create a Kubernetes secret for RCU in the same Kubernetes namespace as the domain, using the create-weblogic-credentials.sh script:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-rcu-credentials $ ./create-rcu-credentials.sh -u \u0026lt;rcu_prefix\u0026gt; -p \u0026lt;rcu_schema_pwd\u0026gt; -a sys -q \u0026lt;sys_db_pwd\u0026gt; -d \u0026lt;domain_uid\u0026gt; -n \u0026lt;domain_namespace\u0026gt; -s \u0026lt;kubernetes_rcu_secret\u0026gt; where:\n-u \u0026lt;rcu_prefix\u0026gt; is the name of the RCU schema prefix created previously\n-p \u0026lt;rcu_schema_pwd\u0026gt; is the password for the RCU schema prefix\n-q \u0026lt;sys_db_pwd\u0026gt; is the sys database password\n-d \u0026lt;domain_uid\u0026gt; is the domain_uid that you created earlier\n-n \u0026lt;domain_namespace\u0026gt; is the domain namespace\n-s \u0026lt;kubernetes_rcu_secret\u0026gt; is the name of the rcu secret to create\nFor example:\n$ cd /scratch/OIGDockerK8S/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-rcu-credentials $ ./create-rcu-credentials.sh -u OIGK8S -p \u0026lt;password\u0026gt; -a sys -q \u0026lt;password\u0026gt; -d oimcluster -n oimcluster -s oimcluster-rcu-credentials The output will look similar to the following:\nsecret/oimcluster-rcu-credentials created secret/oimcluster-rcu-credentials labeled The secret oimcluster-rcu-credentials has been successfully created in the oimcluster namespace.   Verify the secret is created using the following command:\n$ kubectl get secret \u0026lt;kubernetes_rcu_secret\u0026gt; -o yaml -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get secret oimcluster-rcu-credentials -o yaml -n oimcluster The output will look similar to the following:\napiVersion: v1 data: password: V2VsY29tZTE= sys_password: V2VsY29tZTE= sys_username: c3lz username: T0lHSzhT kind: Secret metadata: creationTimestamp: \u0026#34;2020-09-29T11:18:45Z\u0026#34; labels: weblogic.domainName: oimcluster weblogic.domainUID: oimcluster managedFields: - apiVersion: v1 fieldsType: FieldsV1 fieldsV1: f:data: .: {} f:password: {} f:sys_password: {} f:sys_username: {} f:username: {} f:metadata: f:labels: .: {} f:weblogic.domainName: {} f:weblogic.domainUID: {} f:type: {} manager: kubectl operation: Update time: \u0026#34;2020-09-29T11:18:45Z\u0026#34; name: oimcluster-rcu-credentials namespace: oimcluster resourceVersion: \u0026#34;1251020\u0026#34; selfLink: /api/v1/namespaces/oimcluster/secrets/oimcluster-rcu-credentials uid: aee4213e-ffe2-45a6-9b96-11c4e88d12f2 type: Opaque   Create a Kubernetes persistent volume and persistent volume claim In the Kubernetes domain namespace created above, create the persistent volume (PV) and persistent volume claim (PVC) by running the create-pv-pvc.sh script.\n  Make a backup copy of the create-pv-pvc-inputs.yaml file and create required directories:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-weblogic-domain-pv-pvc $ cp create-pv-pvc-inputs.yaml create-pv-pvc-inputs.yaml.orig $ mkdir output_oimcluster $ mkdir -p /\u0026lt;work directory\u0026gt;/oimclusterdomainpv $ chmod -R 777 /\u0026lt;work directory\u0026gt;/oimclusterdomainpv For example:\n$ cd /scratch/OIGDockerK8S/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-weblogic-domain-pv-pvc $ cp create-pv-pvc-inputs.yaml create-pv-pvc-inputs.yaml.orig $ mkdir output_oimcluster $ mkdir -p /scratch/OIGDockerK8S/oimclusterdomainpv $ chmod -R 777 /scratch/OIGDockerK8S/oimclusterdomainpv Note: The persistent volume directory needs to be accessible to both the master and worker node(s) via NFS. Make sure this path has full access permissions, and that the folder is empty. In this example /scratch/OIGDockerK8S/oimclusterdomainpv is accessible from all nodes via NFS.\n  Edit the create-pv-pvc-inputs.yaml file and update the following parameters to reflect your settings. Save the file when complete:\nbaseName: \u0026lt;domain\u0026gt; domainUID: \u0026lt;domain_uid\u0026gt; namespace: \u0026lt;domain_namespace\u0026gt; weblogicDomainStorageType: NFS weblogicDomainStorageNFSServer: \u0026lt;nfs_server\u0026gt; weblogicDomainStoragePath: \u0026lt;physical_path_of_persistent_storage\u0026gt; For example:\n# The base name of the pv and pvc baseName: oim # Unique ID identifying a domain. # If left empty, the generated pv can be shared by multiple domains # This ID must not contain an underscope (\u0026#34;_\u0026#34;), and must be lowercase and unique across all domains in a Kubernetes cluster. domainUID: oimcluster # Name of the namespace for the persistent volume claim namespace: oimcluster # Persistent volume type for the persistent storage. # The value must be \u0026#39;HOST_PATH\u0026#39; or \u0026#39;NFS\u0026#39;. # If using \u0026#39;NFS\u0026#39;, weblogicDomainStorageNFSServer must be specified. weblogicDomainStorageType: NFS # The server name or ip address of the NFS server to use for the persistent storage. # The following line must be uncomment and customized if weblogicDomainStorateType is NFS: weblogicDomainStorageNFSServer: mynfsserver # Physical path of the persistent storage. # When weblogicDomainStorageType is set to HOST_PATH, this value should be set the to path to the # domain storage on the Kubernetes host. # When weblogicDomainStorageType is set to NFS, then weblogicDomainStorageNFSServer should be set # to the IP address or name of the DNS server, and this value should be set to the exported path # on that server. # Note that the path where the domain is mounted in the WebLogic containers is not affected by this # setting, that is determined when you create your domain. # The following line must be uncomment and customized: weblogicDomainStoragePath: /scratch/OIGDockerK8S/oimclusterdomainpv   Execute the create-pv-pvc.sh script to create the PV and PVC configuration files:\n$ ./create-pv-pvc.sh -i create-pv-pvc-inputs.yaml -o output_oimcluster The output will be similar to the following:\nInput parameters being used export version=\u0026#34;create-weblogic-sample-domain-pv-pvc-inputs-v1\u0026#34; export baseName=\u0026#34;oim\u0026#34; export domainUID=\u0026#34;oimcluster\u0026#34; export namespace=\u0026#34;oimcluster\u0026#34; export weblogicDomainStorageType=\u0026#34;NFS\u0026#34; export weblogicDomainStorageNFSServer=\u0026#34;mynfsserver\u0026#34; export weblogicDomainStoragePath=\u0026#34;/scratch/OIGDockerK8S/oimclusterdomainpv\u0026#34; export weblogicDomainStorageReclaimPolicy=\u0026#34;Retain\u0026#34; export weblogicDomainStorageSize=\u0026#34;10Gi\u0026#34; Generating output_oimcluster/pv-pvcs/oimcluster-oim-pv.yaml Generating output_oimcluster/pv-pvcs/oimcluster-oim-pvc.yaml The following files were generated: output_oimcluster/pv-pvcs/oimcluster-oim-pv.yaml output_oimcluster/pv-pvcs/oimcluster-oim-pvc.yaml Completed   Run the following to show the files are created:\n$ ls output_oimcluster/pv-pvcs create-pv-pvc-inputs.yaml oimcluster-oim-pvc.yaml oimcluster-oim-pv.yaml   Run the following kubectl command to create the PV and PVC in the domain namespace:\n$ kubectl create -f output_oimcluster/pv-pvcs/oimcluster-oim-pv.yaml -n \u0026lt;domain_namespace\u0026gt; $ kubectl create -f output_oimcluster/pv-pvcs/oimcluster-oim-pvc.yaml -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl create -f output_oimcluster/pv-pvcs/oimcluster-oim-pv.yaml -n oimcluster $ kubectl create -f output_oimcluster/pv-pvcs/oimcluster-oim-pvc.yaml -n oimcluster The output will look similar to the following:\npersistentvolume/oimcluster-oim-pv created persistentvolumeclaim/oimcluster-oim-pvc created   Run the following commands to verify the PV and PVC were created successfully:\n$ kubectl describe pv \u0026lt;pv_name\u0026gt; $ kubectl describe pvc \u0026lt;pvc_name\u0026gt; -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl describe pv oimcluster-oim-pv $ kubectl describe pvc oimcluster-oim-pvc -n oimcluster The output will look similar to the following:\n$ kubectl describe pv oimcluster-oim-pv Name: oimcluster-oim-pv Labels: weblogic.domainUID=oimcluster Annotations: pv.kubernetes.io/bound-by-controller: yes Finalizers: [kubernetes.io/pv-protection] StorageClass: oimcluster-oim-storage-class Status: Bound Claim: oimcluster/oimcluster-oim-pvc Reclaim Policy: Retain Access Modes: RWX VolumeMode: Filesystem Capacity: 10Gi Node Affinity: \u0026lt;none\u0026gt; Message: Source: Type: NFS (an NFS mount that lasts the lifetime of a pod) Server: mynfsserver Path: /scratch/OIGDockerK8S/oimclusterdomainpv ReadOnly: false Events: \u0026lt;none\u0026gt; $ kubectl describe pvc oimcluster-oim-pvc -n oimcluster Name: oimcluster-oim-pvc Namespace: oimcluster StorageClass: oimcluster-oim-storage-class Status: Bound Volume: oimcluster-oim-pv Labels: weblogic.domainUID=oimcluster Annotations: pv.kubernetes.io/bind-completed: yes pv.kubernetes.io/bound-by-controller: yes Finalizers: [kubernetes.io/pvc-protection] Capacity: 10Gi Access Modes: RWX VolumeMode: Filesystem Mounted By: \u0026lt;none\u0026gt; Events: \u0026lt;none\u0026gt; You are now ready to create the OIG domain as per Create OIG Domains\n  "
},
{
	"uri": "/fmw-kubernetes/20.4.1/oam/manage-oam-domains/wlst-admin-operations/",
	"title": "WLST Administration Operations",
	"tags": [],
	"description": "Describes the steps for WLST administration using helper pod running in the same Kubernetes Cluster as OAM Domain.",
	"content": "To use WLST to administer the OAM domain, use the helper pod in the same Kubernetes cluster as the OAM Domain.\n  Run the following command to start a bash shell in the helper pod (if one is not already running):\n$ kubectl exec -it helper -n \u0026lt;domain_namespace\u0026gt; -- /bin/bash For example:\n$ kubectl exec -it helper -n accessns -- /bin/bash This will take you into a bash shell in the running helper pod:\n[oracle@helper ~]$   Connect to WLST using the following command:\ncd $ORACLE_HOME/oracle_common/common/bin ./wlst.sh The output will look similar to the following:\nInitializing WebLogic Scripting Tool (WLST) ... Jython scans all the jar files it can find at first startup. Depending on the system, this process may take a few minutes to complete, and WLST may not return a prompt right away. Welcome to WebLogic Server Administration Scripting Shell Type help() for help on available commands wls:/offline\u0026gt;   To access t3 for the Administration Server connect as follows:\nconnect(\u0026#39;weblogic\u0026#39;,\u0026#39;\u0026lt;password\u0026gt;\u0026#39;,\u0026#39;t3://accessinfra-adminserver:7001\u0026#39;) The output will look similar to the following:\nConnecting to t3://accessinfra-adminserver:7001 with userid weblogic ... Successfully connected to Admin Server \u0026#34;AdminServer\u0026#34; that belongs to domain \u0026#34;accessinfra\u0026#34;. Warning: An insecure protocol was used to connect to the server. To ensure on-the-wire security, the SSL port or Admin port should be used instead. wls:/accessinfra/serverConfig/\u0026gt; Or to access t3 for the OAM Cluster service, connect as follows:\nconnect(\u0026#39;weblogic\u0026#39;,\u0026#39;\u0026lt;password\u0026gt;\u0026#39;,\u0026#39;t3://accessinfra-cluster-oam-cluster:14100\u0026#39;) The output will look similar to the following:\nConnecting to t3://accessinfra-cluster-oam-cluster:14100 with userid weblogic ... Successfully connected to managed Server \u0026#34;oam_server1\u0026#34; that belongs to domain \u0026#34;accessinfra\u0026#34;. Warning: An insecure protocol was used to connect to the server. To ensure on-the-wire security, the SSL port or Admin port should be used instead. wls:/accessinfra/serverConfig/\u0026gt;   Sample operations For a full list of WLST operations refer to WebLogic Server WLST Online and Offline Command Reference.\nDisplay servers wls:/accessinfra/serverConfig/\u0026gt; cd(\u0026#39;/Servers\u0026#39;) wls:/accessinfra/serverConfig/Servers\u0026gt; ls() dr-- AdminServer dr-- oam_policy_mgr1 dr-- oam_policy_mgr2 dr-- oam_policy_mgr3 dr-- oam_policy_mgr4 dr-- oam_policy_mgr5 dr-- oam_server1 dr-- oam_server2 dr-- oam_server3 dr-- oam_server4 dr-- oam_server5 wls:/accessinfra/serverConfig/Servers\u0026gt; Configure logging for managed servers Connect to the Administration Server and run the following:\nwls:/accessinfra/serverConfig/\u0026gt; domainRuntime() Location changed to domainRuntime tree. This is a read-only tree with DomainMBean as the root MBean. For more help, use help(\u0026#39;domainRuntime\u0026#39;) wls:/accessinfra/domainRuntime/\u0026gt; wls:/accessinfra/domainRuntime/\u0026gt; listLoggers(pattern=\u0026#34;oracle.oam.*\u0026#34;,target=\u0026#34;oam_server1\u0026#34;) ------------------------------------------+----------------- Logger | Level ------------------------------------------+----------------- oracle.oam | \u0026lt;Inherited\u0026gt; oracle.oam.admin.foundation.configuration | \u0026lt;Inherited\u0026gt; oracle.oam.admin.service.config | \u0026lt;Inherited\u0026gt; oracle.oam.agent | \u0026lt;Inherited\u0026gt; oracle.oam.agent-default | \u0026lt;Inherited\u0026gt; oracle.oam.audit | \u0026lt;Inherited\u0026gt; oracle.oam.binding | \u0026lt;Inherited\u0026gt; oracle.oam.certvalidation | \u0026lt;Inherited\u0026gt; oracle.oam.certvalidation.mbeans | \u0026lt;Inherited\u0026gt; oracle.oam.common.healthcheck | \u0026lt;Inherited\u0026gt; oracle.oam.common.runtimeent | \u0026lt;Inherited\u0026gt; oracle.oam.commonutil | \u0026lt;Inherited\u0026gt; oracle.oam.config | \u0026lt;Inherited\u0026gt; oracle.oam.controller | \u0026lt;Inherited\u0026gt; oracle.oam.credcollector | \u0026lt;Inherited\u0026gt; oracle.oam.default | \u0026lt;Inherited\u0026gt; oracle.oam.diagnostic | \u0026lt;Inherited\u0026gt; oracle.oam.engine.authn | \u0026lt;Inherited\u0026gt; oracle.oam.engine.authz | \u0026lt;Inherited\u0026gt; oracle.oam.engine.policy | \u0026lt;Inherited\u0026gt; oracle.oam.engine.ptmetadata | \u0026lt;Inherited\u0026gt; oracle.oam.engine.session | \u0026lt;Inherited\u0026gt; oracle.oam.engine.sso | \u0026lt;Inherited\u0026gt; oracle.oam.engine.token | \u0026lt;Inherited\u0026gt; oracle.oam.esso | \u0026lt;Inherited\u0026gt; oracle.oam.extensibility.lifecycle | \u0026lt;Inherited\u0026gt; oracle.oam.foundation.access | \u0026lt;Inherited\u0026gt; oracle.oam.idm | \u0026lt;Inherited\u0026gt; oracle.oam.install | \u0026lt;Inherited\u0026gt; oracle.oam.install.bootstrap | \u0026lt;Inherited\u0026gt; oracle.oam.install.mbeans | \u0026lt;Inherited\u0026gt; oracle.oam.ipf.rest.api | \u0026lt;Inherited\u0026gt; oracle.oam.oauth | \u0026lt;Inherited\u0026gt; oracle.oam.plugin | \u0026lt;Inherited\u0026gt; oracle.oam.proxy.oam | \u0026lt;Inherited\u0026gt; oracle.oam.proxy.oam.workmanager | \u0026lt;Inherited\u0026gt; oracle.oam.proxy.opensso | \u0026lt;Inherited\u0026gt; oracle.oam.proxy.osso | \u0026lt;Inherited\u0026gt; oracle.oam.pswd.service.provider | \u0026lt;Inherited\u0026gt; oracle.oam.replication | \u0026lt;Inherited\u0026gt; oracle.oam.user.identity.provider | \u0026lt;Inherited\u0026gt; wls:/accessinfra/domainRuntime/\u0026gt; Set the log level to TRACE:32:\nwls:/accessinfra/domainRuntime/\u0026gt; setLogLevel(target=\u0026#39;oam_server1\u0026#39;,logger=\u0026#39;oracle.oam\u0026#39;,level=\u0026#39;TRACE:32\u0026#39;,persist=\u0026#34;1\u0026#34;,addLogger=1) wls:/accessinfra/domainRuntime/\u0026gt; wls:/accessinfra/domainRuntime/\u0026gt; listLoggers(pattern=\u0026#34;oracle.oam.*\u0026#34;,target=\u0026#34;oam_server1\u0026#34;) ------------------------------------------+----------------- Logger | Level ------------------------------------------+----------------- oracle.oam | TRACE:32 oracle.oam.admin.foundation.configuration | \u0026lt;Inherited\u0026gt; oracle.oam.admin.service.config | \u0026lt;Inherited\u0026gt; oracle.oam.agent | \u0026lt;Inherited\u0026gt; oracle.oam.agent-default | \u0026lt;Inherited\u0026gt; oracle.oam.audit | \u0026lt;Inherited\u0026gt; oracle.oam.binding | \u0026lt;Inherited\u0026gt; oracle.oam.certvalidation | \u0026lt;Inherited\u0026gt; oracle.oam.certvalidation.mbeans | \u0026lt;Inherited\u0026gt; oracle.oam.common.healthcheck | \u0026lt;Inherited\u0026gt; oracle.oam.common.runtimeent | \u0026lt;Inherited\u0026gt; oracle.oam.commonutil | \u0026lt;Inherited\u0026gt; oracle.oam.config | \u0026lt;Inherited\u0026gt; oracle.oam.controller | \u0026lt;Inherited\u0026gt; oracle.oam.credcollector | \u0026lt;Inherited\u0026gt; oracle.oam.default | \u0026lt;Inherited\u0026gt; oracle.oam.diagnostic | \u0026lt;Inherited\u0026gt; oracle.oam.engine.authn | \u0026lt;Inherited\u0026gt; oracle.oam.engine.authz | \u0026lt;Inherited\u0026gt; oracle.oam.engine.policy | \u0026lt;Inherited\u0026gt; oracle.oam.engine.ptmetadata | \u0026lt;Inherited\u0026gt; oracle.oam.engine.session | \u0026lt;Inherited\u0026gt; oracle.oam.engine.sso | \u0026lt;Inherited\u0026gt; oracle.oam.engine.token | \u0026lt;Inherited\u0026gt; oracle.oam.esso | \u0026lt;Inherited\u0026gt; oracle.oam.extensibility.lifecycle | \u0026lt;Inherited\u0026gt; oracle.oam.foundation.access | \u0026lt;Inherited\u0026gt; oracle.oam.idm | \u0026lt;Inherited\u0026gt; oracle.oam.install | \u0026lt;Inherited\u0026gt; oracle.oam.install.bootstrap | \u0026lt;Inherited\u0026gt; oracle.oam.install.mbeans | \u0026lt;Inherited\u0026gt; oracle.oam.ipf.rest.api | \u0026lt;Inherited\u0026gt; oracle.oam.oauth | \u0026lt;Inherited\u0026gt; oracle.oam.plugin | \u0026lt;Inherited\u0026gt; oracle.oam.proxy.oam | \u0026lt;Inherited\u0026gt; oracle.oam.proxy.oam.workmanager | \u0026lt;Inherited\u0026gt; oracle.oam.proxy.opensso | \u0026lt;Inherited\u0026gt; oracle.oam.proxy.osso | \u0026lt;Inherited\u0026gt; oracle.oam.pswd.service.provider | \u0026lt;Inherited\u0026gt; oracle.oam.replication | \u0026lt;Inherited\u0026gt; oracle.oam.user.identity.provider | \u0026lt;Inherited\u0026gt; wls:/accessinfra/domainRuntime/\u0026gt; Verify that TRACE:32 log level is set by connecting to the Administration Server and viewing the logs:\n$ kubectl exec -it accessinfra-adminserver -n accessns -- /bin/bash [oracle@accessinfra-adminserver oracle]$ [oracle@accessinfra-adminserver oracle]$ cd /u01/oracle/user_projects/domains/accessinfra/servers/oam_server1/logs [oracle@accessinfra-adminserver logs]$ tail oam_server1-diagnostic.log [2020-09-25T09:02:19.492+00:00] [oam_server1] [TRACE:32] [] [oracle.oam.config] [tid: Configuration Store Observer] [userId: \u0026lt;anonymous\u0026gt;] [ecid: 0dc53783-fada-4709-b7c1-8958bbbaac95-0000000b,0:1062] [APP: oam_server] [partition-name: DOMAIN] [tenant-name: GLOBAL] [SRC_CLASS: oracle.security.am.admin.config.util.store.DbStore] [SRC_METHOD: getSelectSQL] SELECT SQL:SELECT version from IDM_OBJECT_STORE where id = ? and version = (select max(version) from IDM_OBJECT_STORE where id = ?) [2020-09-25T09:02:19.494+00:00] [oam_server1] [TRACE] [] [oracle.oam.config] [tid: Configuration Store Observer] [userId: \u0026lt;anonymous\u0026gt;] [ecid: 0dc53783-fada-4709-b7c1-8958bbbaac95-0000000b,0:1062] [APP: oam_server] [partition-name: DOMAIN] [tenant-name: GLOBAL] [SRC_CLASS: oracle.security.am.admin.config.util.store.DbStore] [SRC_METHOD: load] Time (ms) to load key CONFIG:-1{FIELD_TYPES=INT, SELECT_FIELDS=SELECT version from IDM_OBJECT_STORE }:3 [2020-09-25T09:02:19.494+00:00] [oam_server1] [TRACE:16] [] [oracle.oam.config] [tid: Configuration Store Observer] [userId: \u0026lt;anonymous\u0026gt;] [ecid: 0dc53783-fada-4709-b7c1-8958bbbaac95-0000000b,0:1062] [APP: oam_server] [partition-name: DOMAIN] [tenant-name: GLOBAL] [SRC_CLASS: oracle.security.am.admin.config.util.store.DbStore] [SRC_METHOD: load] RETURN [2020-09-25T09:02:20.050+00:00] [oam_server1] [TRACE:16] [] [oracle.oam.engine.session] [tid: OAM SME Service - 2] [userId: \u0026lt;anonymous\u0026gt;] [ecid: 0dc53783-fada-4709-b7c1-8958bbbaac95-0000000b,0:1777] [APP: oam_server] [partition-name: DOMAIN] [tenant-name: GLOBAL] [SRC_CLASS: oracle.security.am.engines.sme.mgrdb.SessionManagerImpl$3] [SRC_METHOD: run] ENTRY [2020-09-25T09:02:20.057+00:00] [oam_server1] [TRACE] [] [oracle.oam.engine.session] [tid: OAM SME Service - 2] [userId: \u0026lt;anonymous\u0026gt;] [ecid: 0dc53783-fada-4709-b7c1-8958bbbaac95-0000000b,0:1777] [APP: oam_server] [partition-name: DOMAIN] [tenant-name: GLOBAL] [SRC_CLASS: oracle.security.am.engines.sme.mgrdb.SessionManagerImpl$3] [SRC_METHOD: run] Session Store Current status: UP, at time: Fri Sep 25 09:02:20 GMT 2020. Previous known status: UP. Polling Interval: 15000 milliseconds [2020-09-25T09:02:20.057+00:00] [oam_server1] [TRACE:16] [] [oracle.oam.engine.session] [tid: OAM SME Service - 2] [userId: \u0026lt;anonymous\u0026gt;] [ecid: 0dc53783-fada-4709-b7c1-8958bbbaac95-0000000b,0:1777] [APP: oam_server] [partition-name: DOMAIN] [tenant-name: GLOBAL] [SRC_CLASS: oracle.security.am.engines.sme.mgrdb.SessionManagerImpl$3] [SRC_METHOD: run] RETURN [2020-09-25T09:02:22.602+00:00] [oam_server1] [NOTIFICATION] [] [oracle.wsm.agent.handler.jaxrs.RESTJeeResourceFilter] [tid: [ACTIVE].ExecuteThread: \u0026#39;9\u0026#39; for queue: \u0026#39;weblogic.kernel.Default (self-tuning)\u0026#39;] [userId: weblogic] [ecid: 0dc53783-fada-4709-b7c1-8958bbbaac95-000000c8,0] [APP: wls-management-services] [partition-name: DOMAIN] [tenant-name: GLOBAL] Tenant: default, ProcessResponse is set to false [2020-09-25T09:02:27.608+00:00] [oam_server1] [NOTIFICATION] [] [oracle.wsm.agent.handler.jaxrs.RESTJeeResourceFilter] [tid: [ACTIVE].ExecuteThread: \u0026#39;43\u0026#39; for queue: \u0026#39;weblogic.kernel.Default (self-tuning)\u0026#39;] [userId: weblogic] [ecid: 0dc53783-fada-4709-b7c1-8958bbbaac95-000000c9,0] [APP: wls-management-services] [partition-name: DOMAIN] [tenant-name: GLOBAL] Tenant: default, ProcessResponse is set to false Performing WLST Administration via SSL   By default the SSL port is not enabled for the Administration Server or OAM Managed Servers. To configure the SSL port for the Administration Server and Managed Servers login to WebLogic Administration console https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/console and navigate to Lock \u0026amp; Edit -\u0026gt; Environment -\u0026gt;Servers -\u0026gt; server_name -\u0026gt;Configuration -\u0026gt; General -\u0026gt; SSL Listen Port Enabled -\u0026gt; Provide SSL Port ( For Administration Server: 7002 and for OAM Managed Server (oam_server1): 14101) - \u0026gt; Save -\u0026gt; Activate Changes.\nNote: If configuring the OAM Managed Servers for SSL you must enable SSL on the same port for all servers (oam_server1 through oam_server5)\n  Create a myscripts directory as follows:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/scripts $ mkdir myscripts $ cd myscripts   Create a sample yaml template file in the myscripts directory called \u0026lt;domain_uid\u0026gt;-adminserver-ssl.yaml to create a Kubernetes service for the Administration Server:\nNote: Update the domainName, domainUID and namespace based on your environment. For example:\napiVersion: v1 kind: Service metadata: labels: serviceType: SERVER weblogic.domainName: accessinfra weblogic.domainUID: accessinfra weblogic.resourceVersion: domain-v2 weblogic.serverName: AdminServer name: accessinfra-adminserverssl namespace: accessns spec: clusterIP: None ports: - name: default port: 7002 protocol: TCP targetPort: 7002 selector: weblogic.createdByOperator: \u0026#34;true\u0026#34; weblogic.domainUID: accessinfra weblogic.serverName: AdminServer type: ClusterIP and the following sample yaml template file \u0026lt;domain_uid\u0026gt;-oamcluster-ssl.yaml for the OAM Managed Server:\napiVersion: v1 kind: Service metadata: labels: serviceType: SERVER weblogic.domainName: accessinfra weblogic.domainUID: accessinfra weblogic.resourceVersion: domain-v2 name: accessinfra-oamcluster-ssl namespace: accessns spec: clusterIP: None ports: - name: default port: 14101 protocol: TCP targetPort: 14101 selector: weblogic.clusterName: oam_cluster weblogic.createdByOperator: \u0026#34;true\u0026#34; weblogic.domainUID: accessinfra type: ClusterIP   Apply the template using the following command for the AdminServer:\n$ kubectl apply -f \u0026lt;domain_uid\u0026gt;-adminserver-ssl.yaml For example:\n$ kubectl apply -f accessinfra-adminserver-ssl.yaml service/accessinfra-adminserverssl created and using the following command for the OAM Managed Server:\n$ kubectl apply -f \u0026lt;domain_uid\u0026gt;-oamcluster-ssl.yaml For example:\n$ kubectl apply -f accessinfra-oamcluster-ssl.yaml service/accessinfra-oamcluster-ssl created   Validate that the Kubernetes Services to access SSL ports are created successfully:\n$ kubectl get svc -n \u0026lt;domain_namespace\u0026gt; |grep ssl For example:\n$ kubectl get svc -n accessns |grep ssl The output will look similar to the following:\naccessinfra-adminserverssl ClusterIP None \u0026lt;none\u0026gt; 7002/TCP 102s accessinfra-oamcluster-ssl ClusterIP None \u0026lt;none\u0026gt; 14101/TCP 35s   Inside the bash shell of the running helper pod, run the following:\n[oracle@helper bin]$ export WLST_PROPERTIES=\u0026#34;-Dweblogic.security.SSL.ignoreHostnameVerification=true -Dweblogic.security.TrustKeyStore=DemoTrust\u0026#34; [oracle@helper bin]$ cd /u01/oracle/oracle_common/common/bin [oracle@helper bin]$ ./wlst.sh Initializing WebLogic Scripting Tool (WLST) ... Welcome to WebLogic Server Administration Scripting Shell Type help() for help on available commands wls:/offline\u0026gt; To connect to the Administration Server t3s service:\nwls:/offline\u0026gt; connect(\u0026#39;weblogic\u0026#39;,\u0026#39;\u0026lt;password\u0026gt;\u0026#39;,\u0026#39;t3s://accessinfra-adminserverssl:7002\u0026#39;) Connecting to t3s://accessinfra-adminserverssl:7002 with userid weblogic ... \u0026lt;Sep 25, 2020 9:11:24 AM GMT\u0026gt; \u0026lt;Info\u0026gt; \u0026lt;Security\u0026gt; \u0026lt;BEA-090905\u0026gt; \u0026lt;Disabling the CryptoJ JCE Provider self-integrity check for better startup performance. To enable this check, specify -Dweblogic.security.allowCryptoJDefaultJCEVerification=true.\u0026gt; \u0026lt;Sep 25, 2020 9:11:24 AM GMT\u0026gt; \u0026lt;Info\u0026gt; \u0026lt;Security\u0026gt; \u0026lt;BEA-090906\u0026gt; \u0026lt;Changing the default Random Number Generator in RSA CryptoJ from ECDRBG128 to HMACDRBG. To disable this change, specify -Dweblogic.security.allowCryptoJDefaultPRNG=true.\u0026gt; \u0026lt;Sep 25, 2020 9:11:24 AM GMT\u0026gt; \u0026lt;Info\u0026gt; \u0026lt;Security\u0026gt; \u0026lt;BEA-090909\u0026gt; \u0026lt;Using the configured custom SSL Hostname Verifier implementation: weblogic.security.utils.SSLWLSHostnameVerifier$NullHostnameVerifier.\u0026gt; Successfully connected to Admin Server \u0026#34;AdminServer\u0026#34; that belongs to domain \u0026#34;accessinfra\u0026#34;. wls:/accessinfra/serverConfig/\u0026gt; To connect to the OAM Managed Server t3s service:\nwls:/offline\u0026gt; connect(\u0026#39;weblogic\u0026#39;,\u0026#39;\u0026lt;password\u0026gt;\u0026#39;,\u0026#39;t3s://accessinfra-oamcluster-ssl:14101\u0026#39;) Connecting to t3s://accessinfra-oamcluster-ssl:14101 with userid weblogic ... Successfully connected to managed Server \u0026#34;oam_server1\u0026#34; that belongs to domain \u0026#34;accessinfra\u0026#34;.   "
},
{
	"uri": "/fmw-kubernetes/20.4.1/oig/manage-oig-domains/wlst-admin-operations/",
	"title": "WLST Administration Operations",
	"tags": [],
	"description": "Describes the steps for WLST administration using helper pod running in the same Kubernetes Cluster as OIG Domain.",
	"content": "Invoke WLST and Access Administration Server To use WLST to administer the OIG domain, use a helper pod in the same Kubernetes cluster as the OIG Domain.\n  Run the following command to create a helper pod if one doesn\u0026rsquo;t already exist:\n$ kubectl run helper --image \u0026lt;image_name\u0026gt; -n \u0026lt;domain_namespace\u0026gt; -- sleep infinity For example:\n$ kubectl run helper --image oracle/oig:12.2.1.4.0 -n oimcluster -- sleep infinity The output will look similar to the following:\n$ kubectl run helper --image oracle/oig:12.2.1.4.0 -n oimcluster -- sleep infinity pod/helper created   Run the following command to start a bash shell in the helper pod:\n$ kubectl exec -it helper -n \u0026lt;domain_namespace\u0026gt; -- /bin/bash For example:\n$ kubectl exec -it helper -n oimcluster -- /bin/bash This will take you into a bash shell in the running helper pod:\n[oracle@helper ~]$   Connect to WLST using the following commands:\n[oracle@helper ~]$ cd $ORACLE_HOME/oracle_common/common/bin [oracle@helper ~]$ ./wlst.sh The output will look similar to the following:\n[oracle@helper bin]$ ./wlst.sh Initializing WebLogic Scripting Tool (WLST) ... Jython scans all the jar files it can find at first startup. Depending on the system, this process may take a few minutes to complete, and WLST may not return a prompt right away. Welcome to WebLogic Server Administration Scripting Shell Type help() for help on available commands wls:/offline\u0026gt;   To access t3 for the Administration Server connect as follows:\nconnect('weblogic','\u0026lt;password\u0026gt;','t3://oimcluster-adminserver:7001') The output will look similar to the following:\nwls:/offline\u0026gt; connect('weblogic','\u0026lt;password\u0026gt;','t3://oimcluster-adminserver:7001') Connecting to t3://oimcluster-adminserver:7001 with userid weblogic ... Successfully connected to Admin Server \u0026quot;AdminServer\u0026quot; that belongs to domain \u0026quot;oimcluster\u0026quot;. Warning: An insecure protocol was used to connect to the server. To ensure on-the-wire security, the SSL port or Admin port should be used instead. wls:/oimcluster/serverConfig/\u0026gt; Or to access t3 for the OIG Cluster service, connect as follows:\nconnect('weblogic','\u0026lt;password\u0026gt;','t3://oimcluster-cluster-oim-cluster:14100') The output will look similar to the following:\nwls:/offline\u0026gt; connect('weblogic','\u0026lt;password\u0026gt;','t3://oimcluster-cluster-oim-cluster:14000') Connecting to t3://oimcluster-cluster-oim-cluster:14000 with userid weblogic ... Successfully connected to managed Server \u0026quot;oim_server1\u0026quot; that belongs to domain \u0026quot;oimcluster\u0026quot;. Warning: An insecure protocol was used to connect to the server. To ensure on-the-wire security, the SSL port or Admin port should be used instead. wls:/oimcluster/serverConfig/\u0026gt;   Sample operations For a full list of WLST operations refer to WebLogic Server WLST Online and Offline Command Reference.\nDisplay servers wls:/oimcluster/serverConfig/\u0026gt; cd('/Servers') wls:/oimcluster/serverConfig/Servers\u0026gt; ls () dr-- AdminServer dr-- oim_server1 dr-- oim_server2 dr-- oim_server3 dr-- oim_server4 dr-- oim_server5 dr-- soa_server1 dr-- soa_server2 dr-- soa_server3 dr-- soa_server4 dr-- soa_server5 wls:/oimcluster/serverConfig/Servers\u0026gt; Performing WLST Administration via SSL   By default the SSL port is not enabled for the Administration Server or OIG Managed Servers. To configure the SSL port for the Administration Server and Managed Servers login to WebLogic Administration console https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/console and navigate to Lock \u0026amp; Edit -\u0026gt; Environment -\u0026gt;Servers -\u0026gt; server_name -\u0026gt;Configuration -\u0026gt; General -\u0026gt; SSL Listen Port Enabled -\u0026gt; Provide SSL Port ( For Administration Server: 7002 and for OIG Managed Server (oim_server1): 14101) - \u0026gt; Save -\u0026gt; Activate Changes.\nNote: If configuring the OIG Managed Servers for SSL you must enable SSL on the same port for all servers (oim_server1 through oim_server4)\n  Create a myscripts directory as follows:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/scripts $ mkdir myscripts $ cd myscripts   Create a sample yaml template file in the myscripts directory called \u0026lt;domain_uid\u0026gt;-adminserver-ssl.yaml to create a Kubernetes service for the Administration Server:\nNote: Update the domainName, domainUID and namespace based on your environment.\napiVersion: v1 kind: Service metadata: labels: serviceType: SERVER weblogic.domainName: oimcluster weblogic.domainUID: oimcluster weblogic.resourceVersion: domain-v2 weblogic.serverName: AdminServer name: oimcluster-adminserver-ssl namespace: oimcluster spec: clusterIP: None ports: - name: default port: 7002 protocol: TCP targetPort: 7002 selector: weblogic.createdByOperator: \u0026quot;true\u0026quot; weblogic.domainUID: oimcluster weblogic.serverName: AdminServer type: ClusterIP and create the following sample yaml template file \u0026lt;domain_uid\u0026gt;-oim-cluster-ssl.yaml for the OIG Managed Server:\napiVersion: v1 kind: Service metadata: labels: serviceType: SERVER weblogic.domainName: oimcluster weblogic.domainUID: oimcluster weblogic.resourceVersion: domain-v2 name: oimcluster-cluster-oim-cluster-ssl namespace: oimcluster spec: clusterIP: None ports: - name: default port: 14101 protocol: TCP targetPort: 14101 selector: weblogic.clusterName: oim_cluster weblogic.createdByOperator: \u0026quot;true\u0026quot; weblogic.domainUID: oimcluster type: ClusterIP   Apply the template using the following command for the Administration Server:\n$ kubectl apply -f oimcluster-adminserver-ssl.yaml service/oimcluster-adminserver-ssl created or using the following command for the OIG Managed Server:\n$ kubectl apply -f oimcluster-oim-cluster-ssl.yaml service/oimcluster-cluster-oim-cluster-ssl created   Validate that the Kubernetes Services to access SSL ports are created successfully:\n$ kubectl get svc -n \u0026lt;domain_namespace\u0026gt; |grep ssl For example:\n$ kubectl get svc -n oimcluster |grep ssl The output will look similar to the following:\noimcluster-adminserver-ssl ClusterIP None \u0026lt;none\u0026gt; 7002/TCP 74s oimcluster-cluster-oim-cluster-ssl ClusterIP None \u0026lt;none\u0026gt; 14101/TCP 21s   Connect to a bash shell of the helper pod:\n$ kubectl exec -it helper -n oimcluster -- /bin/bash   In the bash shell run the following:\n[oracle@oimcluster-adminserver oracle]$ export WLST_PROPERTIES=\u0026quot;-Dweblogic.security.SSL.ignoreHostnameVerification=true -Dweblogic.security.TrustKeyStore=DemoTrust\u0026quot; [oracle@oimcluster-adminserver oracle]$ cd /u01/oracle/oracle_common/common/bin [oracle@oimcluster-adminserver oracle]$ ./wlst.sh Initializing WebLogic Scripting Tool (WLST) ... Welcome to WebLogic Server Administration Scripting Shell Type help() for help on available commands wls:/offline\u0026gt; Connect to the Administration Server t3s service:\nwls:/offline\u0026gt; connect('weblogic','\u0026lt;password\u0026gt;','t3s://oimcluster-adminserver-ssl:7002') \u0026lt;Sep 30, 2020 3:16:48 PM GMT\u0026gt; \u0026lt;Info\u0026gt; \u0026lt;Security\u0026gt; \u0026lt;BEA-090905\u0026gt; \u0026lt;Disabling the CryptoJ JCE Provider self-integrity check for better startup performance. To enable this check, specify -Dweblogic.security.allowCryptoJDefaultJCEVerification=true.\u0026gt; \u0026lt;Sep 30, 2020 3:16:48 PM GMT\u0026gt; \u0026lt;Info\u0026gt; \u0026lt;Security\u0026gt; \u0026lt;BEA-090906\u0026gt; \u0026lt;Changing the default Random Number Generator in RSA CryptoJ from ECDRBG128 to HMACDRBG. To disable this change, specify -Dweblogic.security.allowCryptoJDefaultPRNG=true.\u0026gt; \u0026lt;Sep 30, 2020 3:16:48 PM GMT\u0026gt; \u0026lt;Info\u0026gt; \u0026lt;Security\u0026gt; \u0026lt;BEA-090909\u0026gt; \u0026lt;Using the configured custom SSL Hostname Verifier implementation: weblogic.security.utils.SSLWLSHostnameVerifier$NullHostnameVerifier.\u0026gt; Successfully connected to Admin Server \u0026quot;AdminServer\u0026quot; that belongs to domain \u0026quot;oimcluster\u0026quot;. wls:/oimcluster/serverConfig/\u0026gt; To connect to the OIG Managed Server t3s service:\nwls:/offline\u0026gt; connect('weblogic','\u0026lt;password\u0026gt;','t3s://oimcluster-cluster-oim-cluster-ssl:14101') Connecting to t3s://oimcluster-cluster-oim-cluster-ssl:14101 with userid weblogic ... Successfully connected to managed Server \u0026quot;oim_server1\u0026quot; that belongs to domain \u0026quot;oimcluster\u0026quot;. wls:/oimcluster/serverConfig/\u0026gt;   "
},
{
	"uri": "/fmw-kubernetes/20.4.1/oud/create-oud-instances/",
	"title": "Create Oracle Unified Directory Instances Using Samples",
	"tags": [],
	"description": "Samples for deploying Oracle Unified Directory instances to a Kubernetes POD.",
	"content": " Introduction Preparing the Environment for Container Creation  Create Kubernetes Namespace Create Secrets for User IDs and Passwords Prepare a Host Directory to be used for Filesystem Based PersistentVolume Create PersistentVolume (PV) and PersistentVolumeClaim (PVC) for your Namespace   Directory Server (instanceType=Directory) Directory Server (instanceType=Directory) as a Kubernetes Service Proxy Server (instanceType=Proxy) as a Kubernetes Service Replication Server (instanceType=Replication) as a Kubernetes Service Directory Server/Service added to existing Replication Server/Service (instanceType=AddDS2RS) Appendix A : Reference  Introduction The Oracle Unified Directory deployment scripts provided in the samples directory of this project demonstrate the creation of different types of Oracle Unified Directory Instances (Directory Service, Proxy, Replication) in containers within a Kubernetes environment.\nNote: The sample files to assist you in creating and configuring your Oracle Unified Directory Kubernetes environment can be found in the project at the following location:\nhttps://github.com/oracle/fmw-kubernetes/tree/master/OracleUnifiedDirectory/kubernetes/samples\nPreparing the Environment for Container Creation In this section you prepare the environment for the Oracle Unified Directory container creation. This involves the following steps:\n Create Kubernetes Namespace Create Secrets for User IDs and Passwords Prepare a host directory to be used for Filesystem based PersistentVolume Create PersistentVolume (PV) and PersistentVolumeClaim (PVC) for your Namespace  Note: Sample files to assist you in creating and configuring your Oracle Unified Directory Kubernetes environment can be found in the project at the following location:\nhttps://github.com/oracle/fmw-kubernetes/tree/master/OracleUnifiedDirectory/kubernetes/samples\nCreate Kubernetes Namespace You should create a Kubernetes namespace to provide a scope for other objects such as pods and services that you create in the environment. To create your namespace you should refer to the oudns.yaml file.\nUpdate the oudns.yaml file and replace %NAMESPACE% with the value of the namespace you would like to create. In the example below the value \u0026lsquo;myoudns\u0026rsquo; is used.\nTo create the namespace apply the file using kubectl:\n$ kubectl apply -f oudns.yaml namespace/myoudns created Confirm that the namespace is created:\n$ kubectl get namespaces NAME STATUS AGE default Active 4d kube-public Active 4d kube-system Active 4d myoudns Active 53s Create Secrets for User IDs and Passwords To protect sensitive information, namely user IDs and passwords, you should create Kubernetes Secrets for the key-value pairs with following keys. The Secret with key-value pairs will be used to pass values to containers created through the Oracle Unified Directory image:\n rootUserDN rootUserPassword adminUID adminPassword bindDN1 bindPassword1 bindDN2 bindPassword2  There are two ways by which a Kubernetes secret object can be created with required key-value pairs.\nUsing samples/secrets.yaml file In this method you update the samples/secrets.yaml file with the value for %SECRET_NAME% and %NAMESPACE%, together with the Base64 value for each secret.\n %rootUserDN% - With Base64 encoded value for rootUserDN parameter. %rootUserPassword% - With Base64 encoded value for rootUserPassword parameter. %adminUID% - With Base64 encoded value for adminUID parameter. %adminPassword% - With Base64 encoded value for adminPassword parameter. %bindDN1% - With Base64 encoded value for bindDN1 parameter. %bindPassword1% - With Base64 encoded value for bindPassword1 parameter. %bindDN2% - With Base64 encoded value for bindDN2 parameter. %bindPassword2% - With Base64 encoded value for bindPassword2 parameter.  Obtain the base64 value for your secrets, for example:\n$ echo -n cn=Directory Manager | base64 Y249RGlyZWN0b3J5IE1hbmFnZXI= $ echo -n Oracle123 | base64 T3JhY2xlMTIz $ echo -n admin | base64 YWRtaW4= Note: Ensure that you use the -n parameter with the echo command. If the parameter is omitted Base64 values willbe generated with a new-line character included.\nUpdate the secrets.yaml file with your values. It should look similar to the file shown below:\napiVersion: v1 kind: Secret metadata: name: oudsecret namespace: myoudns type: Opaque data: rootUserDN: Y249RGlyZWN0b3J5IE1hbmFnZXI= rootUserPassword: T3JhY2xlMTIz adminUID: YWRtaW4= adminPassword: T3JhY2xlMTIz bindDN1: Y249RGlyZWN0b3J5IE1hbmFnZXI= bindPassword1: T3JhY2xlMTIz bindDN2: Y249RGlyZWN0b3J5IE1hbmFnZXI= bindPassword2: T3JhY2xlMTIz Apply the file:\n$ kubectl apply -f secrets.yaml secret/oudsecret created Verify that the secret has been created:\n$ kubectl --namespace myoudns get secret NAME TYPE DATA AGE default-token-fztcb kubernetes.io/service-account-token 3 15m oudsecret Opaque 8 99s Using kubectl create secret command The Kubernetes secret can be created using the command line with the following syntax:\n$ kubectl --namespace %NAMESPACE% create secret generic %SECRET_NAME% \\ --from-literal=rootUserDN=\u0026quot;%rootUserDN%\u0026quot; \\ --from-literal=rootUserPassword=\u0026quot;%rootUserPassword%\u0026quot; \\ --from-literal=adminUID=\u0026quot;%adminUID%\u0026quot; \\ --from-literal=adminPassword=\u0026quot;%adminPassword%\u0026quot; \\ --from-literal=bindDN1=\u0026quot;%bindDN1%\u0026quot; \\ --from-literal=bindPassword1=\u0026quot;%bindPassword1%\u0026quot; \\ --from-literal=bindDN2=\u0026quot;%bindDN2%\u0026quot; \\ --from-literal=bindPassword2=\u0026quot;%bindPassword2%\u0026quot; Update the following placeholders in the command with the relevant value:\n %NAMESPACE% - With name of namespace in which secret is required to be created %SECRET_NAME% - Name for the secret object %rootUserDN% - With Base64 encoded value for rootUserDN parameter. %rootUserPassword% - With Base64 encoded value for rootUserPassword parameter. %adminUID% - With Base64 encoded value for adminUID parameter. %adminPassword% - With Base64 encoded value for adminPassword parameter. %bindDN1% - With Base64 encoded value for bindDN1 parameter. %bindPassword1% - With Base64 encoded value for bindPassword1 parameter. %bindDN2% - With Base64 encoded value for bindDN2 parameter. %bindPassword2% - With Base64 encoded value for bindPassword2 parameter.  After executing the kubectl create secret command, verify that the secret has been created:\n$ kubectl --namespace myoudns get secret NAME TYPE DATA AGE default-token-fztcb kubernetes.io/service-account-token 3 15m oudsecret Opaque 8 99s Prepare a Host Directory to be used for Filesystem Based PersistentVolume It is required to prepare a directory on the Host filesystem to store Oracle Unified Directory Instances and other configuration outside the container filesystem. That directory from the Host filesystem will be associated with a PersistentVolume.\nIn the case of a multi-node Kubernetes cluster, the Host directory to be associated with the PersistentVolume should be accessible on all the nodes at the same path.\nTo prepare a Host directory (for example: /scratch/user_projects) for mounting as a file system based PersistentVolume inside your containers, execute the command below on your Host:\n The userid can be anything but it must have uid:guid as 1000:1000, which is the same as the \u0026lsquo;oracle\u0026rsquo; user running in the container. This ensures the \u0026lsquo;oracle\u0026rsquo; user has access to the shared volume/directory.\n $ sudo su - root $ mkdir -p /scratch/user_projects $ chown 1000:1000 /scratch/user_projects $ exit All container operations are performed as the oracle user.\nNote: If a user already exists with -u 1000 -g 1000 then use the same user. Else modify the existing user to have uid-gid as '-u 1000 -g 1000\u0026rsquo;\nCreate PersistentVolume (PV) and PersistentVolumeClaim (PVC) for your Namespace A PersistentVolume (PV) is a storage resource, while a PersistentVolumeClaim (PVC) is a request for that resource. To provide storage for your namespace, update the persistent-volume.yaml file.\nUpdate the following to values specific to your environment:\n   Param Value Example     %PV_NAME% PV name oudpv   %PV_HOST_PATH% Valid path on localhost /scratch/user_projects   %PVC_NAME% PVC name oudpvc   %NAMESPACE% Namespace myoudns    Apply the file:\n$ kubectl apply -f persistent-volume.yaml persistentvolume/oudpv created persistentvolumeclaim/oudpvc created Verify the PersistentVolume:\n$ kubectl --namespace myoudns describe persistentvolume oudpv Name: oudpv Labels: type=oud-pv Annotations: pv.kubernetes.io/bound-by-controller: yes Finalizers: [kubernetes.io/pv-protection] StorageClass: manual Status: Bound Claim: myoudns/oudpvc Reclaim Policy: Retain Access Modes: RWX VolumeMode: Filesystem Capacity: 10Gi Node Affinity: \u0026lt;none\u0026gt; Message: Source: Type: HostPath (bare host directory volume) Path: /scratch/user_projects HostPathType: Events: \u0026lt;none\u0026gt; Verify the PersistentVolumeClaim:\n$ kubectl --namespace myoudns describe pvc oudpvc Name: oudpvc Namespace: myoudns StorageClass: manual Status: Bound Volume: oudpv Labels: \u0026lt;none\u0026gt; Annotations: pv.kubernetes.io/bind-completed: yes pv.kubernetes.io/bound-by-controller: yes Finalizers: [kubernetes.io/pvc-protection] Capacity: 10Gi Access Modes: RWX VolumeMode: Filesystem Mounted By: \u0026lt;none\u0026gt; Events: \u0026lt;none\u0026gt; Directory Server (instanceType=Directory) In this example you create a POD (oudpod1) which comprises a single container based on an Oracle Unified Directory 12c PS4 (12.2.1.4.0) image.\nTo create the POD update the oud-dir-pod.yaml file.\nUpdate the following to values specific to your environment:\n   Param Value Example     %NAMESPACE% Namespace myoudns   %IMAGE% Oracle image tag oracle/oud:12.2.1.4.0   %SECRET_NAME% Secret name oudsecret   %PV_NAME% PV name oudpv   %PVC_NAME% PVC name oudpvc    Apply the file:\n$ kubectl apply -f oud-dir-pod.yaml pod/oudpod1 created To check the status of the created pod:\n$ kubectl get pods -n myoudns NAME READY STATUS RESTARTS AGE oudpod1 1/1 Running 0 14m If you see any errors then use the following commands to debug the pod/container.\nTo review issues with the pod e.g. CreateContainerConfigError:\n$ kubectl --namespace \u0026lt;namespace\u0026gt; describe pod \u0026lt;pod\u0026gt; For example:\n$ kubectl --namespace myoudns describe pod oudpod1 To tail the container logs while it is initialising use the following command:\n$ kubectl --namespace \u0026lt;namespace\u0026gt; logs -f -c \u0026lt;container\u0026gt; \u0026lt;pod\u0026gt; For example:\n$ kubectl --namespace myoudns logs -f -c oudds1 oudpod1 To view the full container logs:\n$ kubectl --namespace \u0026lt;namespace\u0026gt; logs -c \u0026lt;container\u0026gt; \u0026lt;pod\u0026gt; To validate that the Oracle Unified Directory directory server instance is running, connect to the container:\n$ kubectl --namespace myoudns exec -it -c oudds1 oudpod1 /bin/bash In the container, run ldapsearch to return entries from the directory server:\n$ cd /u01/oracle/user_projects/oudpod1/OUD/bin $ ./ldapsearch -h localhost -p 1389 -D \u0026quot;cn=Directory Manager\u0026quot; -w Oracle123 -b \u0026quot;\u0026quot; -s sub \u0026quot;(objectclass=*)\u0026quot; dn dn: dc=example1,dc=com dn: ou=People,dc=example1,dc=com dn: uid=user.0,ou=People,dc=example1,dc=com ... dn: uid=user.99,ou=People,dc=example1,dc=com Directory Server (instanceType=Directory) as a Kubernetes Service In this example you will create two pods and 2 associated containers, both running Oracle Unified Directory 12c Directory Server instances. This demonstrates how you can expose Oracle Unified Directory 12c as a network service. This provides a way of abstracting access to the backend service independent of the pod details.\nTo create the POD update the oud-dir-svc.yaml file.\nUpdate the following to values specific to your environment:\n   Param Value Example     %NAMESPACE% Namespace myoudns   %IMAGE% Oracle image tag oracle/oud:12.2.1.4.0   %SECRET_NAME% Secret name oudsecret   %PV_NAME% PV name oudpv   %PVC_NAME% PVC name oudpvc    Apply the file:\n$ kubectl apply -f oud-dir-svc.yaml service/oud-dir-svc-1 created pod/oud-dir1 created service/oud-dir-svc-2 created pod/oud-dir2 created To check the status of the created pods (oud-dir1 and oud-dir2) and services (oud-dir-svc-1 and oud-dir-svc-2):\n$ kubectl --namespace myoudns get all NAME READY STATUS RESTARTS AGE pod/oud-dir1 1/1 Running 0 28m pod/oud-dir2 1/1 Running 0 28m pod/oudpod1 1/1 Running 0 22h NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/oud-dir-svc-1 NodePort 10.107.171.235 \u0026lt;none\u0026gt; 1444:30616/TCP,1888:32605/TCP,1389:31405/TCP,1636:32544/TCP,1080:31509/TCP,1081:32395/TCP,1898:31116/TCP 28m service/oud-dir-svc-2 NodePort 10.106.206.229 \u0026lt;none\u0026gt; 1444:30882/TCP,1888:30427/TCP,1389:31299/TCP,1636:31529/TCP,1080:30056/TCP,1081:30458/TCP,1898:31796/TCP 28m From this example you can see that the following service port mappings are available to access the container:\n service/oud-dir-svc-1 : 10.107.171.235 : 1389:31405 service/oud-dir-svc-2 : 10.106.206.229 : 1389:31299  To access the Oracle Unified Directory directory server running in pod/oud-dir1 via the LDAP port 1389 you would use the service port : 31405.\nTo access the Oracle Unified Directory directory server running in pod/oud-dir2 via the LDAP port 1389 you would use the service port : 31299.\nFor example:\nNote: use the ldapsearch from the Oracle Unified Directory ORACLE_HOME when accessing the cluster externally.\n$ ldapsearch -h $HOSTNAME -p 31405 -D \u0026quot;cn=Directory Manager\u0026quot; -w Oracle123 -b \u0026quot;\u0026quot; -s sub \u0026quot;(objectclass=*)\u0026quot; dn dn: dc=example1,dc=com dn: ou=People,dc=example1,dc=com dn: uid=user.0,ou=People,dc=example1,dc=com ... dn: uid=user.98,ou=People,dc=example1,dc=com dn: uid=user.99,ou=People,dc=example1,dc=com $ ldapsearch -h $HOSTNAME -p 31299 -D \u0026quot;cn=Directory Manager\u0026quot; -w Oracle123 -b \u0026quot;\u0026quot; -s sub \u0026quot;(objectclass=*)\u0026quot; dn dn: dc=example2,dc=com dn: ou=People,dc=example2,dc=com dn: uid=user.0,ou=People,dc=example2,dc=com ... dn: uid=user.98,ou=People,dc=example2,dc=com dn: uid=user.99,ou=People,dc=example2,dc=com Validation It is possible to access the Oracle Unified Directory instances and the data within externally from the cluster, using commands like curl commands. In this way you can access interfaces exposed through NodePort. In the example below, two services (service/oud-dir-svc-1 and service/oud-dir-svc-2) expose a set of ports. The following curl commands can be executed against the ports exposed through each service.\nCurl command example for Oracle Unified Directory Admin REST: curl --noproxy \u0026quot;*\u0026quot; --insecure --location --request GET \\ 'https://\u0026lt;HOSTNAME\u0026gt;:\u0026lt;AdminHttps NodePort mapped to 1888\u0026gt;/rest/v1/admin/?scope=base\u0026amp;attributes=%2b' \\ --header 'Content-Type: application/json' \\ --header 'Authorization: Basic Y249RGlyZWN0b3J5IE1hbmFnZXI6T3JhY2xlMTIz' Curl command example for Oracle Unified Directory Data REST : curl --noproxy \u0026quot;*\u0026quot; --insecure --location --request GET \\ 'https://\u0026lt;HOSTNAME\u0026gt;:\u0026lt;Https NodePort mapped to 1081\u0026gt;/rest/v1/directory/?scope=base\u0026amp;attributes=%2b' \\ --header 'Authorization: Basic Y249RGlyZWN0b3J5IE1hbmFnZXI6T3JhY2xlMTIz' Curl command example for Oracle Unified Directory Data SCIM: curl --noproxy \u0026quot;*\u0026quot; --insecure --location --request GET \\ 'https://\u0026lt;HOSTNAME\u0026gt;:\u0026lt;Https NodePort mapped to 1081\u0026gt;/iam/directory/oud/scim/v1/Schemas/urn:ietf:params:scim:schemas:core:2.0:Schema' \\ --header 'Authorization: Basic Y249RGlyZWN0b3J5IE1hbmFnZXI6T3JhY2xlMTIz' Proxy Server (instanceType=Proxy) as a Kubernetes Service In this example you will create a service, pod and associated container, in which an Oracle Unified Directory 12c Proxy Server instance is deployed. This acts as a proxy to the 2 services you created in the previous example.\nTo create the POD update the oud-ds_proxy-svc.yaml file.\nUpdate the following to values specific to your environment:\n   Param Value Example     %NAMESPACE% Namespace myoudns   %IMAGE% Oracle image tag oracle/oud:12.2.1.4.0   %SECRET_NAME% Secret name oudsecret   %PV_NAME% PV name oudpv   %PVC_NAME% PVC name oudpvc    Apply the file:\n$ kubectl apply -f oud-ds_proxy-svc.yaml service/oud-ds-proxy-svc created pod/oudp1 created Check the status of the new pod/service:\n$ kubectl --namespace myoudns get all NAME READY STATUS RESTARTS AGE pod/oud-dir1 1/1 Running 0 166m pod/oud-dir2 1/1 Running 0 166m pod/oudp1 1/1 Running 0 20m pod/oudpod1 1/1 Running 0 25h NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/oud-dir-svc-1 NodePort 10.107.171.235 \u0026lt;none\u0026gt; 1444:30616/TCP,1888:32605/TCP,1389:31405/TCP,1636:32544/TCP,1080:31509/TCP,1081:32395/TCP,1898:31116/TCP 166m service/oud-dir-svc-2 NodePort 10.106.206.229 \u0026lt;none\u0026gt; 1444:30882/TCP,1888:30427/TCP,1389:31299/TCP,1636:31529/TCP,1080:30056/TCP,1081:30458/TCP,1898:31796/TCP 166m service/oud-ds-proxy-svc NodePort 10.103.41.171 \u0026lt;none\u0026gt; 1444:30878/TCP,1888:30847/TCP,1389:31810/TCP,1636:30873/TCP,1080:32076/TCP,1081:30762/TCP,1898:31269/TCP 20m Verify operation of the proxy server, accessing through the external service port:\n$ ldapsearch -h $HOSTNAME -p 31810 -D \u0026quot;cn=Directory Manager\u0026quot; -w Oracle123 -b \u0026quot;\u0026quot; -s sub \u0026quot;(objectclass=*)\u0026quot; dn dn: dc=example1,dc=com dn: ou=People,dc=example1,dc=com dn: uid=user.0,ou=People,dc=example1,dc=com ... dn: uid=user.99,ou=People,dc=example1,dc=com dn: dc=example2,dc=com dn: ou=People,dc=example2,dc=com dn: uid=user.0,ou=People,dc=example2,dc=com ... dn: uid=user.98,ou=People,dc=example2,dc=com dn: uid=user.99,ou=People,dc=example2,dc=com Note: Entries are returned from both backend directory servers (dc=example1,dc=com and dc=example2,dc=com) via the proxy server.\nReplication Server (instanceType=Replication) as a Kubernetes Service In this example you will create a service, pod and associated container, in which an Oracle Unified Directory 12c Replication Server instance is deployed. This creates a single Replication Server which has 2 Directory Servers as its replication group. This example extends the Oracle Unified Directory instances created as part of Directory Server (instanceType=Directory) as a Kubernetes Service.\nTo create the POD update the oud-ds_rs_ds-svc.yaml file.\nUpdate the following to values specific to your environment:\n   Param Value Example     %NAMESPACE% Namespace myoudns   %IMAGE% Oracle image tag oracle/oud:12.2.1.4.0   %SECRET_NAME% Secret name oudsecret   %PV_NAME% PV name oudpv   %PVC_NAME% PVC name oudpvc    Apply the file:\n$ kubectl apply -f oud-ds_rs_ds-svc.yaml service/oud-rs-svc-1 created pod/oudpodrs1 created service/oud-ds-svc-1a created pod/oudpodds1a created service/oud-ds-svc-1b created pod/oudpodds1b created Check the status of the new services:\n$ kubectl --namespace myoudns get all NAME READY STATUS RESTARTS AGE pod/oud-dir1 1/1 Running 0 2d20h pod/oud-dir2 1/1 Running 0 2d20h pod/oudp1 1/1 Running 0 2d18h pod/oudpod1 1/1 Running 0 3d18h pod/oudpodds1a 0/1 Running 0 2m44s pod/oudpodds1b 0/1 Running 0 2m44s pod/oudpodrs1 0/1 Running 0 2m45s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/oud-dir-svc-1 NodePort 10.107.171.235 \u0026lt;none\u0026gt; 1444:30616/TCP,1888:32605/TCP,1389:31405/TCP,1636:32544/TCP,1080:31509/TCP,1081:32395/TCP,1898:31116/TCP 2d20h service/oud-dir-svc-2 NodePort 10.106.206.229 \u0026lt;none\u0026gt; 1444:30882/TCP,1888:30427/TCP,1389:31299/TCP,1636:31529/TCP,1080:30056/TCP,1081:30458/TCP,1898:31796/TCP 2d20h service/oud-ds-proxy-svc NodePort 10.103.41.171 \u0026lt;none\u0026gt; 1444:30878/TCP,1888:30847/TCP,1389:31810/TCP,1636:30873/TCP,1080:32076/TCP,1081:30762/TCP,1898:31269/TCP 2d18h service/oud-ds-svc-1a NodePort 10.102.218.25 \u0026lt;none\u0026gt; 1444:30347/TCP,1888:30392/TCP,1389:32482/TCP,1636:31161/TCP,1080:31241/TCP,1081:32597/TCP 2m45s service/oud-ds-svc-1b NodePort 10.104.6.215 \u0026lt;none\u0026gt; 1444:32031/TCP,1888:31621/TCP,1389:32511/TCP,1636:31698/TCP,1080:30737/TCP,1081:30748/TCP 2m44s service/oud-rs-svc-1 NodePort 10.110.237.193 \u0026lt;none\u0026gt; 1444:32685/TCP,1888:30176/TCP,1898:30543/TCP 2m45s Validation To validate that the Oracle Unified Directory replication group is running, connect to the replication server container (oudrs1):\n$ kubectl --namespace myoudns exec -it -c oudrs1 oudpodrs1 /bin/bash $ cd /u01/oracle/user_projects/oudpodrs1/OUD/bin In the container, run dsreplication to return details of the replication group:\n$ ./dsreplication status --trustAll --hostname localhost --port 1444 --adminUID admin --dataToDisplay compat-view --dataToDisplay rs-connections \u0026gt;\u0026gt;\u0026gt;\u0026gt; Specify Oracle Unified Directory LDAP connection parameters Password for user 'admin': Establishing connections and reading configuration ..... Done. dc=example1,dc=com - Replication Enabled ======================================== Server : Entries : M.C. [1] : A.O.M.C. [2] : Port [3] : Encryption [4] : Trust [5] : U.C. [6] : Status [7] : ChangeLog [8] : Group ID [9] : Connected To [10] --------------------:----------:----------:--------------:----------:----------------:-----------:----------:------------:---------------:--------------:--------------------------- oud-rs-svc-1:1444 : -- [11] : 0 : -- : 1898 : Disabled : -- : -- : Up : -- : 1 : -- oud-ds-svc-1a:1444 : 1 : 0 : 0 : -- [12] : Disabled : Trusted : -- : Normal : Enabled : 1 : oud-rs-svc-1:1898 (GID=1) oud-ds-svc-1b:1444 : 1 : 0 : 0 : -- [12] : Disabled : Trusted : -- : Normal : Enabled : 1 : oud-rs-svc-1:1898 (GID=1) You can see that the Replication Server is running as the oud-rs-svc-1:1444, while you have Directory Server services running on oud-ds-svc-1a:1444 and oud-ds-svc-1b:1444.\nFrom outside the cluster, you can invoke curl commands, as shown in the following examples, to access interfaces exposed through NodePort. In this example, there are two Directory services (service/oud-ds-svc-1a and service/oud-ds-svc-1b) exposing a set of ports. The following curl commands can be executed against ports exposed through each service.\nCurl command example for Oracle Unified Directory Admin REST: curl --noproxy \u0026quot;*\u0026quot; --insecure --location --request GET \\ 'https://\u0026lt;HOSTNAME\u0026gt;:\u0026lt;AdminHttps NodePort mapped to 1888\u0026gt;/rest/v1/admin/?scope=base\u0026amp;attributes=%2b' \\ --header 'Content-Type: application/json' \\ --header 'Authorization: Basic Y249RGlyZWN0b3J5IE1hbmFnZXI6T3JhY2xlMTIz' Note: This can be executed against the replication service (oud-rs-svc-1) as well.\nCurl command example for Oracle Unified Directory Data REST : curl --noproxy \u0026quot;*\u0026quot; --insecure --location --request GET \\ 'https://\u0026lt;HOSTNAME\u0026gt;:\u0026lt;Https NodePort mapped to 1081\u0026gt;/rest/v1/directory/?scope=base\u0026amp;attributes=%2b' \\ --header 'Authorization: Basic Y249RGlyZWN0b3J5IE1hbmFnZXI6T3JhY2xlMTIz' Curl command example for Oracle Unified Directory Data SCIM: curl --noproxy \u0026quot;*\u0026quot; --insecure --location --request GET \\ 'https://\u0026lt;HOSTNAME\u0026gt;:\u0026lt;Https NodePort mapped to 1081\u0026gt;/iam/directory/oud/scim/v1/Schemas/urn:ietf:params:scim:schemas:core:2.0:Schema' \\ --header 'Authorization: Basic Y249RGlyZWN0b3J5IE1hbmFnZXI6T3JhY2xlMTIz' Directory Server/Service added to existing Replication Server/Service (instanceType=AddDS2RS) In this example you will create services, pods and containers, in which Oracle Unified Directory 12c Replication Server instances are deployed. In this case 2 Replication/Directory Server Services are added, in addition the Directory Server created in Directory Server (instanceType=Directory) as a Kubernetes Service (oud-dir-svc-2) is added to the replication group.\nTo create the POD update the oud-ds-plus-rs-svc.yaml file.\nUpdate the following to values specific to your environment:\n   Param Value Example     %NAMESPACE% Namespace myoudns   %IMAGE% Oracle image tag oracle/oud:12.2.1.4.0   %SECRET_NAME% Secret name oudsecret   %PV_NAME% PV name oudpv   %PVC_NAME% PVC name oudpvc    Apply the file:\n$ kubectl apply -f oud-ds-plus-rs-svc.yaml service/oud-dsrs-svc-1 created pod/ouddsrs1 created service/oud-dsrs-svc-2 created pod/ouddsrs2 created Check the status of the new services:\n$ kubectl --namespace myoudns get all NAME READY STATUS RESTARTS AGE pod/oud-dir1 1/1 Running 0 3d pod/oud-dir2 1/1 Running 0 3d pod/ouddsrs1 0/1 Running 0 75s pod/ouddsrs2 0/1 Running 0 75s pod/oudp1 1/1 Running 0 2d21h pod/oudpod1 1/1 Running 0 3d22h pod/oudpodds1a 1/1 Running 0 3h33m pod/oudpodds1b 1/1 Running 0 3h33m pod/oudpodrs1 1/1 Running 0 3h33m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/oud-dir-svc-1 NodePort 10.107.171.235 \u0026lt;none\u0026gt; 1444:30616/TCP,1888:32605/TCP,1389:31405/TCP,1636:32544/TCP,1080:31509/TCP,1081:32395/TCP,1898:31116/TCP 3d service/oud-dir-svc-2 NodePort 10.106.206.229 \u0026lt;none\u0026gt; 1444:30882/TCP,1888:30427/TCP,1389:31299/TCP,1636:31529/TCP,1080:30056/TCP,1081:30458/TCP,1898:31796/TCP 3d service/oud-ds-proxy-svc NodePort 10.103.41.171 \u0026lt;none\u0026gt; 1444:30878/TCP,1888:30847/TCP,1389:31810/TCP,1636:30873/TCP,1080:32076/TCP,1081:30762/TCP,1898:31269/TCP 2d21h service/oud-ds-svc-1a NodePort 10.102.218.25 \u0026lt;none\u0026gt; 1444:30347/TCP,1888:30392/TCP,1389:32482/TCP,1636:31161/TCP,1080:31241/TCP,1081:32597/TCP 3h33m service/oud-ds-svc-1b NodePort 10.104.6.215 \u0026lt;none\u0026gt; 1444:32031/TCP,1888:31621/TCP,1389:32511/TCP,1636:31698/TCP,1080:30737/TCP,1081:30748/TCP 3h33m service/oud-dsrs-svc-1 NodePort 10.102.118.29 \u0026lt;none\u0026gt; 1444:30738/TCP,1888:30935/TCP,1389:32438/TCP,1636:32109/TCP,1080:31776/TCP,1081:31897/TCP,1898:30874/TCP 75s service/oud-dsrs-svc-2 NodePort 10.98.139.53 \u0026lt;none\u0026gt; 1444:32312/TCP,1888:30595/TCP,1389:31376/TCP,1636:30090/TCP,1080:31238/TCP,1081:31174/TCP,1898:31863/TCP 75s service/oud-rs-svc-1 NodePort 10.110.237.193 \u0026lt;none\u0026gt; 1444:32685/TCP,1888:30176/TCP,1898:30543/TCP 3h33m Validation To validate that the Oracle Unified Directory replication group is running, connect to the replication server container (oudrs1):\n$ kubectl --namespace myoudns exec -it -c ouddsrs ouddsrs1 /bin/bash $ cd /u01/oracle/user_projects/ouddsrs1/OUD/bin In the container, run dsreplication to return details of the replication group:\n$ ./dsreplication status --trustAll --hostname localhost --port 1444 --adminUID admin --dataToDisplay compat-view --dataToDisplay rs-connections \u0026gt;\u0026gt;\u0026gt;\u0026gt; Specify Oracle Unified Directory LDAP connection parameters Password for user 'admin': Establishing connections and reading configuration ..... Done. dc=example2,dc=com - Replication Enabled ======================================== Server : Entries : M.C. [1] : A.O.M.C. [2] : Port [3] : Encryption [4] : Trust [5] : U.C. [6] : Status [7] : ChangeLog [8] : Group ID [9] : Connected To [10] --------------------:---------:----------:--------------:----------:----------------:-----------:----------:------------:---------------:--------------:----------------------------- oud-dir-svc-2:1444 : 102 : 0 : 0 : 1898 : Disabled : Trusted : -- : Normal : Enabled : 1 : oud-dir-svc-2:1898 (GID=1) oud-dsrs-svc-1:1444 : 102 : 0 : 0 : 1898 : Disabled : Trusted : -- : Normal : Enabled : 2 : oud-dsrs-svc-1:1898 (GID=2) oud-dsrs-svc-2:1444 : 102 : 0 : 0 : 1898 : Disabled : Trusted : -- : Normal : Enabled : 2 : oud-dsrs-svc-2:1898 (GID=2) Replication Server [11] : RS #1 : RS #2 : RS #3 --------------------------:-------:-------:------- oud-dir-svc-2:1898 (#1) : -- : Yes : Yes oud-dsrs-svc-1:1898 (#2) : Yes : -- : Yes oud-dsrs-svc-2:1898 (#3) : Yes : Yes : -- From outside the cluster, you can invoke curl commands like following for accessing interfaces exposed through NodePort. In this example, there are two services (service/oud-dsrs-svc-1 and service/oud-dsrs-svc-2) exposing set of ports. Following curl commands can be executed against ports exposed through each service.\nCurl command example for Oracle Unified Directory Admin REST: curl --noproxy \u0026quot;*\u0026quot; --insecure --location --request GET \\ 'https://\u0026lt;HOSTNAME\u0026gt;:\u0026lt;AdminHttps NodePort mapped to 1888\u0026gt;/rest/v1/admin/?scope=base\u0026amp;attributes=%2b' \\ --header 'Content-Type: application/json' \\ --header 'Authorization: Basic Y249RGlyZWN0b3J5IE1hbmFnZXI6T3JhY2xlMTIz' Curl command example for Oracle Unified Directory Data REST : curl --noproxy \u0026quot;*\u0026quot; --insecure --location --request GET \\ 'https://\u0026lt;HOSTNAME\u0026gt;:\u0026lt;Https NodePort mapped to 1081\u0026gt;/rest/v1/directory/?scope=base\u0026amp;attributes=%2b' \\ --header 'Authorization: Basic Y249RGlyZWN0b3J5IE1hbmFnZXI6T3JhY2xlMTIz' Curl command example for Oracle Unified Directory Data SCIM: curl --noproxy \u0026quot;*\u0026quot; --insecure --location --request GET \\ 'https://\u0026lt;HOSTNAME\u0026gt;:\u0026lt;Https NodePort mapped to 1081\u0026gt;/iam/directory/oud/scim/v1/Schemas/urn:ietf:params:scim:schemas:core:2.0:Schema' \\ --header 'Authorization: Basic Y249RGlyZWN0b3J5IE1hbmFnZXI6T3JhY2xlMTIz' Appendix A : Reference Before using these sample yaml files, the following variables must be updated:\n %NAMESPACE% - with value for Kubernetes namespace of your choice %IMAGE% - with exact docker image for oracle/oud:12.2.1.x.x %PV_NAME% - with value of the persistent volume name of your choice %PV_HOST_PATH% - with value of the persistent volume Host Path (Directory Path which would be used as storage path for volume) %PVC_NAME% - with value of the persistent volume claim name of your choice %SECRET_NAME% - with value of the secret name which can be created using secrets.yaml file. %rootUserDN% - With Base64 encoded value for rootUserDN parameter. %rootUserPassword% - With Base64 encoded value for rootUserPassword parameter. %adminUID% - With Base64 encoded value for adminUID parameter. %adminPassword% - With Base64 encoded value for adminPassword parameter. %bindDN1% - With Base64 encoded value for bindDN1 parameter. %bindPassword1% - With Base64 encoded value for bindPassword1 parameter. %bindDN2% - With Base64 encoded value for bindDN2 parameter. %bindPassword2% - With Base64 encoded value for bindPassword2 parameter.  oudns.yaml This is a sample file to create a Kubernetes namespace.\npersistent-volume.yaml This is a sample file to create Persistent Volume and Persistent Volume Claim\nsecrets.yaml This is a sample file to create the Kubernetes secrets which can be used to substitute values during Pod creation.\nThe keys below will be honoured by the different Oracle Unified Directory yaml files\n rootUserDN rootUserPassword adminUID adminPassword bindDN1 bindPassword1 bindDN2 bindPassword2  All the values of the keys should be encoded using the command below and the encoded value should be used in the secrets.yaml file.\nTo generate an encoded value for keys in Base64 format, execute the following command:\n$ echo -n 'MyPassword' | base64 TXlQYXNzd29yZA== oud-dir-pod.yaml This is a sample file to create POD (oudpod1) and a container for an Oracle Unified Directory Directory Instance.\noud-ds_proxy-svc.yaml This is a sample file to create:\n POD (oudds1) with container for Oracle Unified Directory Directory Instance (dc=example1,dc=com) POD (oudds2) with container for Oracle Unified Directory Directory Instance (dc=example2,dc=com) POD (oudp1) with container for Oracle Unified Directory Directory Proxy referring to Oracle Unified Directory Directory Instances (oudds1 and oudds2) for dc=example1,dc=com and dc=example2,dc=com Service (oud-ds-proxy-svc) referring to POD with Oracle Unified Directory Directory Proxy (oudp1)  oud-ds_rs_ds-svc.yaml This is a sample file to create:\n POD (oudpodds1) with container for Oracle Unified Directory Directory Instance (dc=example1,dc=com) POD (oudpodrs1) with container for Oracle Unified Directory Replication Server Instance connected to Oracle Unified Directory Directory Instance (oudpodds1) POD (oudpodds1a) with container for Oracle Unified Directory Directory Instance having replication enabled through Replication Server Instance (oudpodrs1) POD (oudpodds1b) with container for Oracle Unified Directory Directory Instance having replication enabled through Replication Server Instance (oudpodrs1) Service (oud-ds-rs-ds-svc) referring to all PODs  The following command can be executed in the container to check the status of the replicated instances:\n$ /u01/oracle/user_projects/oudpodrs1/OUD/bin/dsreplication status \\ --trustAll --hostname oudpodrs1.oud-ds-rs-ds-svc.myoudns.svc.cluster.local --port 1444 \\ --dataToDisplay compat-view oud-ds-plus-rs-svc.yaml This is a sample file to create 3 replicated DS+RS Instances:\n POD (ouddsrs1) with container for Oracle Unified Directory Directory Server (dc=example1,dc=com) and Replication Server POD (ouddsrs2) with container for Oracle Unified Directory Directory Server (dc=example1,dc=com) and Replication Server Service (oud-dsrs-svc) referring to all PODs  The following command can be executed in the container to check the status of the replicated instances:\n$ /u01/oracle/user_projects/ouddsrs1/OUD/bin/dsreplication status \\ --trustAll --hostname ouddsrs1.oud-dsrs-svc.myoudns.svc.cluster.local --port 1444 \\ --dataToDisplay compat-view "
},
{
	"uri": "/fmw-kubernetes/20.4.1/oudsm/create-oudsm-instances/",
	"title": "Create Oracle Unified Directory Services Manager Instances Using Samples",
	"tags": [],
	"description": "Samples for deploying Oracle Unified Directory Services Manager instances to a Kubernetes POD.",
	"content": " Introduction Preparing the Environment for Container Creation  Create Kubernetes Namespace Create Secrets for User IDs and Passwords Prepare a Host Directory to be used for Filesystem Based PersistentVolume Create PersistentVolume (PV) and PersistentVolumeClaim (PVC) for your Namespace   Oracle Unified Directory Services Manager POD Oracle Unified Directory Services Manager Deployment  Introduction The Oracle Unified Directory Services Manager deployment scripts provided in the code repository demonstrate how to deploy Oracle Unified Directory Services Manager in containers within a Kubernetes environment.\nNote: The sample files to assist you in creating and configuring your Oracle Unified Directory Services Manager Kubernetes environment can be found in the project at the following location:\n\u0026lt;work directory\u0026gt;/fmw-kubernetes/OracleUnifiedDirectorySM/kubernetes/samples\nPreparing the Environment for Container Creation In this section you prepare the environment for the Oracle Unified Directory Services Manager container creation. This involves the following steps:\n Create Kubernetes Namespace Create Secrets for User IDs and Passwords Prepare a host directory to be used for Filesystem based PersistentVolume Create PersistentVolume (PV) and PersistentVolumeClaim (PVC) for your Namespace  Create Kubernetes Namespace You should create a Kubernetes namespace to provide a scope for other objects such as pods and services that you create in the environment. To create your namespace you should refer to the oudsmns.yaml file.\nUpdate the oudsmns.yaml file and replace %NAMESPACE% with the value of the namespace you would like to create. In the example below the value \u0026lsquo;myoudsmns\u0026rsquo; is used.\nTo create the namespace apply the file using kubectl:\n$ kubectl apply -f oudsmns.yaml namespace/myoudsmns created Confirm that the namespace is created:\n$ kubectl get namespaces NAME STATUS AGE default Active 4d kube-public Active 4d kube-system Active 4d myoudsmns Active 53s Create Secrets for User IDs and Passwords To protect sensitive information, namely user IDs and passwords, you should create Kubernetes Secrets for the key-value pairs with following keys. The Secret with key-value pairs will be used to pass values to containers created through the OUD image:\n adminUser adminPass  There are two ways by which a Kubernetes secret object can be created with required key-value pairs.\nUsing samples/secrets.yaml file In this method you update the samples/secrets.yaml file with the value for %SECRET_NAME% and %NAMESPACE%, together with the Base64 value for each secret.\n %adminUser% - With Base64 encoded value for adminUser parameter. %adminPass% - With Base64 encoded value for adminPass parameter.  Obtain the base64 value for your secrets, for example:\n$ echo -n weblogic | base64 d2VibG9naWM= $ echo -n Oracle123 | base64 T3JhY2xlMTIz Note: Ensure that you use the -n parameter with the echo command. If the parameter is omitted Base64 values will be generated with a new-line character included.\nUpdate the secrets.yaml file with your values. It should look similar to the file shown below:\napiVersion: v1 kind: Secret metadata: name: oudsmsecret namespace: myoudsmns type: Opaque data: adminUser: d2VibG9naWM= adminPass: T3JhY2xlMTIz Apply the file:\n$ kubectl apply -f secrets.yaml secret/oudsmsecret created Verify that the secret has been created:\n$ kubectl --namespace myoudsmns get secret NAME TYPE DATA AGE default-token-fztcb kubernetes.io/service-account-token 3 15m oudsmsecret Opaque 8 99s Using kubectl create secret command The Kubernetes secret can be created using the command line with the following syntax:\n$ kubectl --namespace %NAMESPACE% create secret generic %SECRET_NAME% \\ --from-literal=adminUser=\u0026quot;%adminUser%\u0026quot; \\ --from-literal=adminPass=\u0026quot;%adminPass%\u0026quot; Update the following placeholders in the command with the relevant value:\n %NAMESPACE% - With name of namespace in which secret is required to be created %SECRET_NAME% - Name for the secret object %adminUser% - With Base64 encoded value for adminUser parameter. %adminPass%- With Base64 encoded value for adminPass parameter.  After executing the kubectl create secret command, verify that the secret has been created:\n$ kubectl --namespace myoudsmns get secret NAME TYPE DATA AGE default-token-fztcb kubernetes.io/service-account-token 3 15m oudsmsecret Opaque 8 99s Prepare a Host Directory to be used for Filesystem Based PersistentVolume It is required to prepare a directory on the Host filesystem to store Oracle Unified Directory Services Manager Instances and other configuration outside the container filesystem. That directory from the Host filesystem will be associated with a PersistentVolume.\nIn the case of a multi-node Kubernetes cluster, the Host directory to be associated with the PersistentVolume should be accessible on all the nodes at the same path.\nTo prepare a Host directory (for example: /scratch/user_projects) for mounting as a file system based PersistentVolume inside your containers, execute the command below on your Host:\n The userid can be anything but it must have uid:guid as 1000:1000, which is the same as the \u0026lsquo;oracle\u0026rsquo; user running in the container. This ensures the \u0026lsquo;oracle\u0026rsquo; user has access to the shared volume/directory.\n $ sudo su - root $ mkdir -p /scratch/user_projects $ chown 1000:1000 /scratch/user_projects $ exit All container operations are performed as the oracle user.\nNote: If a user already exists with -u 1000 -g 1000 then use the same user. Else modify the existing user to have uid-gid as '-u 1000 -g 1000\u0026rsquo;\nCreate PersistentVolume (PV) and PersistentVolumeClaim (PVC) for your Namespace A PersistentVolume (PV) is a storage resource, while a PersistentVolumeClaim (PVC) is a request for that resource. To provide storage for your namespace, update the persistent-volume.yaml file.\nUpdate the following to values specific to your environment:\n   Param Value Example     %PV_NAME% PV name oudsmpv   %PV_HOST_PATH% Valid path on localhost /scratch/user_projects   %PVC_NAME% PVC name oudsmpvc   %NAMESPACE% Namespace myoudsmns    Apply the file:\n$ kubectl apply -f persistent-volume.yaml persistentvolume/oudsmpv created persistentvolumeclaim/oudsmpvc created Verify the PersistentVolume:\n$ kubectl describe persistentvolume oudsmpv Name: oudsmpv Labels: type=oud-pv Annotations: pv.kubernetes.io/bound-by-controller: yes Finalizers: [kubernetes.io/pv-protection] StorageClass: manual Status: Bound Claim: myoudsmns/oudsmpvc Reclaim Policy: Retain Access Modes: RWX VolumeMode: Filesystem Capacity: 10Gi Node Affinity: \u0026lt;none\u0026gt; Message: Source: Type: HostPath (bare host directory volume) Path: /scratch/user_projects HostPathType: Events: \u0026lt;none\u0026gt; Verify the PersistentVolumeClaim:\n$ kubectl --namespace myoudsmns describe pvc oudsmpvc Name: oudsmpvc Namespace: myoudsmns StorageClass: manual Status: Bound Volume: oudsmpv Labels: \u0026lt;none\u0026gt; Annotations: pv.kubernetes.io/bind-completed: yes pv.kubernetes.io/bound-by-controller: yes Finalizers: [kubernetes.io/pvc-protection] Capacity: 10Gi Access Modes: RWX VolumeMode: Filesystem Mounted By: \u0026lt;none\u0026gt; Events: \u0026lt;none\u0026gt; Oracle Unified Directory Services Manager POD In this example you create a POD (oudsmpod) which holds a single container based on an Oracle Unified Directory Services Manager 12c PS4 (12.2.1.4.0) image. This container is configured to run Oracle Unified Directory Services Manager. You also create a service (oudsm) through which you can access the Oracle Unified Directory Services Manager GUI.\nTo create the POD update the samples/oudsm-pod.yaml file.\nUpdate the following parameters to values specific to your environment:\n   Param Value Example     %NAMESPACE% Namespace myoudsmns   %IMAGE% Oracle image tag oracle/oudsm:12.2.1.4.0   %SECRET_NAME% Secret name oudsmsecret   %PV_NAME% PV name oudsmpv   %PVC_NAME% PVC name oudsmpvc    Apply the file:\n$ kubectl apply -f samples/oudsm-pod.yaml service/oudsm-svc created pod/oudsmpod created To check the status of the created pod:\n$ kubectl get pods -n myoudsmns NAME READY STATUS RESTARTS AGE oudsmpod 1/1 Running 0 22m If you see any errors then use the following commands to debug the pod/container.\nTo review issues with the pod e.g. CreateContainerConfigError:\n$ kubectl --namespace \u0026lt;namespace\u0026gt; describe pod \u0026lt;pod\u0026gt; For example:\n$ kubectl --namespace myoudsmns describe pod oudsmpod To tail the container logs while it is initialising use the following command:\n$ kubectl --namespace \u0026lt;namespace\u0026gt; logs -f -c \u0026lt;container\u0026gt; \u0026lt;pod\u0026gt; For example:\n$ kubectl --namespace myoudsmns logs -f -c oudsm oudsmpod To view the full container logs:\n$ kubectl --namespace \u0026lt;namespace\u0026gt; logs -c \u0026lt;container\u0026gt; \u0026lt;pod\u0026gt; To validate that the POD is running:\n$ kubectl --namespace \u0026lt;namespace\u0026gt; get all,pv,pvc,secret For example:\n$ kubectl --namespace myoudsmns get all,pv,pvc,secret NAME READY STATUS RESTARTS AGE pod/oudsmpod 1/1 Running 0 24m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/oudsm-svc NodePort 10.109.142.163 \u0026lt;none\u0026gt; 7001:31674/TCP,7002:31490/TCP 24m NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE persistentvolume/oudsmpv 10Gi RWX Delete Bound myoudsmns/oudsmpvc manual 45m NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistentvolumeclaim/oudsmpvc Bound oudsmpv 10Gi RWX manual 45m NAME TYPE DATA AGE secret/default-token-5kbxk kubernetes.io/service-account-token 3 84m secret/oudsmsecret Opaque 2 80m Once the container is running (READY shows as \u0026lsquo;1/1\u0026rsquo;) check the value of the service port (PORT/s value : here 7001:31674/TCP,7002:31490/TCP) for the Oracle Unified Directory Services Manager service and use this to access Oracle Unified Directory Services Manager in a browser:\nhttp://\u0026lt;hostname\u0026gt;:\u0026lt;svcport\u0026gt;/oudsm  In the case here:\nhttp://\u0026lt;myhost\u0026gt;:31674/oudsm  If you need to release the resources created in this example (POD, service) then issue the following command:\n$ kubectl delete -f samples/oudsm-pod.yaml service \u0026quot;oudsm-svc\u0026quot; deleted pod \u0026quot;oudsmpod\u0026quot; deleted This will avoid conflicts when running the following example for Deployments.\nOracle Unified Directory Services Manager Deployment In this example you create multiple Oracle Unified Directory Services Manager PODs/Services using Kubernetes deployments.\nTo create the deployment update the samples/oudsm-deployment.yaml file.\nUpdate the following to values specific to your environment:\n   Param Value Example     %NAMESPACE% Namespace myoudsmns   %IMAGE% Oracle image tag oracle/oudsm:12.2.1.4.0   %SECRET_NAME% Secret name oudsmsecret    Apply the file:\n$ kubectl apply -f samples/oudsm-deployment.yaml service/oudsm created deployment.apps/oudsmdeploypod created To validate that the POD is running:\n$ kubectl --namespace \u0026lt;namespace\u0026gt; get all,pv,pvc,secret For example:\n$ kubectl --namespace myoudsmns get all,pv,pvc,secret For example:\n$ kubectl --namespace myoudsmns get all,pv,pvc,secret NAME READY STATUS RESTARTS AGE pod/oudsmdeploypod-7c6bb5476-6zcmc 1/1 Running 0 13m pod/oudsmdeploypod-7c6bb5476-nldd8 1/1 Running 0 13m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/oudsm NodePort 10.97.245.58 \u0026lt;none\u0026gt; 7001:31342/TCP,7002:31222/TCP 13m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/oudsmdeploypod 2/2 2 2 13m NAME DESIRED CURRENT READY AGE replicaset.apps/oudsmdeploypod-7c6bb5476 2 2 2 13m NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE persistentvolume/oudsmpv 10Gi RWX Delete Bound myoudsmns/oudsmpvc manual 16h NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistentvolumeclaim/oudsmpvc Bound oudsmpv 10Gi RWX manual 16h NAME TYPE DATA AGE secret/default-token-5kbxk kubernetes.io/service-account-token 3 16h secret/oudsmsecret Opaque 2 16h Once the container is running (READY shows as \u0026lsquo;1/1\u0026rsquo;) check the value of the service port (PORT/s value : here 7001:31421/TCP,7002:31737/TCP) for the Oracle Unified Directory Services Manager service and use this to access Oracle Unified Directory Services Manager in a browser:\n http://\u0026lt;hostname\u0026gt;:\u0026lt;svcport\u0026gt;/oudsm  In the case here:\n http://\u0026lt;myhost\u0026gt;:31342/oudsm  Notice that in the output above we have created 2 Oracle Unified Directory Services Manager PODs (pod/oudsmdeploypod-7bb67b685c-78sq5, pod/oudsmdeploypod-7bb67b685c-xssbq) which are accessed via a service (service/oudsm).\nThe number of PODs is governed by the replicas parameter in the samples/oudsm-deployment.yaml file:\n... kind: Deployment metadata: name: oudsmdeploypod namespace: myoudsmns labels: app: oudsmdeploypod spec: replicas: 2 selector: matchLabels: app: oudsmdeploypod ... If you have a requirement to add additional PODs to your cluster you can update the samples/oudsm-deployment.yaml file with the new value for replicas and apply the file. For example, setting replicas to \u0026lsquo;3\u0026rsquo; would start an additional POD as shown below:\n... kind: Deployment metadata: name: oudsmdeploypod namespace: myoudsmns labels: app: oudsmdeploypod spec: replicas: 3 selector: matchLabels: app: oudsmdeploypod ... $ kubectl apply -f samples/oudsm-deployment.yaml.tmp service/oudsm unchanged deployment.apps/oudsmdeploypod configured Check the number of PODs have increased to 3.\n$ kubectl --namespace myoudsmns get all,pv,pvc,secret NAME READY STATUS RESTARTS AGE pod/oudsmdeploypod-7c6bb5476-6zcmc 1/1 Running 0 17m pod/oudsmdeploypod-7c6bb5476-nldd8 1/1 Running 0 17m pod/oudsmdeploypod-7c6bb5476-vqmz7 0/1 Running 0 26s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/oudsm NodePort 10.97.245.58 \u0026lt;none\u0026gt; 7001:31342/TCP,7002:31222/TCP 17m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/oudsmdeploypod 2/3 3 2 17m NAME DESIRED CURRENT READY AGE replicaset.apps/oudsmdeploypod-7c6bb5476 3 3 2 17m NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE persistentvolume/mike-oud-ds-rs-espv1 20Gi RWX Retain Bound mikens/data-mike-oud-ds-rs-es-cluster-0 elk 4d18h persistentvolume/mike-oud-ds-rs-pv 30Gi RWX Retain Bound mikens/mike-oud-ds-rs-pvc manual 4d18h persistentvolume/oimcluster-oim-pv 10Gi RWX Retain Bound oimcluster/oimcluster-oim-pvc oimcluster-oim-storage-class 69d persistentvolume/oudsmpv 10Gi RWX Delete Bound myoudsmns/oudsmpvc manual 16h NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistentvolumeclaim/oudsmpvc Bound oudsmpv 10Gi RWX manual 16h NAME TYPE DATA AGE secret/default-token-5kbxk kubernetes.io/service-account-token 3 16h secret/oudsmsecret Opaque 2 16h bash-4.2$ In this example, the POD pod/oudsmdeploypod-7c6bb5476-vqmz7 has been added.\nAppendix A : Reference  samples/oudsm-pod.yaml : This yaml file is use to create the pod and bring up the Oracle Unified Directory Services Manager services samples/oudsm-deployment.yaml : This yaml file is used to create replicas of Oracle Unified Directory Services Manager and bring up the Oracle Unified Directory Services Manager services based on the deployment  "
},
{
	"uri": "/fmw-kubernetes/20.4.1/oam/create-oam-domains/",
	"title": "Create OAM domains",
	"tags": [],
	"description": "Sample for creating an OAM domain home on an existing PV or PVC, and the domain resource YAML file for deploying the generated OAM domain.",
	"content": "The OAM deployment scripts demonstrate the creation of an OAM domain home on an existing Kubernetes persistent volume (PV) and persistent volume claim (PVC). The scripts also generate the domain YAML file, which can then be used to start the Kubernetes artifacts of the corresponding domain.\nPrerequisites Before you begin, perform the following steps:\n Review the Domain resource documentation. Ensure that you have executed all the preliminary steps documented in Prepare your environment. Ensure that the database is up and running.  Prepare to use the create domain script The sample scripts for Oracle Access Management domain deployment are available at \u0026lt;weblogic-kubernetes-operator-project\u0026gt;/kubernetes/samples/scripts/create-access-domain.\n  Make a copy of the create-domain-inputs.yaml file:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-access-domain/domain-home-on-pv $ cp create-domain-inputs.yaml create-domain-inputs.yaml.orig For example:\n$ cd /scratch/OAMDockerK8S/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-access-domain/domain-home-on-pv $ cp create-domain-inputs.yaml create-domain-inputs.yaml.orig   You must edit create-domain-inputs.yaml (or a copy of it) to provide the details for your domain. Please refer to the configuration parameters below to understand the information that you must provide in this file.\nEdit configuration parameters   Edit the create-domain-inputs.yaml and modify the following parameters. Save the file when complete:\ndomainUID: \u0026lt;domain_uid\u0026gt; domainHome: /u01/oracle/user_projects/domains/\u0026lt;domain_uid\u0026gt; image: \u0026lt;image_name\u0026gt; namespace: \u0026lt;domain_namespace\u0026gt; weblogicCredentialsSecretName: \u0026lt;kubernetes_domain_secret\u0026gt; persistentVolumeClaimName: \u0026lt;pvc_name\u0026gt; logHome: /u01/oracle/user_projects/domains/logs/\u0026lt;domain_uid\u0026gt; rcuSchemaPrefix: \u0026lt;rcu_prefix\u0026gt; rcuDatabaseURL: \u0026lt;rcu_db_host\u0026gt;:\u0026lt;rcu_db_port\u0026gt;/\u0026lt;rcu_db_service_name\u0026gt; rcuCredentialsSecret: \u0026lt;kubernetes_rcu_secret\u0026gt; For example:\ndomainUID: accessinfra domainHome: /u01/oracle/user_projects/domains/accessinfra image: oracle/oam:12.2.1.4.0 namespace: accessns weblogicCredentialsSecretName: accessinfra-domain-credentials persistentVolumeClaimName: accessinfra-domain-pvc logHome: /u01/oracle/user_projects/domains/logs/accessinfra rcuSchemaPrefix: OAMK8S rcuDatabaseURL: mydatabasehost.example.com:1521/orcl.example.com rcuCredentialsSecret: accessinfra-rcu-credentials   A full list of parameters in the create-domain-inputs.yaml file are shown below:\n   Parameter Definition Default     adminPort Port number for the Administration Server inside the Kubernetes cluster. 7001   adminNodePort Port number of the Administration Server outside the Kubernetes cluster. 30701   adminServerName Name of the Administration Server. AdminServer   clusterName Name of the WebLogic cluster instance to generate for the domain. By default the cluster name is oam_cluster for the OAM domain. oam_cluster   configuredManagedServerCount Number of Managed Server instances to generate for the domain. 5   createDomainFilesDir Directory on the host machine to locate all the files to create a WebLogic domain, including the script that is specified in the createDomainScriptName property. By default, this directory is set to the relative path wlst, and the create script will use the built-in WLST offline scripts in the wlst directory to create the WebLogic domain. It can also be set to the relative path wdt, and then the built-in WDT scripts will be used instead. An absolute path is also supported to point to an arbitrary directory in the file system. The built-in scripts can be replaced by the user-provided scripts or model files as long as those files are in the specified directory. Files in this directory are put into a Kubernetes config map, which in turn is mounted to the createDomainScriptsMountPath, so that the Kubernetes pod can use the scripts and supporting files to create a domain home. wlst   createDomainScriptsMountPath Mount path where the create domain scripts are located inside a pod. The create-domain.sh script creates a Kubernetes job to run the script (specified in the createDomainScriptName property) in a Kubernetes pod to create a domain home. Files in the createDomainFilesDir directory are mounted to this location in the pod, so that the Kubernetes pod can use the scripts and supporting files to create a domain home. /u01/weblogic   createDomainScriptName Script that the create domain script uses to create a WebLogic domain. The create-domain.sh script creates a Kubernetes job to run this script to create a domain home. The script is located in the in-pod directory that is specified in the createDomainScriptsMountPath property. If you need to provide your own scripts to create the domain home, instead of using the built-it scripts, you must use this property to set the name of the script that you want the create domain job to run. create-domain-job.sh   domainHome Home directory of the OAM domain. If not specified, the value is derived from the domainUID as /shared/domains/\u0026lt;domainUID\u0026gt;. /u01/oracle/user_projects/domains/accessinfra   domainPVMountPath Mount path of the domain persistent volume. /u01/oracle/user_projects   domainUID Unique ID that will be used to identify this particular domain. Used as the name of the generated WebLogic domain as well as the name of the Kubernetes domain resource. This ID must be unique across all domains in a Kubernetes cluster. This ID cannot contain any character that is not valid in a Kubernetes service name. accessinfra   domainType Type of the domain. Mandatory input for OAM domains. You must provide one of the supported domain type value: oam (deploys an OAM domain) oam   exposeAdminNodePort Boolean indicating if the Administration Server is exposed outside of the Kubernetes cluster. false   exposeAdminT3Channel Boolean indicating if the T3 administrative channel is exposed outside the Kubernetes cluster. true   image OAM Docker image. The operator requires OAM 12.2.1.4. Refer to OAM domains for details on how to obtain or create the image. oracle/oam:12.2.1.4.0   imagePullPolicy WebLogic Docker image pull policy. Legal values are IfNotPresent, Always, or Never IfNotPresent   imagePullSecretName Name of the Kubernetes secret to access the Docker Store to pull the WebLogic Server Docker image. The presence of the secret will be validated when this parameter is specified.    includeServerOutInPodLog Boolean indicating whether to include the server .out to the pod\u0026rsquo;s stdout. true   initialManagedServerReplicas Number of Managed Servers to initially start for the domain. 2   javaOptions Java options for starting the Administration Server and Managed Servers. A Java option can have references to one or more of the following pre-defined variables to obtain WebLogic domain information: $(DOMAIN_NAME), $(DOMAIN_HOME), $(ADMIN_NAME), $(ADMIN_PORT), and $(SERVER_NAME). -Dweblogic.StdoutDebugEnabled=false   logHome The in-pod location for the domain log, server logs, server out, and Node Manager log files. If not specified, the value is derived from the domainUID as /shared/logs/\u0026lt;domainUID\u0026gt;. /u01/oracle/user_projects/domains/logs/accessinfra   managedServerNameBase Base string used to generate Managed Server names. oam_server   managedServerPort Port number for each Managed Server. 8001   namespace Kubernetes namespace in which to create the domain. accessns   persistentVolumeClaimName Name of the persistent volume claim created to host the domain home. If not specified, the value is derived from the domainUID as \u0026lt;domainUID\u0026gt;-weblogic-sample-pvc. accessinfra-domain-pvc   productionModeEnabled Boolean indicating if production mode is enabled for the domain. true   serverStartPolicy Determines which WebLogic Server instances will be started. Legal values are NEVER, IF_NEEDED, ADMIN_ONLY. IF_NEEDED   t3ChannelPort Port for the T3 channel of the NetworkAccessPoint. 30012   t3PublicAddress Public address for the T3 channel. This should be set to the public address of the Kubernetes cluster. This would typically be a load balancer address. For development environments only: In a single server (all-in-one) Kubernetes deployment, this may be set to the address of the master, or at the very least, it must be set to the address of one of the worker nodes. If not provided, the script will attempt to set it to the IP address of the Kubernetes cluster   weblogicCredentialsSecretName Name of the Kubernetes secret for the Administration Server\u0026rsquo;s user name and password. If not specified, then the value is derived from the domainUID as \u0026lt;domainUID\u0026gt;-weblogic-credentials. accessinfra-domain-credentials   weblogicImagePullSecretName Name of the Kubernetes secret for the Docker Store, used to pull the WebLogic Server image.    serverPodCpuRequest, serverPodMemoryRequest, serverPodCpuCLimit, serverPodMemoryLimit The maximum amount of compute resources allowed, and minimum amount of compute resources required, for each server pod. Please refer to the Kubernetes documentation on Managing Compute Resources for Containers for details. Resource requests and resource limits are not specified.   rcuSchemaPrefix The schema prefix to use in the database, for example OAM1. You may wish to make this the same as the domainUID in order to simplify matching domains to their RCU schemas. OAM1   rcuDatabaseURL The database URL. oracle-db.default.svc.cluster.local:1521/devpdb.k8s   rcuCredentialsSecret The Kubernetes secret containing the database credentials. accessinfra-rcu-credentials    Note that the names of the Kubernetes resources in the generated YAML files may be formed with the value of some of the properties specified in the create-inputs.yaml file. Those properties include the adminServerName, clusterName and managedServerNameBase. If those values contain any characters that are invalid in a Kubernetes service name, those characters are converted to valid values in the generated YAML files. For example, an uppercase letter is converted to a lowercase letter and an underscore (\u0026quot;_\u0026quot;) is converted to a hyphen (\u0026quot;-\u0026quot;).\nThe sample demonstrates how to create an OAM domain home and associated Kubernetes resources for a domain that has one cluster only. In addition, the sample provides the capability for users to supply their own scripts to create the domain home for other use cases. The generated domain YAML file could also be modified to cover more use cases.\nRun the create domain script   Run the create domain script, specifying your inputs file and an output directory to store the generated artifacts:\ncd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-access-domain/domain-home-on-pv $ ./create-domain.sh -i create-domain-inputs.yaml -o /\u0026lt;path to output-directory\u0026gt; For example:\n$ cd /scratch/OAMDockerK8S/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-access-domain/domain-home-on-pv $ ./create-domain.sh -i create-domain-inputs.yaml -o output_access The output will look similar to the following:\nInput parameters being used export version=\u0026#34;create-weblogic-sample-domain-inputs-v1\u0026#34; export adminPort=\u0026#34;7001\u0026#34; export adminServerName=\u0026#34;AdminServer\u0026#34; export domainUID=\u0026#34;accessinfra\u0026#34; export domainType=\u0026#34;oam\u0026#34; export domainHome=\u0026#34;/u01/oracle/user_projects/domains/accessinfra\u0026#34; export serverStartPolicy=\u0026#34;IF_NEEDED\u0026#34; export clusterName=\u0026#34;oam_cluster\u0026#34; export configuredManagedServerCount=\u0026#34;5\u0026#34; export initialManagedServerReplicas=\u0026#34;2\u0026#34; export managedServerNameBase=\u0026#34;oam_server\u0026#34; export managedServerPort=\u0026#34;14100\u0026#34; export image=\u0026#34;oracle/oam:12.2.1.4.0\u0026#34; export imagePullPolicy=\u0026#34;IfNotPresent\u0026#34; export productionModeEnabled=\u0026#34;true\u0026#34; export weblogicCredentialsSecretName=\u0026#34;accessinfra-domain-credentials\u0026#34; export includeServerOutInPodLog=\u0026#34;true\u0026#34; export logHome=\u0026#34;/u01/oracle/user_projects/domains/logs/accessinfra\u0026#34; export t3ChannelPort=\u0026#34;30012\u0026#34; export exposeAdminT3Channel=\u0026#34;false\u0026#34; export adminNodePort=\u0026#34;30701\u0026#34; export exposeAdminNodePort=\u0026#34;false\u0026#34; export namespace=\u0026#34;accessns\u0026#34; javaOptions=-Dweblogic.StdoutDebugEnabled=false export persistentVolumeClaimName=\u0026#34;accessinfra-domain-pvc\u0026#34; export domainPVMountPath=\u0026#34;/u01/oracle/user_projects/domains\u0026#34; export createDomainScriptsMountPath=\u0026#34;/u01/weblogic\u0026#34; export createDomainScriptName=\u0026#34;create-domain-job.sh\u0026#34; export createDomainFilesDir=\u0026#34;wlst\u0026#34; export rcuSchemaPrefix=\u0026#34;OAMK8S\u0026#34; export rcuDatabaseURL=\u0026#34;mydatabasehost.example.com:1521/orcl.example.com\u0026#34; export rcuCredentialsSecret=\u0026#34;accessinfra-rcu-credentials\u0026#34; Generating output_access/weblogic-domains/accessinfra/create-domain-job.yaml Generating output_access/weblogic-domains/accessinfra/delete-domain-job.yaml Generating output_access/weblogic-domains/accessinfra/domain.yaml Checking to see if the secret accessinfra-domain-credentials exists in namespace accessns configmap/accessinfra-create-oam-infra-domain-job-cm created Checking the configmap accessinfra-create-oam-infra-domain-job-cm was created configmap/accessinfra-create-oam-infra-domain-job-cm labeled Checking if object type job with name accessinfra-create-oam-infra-domain-job exists No resources found in accessns namespace. Creating the domain by creating the job output_access/weblogic-domains/accessinfra/create-domain-job.yaml job.batch/accessinfra-create-oam-infra-domain-job created Waiting for the job to complete... status on iteration 1 of 20 pod accessinfra-create-oam-infra-domain-job-vj69h status is Running status on iteration 2 of 20 pod accessinfra-create-oam-infra-domain-job-vj69h status is Running status on iteration 3 of 20 pod accessinfra-create-oam-infra-domain-job-vj69h status is Running status on iteration 4 of 20 pod accessinfra-create-oam-infra-domain-job-vj69h status is Running status on iteration 5 of 20 pod accessinfra-create-oam-infra-domain-job-vj69h status is Completed Domain accessinfra was created and will be started by the Oracle WebLogic Kubernetes Operator The following files were generated: output_access/weblogic-domains/accessinfra/create-domain-inputs.yaml output_access/weblogic-domains/accessinfra/create-domain-job.yaml output_access/weblogic-domains/accessinfra/domain.yaml Completed Note: If the domain creation fails, refer to the Troubleshooting section.\nThe command creates a domain.yaml file required for domain creation.\n  Navigate to the /output_access/weblogic-domains/\u0026lt;domain_uid\u0026gt; directory:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-access-domain/domain-home-on-pv/output_access/weblogic-domains/\u0026lt;domain_uid\u0026gt; For example:\n$ cd /scratch/OAMDockerK8S/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-access-domain/domain-home-on-pv/output_access/weblogic-domains/accessinfra Edit the domain.yaml file, increase the min and max heap size save the file. Change the following value from:\n- name: USER_MEM_ARGS\u0026quot; value: \u0026quot;-Djava.security.egd=file:/dev/./urandom -Xms256m -Xmx1024m \u0026quot; to:\n- name: USER_MEM_ARGS\u0026quot; value: \u0026quot;-XX:+UseContainerSupport -Djava.security.egd=file:/dev/./urandom -Xms8192m -Xmx8192m\u0026quot;   If required, you can add the optional parameter maxClusterConcurrentStartup to the spec section of the domain.yaml. This parameter specifies the number of managed servers to be started in sequence per cluster. For example if you updated the initialManagedServerReplicas to 4 in create-domain-inputs.yaml and only had 2 nodes, then setting maxClusterConcurrentStartup: 1 will start one managed server at a time on each node, rather than starting them all at once. This can be useful to take the strain off individual nodes at startup. Below is an example with the parameter added:\napiVersion: \u0026quot;weblogic.oracle/v8\u0026quot; kind: Domain metadata: name: accessinfra namespace: accessns labels: weblogic.domainUID: accessinfra spec: # The WebLogic Domain Home domainHome: /u01/oracle/user_projects/domains/accessinfra maxClusterConcurrentStartup: 1 # The domain home source type # Set to PersistentVolume for domain-in-pv, Image for domain-in-image, or FromModel for model-in-image domainHomeSourceType: PersistentVolume # The WebLogic Server Docker image that the Operator uses to start the domain image: \u0026quot;oracle/oam:12.2.1.4.0\u0026quot; ....   Create the Kubernetes resource using the following command:\n$ kubectl apply -f \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-access-domain/domain-home-on-pv/output_access/weblogic-domains/accessinfra/domain.yaml For example:\n$ kubectl apply -f /scratch/OAMDockerK8S/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-access-domain/domain-home-on-pv/output_access/weblogic-domains/accessinfra/domain.yaml The output will look similar to the following:\ndomain.weblogic.oracle/accessinfra created   Verify the domain   Verify the domain, servers pods and services are created and in the READY state with a status of 1/1, by running the following command:\n$ kubectl get all,domains -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get all,domains -n accessns The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE pod/accessinfra-adminserver 1/1 Running 0 17m pod/accessinfra-create-oam-infra-domain-job-vj69h 0/1 Completed 0 42m pod/accessinfra-oam-policy-mgr1 1/1 Running 0 9m7s pod/accessinfra-oam-server1 1/1 Running 0 9m7s pod/accessinfra-oam-server2 1/1 Running 0 9m7s pod/helper 1/1 Running 0 23h NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/accessinfra-adminserver ClusterIP None \u0026lt;none\u0026gt; 7001/TCP 17m service/accessinfra-cluster-oam-cluster ClusterIP 10.110.50.168 \u0026lt;none\u0026gt; 14100/TCP 9m8s service/accessinfra-cluster-policy-cluster ClusterIP 10.102.32.247 \u0026lt;none\u0026gt; 15100/TCP 9m8s service/accessinfra-oam-policy-mgr1 ClusterIP None \u0026lt;none\u0026gt; 15100/TCP 9m8s service/accessinfra-oam-policy-mgr2 ClusterIP 10.104.147.108 \u0026lt;none\u0026gt; 15100/TCP 9m8s service/accessinfra-oam-policy-mgr3 ClusterIP 10.108.233.86 \u0026lt;none\u0026gt; 15100/TCP 9m8s service/accessinfra-oam-policy-mgr4 ClusterIP 10.105.15.228 \u0026lt;none\u0026gt; 15100/TCP 9m7s service/accessinfra-oam-policy-mgr5 ClusterIP 10.99.66.92 \u0026lt;none\u0026gt; 15100/TCP 9m8s service/accessinfra-oam-server1 ClusterIP None \u0026lt;none\u0026gt; 14100/TCP 9m8s service/accessinfra-oam-server2 ClusterIP None \u0026lt;none\u0026gt; 14100/TCP 9m8s service/accessinfra-oam-server3 ClusterIP 10.111.231.33 \u0026lt;none\u0026gt; 14100/TCP 9m8s service/accessinfra-oam-server4 ClusterIP 10.110.10.183 \u0026lt;none\u0026gt; 14100/TCP 9m7s service/accessinfra-oam-server5 ClusterIP 10.103.192.174 \u0026lt;none\u0026gt; 14100/TCP 9m8s NAME COMPLETIONS DURATION AGE job.batch/accessinfra-create-oam-infra-domain-job 1/1 2m14s 42m NAME AGE domain.weblogic.oracle/accessinfra 25m Note: It will take several minutes before all the services listed above show. When a pod has a STATUS of 0/1 the pod is started but the OAM server associated with it is currently starting. While the pods are starting you can check the startup status in the pod logs, by running the following command:\n$ kubectl logs accessinfra-adminserver -n accessns $ kubectl logs accessinfra-oam-policy-mgr1 -n accessns $ kubectl logs accessinfra-oam-server1 -n accessns etc.. The default domain created by the script has the following characteristics:\n An Administration Server named AdminServer listening on port 7001. A configured OAM cluster named oam_cluster of size 5. A configured Policy Manager cluster named policy_cluster of size 5. Two started OAM managed Servers, named oam_server1 and oam_server2, listening on port 14100. One started Policy Manager managed server named oam-policy-mgr1, listening on port 15100. Log files that are located in \u0026lt;persistent_volume\u0026gt;/logs/\u0026lt;domainUID\u0026gt;.    Run the following command to describe the domain:\n$ kubectl describe domain \u0026lt;domain_uid\u0026gt; -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl describe domain accessinfra -n accessns The output will look similar to the following:\nName: accessinfra Namespace: accessns Labels: weblogic.domainUID=accessinfra Annotations: API Version: weblogic.oracle/v8 Kind: Domain Metadata: Creation Timestamp: 2020-09-24T14:00:34Z Generation: 1 Managed Fields: API Version: weblogic.oracle/v8 Fields Type: FieldsV1 fieldsV1: f:metadata: f:annotations: .: f:kubectl.kubernetes.io/last-applied-configuration: f:labels: .: f:weblogic.domainUID: Manager: kubectl Operation: Update Time: 2020-09-24T14:00:34Z API Version: weblogic.oracle/v8 Fields Type: FieldsV1 fieldsV1: f:status: .: f:clusters: f:conditions: f:servers: f:startTime: Manager: OpenAPI-Generator Operation: Update Time: 2020-09-24T14:12:51Z Resource Version: 244336 Self Link: /apis/weblogic.oracle/v8/namespaces/accessns/domains/accessinfra UID: 0edf8266-4419-45f1-bd50-e26ac41340e5 Spec: Admin Server: Server Pod: Env: Name: USER_MEM_ARGS Value: -Djava.security.egd=file:/dev/./urandom -Xms512m -Xmx1024m Server Start State: RUNNING Clusters: Cluster Name: policy_cluster Replicas: 1 Server Pod: Affinity: Pod Anti Affinity: Preferred During Scheduling Ignored During Execution: Pod Affinity Term: Label Selector: Match Expressions: Key: weblogic.clusterName Operator: In Values: $(CLUSTER_NAME) Topology Key: kubernetes.io/hostname Weight: 100 Server Service: Precreate Service: true Server Start State: RUNNING Cluster Name: oam_cluster Replicas: 2 Server Pod: Affinity: Pod Anti Affinity: Preferred During Scheduling Ignored During Execution: Pod Affinity Term: Label Selector: Match Expressions: Key: weblogic.clusterName Operator: In Values: $(CLUSTER_NAME) Topology Key: kubernetes.io/hostname Weight: 100 Server Service: Precreate Service: true Server Start State: RUNNING Data Home: Domain Home: /u01/oracle/user_projects/domains/accessinfra Domain Home Source Type: PersistentVolume Http Access Log In Log Home: true Image: oracle/oam:12.2.1.4.0 Image Pull Policy: IfNotPresent Include Server Out In Pod Log: true Log Home: /u01/oracle/user_projects/domains/logs/accessinfra Log Home Enabled: true Server Pod: Env: Name: JAVA_OPTIONS Value: -Dweblogic.StdoutDebugEnabled=false Name: USER_MEM_ARGS Value: -Djava.security.egd=file:/dev/./urandom -Xms256m -Xmx1024m Volume Mounts: Mount Path: /u01/oracle/user_projects/domains Name: weblogic-domain-storage-volume Volumes: Name: weblogic-domain-storage-volume Persistent Volume Claim: Claim Name: accessinfra-domain-pvc Server Start Policy: IF_NEEDED Web Logic Credentials Secret: Name: accessinfra-domain-credentials Status: Clusters: Cluster Name: oam_cluster Maximum Replicas: 5 Minimum Replicas: 0 Ready Replicas: 2 Replicas: 2 Replicas Goal: 2 Cluster Name: policy_cluster Maximum Replicas: 5 Minimum Replicas: 0 Ready Replicas: 1 Replicas: 1 Replicas Goal: 1 Conditions: Last Transition Time: 2020-09-24T14:12:02.037Z Reason: ServersReady Status: True Type: Available Servers: Desired State: RUNNING Health: Activation Time: 2020-09-24T14:09:01.164Z Overall Health: ok Subsystems: Subsystem Name: ServerRuntime Symptoms: Node Name: 10.250.111.112 Server Name: AdminServer State: RUNNING Cluster Name: oam_cluster Desired State: RUNNING Health: Activation Time: 2020-09-24T14:11:06.015Z Overall Health: ok Subsystems: Subsystem Name: ServerRuntime Symptoms: Node Name: 10.250.111.111 Server Name: oam_server1 State: RUNNING Cluster Name: oam_cluster Desired State: RUNNING Health: Activation Time: 2020-09-24T14:11:35.454Z Overall Health: ok Subsystems: Subsystem Name: ServerRuntime Symptoms: Node Name: 10.250.111.112 Server Name: oam_server2 State: RUNNING Cluster Name: oam_cluster Desired State: SHUTDOWN Server Name: oam_server3 Cluster Name: oam_cluster Desired State: SHUTDOWN Server Name: oam_server4 Cluster Name: oam_cluster Desired State: SHUTDOWN Server Name: oam_server5 Cluster Name: policy_cluster Desired State: RUNNING Health: Activation Time: 2020-09-24T14:11:54.938Z Overall Health: ok Subsystems: Subsystem Name: ServerRuntime Symptoms: Node Name: 10.250.111.112 Server Name: oam_policy_mgr1 State: RUNNING Cluster Name: policy_cluster Desired State: SHUTDOWN Server Name: oam_policy_mgr2 Cluster Name: policy_cluster Desired State: SHUTDOWN Server Name: oam_policy_mgr3 Cluster Name: policy_cluster Desired State: SHUTDOWN Server Name: oam_policy_mgr4 Cluster Name: policy_cluster Desired State: SHUTDOWN Server Name: oam_policy_mgr5 Start Time: 2020-09-24T14:00:34.395Z Events: \u0026lt;none\u0026gt; In the Status section of the output, the available servers and clusters are listed.\n  Run the following command to see the pods running the servers and which nodes they are running on:\n$ kubectl get pods -n \u0026lt;domain_namespace\u0026gt; -o wide For example:\n$ kubectl get pods -n accessns -o wide The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES accessinfra-adminserver 1/1 Running 0 26m 10.244.1.7 10.250.111.112 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; accessinfra-create-oam-infra-domain-job-vj69h 0/1 Completed 0 5h55m 10.244.1.5 10.250.111.112 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; accessinfra-oam-policy-mgr1 1/1 Running 0 18m 10.244.1.9 10.250.111.112 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; accessinfra-oam-server1 1/1 Running 0 18m 10.244.2.3 10.250.111.111 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; accessinfra-oam-server2 1/1 Running 0 18m 10.244.1.8 10.250.111.112 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; helper 1/1 Running 0 22h 10.244.1.4 10.250.111.112 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; You are now ready to configure an Ingress to direct traffic for your OAM domain as per Configure an Ingress for an OAM domain.\n  "
},
{
	"uri": "/fmw-kubernetes/20.4.1/oig/create-oig-domains/",
	"title": "Create OIG domains",
	"tags": [],
	"description": "Sample for creating an OIG domain home on an existing PV or PVC, and the domain resource YAML file for deploying the generated OIG domain.",
	"content": " Introduction Prerequisites Prepare the Create Domain Script  Edit Configuration Parameters   Run the Create Domain Script  Generate the Create Domain Script Create Docker Registry Secret Run the Create Domain Scripts   Verify the Results  Verify the Domain, Pods and Services Verify the Domain Verify the Pods    Introduction The OIG deployment scripts demonstrate the creation of an OIG domain home on an existing Kubernetes persistent volume (PV) and persistent volume claim (PVC). The scripts also generate the domain YAML file, which can then be used to start the Kubernetes artifacts of the corresponding domain.\nPrerequisites Before you begin, perform the following steps:\n Review the Domain resource documentation. Ensure that you have executed all the preliminary steps documented in Prepare your environment. Ensure that the database is up and running.  Prepare the Create Domain Script The sample scripts for Oracle Identity Governance domain deployment are available at \u0026lt;weblogic-kubernetes-operator-project\u0026gt;/kubernetes/samples/scripts/create-oim-domain.\n  Make a copy of the create-domain-inputs.yaml file:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-oim-domain/domain-home-on-pv $ cp create-domain-inputs.yaml create-domain-inputs.yaml.orig For example:\n$ cd /scratch/OIGDockerK8S/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-oim-domain/domain-home-on-pv $ cp create-domain-inputs.yaml create-domain-inputs.yaml.orig You must edit create-domain-inputs.yaml (or a copy of it) to provide the details for your domain. Please refer to the configuration parameters below to understand the information that you must provide in this file.\n  Edit Configuration Parameters   Edit the create-domain-inputs.yaml and modify the following parameters. Save the file when complete:\ndomainUID: \u0026lt;domain_uid\u0026gt; domainHome: /u01/oracle/user_projects/domains/\u0026lt;domain_uid\u0026gt; image: \u0026lt;image_name\u0026gt; namespace: \u0026lt;domain_namespace\u0026gt; weblogicCredentialsSecretName: \u0026lt;kubernetes_domain_secret\u0026gt; persistentVolumeClaimName: \u0026lt;pvc_name\u0026gt; logHome: /u01/oracle/user_projects/domains/logs/\u0026lt;domain_id\u0026gt; rcuSchemaPrefix: \u0026lt;rcu_prefix\u0026gt; rcuDatabaseURL: \u0026lt;rcu_db_host\u0026gt;:\u0026lt;rcu_db_port\u0026gt;/\u0026lt;rcu_db_service_name\u0026gt; rcuCredentialsSecret: \u0026lt;kubernetes_rcu_secret\u0026gt; For example:\ndomainUID: oimcluster domainHome: /u01/oracle/user_projects/domains/oimcluster image: oracle/oig:12.2.1.4.0 namespace: oimcluster weblogicCredentialsSecretName: oimcluster-domain-credentials persistentVolumeClaimName: oimcluster-domain-pvc logHome: /u01/oracle/user_projects/domains/logs/oimcluster rcuSchemaPrefix: OIGK8S rcuDatabaseURL: mydatabasehost.example.com:1521/orcl.example.com rcuCredentialsSecret: oimcluster-rcu-credentials   A full list of parameters in the create-domain-inputs.yaml file are shown below:\n   Parameter Definition Default     adminPort Port number for the Administration Server inside the Kubernetes cluster. 7001   adminNodePort Port number of the Administration Server outside the Kubernetes cluster. 30701   adminServerName Name of the Administration Server. AdminServer   clusterName Name of the WebLogic cluster instance to generate for the domain. By default the cluster name is oimcluster for the OIG domain. oimcluster   configuredManagedServerCount Number of Managed Server instances to generate for the domain. 5   createDomainFilesDir Directory on the host machine to locate all the files to create a WebLogic domain, including the script that is specified in the createDomainScriptName property. By default, this directory is set to the relative path wlst, and the create script will use the built-in WLST offline scripts in the wlst directory to create the WebLogic domain. It can also be set to the relative path wdt, and then the built-in WDT scripts will be used instead. An absolute path is also supported to point to an arbitrary directory in the file system. The built-in scripts can be replaced by the user-provided scripts or model files as long as those files are in the specified directory. Files in this directory are put into a Kubernetes config map, which in turn is mounted to the createDomainScriptsMountPath, so that the Kubernetes pod can use the scripts and supporting files to create a domain home. wlst   createDomainScriptsMountPath Mount path where the create domain scripts are located inside a pod. The create-domain.sh script creates a Kubernetes job to run the script (specified in the createDomainScriptName property) in a Kubernetes pod to create a domain home. Files in the createDomainFilesDir directory are mounted to this location in the pod, so that the Kubernetes pod can use the scripts and supporting files to create a domain home. /u01/weblogic   createDomainScriptName Script that the create domain script uses to create a WebLogic domain. The create-domain.sh script creates a Kubernetes job to run this script to create a domain home. The script is located in the in-pod directory that is specified in the createDomainScriptsMountPath property. If you need to provide your own scripts to create the domain home, instead of using the built-it scripts, you must use this property to set the name of the script that you want the create domain job to run. create-domain-job.sh   domainHome Home directory of the OIG domain. If not specified, the value is derived from the domainUID as /shared/domains/\u0026lt;domainUID\u0026gt;. /u01/oracle/user_projects/domains/oimcluster   domainPVMountPath Mount path of the domain persistent volume. /u01/oracle/user_projects   domainUID Unique ID that will be used to identify this particular domain. Used as the name of the generated WebLogic domain as well as the name of the Kubernetes domain resource. This ID must be unique across all domains in a Kubernetes cluster. This ID cannot contain any character that is not valid in a Kubernetes service name. oimcluster   exposeAdminNodePort Boolean indicating if the Administration Server is exposed outside of the Kubernetes cluster. false   exposeAdminT3Channel Boolean indicating if the T3 administrative channel is exposed outside the Kubernetes cluster. true   image OIG Docker image. The operator requires OIG 12.2.1.4. Refer to OIG domains for details on how to obtain or create the image. oracle/oig:12.2.1.4.0   imagePullPolicy WebLogic Docker image pull policy. Legal values are IfNotPresent, Always, or Never IfNotPresent   imagePullSecretName Name of the Kubernetes secret to access the Docker Store to pull the WebLogic Server Docker image. The presence of the secret will be validated when this parameter is specified.    includeServerOutInPodLog Boolean indicating whether to include the server .out to the pod\u0026rsquo;s stdout. true   initialManagedServerReplicas Number of Managed Servers to initially start for the domain. 2   javaOptions Java options for starting the Administration Server and Managed Servers. A Java option can have references to one or more of the following pre-defined variables to obtain WebLogic domain information: $(DOMAIN_NAME), $(DOMAIN_HOME), $(ADMIN_NAME), $(ADMIN_PORT), and $(SERVER_NAME). -Dweblogic.StdoutDebugEnabled=false   logHome The in-pod location for the domain log, server logs, server out, and Node Manager log files. If not specified, the value is derived from the domainUID as /shared/logs/\u0026lt;domainUID\u0026gt;. /u01/oracle/user_projects/domains/logs/oimcluster   managedServerNameBase Base string used to generate Managed Server names. oim_server   managedServerPort Port number for each Managed Server. 8001   namespace Kubernetes namespace in which to create the domain. oimcluster   persistentVolumeClaimName Name of the persistent volume claim created to host the domain home. If not specified, the value is derived from the domainUID as \u0026lt;domainUID\u0026gt;-weblogic-sample-pvc. oimcluster-domain-pvc   productionModeEnabled Boolean indicating if production mode is enabled for the domain. true   serverStartPolicy Determines which WebLogic Server instances will be started. Legal values are NEVER, IF_NEEDED, ADMIN_ONLY. IF_NEEDED   t3ChannelPort Port for the T3 channel of the NetworkAccessPoint. 30012   t3PublicAddress Public address for the T3 channel. This should be set to the public address of the Kubernetes cluster. This would typically be a load balancer address. For development environments only: In a single server (all-in-one) Kubernetes deployment, this may be set to the address of the master, or at the very least, it must be set to the address of one of the worker nodes. If not provided, the script will attempt to set it to the IP address of the Kubernetes cluster   weblogicCredentialsSecretName Name of the Kubernetes secret for the Administration Server\u0026rsquo;s user name and password. If not specified, then the value is derived from the domainUID as \u0026lt;domainUID\u0026gt;-weblogic-credentials. oimcluster-domain-credentials   weblogicImagePullSecretName Name of the Kubernetes secret for the Docker Store, used to pull the WebLogic Server image.    serverPodCpuRequest, serverPodMemoryRequest, serverPodCpuCLimit, serverPodMemoryLimit The maximum amount of compute resources allowed, and minimum amount of compute resources required, for each server pod. Please refer to the Kubernetes documentation on Managing Compute Resources for Containers for details. Resource requests and resource limits are not specified.   rcuSchemaPrefix The schema prefix to use in the database, for example OIGK8S. You may wish to make this the same as the domainUID in order to simplify matching domains to their RCU schemas. OIGK8S   rcuDatabaseURL The database URL. oracle-db.default.svc.cluster.local:1521/devpdb.k8s   rcuCredentialsSecret The Kubernetes secret containing the database credentials. oimcluster-rcu-credentials    Note that the names of the Kubernetes resources in the generated YAML files may be formed with the value of some of the properties specified in the create-inputs.yaml file. Those properties include the adminServerName, clusterName and managedServerNameBase. If those values contain any characters that are invalid in a Kubernetes service name, those characters are converted to valid values in the generated YAML files. For example, an uppercase letter is converted to a lowercase letter and an underscore (\u0026quot;_\u0026quot;) is converted to a hyphen (\u0026quot;-\u0026quot;).\nThe sample demonstrates how to create an OIG domain home and associated Kubernetes resources for a domain that has one cluster only. In addition, the sample provides the capability for users to supply their own scripts to create the domain home for other use cases. The generated domain YAML file could also be modified to cover more use cases.\nRun the Create Domain Script Generate the Create Domain Script   Run the create domain script, specifying your inputs file and an output directory to store the generated artifacts:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-oim-domain/domain-home-on-pv $ mkdir output_oimcluster $ ./create-domain.sh -i create-domain-inputs.yaml -o /\u0026lt;path to output-directory\u0026gt; For example:\n$ cd /scratch/OIGDockerK8S/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-oim-domain/domain-home-on-pv $ mkdir output_oimcluster $ ./create-domain.sh -i create-domain-inputs.yaml -o output_oimcluster The output will look similar to the following:\n$ ./create-domain.sh -i create-domain-inputs.yaml -o output_oimcluster Input parameters being used export version=\u0026#34;create-weblogic-sample-domain-inputs-v1\u0026#34; export adminPort=\u0026#34;7001\u0026#34; export adminServerName=\u0026#34;AdminServer\u0026#34; export domainUID=\u0026#34;oimcluster\u0026#34; export domainHome=\u0026#34;/u01/oracle/user_projects/domains/oimcluster\u0026#34; export serverStartPolicy=\u0026#34;IF_NEEDED\u0026#34; export clusterName=\u0026#34;oim_cluster\u0026#34; export configuredManagedServerCount=\u0026#34;5\u0026#34; export initialManagedServerReplicas=\u0026#34;1\u0026#34; export managedServerNameBase=\u0026#34;oim_server\u0026#34; export managedServerPort=\u0026#34;14000\u0026#34; export image=\u0026#34;oracle/oig:12.2.1.4.0\u0026#34; export imagePullPolicy=\u0026#34;IfNotPresent\u0026#34; export imagePullSecretName=\u0026#34;oig-docker\u0026#34; export productionModeEnabled=\u0026#34;true\u0026#34; export weblogicCredentialsSecretName=\u0026#34;oimcluster-domain-credentials\u0026#34; export includeServerOutInPodLog=\u0026#34;true\u0026#34; export logHome=\u0026#34;/u01/oracle/user_projects/domains/logs/oimcluster\u0026#34; export t3ChannelPort=\u0026#34;30012\u0026#34; export exposeAdminT3Channel=\u0026#34;false\u0026#34; export adminNodePort=\u0026#34;30701\u0026#34; export exposeAdminNodePort=\u0026#34;false\u0026#34; export namespace=\u0026#34;oimcluster\u0026#34; javaOptions=-Dweblogic.StdoutDebugEnabled=false export persistentVolumeClaimName=\u0026#34;oimcluster-oim-pvc\u0026#34; export domainPVMountPath=\u0026#34;/u01/oracle/user_projects/domains\u0026#34; export createDomainScriptsMountPath=\u0026#34;/u01/weblogic\u0026#34; export createDomainScriptName=\u0026#34;create-domain-job.sh\u0026#34; export createDomainFilesDir=\u0026#34;wlst\u0026#34; export rcuSchemaPrefix=\u0026#34;OIGK8S\u0026#34; export rcuDatabaseURL=\u0026#34;mydatabasehost.example.com:1521/orcl.example.com\u0026#34; export rcuCredentialsSecret=\u0026#34;oimcluster-rcu-credentials\u0026#34; export frontEndHost=\u0026#34;100.102.48.49\u0026#34; export frontEndPort=\u0026#34;80\u0026#34; Generating output_oimcluster/weblogic-domains/oimcluster/create-domain-job.yaml Generating output_oimcluster/weblogic-domains/oimcluster/delete-domain-job.yaml Generating output_oimcluster/weblogic-domains/oimcluster/domain.yaml Checking to see if the secret oimcluster-domain-credentials exists in namespace oimcluster configmap/oimcluster-create-fmw-infra-sample-domain-job-cm created Checking the configmap oimcluster-create-fmw-infra-sample-domain-job-cm was created configmap/oimcluster-create-fmw-infra-sample-domain-job-cm labeled Checking if object type job with name oimcluster-create-fmw-infra-sample-domain-job exists No resources found in oimcluster namespace. Creating the domain by creating the job output_oimcluster/weblogic-domains/oimcluster/create-domain-job.yaml job.batch/oimcluster-create-fmw-infra-sample-domain-job created Waiting for the job to complete... status on iteration 1 of 40 pod oimcluster-create-fmw-infra-sample-domain-job-dktkk status is Running status on iteration 2 of 40 pod oimcluster-create-fmw-infra-sample-domain-job-dktkk status is Running status on iteration 3 of 40 pod oimcluster-create-fmw-infra-sample-domain-job-dktkk status is Running status on iteration 4 of 40 pod oimcluster-create-fmw-infra-sample-domain-job-dktkk status is Running status on iteration 5 of 40 pod oimcluster-create-fmw-infra-sample-domain-job-dktkk status is Running status on iteration 6 of 40 pod oimcluster-create-fmw-infra-sample-domain-job-dktkk status is Running status on iteration 7 of 40 pod oimcluster-create-fmw-infra-sample-domain-job-dktkk status is Running status on iteration 8 of 40 pod oimcluster-create-fmw-infra-sample-domain-job-dktkk status is Running status on iteration 9 of 40 pod oimcluster-create-fmw-infra-sample-domain-job-dktkk status is Running status on iteration 10 of 40 pod oimcluster-create-fmw-infra-sample-domain-job-dktkk status is Running status on iteration 11 of 40 pod oimcluster-create-fmw-infra-sample-domain-job-dktkk status is Completed Domain oimcluster was created and will be started by the WebLogic Kubernetes Operator The following files were generated: output_oimcluster/weblogic-domains/oimcluster/create-domain-inputs.yaml output_oimcluster/weblogic-domains/oimcluster/create-domain-job.yaml output_oimcluster/weblogic-domains/oimcluster/domain.yaml sed Completed $ Note: If the create domain script creation fails, refer to the Troubleshooting section.\n  Create Docker Registry Secret   Create a Docker Registry Secret with name oig-docker. The operator validates the presence of this secret. The OIG image has been manually loaded in Install the OIG Docker Image so you can run this command as is. The presence of the secret is sufficient for creating the Kubernetes resource in the next step.\n$ kubectl create secret docker-registry oig-docker -n \u0026lt;domain_namespace\u0026gt; --docker-username=\u0026#39;\u0026lt;user_name\u0026gt;\u0026#39; --docker-password=\u0026#39;\u0026lt;password\u0026gt;\u0026#39; --docker-server=\u0026#39;\u0026lt;docker_registry_url\u0026gt;\u0026#39; --docker-email=\u0026#39;\u0026lt;email_address\u0026gt;\u0026#39; For example:\n$ kubectl create secret docker-registry oig-docker -n oimcluster --docker-username=\u0026#39;\u0026lt;user_name\u0026gt;\u0026#39; --docker-password=\u0026#39;\u0026lt;password\u0026gt;\u0026#39; --docker-server=\u0026#39;\u0026lt;docker_registry_url\u0026gt;\u0026#39; --docker-email=\u0026#39;\u0026lt;email_address\u0026gt;\u0026#39; Note: The above command should be run as described. Do not change anything other than the \u0026lt;domain_namespace\u0026gt;.\nThe output will look similar to the following:\nsecret/oig-docker created   Run the Create Domain Scripts   Create the Kubernetes resource using the following command:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-oim-domain/domain-home-on-pv/output_oimcluster/weblogic-domains/oimcluster $ kubectl apply -f domain.yaml For example:\n$ cd /scratch/OIGDockerK8S/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-oim-domain/domain-home-on-pv/output_oimcluster/weblogic-domains/oimcluster $ kubectl apply -f domain.yaml The output will look similar to the following:\ndomain.weblogic.oracle/oimcluster created   Run the following command to view the status of the OIG pods:\n$ kubectl get pods -n oimcluster The output will initially look similar to the following:\nNAME READY STATUS RESTARTS AGE helper 1/1 Running 0 3h30m oimcluster-create-fmw-infra-sample-domain-job-dktkk 0/1 Completed 0 27m oimcluster-introspect-domain-job-p4brt 1/1 Running 0 6s The introspect-domain-job pod will be displayed first. Run the command again after several minutes and check to see that the AdminServer and SOA Server are both started. When started they should have STATUS = Running and READY = 1/1.\nNAME READY STATUS RESTARTS AGE helper 1/1 Running 0 3h38m oimcluster-adminserver 1/1 Running 0 7m30s oimcluster-create-fmw-infra-sample-domain-job-dktkk 0/1 Completed 0 35m oimcluster-soa-server1 1/1 Running 0 4m Note: It will take several minutes before all the pods listed above show. When a pod has a STATUS of 0/1 the pod is started but the OIG server associated with it is currently starting. While the pods are starting you can check the startup status in the pod logs, by running the following command:\n$ kubectl logs oimcluster-adminserver -n oimcluster $ kubectl logs oimcluster-soa-server1 -n oimcluster   Once both pods are running, start the OIM Server using the following command:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-oim-domain/domain-home-on-pv/output_oimcluster/weblogic-domains/oimcluster $ kubectl apply -f domain_oim_soa.yaml For example:\n$ cd /scratch/OIGDockerK8S/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-oim-domain/domain-home-on-pv/output_oimcluster/weblogic-domains/oimcluster $ kubectl apply -f domain_oim_soa.yaml The output will look similar to the following:\ndomain.weblogic.oracle/oimcluster configured   Verify the Results Verify the Domain, Pods and Services   Verify the domain, servers pods and services are created and in the READY state with a STATUS of 1/1, by running the following command:\n$ kubectl get all,domains -n oimcluster The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE pod/helper 1/1 Running 0 3h40m pod/oimcluster-adminserver 1/1 Running 0 16m pod/oimcluster-create-fmw-infra-sample-domain-job-dktkk 0/1 Completed 0 36m pod/oimcluster-oim-server1 1/1 Running 0 5m57s pod/oimcluster-soa-server1 1/1 Running 0 13m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/oimcluster-adminserver ClusterIP None \u0026lt;none\u0026gt; 7001/TCP 16m service/oimcluster-cluster-oim-cluster ClusterIP 10.97.121.159 \u0026lt;none\u0026gt; 14000/TCP 13m service/oimcluster-cluster-soa-cluster ClusterIP 10.111.231.242 \u0026lt;none\u0026gt; 8001/TCP 13m service/oimcluster-oim-server1 ClusterIP None \u0026lt;none\u0026gt; 14000/TCP 5m57s service/oimcluster-oim-server2 ClusterIP 10.108.139.30 \u0026lt;none\u0026gt; 14000/TCP 5m57s service/oimcluster-oim-server3 ClusterIP 10.97.170.104 \u0026lt;none\u0026gt; 14000/TCP 5m57s service/oimcluster-oim-server4 ClusterIP 10.99.82.214 \u0026lt;none\u0026gt; 14000/TCP 5m57s service/oimcluster-oim-server5 ClusterIP 10.98.75.228 \u0026lt;none\u0026gt; 14000/TCP 5m57s service/oimcluster-soa-server1 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP 13m service/oimcluster-soa-server2 ClusterIP 10.107.232.220 \u0026lt;none\u0026gt; 8001/TCP 13m service/oimcluster-soa-server3 ClusterIP 10.108.203.6 \u0026lt;none\u0026gt; 8001/TCP 13m service/oimcluster-soa-server4 ClusterIP 10.96.178.0 \u0026lt;none\u0026gt; 8001/TCP 13m service/oimcluster-soa-server5 ClusterIP 10.107.83.62 \u0026lt;none\u0026gt; 8001/TCP 13m NAME COMPLETIONS DURATION AGE job.batch/oimcluster-create-fmw-infra-sample-domain-job 1/1 5m30s 36m NAME AGE domain.weblogic.oracle/oimcluster 17m Note: It will take several minutes before all the services listed above show. While the oimcluster-oim-server1 pod has a STATUS of 0/1 the pod is started but the OIG server associated with it is currently starting. While the pod is starting you can check the startup status in the pod logs, by running the following command:\n$ kubectl logs oimcluster-soa-server1 -n oimcluster   The default domain created by the script has the following characteristics:\n An Administration Server named AdminServer listening on port 7001. A configured OIG cluster named oig_cluster of size 5. A configured SOA cluster named soa_cluster of size 5. One started OIG managed Server, named oim_server1, listening on port 14000. One started SOA managed Server, named soa_server1, listening on port 8001. Log files that are located in \u0026lt;persistent_volume\u0026gt;/logs/\u0026lt;domainUID\u0026gt;  Verify the Domain To confirm that the domain was created, use this command:\n$ kubectl describe domain \u0026lt;domain_uid\u0026gt; -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl describe domain oimcluster -n oimcluster Here is an example of the output of this command:\nName: oimcluster Namespace: oimcluster Labels: weblogic.domainUID=oimcluster Annotations: API Version: weblogic.oracle/v8 Kind: Domain Metadata: Creation Timestamp: 2020-09-29T14:08:09Z Generation: 2 Managed Fields: API Version: weblogic.oracle/v8 Fields Type: FieldsV1 fieldsV1: f:metadata: f:annotations: .: f:kubectl.kubernetes.io/last-applied-configuration: f:labels: .: f:weblogic.domainUID: Manager: kubectl Operation: Update Time: 2020-09-29T14:19:58Z API Version: weblogic.oracle/v8 Fields Type: FieldsV1 fieldsV1: f:status: .: f:clusters: f:conditions: f:servers: f:startTime: Manager: OpenAPI-Generator Operation: Update Time: 2020-09-29T14:27:30Z Resource Version: 1278400 Self Link: /apis/weblogic.oracle/v8/namespaces/oimcluster/domains/oimcluster UID: 94604c47-6995-43c5-8848-5c5975ba5ace Spec: Admin Server: Server Pod: Env: Name: USER_MEM_ARGS Value: -Djava.security.egd=file:/dev/./urandom -Xms512m -Xmx1024m Server Start State: RUNNING Clusters: Cluster Name: soa_cluster Replicas: 1 Server Pod: Affinity: Pod Anti Affinity: Preferred During Scheduling Ignored During Execution: Pod Affinity Term: Label Selector: Match Expressions: Key: weblogic.clusterName Operator: In Values: $(CLUSTER_NAME) Topology Key: kubernetes.io/hostname Weight: 100 Server Service: Precreate Service: true Server Start State: RUNNING Cluster Name: oim_cluster Replicas: 1 Server Pod: Affinity: Pod Anti Affinity: Preferred During Scheduling Ignored During Execution: Pod Affinity Term: Label Selector: Match Expressions: Key: weblogic.clusterName Operator: In Values: $(CLUSTER_NAME) Topology Key: kubernetes.io/hostname Weight: 100 Server Service: Precreate Service: true Server Start State: RUNNING Data Home: Domain Home: /u01/oracle/user_projects/domains/oimcluster Domain Home Source Type: PersistentVolume Http Access Log In Log Home: true Image: oracle/oig:12.2.1.4.0 Image Pull Policy: IfNotPresent Image Pull Secrets: Name: oig-docker Include Server Out In Pod Log: true Log Home: /u01/oracle/user_projects/domains/logs/oimcluster Log Home Enabled: true Server Pod: Env: Name: JAVA_OPTIONS Value: -Dweblogic.StdoutDebugEnabled=false Name: USER_MEM_ARGS Value: -Djava.security.egd=file:/dev/./urandom -Xms256m -Xmx1024m Volume Mounts: Mount Path: /u01/oracle/user_projects/domains Name: weblogic-domain-storage-volume Volumes: Name: weblogic-domain-storage-volume Persistent Volume Claim: Claim Name: oimcluster-oim-pvc Server Start Policy: IF_NEEDED Web Logic Credentials Secret: Name: oimcluster-domain-credentials Status: Clusters: Cluster Name: oim_cluster Maximum Replicas: 5 Minimum Replicas: 0 Ready Replicas: 1 Replicas: 1 Replicas Goal: 1 Cluster Name: soa_cluster Maximum Replicas: 5 Minimum Replicas: 0 Ready Replicas: 1 Replicas: 1 Replicas Goal: 1 Conditions: Last Transition Time: 2020-09-29T14:25:51.338Z Reason: ServersReady Status: True Type: Available Servers: Desired State: RUNNING Health: Activation Time: 2020-09-29T14:12:23.439Z Overall Health: ok Subsystems: Subsystem Name: ServerRuntime Symptoms: Node Name: 10.250.111.112 Server Name: AdminServer State: RUNNING Cluster Name: oim_cluster Desired State: RUNNING Health: Activation Time: 2020-09-29T14:25:46.339Z Overall Health: ok Subsystems: Subsystem Name: ServerRuntime Symptoms: Node Name: 10.250.111.112 Server Name: oim_server1 State: RUNNING Cluster Name: oim_cluster Desired State: SHUTDOWN Server Name: oim_server2 Cluster Name: oim_cluster Desired State: SHUTDOWN Server Name: oim_server3 Cluster Name: oim_cluster Desired State: SHUTDOWN Server Name: oim_server4 Cluster Name: oim_cluster Desired State: SHUTDOWN Server Name: oim_server5 Cluster Name: soa_cluster Desired State: RUNNING Health: Activation Time: 2020-09-29T14:15:11.288Z Overall Health: ok Subsystems: Subsystem Name: ServerRuntime Symptoms: Node Name: 10.250.111.112 Server Name: soa_server1 State: RUNNING Cluster Name: soa_cluster Desired State: SHUTDOWN Server Name: soa_server2 Cluster Name: soa_cluster Desired State: SHUTDOWN Server Name: soa_server3 Cluster Name: soa_cluster Desired State: SHUTDOWN Server Name: soa_server4 Cluster Name: soa_cluster Desired State: SHUTDOWN Server Name: soa_server5 Start Time: 2020-09-29T14:08:10.085Z Events: \u0026lt;none\u0026gt; In the Status section of the output, the available servers and clusters are listed.\nVerify the Pods Use the following command to see the pods running the servers and which nodes they are running on:\n$ kubectl get pods -n \u0026lt;namespace\u0026gt; -o wide For example:\n$ kubectl get pods -n oimcluster -o wide The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES helper 1/1 Running 0 3h50m 10.244.1.39 10.250.111.112 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; oimcluster-adminserver 1/1 Running 0 27m 10.244.1.42 10.250.111.112 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; oimcluster-create-fmw-infra-sample-domain-job-dktkk 0/1 Completed 0 47m 10.244.1.40 10.250.111.112 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; oimcluster-oim-server1 1/1 Running 0 16m 10.244.1.44 10.250.111.112 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; oimcluster-soa-server1 1/1 Running 0 24m 10.244.1.43 10.250.111.112 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; You are now ready to configure an Ingress to direct traffic for your OIG domain as per Configure an Ingress for an OIG domain.\n"
},
{
	"uri": "/fmw-kubernetes/20.4.1/oam/manage-oam-domains/logging-and-visualization/",
	"title": "Logging and Visualization",
	"tags": [],
	"description": "Describes the steps for logging and visualization with Elasticsearch and Kibana.",
	"content": "After the OAM domain is set up you can publish operator and WebLogic Server logs into Elasticsearch and interact with them in Kibana.\nIn Prepare your environment if you decided to use the Elasticsearch and Kibana by setting the parameter elkIntegrationEnabled to true, then the steps below must be followed to complete the setup.\nIf you did not set elkIntegrationEnabled to true and want to do so post configuration, run the following command:\n$ helm upgrade --reuse-values --namespace operator --set \u0026#34;elkIntegrationEnabled=true\u0026#34; --set \u0026#34;logStashImage=logstash:6.6.0\u0026#34; --set \u0026#34;elasticSearchHost=elasticsearch.default.svc.cluster.local\u0026#34; --set \u0026#34;elasticSearchPort=9200\u0026#34; --wait weblogic-kubernetes-operator kubernetes/charts/weblogic-operator The output will look similar to the following:\nRelease \u0026#34;weblogic-kubernetes-operator\u0026#34; has been upgraded. Happy Helming! NAME: weblogic-kubernetes-operator LAST DEPLOYED: Fri Sep 25 09:57:11 2020 NAMESPACE: operator STATUS: deployed REVISION: 3 TEST SUITE: None Install Elasticsearch and Kibana   Create the Kubernetes resource using the following command:\n$ kubectl apply -f \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/scripts/elasticsearch-and-kibana/elasticsearch_and_kibana.yaml For example:\n$ kubectl apply -f /scratch/OAMDockerK8S/weblogic-kubernetes-operator/kubernetes/samples/scripts/elasticsearch-and-kibana/elasticsearch_and_kibana.yaml The output will look similar to the following:\ndeployment.apps/elasticsearch created service/elasticsearch created deployment.apps/kibana created service/kibana created   Run the following command to ensure Elasticsearch is used by the operator:\n$ helm get values --all weblogic-kubernetes-operator -n opns The output will look similar to the following:\nCOMPUTED VALUES: dedicated: false domainNamespaces: - accessns elasticSearchHost: elasticsearch.default.svc.cluster.local elasticSearchPort: 9200 elkIntegrationEnabled: true externalDebugHttpPort: 30999 externalRestEnabled: false externalRestHttpsPort: 31001 image: weblogic-kubernetes-operator:3.0.1 imagePullPolicy: IfNotPresent internalDebugHttpPort: 30999 istioEnabled: false javaLoggingLevel: FINE logStashImage: logstash:6.6.0 remoteDebugNodePortEnabled: false serviceAccount: op-sa suspendOnDebugStartup: false   To check that Elasticsearch and Kibana are deployed in the Kubernetes cluster, run the following command:\n$ kubectl get pods The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE elasticsearch-857bd5ff6b-h8bxm 1/1 Running 0 67s kibana-594465687d-84hxz 1/1 Running 0 67s   Create the logstash pod OAM Server logs can be pushed to the Elasticsearch server using the logstash pod. The logstash pod needs access to the persistent volume of the OAM domain created previously, for example accessinfra-domain-pv. The steps to create the logstash pod are as follows:\n  Obtain the OAM domain persistence volume details:\n$ kubectl get pv -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get pv -n accessns The output will look similar to the following:\nNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE accessinfra-domain-pv 10Gi RWX Retain Bound accessns/accessinfra-domain-pvc accessinfra-domain-storage-class 1h12m Make note of the CLAIM value, for example in this case accessinfra-domain-pvc\n  Run the following command to get the mountPath of your domain:\n$ kubectl describe domains \u0026lt;domain_uid\u0026gt; -n \u0026lt;domain_namespace\u0026gt; | grep \u0026#34;Mount Path\u0026#34; For example:\n$ kubectl describe domains accessinfra -n accessns | grep \u0026#34;Mount Path\u0026#34; The output will look similar to the following:\nMount Path: /u01/oracle/user_projects/domains   Navigate to the \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/scripts/elasticsearch-and-kibana directory and create a logstash.yaml file as follows. Change the claimName and mountPath values to match the values returned in the previous commands:\napiVersion: apps/v1 kind: Deployment metadata: name: logstash-wls namespace: accessns spec: selector: matchLabels: k8s-app: logstash-wls template: # create pods using pod definition in this template metadata: labels: k8s-app: logstash-wls spec: volumes: - name: weblogic-domain-storage-volume persistentVolumeClaim: claimName: accessinfra-domain-pvc - name: shared-logs emptyDir: {} containers: - name: logstash image: logstash:6.6.0 command: [\u0026quot;/bin/sh\u0026quot;] args: [\u0026quot;/usr/share/logstash/bin/logstash\u0026quot;, \u0026quot;-f\u0026quot;, \u0026quot;/u01/oracle/user_projects/domains/logstash/logstash.conf\u0026quot;] imagePullPolicy: IfNotPresent volumeMounts: - mountPath: /u01/oracle/user_projects/domains name: weblogic-domain-storage-volume - name: shared-logs mountPath: /shared-logs ports: - containerPort: 5044 name: logstash   In the NFS persistent volume directory that corresponds to the mountPath /u01/oracle/user_projects/domains, create a logstash directory. For example:\nmkdir -p /scratch/OAMDockerK8S/accessdomainpv/logstash   Create a logstash.conf in the newly created logstash directory that contains the following. Make sure the paths correspond to your mountPath and domain name:\ninput { file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/logs/accessinfra/AdminServer*.log\u0026quot; tags =\u0026gt; \u0026quot;Adminserver_log\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/logs/accessinfra/oam_policy_mgr*.log\u0026quot; tags =\u0026gt; \u0026quot;Policymanager_log\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/logs/accessinfra/oam_server*.log\u0026quot; tags =\u0026gt; \u0026quot;Oamserver_log\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/accessinfra/servers/AdminServer/logs/AdminServer-diagnostic.log\u0026quot; tags =\u0026gt; \u0026quot;Adminserver_diagnostic\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/accessinfra/servers/**/logs/oam_policy_mgr*-diagnostic.log\u0026quot; tags =\u0026gt; \u0026quot;Policy_diagnostic\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/accessinfra/servers/**/logs/oam_server*-diagnostic.log\u0026quot; tags =\u0026gt; \u0026quot;Oamserver_diagnostic\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/accessinfra/servers/**/logs/access*.log\u0026quot; tags =\u0026gt; \u0026quot;Access_logs\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/accessinfra/servers/AdminServer/logs/auditlogs/OAM/audit.log\u0026quot; tags =\u0026gt; \u0026quot;Audit_logs\u0026quot; start_position =\u0026gt; beginning } } filter { grok { match =\u0026gt; [ \u0026quot;message\u0026quot;, \u0026quot;\u0026lt;%{DATA:log_timestamp}\u0026gt; \u0026lt;%{WORD:log_level}\u0026gt; \u0026lt;%{WORD:thread}\u0026gt; \u0026lt;%{HOSTNAME:hostname}\u0026gt; \u0026lt;%{HOSTNAME:servername}\u0026gt; \u0026lt;%{DATA:timer}\u0026gt; \u0026lt;\u0026lt;%{DATA:kernel}\u0026gt;\u0026gt; \u0026lt;\u0026gt; \u0026lt;%{DATA:uuid}\u0026gt; \u0026lt;%{NUMBER:timestamp}\u0026gt; \u0026lt;%{DATA:misc}\u0026gt; \u0026lt;%{DATA:log_number}\u0026gt; \u0026lt;%{DATA:log_message}\u0026gt;\u0026quot; ] } if \u0026quot;_grokparsefailure\u0026quot; in [tags] { mutate { remove_tag =\u0026gt; [ \u0026quot;_grokparsefailure\u0026quot; ] } } } output { elasticsearch { hosts =\u0026gt; [\u0026quot;elasticsearch.default.svc.cluster.local:9200\u0026quot;] } }   Deploy the logstash pod by executing the following command:\n$ kubectl create -f \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/scripts/elasticsearch-and-kibana/logstash.yaml The output will look similar to the following:\ndeployment.apps/logstash-wls created   Run the following command to check the logstash pod is created correctly:\n$ kubectl get pods -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl get pods -n accessns The output should look similar to the following:\nNAME READY STATUS RESTARTS AGE accessinfra-adminserver 1/1 Running 0 36m accessinfra-create-oam-infra-domain-job-vj69h 0/1 Completed 0 24h accessinfra-oam-policy-mgr1 1/1 Running 0 33m accessinfra-oam-server1 1/1 Running 0 33m accessinfra-oam-server2 1/1 Running 0 33m helper 1/1 Running 0 41h logstash-wls-7957897645-67c4k 1/1 Running 0 7s voyager-accessinfra-voyager-698764d6d-w8pbt 1/1 Running 0 66m Then run the following to get the Elasticsearch pod name:\n$ kubectl get pods The output should look similar to the following:\nNAME READY STATUS RESTARTS AGE elasticsearch-857bd5ff6b-h8bxm 1/1 Running 0 5m45s kibana-594465687d-84hxz 1/1 Running 0 5m45s   Verify and access the Kibana console   Check if the indices are created correctly in the elasticsearch pod:\n$ kubectl exec -it elasticsearch-857bd5ff6b-h8bxm -- /bin/bash This will take you into a bash shell in the elasticsearch pod:\n[root@elasticsearch-857bd5ff6b-h8bxm elasticsearch]#   In the elasticsearch bash shell, run the following to check the indices:\n[root@elasticsearch-857bd5ff6b-h8bxm elasticsearch]# curl -i \u0026#34;127.0.0.1:9200/_cat/indices?v\u0026#34; The output will look similar to the following:\nHTTP/1.1 200 OK content-type: text/plain; charset=UTF-8 content-length: 696 health status index uuid pri rep docs.count docs.deleted store.size pri.store.size yellow open logstash-2020.09.23 -kVgdpB7TPSwnjvhEDD2RA 5 1 825 0 406.6kb 406.6kb green open .kibana_1 F6DNmwQ5SZaOM7I2LonEVw 1 0 2 0 7.6kb 7.6kb yellow open logstash-2020.09.25 9QQA-DwvQay8uOAe3dvKuQ 5 1 149293 0 39.3mb 39.3mb yellow open logstash-2020.09.24 t5N8O0LxRRabND6StHFgSg 5 1 69748 0 21.1mb 21.1mb green open .kibana_task_manager kt1uSgpnSGWgWR8nKDuiVA 1 0 2 0 12.5kb 12.5kb Exit the bash shell by typing exit.\n  Find the Kibana port by running the following command:\n$ kubectl get svc The output will look similar to the following:\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE elasticsearch ClusterIP 10.97.144.163 \u0026lt;none\u0026gt; 9200/TCP,9300/TCP 9m25s kibana NodePort 10.103.150.116 \u0026lt;none\u0026gt; 5601:30707/TCP 9m25s kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 47h In the example above the Kibana port is 30707.\n  Access the Kibana console with http://${MASTERNODE-HOSTNAME}:${KIBANA-PORT}/app/kibana.\n  Click Dashboard and in the Create index pattern page enter logstash*. Click Next Step.\n  From the Time Filter field name drop down menu select @timestamp and click Create index pattern.\n  Once the index pattern is created click on Discover in the navigation menu to view the logs.\n  For more details on how to use the Kibana console see the Kibana Guide\n"
},
{
	"uri": "/fmw-kubernetes/20.4.1/oig/manage-oig-domains/running-oig-utilities/",
	"title": "Runnning OIG Utilities",
	"tags": [],
	"description": "Describes the steps for running OIG utilities in Kubernetes.",
	"content": "Run OIG utlities inside the OIG Kubernetes cluster.\nRun utilities in an interactive bash shell   Access a bash shell inside the oimcluster-oim-server1 pod:\n$ kubectl -n oimcluster exec -it oimcluster-oim-server1 -- bash This will take you into a bash shell in the running oimcluster-oim-server1 pod:\n[oracle@oimcluster-oim-server1 oracle]$   Navigate to the /u01/oracle/idm/server/bin directory and execute the utility as required. For example:\n[oracle@oimcluster-oim-server1 oracle] cd /u01/oracle/idm/server/bin [oracle@oimcluster-oim-server1 bin]$ ./\u0026lt;filename\u0026gt;.sh   Passing inputs as a jar/xml file   Copy the input file to pass to a directory of your choice.\n  Run the following command to copy the input file to the running oimcluster-oim-server1 pod.\n$ kubectl -n oimcluster cp /\u0026lt;path\u0026gt;/\u0026lt;inputFile\u0026gt; oimcluster-oim-server1:/u01/oracle/idm/server/bin/   Access a bash shell inside the oimcluster-oim-server1 pod:\n$ kubectl -n oimcluster exec -it oimcluster-oim-server1 -- bash This will take you into a bash shell in the running oimcluster-oim-server1 pod:\n[oracle@oimcluster-oim-server1 oracle]$   Navigate to the /u01/oracle/idm/server/bin directory and execute the utility as required, passing the input file. For example:\n[oracle@oimcluster-oim-server1 oracle] cd /u01/oracle/idm/server/bin [oracle@oimcluster-oim-server1 bin]$ ./\u0026lt;filename\u0026gt;.sh -inputFile \u0026lt;inputFile\u0026gt; Note As pods are stateless the copied input file will remain until the pod restarts.\n  Editing property/profile files To edit a property/profile file in the Kubernetes cluster:\n  Copy the input file from the pod to a on the local system, for example:\n$ kubectl -n oimcluster cp oimcluster-oim-server1:/u01/oracle/idm/server/bin/\u0026lt;file.properties_profile\u0026gt; /\u0026lt;path\u0026gt;/\u0026lt;file.properties_profile\u0026gt; Note: If you see the message tar: Removing leading '/' from member names this can be ignored.\n  Edit the \u0026lt;/path\u0026gt;/\u0026lt;file.properties_profile\u0026gt; in an editor of your choice.\n  Copy the file back to the pod:\n$ kubectl -n oimcluster cp /\u0026lt;path\u0026gt;/\u0026lt;file.properties_profile\u0026gt; oimcluster-oim-server1:/u01/oracle/idm/server/bin/ Note: As pods are stateless the copied input file will remain until the pod restarts. Preserve a local copy in case you need to copy files back after pod restart.\n  "
},
{
	"uri": "/fmw-kubernetes/20.4.1/oud/create-oud-instances-helm/",
	"title": "Create Oracle Unified Directory Instances Using Helm",
	"tags": [],
	"description": "This document provides steps to create Oracle Unified Directory instances using Helm Charts.",
	"content": " Introduction Install Helm Deploy an Application using the Helm Chart Undeploy an Application using the Helm Chart Helm Chart(s) for Oracle Unified Directory  Introduction This chapter demonstrates how to deploy Oracle Unified Directory 12c instance(s) using the Helm package manager for Kubernetes. Helm Chart(s) described here can be used to facilitate installation, configuration, and environment setup within a Kubernetes environment.\nInstall Helm Helm can be used to create and deploy the Oracle Unified Directory resources in a Kubernetes cluster. For Helm installation and usage information, refer to the README.\nDeploy an Application using the Helm Chart The helm install command is used to deploy applications to a Kubernetes environment, using the Helm Chart supplied.\n$ helm install [Deployment NAME] [CHART Reference] [flags] For example:\n$ helm install my-oud-ds-rs oud-ds-rs --namespace myhelmns Undeploy an Application using the Helm Chart To uninstall an application deployed using a Helm chart you need to identify the release name and then issue a delete command:\nTo get the release name:\n$ helm --namespace \u0026lt;namespace\u0026gt; list For example:\n$ helm --namespace myhelmns list NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION my-oud-ds-rs- myhelmns 1 2020-03-31 10:37:30.616927678 -0700 PDT deployed oud-ds-rs-12.2.1.4.0 12.2.1.4.0 To delete the chart:\n$ helm uninstall --namespace \u0026lt;namespace\u0026gt; \u0026lt;release\u0026gt; For example:\n$ helm uninstall --namespace myhelmns my-oud-ds-rs release \u0026quot;my-oud-ds-rs\u0026quot; uninstalled Helm Chart(s) for Oracle Unified Directory The following list provides Helm charts for deploying Oracle Unified Directory in a Kubernetes environment. Helm charts provided can be found in the project at the following location:\nhttps://github.com/oracle/fmw-kubernetes/tree/master/OracleUnifiedDirectory/kubernetes/helm\nDetails about each Helm Chart can be found in the relevant README listed below:\n oud-ds-rs : A Helm chart for deployment of Oracle Unified Directory Directory (DS+RS) instances on Kubernetes.  "
},
{
	"uri": "/fmw-kubernetes/20.4.1/oudsm/create-oudsm-instances-helm/",
	"title": "Create Oracle Unified Directory Services Manager Instances Using Helm",
	"tags": [],
	"description": "This document provides steps to create OUDSM instances using Helm Charts.",
	"content": " Introduction Install Helm Deploy an Application using the Helm Chart Undeploy an Application using the Helm Chart Helm Chart(s) for Oracle Unified Directory Services Manager  Introduction This chapter demonstrates how to deploy Oracle Unified Directory Services Manager 12c instance(s) using the Helm package manager for Kubernetes. Helm Chart(s) described here can be used to facilitate installation, configuration, and environment setup within a Kubernetes environment.\nInstall Helm Helm can be used to create and deploy the Oracle Unified Directory Services Manager resources in a Kubernetes cluster. For Helm installation and usage information, refer to the README.\nDeploy an Application using the Helm Chart The helm install command is used to deploy applications to a Kubernetes environment, using the Helm Chart supplied.\n$ helm install [Deployment NAME] [CHART Reference] [flags] For example:\n$ helm install my-oudsm oudsm --namespace myhelmns Undeploy an Application using the Helm Chart To uninstall an application deployed using a Helm chart you need to identify the release name and then issue a delete command:\nTo get the release name:\n$ helm --namespace \u0026lt;namespace\u0026gt; list For example:\n$ helm --namespace myhelmns list NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION my-oudsm myhelmns 1 2020-03-31 10:37:30.616927678 -0700 PDT deployed my-oudsm-12.2.1.4.0 12.2.1.4.0 To delete the chart:\n$ helm uninstall --namespace \u0026lt;namespace\u0026gt; \u0026lt;release\u0026gt; For example:\n$ helm uninstall --namespace myhelmns my-oudsm release \u0026quot;my-oudsm\u0026quot; uninstalled Helm Chart(s) for Oracle Unified Directory Services Manager The following list provides Helm charts for deploying Oracle Unified Directory Services Manager in a Kubernetes environment. The following list provides Helm charts for deploying Oracle Unified Directory Services Manager in a Kubernetes environment. Helm charts provided can be found in the project at the following location:\nhttps://github.com/oracle/fmw-kubernetes/tree/master/OracleUnifiedDirectorySM/kubernetes/helm\nDetails about each Helm Chart can be found in the relevant README listed below:\n oudsm : A Helm chart for deployment of Oracle Unified Directory Services Manager instances on Kubernetes.  "
},
{
	"uri": "/fmw-kubernetes/20.4.1/oud/",
	"title": "Oracle Unified Directory",
	"tags": [],
	"description": "Oracle Unified Directory provides a comprehensive Directory Solution for robust Identity Management",
	"content": "Oracle Unified Directory provides a comprehensive Directory Solution for robust Identity Management. Oracle Unified Directory is an all-in-one directory solution with storage, proxy, synchronization and virtualization capabilities. While unifying the approach, it provides all the services required for high-performance Enterprise and carrier-grade environments. Oracle Unified Directory ensures scalability to billions of entries, ease of installation, elastic deployments, enterprise manageability and effective monitoring.\nThis project supports deployment of Oracle Unified Directory (OUD) Docker images based on the 12cPS4 (12.2.1.4.0) release within a Kubernetes environment. The OUD Docker Image refers to binaries for OUD Release 12.2.1.4.0 and it has the capability to create different types of OUD Instances (Directory Service, Proxy, Replication) in containers.\nImage: oracle/oud:12.2.1.4.0\nThis project has several key features to assist you with deploying and managing Oracle Unified Directory in a Kubernetes environment. You can:\n Create Oracle Unified Directory instances in a Kubernetes persistent volume (PV). This PV can reside in an NFS file system or other Kubernetes volume types. Start servers based on declarative startup parameters and desired states. Expose the Oracle Unified Directory services for external access. Scale Oracle Unified Directory by starting and stopping servers on demand. Monitor the Oracle Unified Directory instance using Prometheus and Grafana.  Follow the instructions in this guide to set up Oracle Unified Directory on Kubernetes.\nGetting started For detailed information about deploying Oracle Unified Directory, start at Prerequisites and follow this documentation sequentially.\nCurrent release The current supported release of Oracle Unified Directory is OUD 12c PS4 (12.2.1.4.0)\n"
},
{
	"uri": "/fmw-kubernetes/20.4.1/oam/configure-ingress/",
	"title": "Configure an Ingress for an OAM domain",
	"tags": [],
	"description": "This document provides steps to configure an Ingress to direct traffic to the OAM domain.",
	"content": "Choose one of the following supported methods to configure an Ingress to direct traffic for your OAM domain.\n a. Using an Ingress with NGINX  Steps to set up an Ingress for NGINX to direct traffic to the OAM domain.\n b. Using an Ingress with Voyager  Steps to set up an Ingress for Voyager to direct traffic to the OAM domain.\n "
},
{
	"uri": "/fmw-kubernetes/20.4.1/oig/configure-ingress/",
	"title": "Configure an Ingress for an OIG domain",
	"tags": [],
	"description": "This document provides steps to configure an Ingress to direct traffic to the OIG domain.",
	"content": "Choose one of the following supported methods to configure an Ingress to direct traffic for your OIG domain.\n a. Using an Ingress with NGINX (non-SSL)  Steps to set up an Ingress for NGINX to direct traffic to the OIG domain (non-SSL).\n b. Using an Ingress with NGINX (SSL)  Steps to set up an Ingress for NGINX to direct traffic to the OIG domain (SSL).\n c. Using an Ingress with Voyager (non-SSL)  Steps to set up an Ingress for Voyager to direct traffic to the OIG domain (non-SSL).\n d. Using an Ingress with Voyager (SSL)  Steps to set up an Ingress for Voyager to direct traffic to the OIG domain (SSL).\n "
},
{
	"uri": "/fmw-kubernetes/20.4.1/oig/manage-oig-domains/logging-and-visualization/",
	"title": "Logging and Visualization",
	"tags": [],
	"description": "Describes the steps for logging and visualization with Elasticsearch and Kibana.",
	"content": "After the OIG domain is set up you can publish operator and WebLogic Server logs into Elasticsearch and interact with them in Kibana.\nIn Prepare your environment if you decided to use the Elasticsearch and Kibana by setting the parameter elkIntegrationEnabled to true, then the steps below must be followed to complete the setup.\nIf you did not set elkIntegrationEnabled to true and want to do so post configuration, run the following command:\n$ helm upgrade --reuse-values --namespace operator --set \u0026#34;elkIntegrationEnabled=true\u0026#34; --set \u0026#34;logStashImage=logstash:6.6.0\u0026#34; --set \u0026#34;elasticSearchHost=elasticsearch.default.svc.cluster.local\u0026#34; --set \u0026#34;elasticSearchPort=9200\u0026#34; --wait weblogic-kubernetes-operator kubernetes/charts/weblogic-operator The output will look similar to the following:\nRelease \u0026#34;weblogic-kubernetes-operator\u0026#34; has been upgraded. Happy Helming! NAME: weblogic-kubernetes-operator LAST DEPLOYED: Tue Aug 18 05:57:11 2020 NAMESPACE: operator STATUS: deployed REVISION: 3 TEST SUITE: None Install Elasticsearch and Kibana   Create the Kubernetes resource using the following command:\n$ kubectl apply -f \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/scripts/elasticsearch-and-kibana/elasticsearch_and_kibana.yaml For example:\n$ kubectl apply -f /scratch/OIGDockerK8S/weblogic-kubernetes-operator/kubernetes/samples/scripts/elasticsearch-and-kibana/elasticsearch_and_kibana.yaml The output will look similar to the following:\ndeployment.apps/elasticsearch created service/elasticsearch created deployment.apps/kibana created service/kibana created   Run the following command to ensure Elasticsearch is used by the operator:\n$ helm get values --all weblogic-kubernetes-operator -n operator The output will look similar to the following:\nCOMPUTED VALUES: dedicated: false domainNamespaces: - oimcluster elasticSearchHost: elasticsearch.default.svc.cluster.local elasticSearchPort: 9200 elkIntegrationEnabled: true externalDebugHttpPort: 30999 externalRestEnabled: false externalRestHttpsPort: 31001 image: weblogic-kubernetes-operator:3.0.1 imagePullPolicy: IfNotPresent internalDebugHttpPort: 30999 istioEnabled: false javaLoggingLevel: INFO logStashImage: logstash:6.6.0 remoteDebugNodePortEnabled: false serviceAccount: operator-serviceaccount suspendOnDebugStartup: false   To check that Elasticsearch and Kibana are deployed in the Kubernetes cluster, run the following command:\n$ kubectl get pods The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE elasticsearch-857bd5ff6b-tvqdn 1/1 Running 0 2m9s kibana-594465687d-zc2rt 1/1 Running 0 2m9s   Create the logstash pod OIG Server logs can be pushed to the Elasticsearch server using the logstash pod. The logstash pod needs access to the persistent volume of the OIG domain created previously, for example oimcluster-oim-pv. The steps to create the logstash pod are as follows:\n  Obtain the OIG domain persistence volume details:\n$ kubectl get pv -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get pv -n oimcluster The output will look similar to the following:\nNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE oimcluster-oim-pv 10Gi RWX Retain Bound oimcluster/oimcluster-oim-pvc oimcluster-oim-storage-class 28h Make note of the CLAIM value, for example in this case oimcluster-oim-pvc\n  Run the following command to get the mountPath of your domain:\n$ kubectl describe domains \u0026lt;domain_uid\u0026gt; -n \u0026lt;domain_namespace\u0026gt; | grep \u0026#34;Mount Path\u0026#34; For example:\n$ kubectl describe domains oimcluster -n oimcluster | grep \u0026#34;Mount Path\u0026#34; The output will look similar to the following:\nMount Path: /u01/oracle/user_projects/domains   Navigate to the \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/scripts/elasticsearch-and-kibana directory and create a logstash.yaml file as follows. Change the claimName and mountPath values to match the values returned in the previous commands:\napiVersion: apps/v1 kind: Deployment metadata: name: logstash-wls namespace: oimcluster spec: selector: matchLabels: k8s-app: logstash-wls template: # create pods using pod definition in this template metadata: labels: k8s-app: logstash-wls spec: volumes: - name: weblogic-domain-storage-volume persistentVolumeClaim: claimName: oimcluster-oim-pvc - name: shared-logs emptyDir: {} containers: - name: logstash image: logstash:6.6.0 command: [\u0026quot;/bin/sh\u0026quot;] args: [\u0026quot;/usr/share/logstash/bin/logstash\u0026quot;, \u0026quot;-f\u0026quot;, \u0026quot;/u01/oracle/user_projects/domains/logstash/logstash.conf\u0026quot;] imagePullPolicy: IfNotPresent volumeMounts: - mountPath: /u01/oracle/user_projects/domains name: weblogic-domain-storage-volume - name: shared-logs mountPath: /shared-logs ports: - containerPort: 5044 name: logstash   In the NFS persistent volume directory that corresponds to the mountPath /u01/oracle/user_projects/domains, create a logstash directory. For example:\n$ mkdir -p /scratch/OIGDockerK8S/oimclusterdomainpv/logstash   Create a logstash.conf in the newly created logstash directory that contains the following. Make sure the paths correspond to your mountPath and domain name:\ninput { file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/logs/oimcluster/AdminServer*.log\u0026quot; tags =\u0026gt; \u0026quot;Adminserver_log\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/logs/oimcluster/soa_server*.log\u0026quot; tags =\u0026gt; \u0026quot;soaserver_log\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/logs/oimcluster/oim_server*.log\u0026quot; tags =\u0026gt; \u0026quot;Oimserver_log\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/oimcluster/servers/AdminServer/logs/AdminServer-diagnostic.log\u0026quot; tags =\u0026gt; \u0026quot;Adminserver_diagnostic\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/oimcluster/servers/**/logs/soa_server*-diagnostic.log\u0026quot; tags =\u0026gt; \u0026quot;Soa_diagnostic\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/oimcluster/servers/**/logs/oim_server*-diagnostic.log\u0026quot; tags =\u0026gt; \u0026quot;Oimserver_diagnostic\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/oimcluster/servers/**/logs/access*.log\u0026quot; tags =\u0026gt; \u0026quot;Access_logs\u0026quot; start_position =\u0026gt; beginning } } filter { grok { match =\u0026gt; [ \u0026quot;message\u0026quot;, \u0026quot;\u0026lt;%{DATA:log_timestamp}\u0026gt; \u0026lt;%{WORD:log_level}\u0026gt; \u0026lt;%{WORD:thread}\u0026gt; \u0026lt;%{HOSTNAME:hostname}\u0026gt; \u0026lt;%{HOSTNAME:servername}\u0026gt; \u0026lt;%{DATA:timer}\u0026gt; \u0026lt;\u0026lt;%{DATA:kernel}\u0026gt;\u0026gt; \u0026lt;\u0026gt; \u0026lt;%{DATA:uuid}\u0026gt; \u0026lt;%{NUMBER:timestamp}\u0026gt; \u0026lt;%{DATA:misc}\u0026gt; \u0026lt;%{DATA:log_number}\u0026gt; \u0026lt;%{DATA:log_message}\u0026gt;\u0026quot; ] } if \u0026quot;_grokparsefailure\u0026quot; in [tags] { mutate { remove_tag =\u0026gt; [ \u0026quot;_grokparsefailure\u0026quot; ] } } } output { elasticsearch { hosts =\u0026gt; [\u0026quot;elasticsearch.default.svc.cluster.local:9200\u0026quot;] } }   Deploy the logstash pod by executing the following command:\n$ kubectl create -f \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/scripts/elasticsearch-and-kibana/logstash.yaml The output will look similar to the following:\ndeployment.apps/logstash-wls created   Run the following command to check the logstash pod is created correctly:\n$ kubectl get pods -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl get pods -n oimcluster The output should look similar to the following:\nNAME READY STATUS RESTARTS AGE logstash-wls-85867765bc-bhs54 1/1 Running 0 9s oig-design-console 1/1 Running 1 160m oimcluster-adminserver 1/1 Running 0 90m oimcluster-create-fmw-infra-sample-domain-job-dktkk 0/1 Completed 0 25h oimcluster-oim-server1 1/1 Running 0 87m oimcluster-soa-server1 1/1 Running 0 87m Then run the following to get the Elasticsearch pod name:\n$ kubectl get pods The output should look similar to the following:\nNAME READY STATUS RESTARTS AGE elasticsearch-857bd5ff6b-tvqdn 1/1 Running 0 7m48s kibana-594465687d-zc2rt 1/1 Running 0 7m48s   Verify and access the Kibana console   Check if the indices are created correctly in the elasticsearch pod:\n$ kubectl exec -it elasticsearch-857bd5ff6b-tvqdn -- /bin/bash This will take you into a bash shell in the elasticsearch pod:\n[root@elasticsearch-857bd5ff6b-tvqdn elasticsearch]#   In the elasticsearch bash shell run the following to check the indices:\n[root@elasticsearch-857bd5ff6b-tvqdn elasticsearch]# curl -i \u0026quot;127.0.0.1:9200/_cat/indices?v\u0026quot; The output will look similar to the following:\nHTTP/1.1 200 OK content-type: text/plain; charset=UTF-8 content-length: 580 health status index uuid pri rep docs.count docs.deleted store.size pri.store.size green open .kibana_task_manager 1qQ-C21GQJa38lAR28_7iA 1 0 2 0 12.6kb 12.6kb green open .kibana_1 TwIdqENXTqm6mZlBRVy__A 1 0 2 0 7.6kb 7.6kb yellow open logstash-2020.09.30 6LuZLYgARYCGGN-yZT5bJA 5 1 90794 0 22mb 22mb yellow open logstash-2020.09.29 QBYQrolXRiW9l8Ct3DrSyQ 5 1 38 0 86.3kb 86.3kb Exit the bash shell by typing exit.\n  Find the Kibana port by running the following command:\n$ kubectl get svc The output will look similar to the following:\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE elasticsearch ClusterIP 10.107.79.44 \u0026lt;none\u0026gt; 9200/TCP,9300/TCP 11m kibana NodePort 10.103.60.126 \u0026lt;none\u0026gt; 5601:31490/TCP 11m kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 7d5h In the example above the Kibana port is 31490.\n  Access the Kibana console with http://${MASTERNODE-HOSTNAME}:${KIBANA-PORT}/app/kibana.\n  Click on Dashboard in the left hand Navigation Menu.\n  In the Create index pattern page enter logstash* and click Next Step.\n  From the Time Filter field name drop down menu select @timestamp and click Create index pattern.\n  Once the index pattern is created click on Discover in the navigation menu to view the logs.\n  For more details on how to use the Kibana console see the Kibana Guide\n"
},
{
	"uri": "/fmw-kubernetes/20.4.1/oam/manage-oam-domains/monitoring-oam-domains/",
	"title": "Monitoring an OAM domain",
	"tags": [],
	"description": "Describes the steps for Monitoring the OAM domain.",
	"content": "After the OAM domain is set up you can monitor the OAM instance using Prometheus and Grafana. See Monitoring a domain.\nThe WebLogic Monitoring Exporter uses the WLS RESTful Management API to scrape runtime information and then exports Prometheus-compatible metrics. It is deployed as a web application in a WebLogic Server (WLS) instance, version 12.2.1 or later, typically, in the instance from which you want to get metrics.\nDeploy the Prometheus operator   Clone Prometheus by running the following commands:\n$ cd \u0026lt;work directory\u0026gt; $ git clone https://github.com/coreos/kube-prometheus.git Note: Please refer the compatibility matrix of Kube Prometheus. Please download the release of the repository according to the Kubernetes version of your cluster. In the above example the latest release will be downloaded.\nFor example:\n$ cd /scratch/OAMDockerK8S $ git clone https://github.com/coreos/kube-prometheus.git   Run the following command to create the namespace and custom resource definitions:\n$ cd kube-prometheus $ kubectl create -f manifests/setup The output will look similar to the following:\nkubectl create -f manifests/setup namespace/monitoring created customresourcedefinition.apiextensions.k8s.io/alertmanagers.monitoring.coreos.com created customresourcedefinition.apiextensions.k8s.io/podmonitors.monitoring.coreos.com created customresourcedefinition.apiextensions.k8s.io/probes.monitoring.coreos.com created customresourcedefinition.apiextensions.k8s.io/prometheuses.monitoring.coreos.com created customresourcedefinition.apiextensions.k8s.io/prometheusrules.monitoring.coreos.com created customresourcedefinition.apiextensions.k8s.io/servicemonitors.monitoring.coreos.com created customresourcedefinition.apiextensions.k8s.io/thanosrulers.monitoring.coreos.com created clusterrole.rbac.authorization.k8s.io/prometheus-operator created clusterrolebinding.rbac.authorization.k8s.io/prometheus-operator created deployment.apps/prometheus-operator created service/prometheus-operator created serviceaccount/prometheus-operator created   Run the following command to created the rest of the resources:\n$ kubectl create -f manifests/ The output will look similar to the following:\nalertmanager.monitoring.coreos.com/main created secret/alertmanager-main created service/alertmanager-main created serviceaccount/alertmanager-main created servicemonitor.monitoring.coreos.com/alertmanager created secret/grafana-datasources created configmap/grafana-dashboard-apiserver created configmap/grafana-dashboard-cluster-total created configmap/grafana-dashboard-controller-manager created configmap/grafana-dashboard-k8s-resources-cluster created configmap/grafana-dashboard-k8s-resources-namespace created configmap/grafana-dashboard-k8s-resources-node created configmap/grafana-dashboard-k8s-resources-pod created configmap/grafana-dashboard-k8s-resources-workload created configmap/grafana-dashboard-k8s-resources-workloads-namespace created configmap/grafana-dashboard-kubelet created configmap/grafana-dashboard-namespace-by-pod created configmap/grafana-dashboard-namespace-by-workload created configmap/grafana-dashboard-node-cluster-rsrc-use created configmap/grafana-dashboard-node-rsrc-use created configmap/grafana-dashboard-nodes created configmap/grafana-dashboard-persistentvolumesusage created configmap/grafana-dashboard-pod-total created configmap/grafana-dashboard-prometheus-remote-write created configmap/grafana-dashboard-prometheus created configmap/grafana-dashboard-proxy created configmap/grafana-dashboard-scheduler created configmap/grafana-dashboard-statefulset created configmap/grafana-dashboard-workload-total created configmap/grafana-dashboards created deployment.apps/grafana created service/grafana created serviceaccount/grafana created servicemonitor.monitoring.coreos.com/grafana created clusterrole.rbac.authorization.k8s.io/kube-state-metrics created clusterrolebinding.rbac.authorization.k8s.io/kube-state-metrics created deployment.apps/kube-state-metrics created service/kube-state-metrics created serviceaccount/kube-state-metrics created servicemonitor.monitoring.coreos.com/kube-state-metrics created clusterrole.rbac.authorization.k8s.io/node-exporter created clusterrolebinding.rbac.authorization.k8s.io/node-exporter created daemonset.apps/node-exporter created service/node-exporter created serviceaccount/node-exporter created servicemonitor.monitoring.coreos.com/node-exporter created apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created clusterrole.rbac.authorization.k8s.io/prometheus-adapter created clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created clusterrolebinding.rbac.authorization.k8s.io/prometheus-adapter created clusterrolebinding.rbac.authorization.k8s.io/resource-metrics:system:auth-delegator created clusterrole.rbac.authorization.k8s.io/resource-metrics-server-resources created configmap/adapter-config created deployment.apps/prometheus-adapter created rolebinding.rbac.authorization.k8s.io/resource-metrics-auth-reader created service/prometheus-adapter created serviceaccount/prometheus-adapter created servicemonitor.monitoring.coreos.com/prometheus-adapter created clusterrole.rbac.authorization.k8s.io/prometheus-k8s created clusterrolebinding.rbac.authorization.k8s.io/prometheus-k8s created servicemonitor.monitoring.coreos.com/prometheus-operator created prometheus.monitoring.coreos.com/k8s created rolebinding.rbac.authorization.k8s.io/prometheus-k8s-config created rolebinding.rbac.authorization.k8s.io/prometheus-k8s created rolebinding.rbac.authorization.k8s.io/prometheus-k8s created rolebinding.rbac.authorization.k8s.io/prometheus-k8s created role.rbac.authorization.k8s.io/prometheus-k8s-config created role.rbac.authorization.k8s.io/prometheus-k8s created role.rbac.authorization.k8s.io/prometheus-k8s created role.rbac.authorization.k8s.io/prometheus-k8s created prometheusrule.monitoring.coreos.com/prometheus-k8s-rules created service/prometheus-k8s created serviceaccount/prometheus-k8s created servicemonitor.monitoring.coreos.com/prometheus created servicemonitor.monitoring.coreos.com/kube-apiserver created servicemonitor.monitoring.coreos.com/coredns created servicemonitor.monitoring.coreos.com/kube-controller-manager created servicemonitor.monitoring.coreos.com/kube-scheduler created servicemonitor.monitoring.coreos.com/kubelet created   Kube-Prometheus requires all nodes to be labelled with kubernetes.io/os=linux. To check if your nodes are labelled, run the following:\n$ kubectl get nodes --show-labels If the nodes are labelled the output will look similar to the following:\nNAME STATUS ROLES AGE VERSION LABELS worker-node1 Ready \u0026lt;none\u0026gt; 42d v1.18.4 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=worker-node1,kubernetes.io/os=linux worker-node2 Ready \u0026lt;none\u0026gt; 42d v1.18.4 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=worker-node2,kubernetes.io/os=linux master-node Ready master 42d v1.18.4 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=master-node,kubernetes.io/os=linux,node-role.kubernetes.io/master= If the nodes are not labelled, run the following command:\n$ kubectl label nodes --all kubernetes.io/os=linux   Provide external access for Grafana, Prometheus, and Alertmanager, by running the following commands:\n$ kubectl patch svc grafana -n monitoring --type=json -p \u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/type\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;NodePort\u0026#34; },{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/ports/0/nodePort\u0026#34;, \u0026#34;value\u0026#34;: 32100 }]\u0026#39; $ kubectl patch svc prometheus-k8s -n monitoring --type=json -p \u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/type\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;NodePort\u0026#34; },{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/ports/0/nodePort\u0026#34;, \u0026#34;value\u0026#34;: 32101 }]\u0026#39; $ kubectl patch svc alertmanager-main -n monitoring --type=json -p \u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/type\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;NodePort\u0026#34; },{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/ports/0/nodePort\u0026#34;, \u0026#34;value\u0026#34;: 32102 }]\u0026#39; Note: This assigns port 32100 to Grafana, 32101 to Prometheus, and 32102 to Alertmanager.\nThe output will look similar to the following:\nservice/grafana patched service/prometheus-k8s patched service/alertmanager-main patched   Verify that the Prometheus, Grafana, and Alertmanager pods are running in the monitoring namespace and the respective services have the exports configured correctly:\n$ kubectl get pods,services -o wide -n monitoring The output should look similar to the following:\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/alertmanager-main-0 2/2 Running 0 62s 10.244.2.10 worker-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/alertmanager-main-1 2/2 Running 0 62s 10.244.1.19 worker-node1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/alertmanager-main-2 2/2 Running 0 62s 10.244.2.11 worker-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/grafana-86445dccbb-xz5d5 1/1 Running 0 62s 10.244.1.20 worker-node1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/kube-state-metrics-b5b74495f-8bglg 3/3 Running 0 62s 10.244.1.21 worker-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/node-exporter-wj4jw 2/2 Running 0 62s 10.196.4.112 master-node \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/node-exporter-wl2jv 2/2 Running 0 62s 10.250.111.112 worker-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/node-exporter-wt88k 2/2 Running 0 62s 10.250.111.111 worker-node1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/prometheus-adapter-66b855f564-4pmwk 1/1 Running 0 62s 10.244.2.12 worker-node1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/prometheus-k8s-0 3/3 Running 1 62s 10.244.2.13 worker-node1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/prometheus-k8s-1 3/3 Running 1 62s 10.244.1.22 worker-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/prometheus-operator-8ff9cc68-6q9lc 2/2 Running 0 69s 10.244.2.18 worker-node1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service/alertmanager-main NodePort 10.106.217.213 \u0026lt;none\u0026gt; 9093:32102/TCP 62s alertmanager=main,app=alertmanager service/alertmanager-operated ClusterIP None \u0026lt;none\u0026gt; 9093/TCP,9094/TCP,9094/UDP 62s app=alertmanager service/grafana NodePort 10.97.246.92 \u0026lt;none\u0026gt; 3000:32100/TCP 62s app=grafana service/kube-state-metrics ClusterIP None \u0026lt;none\u0026gt; 8443/TCP,9443/TCP 62s app.kubernetes.io/name=kube-state-metrics service/node-exporter ClusterIP None \u0026lt;none\u0026gt; 9100/TCP 62s app.kubernetes.io/name=node-exporter service/prometheus-adapter ClusterIP 10.109.14.232 \u0026lt;none\u0026gt; 443/TCP 62s name=prometheus-adapter service/prometheus-k8s NodePort 10.101.68.142 \u0026lt;none\u0026gt; 9090:32101/TCP 62s app=prometheus,prometheus=k8s service/prometheus-operated ClusterIP None \u0026lt;none\u0026gt; 9090/TCP 62s app=prometheus service/prometheus-operator ClusterIP None \u0026lt;none\u0026gt; 8443/TCP 70 app.kubernetes.io/component=controller,app.kubernetes.io/name=prometheus-operator   Deploy WebLogic Monitoring Exporter   Download WebLogic Monitoring Exporter:\n$ mkdir -p \u0026lt;work_directory\u0026gt;/wls_exporter $ cd \u0026lt;work_directory\u0026gt;/wls_exporter $ wget https://github.com/oracle/weblogic-monitoring-exporter/releases/download/\u0026lt;version\u0026gt;/wls-exporter.war $ wget https://github.com/oracle/weblogic-monitoring-exporter/releases/download/\u0026lt;version\u0026gt;/get\u0026lt;version\u0026gt;.sh Note: see WebLogic Monitoring Exporter Releases for latest releases.\nFor example:\n$ mkdir -p /scratch/OAMDockerK8S/wls_exporter $ cd /scratch/OAMDockerK8S/wls_exporter $ wget https://github.com/oracle/weblogic-monitoring-exporter/releases/download/v1.2.0/wls-exporter.war $ wget https://github.com/oracle/weblogic-monitoring-exporter/releases/download/v1.2.0/get1.2.0.sh   Create a configuration file config-admin.yaml in the \u0026lt;work_directory\u0026gt;/wls_exporter directory that contains the following. Modify the restPort to match the server port for the OAM Administration Server:\nmetricsNameSnakeCase: true restPort: 7001 queries: - key: name keyName: location prefix: wls_server_ applicationRuntimes: key: name keyName: app componentRuntimes: prefix: wls_webapp_config_ type: WebAppComponentRuntime key: name values: [deploymentState, contextRoot, sourceInfo, openSessionsHighCount, openSessionsCurrentCount, sessionsOpenedTotalCount, sessionCookieMaxAgeSecs, sessionInvalidationIntervalSecs, sessionTimeoutSecs, singleThreadedServletPoolSize, sessionIDLength, servletReloadCheckSecs, jSPPageCheckSecs] servlets: prefix: wls_servlet_ key: servletName - JVMRuntime: prefix: wls_jvm_ key: name - executeQueueRuntimes: prefix: wls_socketmuxer_ key: name values: [pendingRequestCurrentCount] - workManagerRuntimes: prefix: wls_workmanager_ key: name values: [stuckThreadCount, pendingRequests, completedRequests] - threadPoolRuntime: prefix: wls_threadpool_ key: name values: [executeThreadTotalCount, queueLength, stuckThreadCount, hoggingThreadCount] - JMSRuntime: key: name keyName: jmsruntime prefix: wls_jmsruntime_ JMSServers: prefix: wls_jms_ key: name keyName: jmsserver destinations: prefix: wls_jms_dest_ key: name keyName: destination - persistentStoreRuntimes: prefix: wls_persistentstore_ key: name - JDBCServiceRuntime: JDBCDataSourceRuntimeMBeans: prefix: wls_datasource_ key: name - JTARuntime: prefix: wls_jta_ key: name   Create a configuration file config-oamserver.yaml in the \u0026lt;work_directory\u0026gt;/wls_exporter directory that contains the following. Modify the restPort to match the server port for the OAM Managed Servers:\nmetricsNameSnakeCase: true restPort: 14100 queries: - key: name keyName: location prefix: wls_server_ applicationRuntimes: key: name keyName: app componentRuntimes: prefix: wls_webapp_config_ type: WebAppComponentRuntime key: name values: [deploymentState, contextRoot, sourceInfo, openSessionsHighCount, openSessionsCurrentCount, sessionsOpenedTotalCount, sessionCookieMaxAgeSecs, sessionInvalidationIntervalSecs, sessionTimeoutSecs, singleThreadedServletPoolSize, sessionIDLength, servletReloadCheckSecs, jSPPageCheckSecs] servlets: prefix: wls_servlet_ key: servletName - JVMRuntime: prefix: wls_jvm_ key: name - executeQueueRuntimes: prefix: wls_socketmuxer_ key: name values: [pendingRequestCurrentCount] - workManagerRuntimes: prefix: wls_workmanager_ key: name values: [stuckThreadCount, pendingRequests, completedRequests] - threadPoolRuntime: prefix: wls_threadpool_ key: name values: [executeThreadTotalCount, queueLength, stuckThreadCount, hoggingThreadCount] - JMSRuntime: key: name keyName: jmsruntime prefix: wls_jmsruntime_ JMSServers: prefix: wls_jms_ key: name keyName: jmsserver destinations: prefix: wls_jms_dest_ key: name keyName: destination - persistentStoreRuntimes: prefix: wls_persistentstore_ key: name - JDBCServiceRuntime: JDBCDataSourceRuntimeMBeans: prefix: wls_datasource_ key: name - JTARuntime: prefix: wls_jta_ key: name   Create a configuration file config-policyserver.yaml in the \u0026lt;work_directory\u0026gt;/wls_exporter directory that contains the following. Modify the restPort to match the server port for the OAM Policy Manager Servers:\nmetricsNameSnakeCase: true restPort: 15100 queries: - key: name keyName: location prefix: wls_server_ applicationRuntimes: key: name keyName: app componentRuntimes: prefix: wls_webapp_config_ type: WebAppComponentRuntime key: name values: [deploymentState, contextRoot, sourceInfo, openSessionsHighCount, openSessionsCurrentCount, sessionsOpenedTotalCount, sessionCookieMaxAgeSecs, sessionInvalidationIntervalSecs, sessionTimeoutSecs, singleThreadedServletPoolSize, sessionIDLength, servletReloadCheckSecs, jSPPageCheckSecs] servlets: prefix: wls_servlet_ key: servletName - JVMRuntime: prefix: wls_jvm_ key: name - executeQueueRuntimes: prefix: wls_socketmuxer_ key: name values: [pendingRequestCurrentCount] - workManagerRuntimes: prefix: wls_workmanager_ key: name values: [stuckThreadCount, pendingRequests, completedRequests] - threadPoolRuntime: prefix: wls_threadpool_ key: name values: [executeThreadTotalCount, queueLength, stuckThreadCount, hoggingThreadCount] - JMSRuntime: key: name keyName: jmsruntime prefix: wls_jmsruntime_ JMSServers: prefix: wls_jms_ key: name keyName: jmsserver destinations: prefix: wls_jms_dest_ key: name keyName: destination - persistentStoreRuntimes: prefix: wls_persistentstore_ key: name - JDBCServiceRuntime: JDBCDataSourceRuntimeMBeans: prefix: wls_datasource_ key: name - JTARuntime: prefix: wls_jta_ key: name   Generate the deployment package for the OAM Administration Server:\n$ chmod 777 get\u0026lt;version\u0026gt;.sh $ ./get\u0026lt;version\u0026gt; config-admin.yaml For example:\n$ chmod 777 get1.2.0.sh $ ./get1.2.0.sh config-admin.yaml The output will look similar to the following:\n% Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 642 100 642 0 0 1272 0 --:--:-- --:--:-- --:--:-- 1273 100 2033k 100 2033k 0 0 1224k 0 0:00:01 0:00:01 --:--:-- 2503k created /tmp/ci-AcBAO1eTer /tmp/ci-AcBAO1eTer /scratch/OAMDockerK8S/wls_exporter in temp dir adding: config.yml (deflated 65%) /scratch/OAMDockerK8S/wls_exporter This will generate a wls-exporter.war file in the same directory. This war file contains a config.yml that corresponds to config-admin.yaml. Rename the file as follows:\nmv wls-exporter.war wls-exporter-admin.war   Generate the deployment package for the OAM Managed Server and Policy Manager Server, for example:\n$ ./get1.2.0.sh config-oamserver.yaml $ mv wls-exporter.war wls-exporter-oamserver.war $ ./get1.2.0.sh config-policyserver.yaml $ mv wls-exporter.war wls-exporter-policyserver.war   Copy the war files to the persistent volume directory:\n$ cp wls-exporter*.war \u0026lt;work_directory\u0026gt;/\u0026lt;persistent_volume\u0026gt;/ For example:\n$ cp wls-exporter*.war /scratch/OAMDockerK8S/accessdomainpv/   Deploy the wls-exporter war files in OAM WebLogic server   Login to the Oracle Enterprise Manager Console using the URL https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/em.\n  Navigate to WebLogic Domain \u0026gt; Deployments. Click on the padlock in the upper right hand corner and select Lock and Edit.\n  From the \u0026lsquo;Deployment\u0026rsquo; drop down menu select Deploy.\n  In the Select Archive screen, under Archive or exploded directory is on the server where Enterprise Manager is running, click Browse. Navigate to the /u01/oracle/user_projects/domains directory and select wls-exporter-admin.war. Click OK and then Next.\n  In Select Target check AdminServer and click Next.\n  In Application Attributes set the following and click Next:\n Application Name: wls-exporter-admin Context Root: wls-exporter Distribution: Install and start application (servicing all requests)    In Deployment Settings click Deploy.\n  Once you see the message Deployment Succeeded, click Close.\n  Click on the padlock in the upper right hand corner and select Activate Changes.\n  Repeat the above steps to deploy wls-exporter-oamserver.war with the following caveats:\n In Select Target choose oam_cluster In Application Attributes set Application Name: wls-exporter-oamserver, Context Root: wls-exporter In Distribution select Install and start application (servicing all requests)    Repeat the above steps to deploy wls-exporter-policyserver.war with the following caveats:\n In Select Target choose policy_cluster In Application Attributes set Application Name: wls-exporter-policyserver, Context Root: wls-exporter In Distribution select Install and start application (servicing all requests)    Check the wls-exporter is accessible using the URL: https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/wls-exporter.\nYou should see a page saying This is the WebLogic Monitoring Exporter.\n  Prometheus Operator Configuration Prometheus has to be configured to collect the metrics from the weblogic-monitor-exporter. The Prometheus operator identifies the targets using service discovery. To get the weblogic-monitor-exporter end point discovered as a target, you will need to create a service monitor to point to the service.\n  Create a wls-exporter-service-monitor.yaml in the \u0026lt;work_directory\u0026gt;/wls_exporter directory with the following contents:\napiVersion: v1 kind: Secret metadata: name: basic-auth namespace: monitoring data: password: V2VsY29tZTE= ## \u0026lt;password\u0026gt; base64 user: d2VibG9naWM= ## weblogic base64 type: Opaque --- apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: wls-exporter-accessinfra namespace: monitoring labels: k8s-app: wls-exporter spec: namespaceSelector: matchNames: - accessns selector: matchLabels: weblogic.domainName: accessinfra endpoints: - basicAuth: password: name: basic-auth key: password username: name: basic-auth key: user port: default relabelings: - action: labelmap regex: __meta_kubernetes_service_label_(.+) interval: 10s honorLabels: true path: /wls-exporter/metrics Note: In the above example, change the password: V2VsY29tZTE= value to the base64 encoded version of your weblogic password. To find the base64 value run the following:\n$ echo -n \u0026#34;\u0026lt;password\u0026gt;\u0026#34; | base64 If using a different namespace from accessns or a different domain_UID from accessinfra, then change accordingly.\n  Add Rolebinding for the WebLogic OAM domain namespace:\n$ cd \u0026lt;work_directory\u0026gt;/kube-prometheus/manifests Edit the prometheus-roleBindingSpecificNamespaces.yaml file and add the following to the file for your OAM domain namespace, for example accessns.\n- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: prometheus-k8s namespace: accessns roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: prometheus-k8s subjects: - kind: ServiceAccount name: prometheus-k8s namespace: monitoring For example the file should now read:\napiVersion: rbac.authorization.k8s.io/v1 items: - apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: prometheus-k8s namespace: accessns roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: prometheus-k8s subjects: - kind: ServiceAccount name: prometheus-k8s namespace: monitoring - apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: prometheus-k8s namespace: default ....   Add the Role for WebLogic OAM domain namespace. Edit the prometheus-roleSpecificNamespaces.yaml and change the namespace to your OAM domain namespace, for example accessns.\n- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: prometheus-k8s namespace: accessns rules: - apiGroups: - \u0026quot;\u0026quot; resources: - services - endpoints - pods verbs: - get - list - watch ....   Apply the yaml files as follows:\n$ kubectl apply -f prometheus-roleBindingSpecificNamespaces.yaml $ kubectl apply -f prometheus-roleSpecificNamespaces.yaml The output should look similar to the following:\nkubectl apply -f prometheus-roleBindingSpecificNamespaces.yaml rolebinding.rbac.authorization.k8s.io/prometheus-k8s created Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply rolebinding.rbac.authorization.k8s.io/prometheus-k8s configured Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply rolebinding.rbac.authorization.k8s.io/prometheus-k8s configured Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply rolebinding.rbac.authorization.k8s.io/prometheus-k8s configured $ kubectl apply -f prometheus-roleSpecificNamespaces.yaml role.rbac.authorization.k8s.io/prometheus-k8s created Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply role.rbac.authorization.k8s.io/prometheus-k8s configured Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply role.rbac.authorization.k8s.io/prometheus-k8s configured Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply role.rbac.authorization.k8s.io/prometheus-k8s configured   Deploy the ServiceMonitor   Run the following command to create the ServiceMonitor:\n$ cd \u0026lt;work_directory\u0026gt;/wls_exporter $ kubectl create -f wls-exporter-service-monitor.yaml The output will look similar to the following:\nservicemonitor.monitoring.coreos.com/wls-exporter-accessinfra created   Prometheus Service Discovery After ServiceMonitor is deployed, the wls-exporter should be discovered by Prometheus and be able to scrape metrics.\n  Access the following URL to view Prometheus service discovery: http://${MASTERNODE-HOSTNAME}:32101/service-discovery\n  Click on monitoring/wls-exporter-accessinfra/0  and then show more. Verify all the targets are mentioned.\n  Grafana Dashboard   Access the Grafana dashboard with the following URL: http://${MASTERNODE-HOSTNAME}:32100 and login with admin/admin. Change your password when prompted.\n  Import the Grafana dashboard by navigating on the left hand menu to Create \u0026gt; Import. Copy the content from \u0026lt;work_directory\u0026gt;/FMW-DockerImages/OracleAccessManagement/kubernetes/3.0.1/grafana/weblogic_dashboard.json and paste. Then click Load and Import.\n  "
},
{
	"uri": "/fmw-kubernetes/20.4.1/oud/manage-oud-containers/",
	"title": "Manage Oracle Unified Directory Containers",
	"tags": [],
	"description": "This document provides steps manage Oracle Unified Directory containers.",
	"content": "Important considerations for Oracle Unified Directory instances in Kubernetes.\n a) Logging and Visualization for Helm Chart oud-ds-rs Deployment  Describes the steps for logging and visualization with Elasticsearch and Kibana.\n b) Monitoring an Oracle Unified Directory Instance  Describes the steps for Monitoring the Oracle Unified Directory environment.\n "
},
{
	"uri": "/fmw-kubernetes/20.4.1/oudsm/manage-oudsm-containers/",
	"title": "Manage Oracle Unified Directory Services Manager Containers",
	"tags": [],
	"description": "This document provides steps to manage Oracle Unified Directory Services Manager containers.",
	"content": "Important considerations for Oracle Unified Directory Services Manager instances in Kubernetes.\n a) Logging and Visualization for Helm Chart oudsm Deployment  Describes the steps for logging and visualization with Elasticsearch and Kibana.\n b) Monitoring an Oracle Unified Directory Services Manager Instance  Describes the steps for Monitoring the Oracle Unified Directory Services Manager environment.\n "
},
{
	"uri": "/fmw-kubernetes/20.4.1/oudsm/",
	"title": "Oracle Unified Directory Services Manager",
	"tags": [],
	"description": "Oracle Unified Directory Services Manager provides an interface for managing instances of Oracle Unified Directory",
	"content": "Oracle Unified Directory Services Manager is an interface for managing instances of Oracle Unified Directory. Oracle Unified Directory Services Manager enables you to configure the structure of the directory, define objects in the directory, add and configure users, groups, and other entries. Oracle Unified Directory Services Manager is also the interface you use to manage entries, schema, security, and other directory features.\nThis project supports deployment of Oracle Unified Directory Services Manager images based on the 12cPS4 (12.2.1.4.0) release within a Kubernetes environment. The Oracle Unified Directory Services Manager Image refers to binaries for Oracle Unified Directory Services Manager Release 12.2.1.4.0.\nImage: oracle/oudsm:12.2.1.4.0\nFollow the instructions in this guide to set up Oracle Unified Directory Services Manager on Kubernetes.\nGetting started For detailed information about deploying Oracle Unified Directory Services Manager, start at Prerequisites and follow this documentation sequentially.\nCurrent release The current supported release of Oracle Unified Directory Services Manager is OUD 12c PS4 (12.2.1.4.0)\n"
},
{
	"uri": "/fmw-kubernetes/20.4.1/oam/manage-oam-domains/delete-domain-home/",
	"title": "Delete the OAM domain home",
	"tags": [],
	"description": "Learn about the steps to cleanup the OAM domain home.",
	"content": "Sometimes in production, but most likely in testing environments, you might want to remove the domain home that is generated using the create-domain.sh script.\n  Run the following command to delete the jobs, domain, and configmaps:\n$ kubectl delete jobs \u0026lt;domain_job\u0026gt; -n \u0026lt;domain_namespace\u0026gt; $ kubectl delete domain \u0026lt;domain_uid\u0026gt; -n \u0026lt;domain_namespace\u0026gt; $ kubectl delete configmaps \u0026lt;domain_job\u0026gt;-cm -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl delete jobs accessinfra-create-oam-infra-domain-job -n accessns $ kubectl delete domain accessinfra -n accessns $ kubectl delete configmaps accessinfra-create-oam-infra-domain-job-cm -n accessns   Drop the RCU schemas as follows:\n$ kubectl exec -it helper -n \u0026lt;domain_namespace\u0026gt; -- /bin/bash [oracle@helper ~]$ [oracle@helper ~]$ export CONNECTION_STRING=\u0026lt;db_host.domain\u0026gt;:\u0026lt;db_port\u0026gt;/\u0026lt;service_name\u0026gt; [oracle@helper ~]$ export RCUPREFIX=\u0026lt;rcu_schema_prefix\u0026gt; /u01/oracle/oracle_common/bin/rcu -silent -dropRepository -databaseType ORACLE -connectString $CONNECTION_STRING \\ -dbUser sys -dbRole sysdba -selectDependentsForComponents true -schemaPrefix $RCUPREFIX \\ -component MDS -component IAU -component IAU_APPEND -component IAU_VIEWER -component OPSS \\ -component WLS -component STB -component OAM -f \u0026lt; /tmp/pwd.txt For example:\n$ kubectl exec -it helper -n accessns -- /bin/bash [oracle@helper ~]$ export CONNECTION_STRING=mydatabasehost.example.com:1521/orcl.example.com [oracle@helper ~]$ export RCUPREFIX=OAMK8S /u01/oracle/oracle_common/bin/rcu -silent -dropRepository -databaseType ORACLE -connectString $CONNECTION_STRING \\ -dbUser sys -dbRole sysdba -selectDependentsForComponents true -schemaPrefix $RCUPREFIX \\ -component MDS -component IAU -component IAU_APPEND -component IAU_VIEWER -component OPSS \\ -component WLS -component STB -component OAM -f \u0026lt; /tmp/pwd.txt   Delete the Persistent Volume and Persistent Volume Claim:\n$ kubectl delete pv \u0026lt;pv-name\u0026gt; -n \u0026lt;domain_namespace\u0026gt; $ kubectl delete pvc \u0026lt;pvc-name\u0026gt; -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl delete pv accessinfra-domain-pv -n accessns $ kubectl delete pvc accessinfra-domain-pvc -n accessns   Delete the contents of the persistent volume, for example:\n$ rm -rf \u0026lt;work directory\u0026gt;/accessdomainpv/* For example:\n$ rm -rf /scratch/OAMDockerK8S/accessdomainpv/*   Delete the Oracle WebLogic Server Kubernetes Operator, by running the following command:\n$ helm delete weblogic-kubernetes-operator   To delete NGINX:\ncd \u0026lt;work_directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain $ kubectl delete -f ssl-nginx-ingress.yaml Then run:\n$ helm delete nginx-ingress   To delete Voyager:\nhelm delete voyager-operator -n voyager then:\n$ helm delete oam-voyager-ingress -n \u0026lt;domain_namespace\u0026gt; For example:\n$ helm delete oam-voyager-ingress -n accessns   "
},
{
	"uri": "/fmw-kubernetes/20.4.1/oig/manage-oig-domains/monitoring-oim-domains/",
	"title": "Monitoring an OIG domain",
	"tags": [],
	"description": "Describes the steps for Monitoring the OIG domain and Publising the logs to Elasticsearch.",
	"content": "After the OIG domain is set up you can monitor the OIG instance using Prometheus and Grafana. See Monitoring a domain.\nThe WebLogic Monitoring Exporter uses the WLS RESTful Management API to scrape runtime information and then exports Prometheus-compatible metrics. It is deployed as a web application in a WebLogic Server (WLS) instance, version 12.2.1 or later, typically, in the instance from which you want to get metrics.\nDeploy the Prometheus operator   Clone Prometheus by running the following commands:\n$ cd \u0026lt;work directory\u0026gt; $ git clone https://github.com/coreos/kube-prometheus.git Note: Please refer the compatibility matrix of Kube Prometheus. Please download the release of the repository according to the Kubernetes version of your cluster. In the above example the latest release will be downloaded.\nFor example:\n$ cd /scratch/OIGDockerK8S $ git clone https://github.com/coreos/kube-prometheus.git   Run the following command to create the namespace and custom resource definitions:\n$ cd kube-prometheus $ kubectl create -f manifests/setup The output will look similar to the following:\nkubectl create -f manifests/setup namespace/monitoring created customresourcedefinition.apiextensions.k8s.io/alertmanagers.monitoring.coreos.com created customresourcedefinition.apiextensions.k8s.io/podmonitors.monitoring.coreos.com created customresourcedefinition.apiextensions.k8s.io/probes.monitoring.coreos.com created customresourcedefinition.apiextensions.k8s.io/prometheuses.monitoring.coreos.com created customresourcedefinition.apiextensions.k8s.io/prometheusrules.monitoring.coreos.com created customresourcedefinition.apiextensions.k8s.io/servicemonitors.monitoring.coreos.com created customresourcedefinition.apiextensions.k8s.io/thanosrulers.monitoring.coreos.com created clusterrole.rbac.authorization.k8s.io/prometheus-operator created clusterrolebinding.rbac.authorization.k8s.io/prometheus-operator created deployment.apps/prometheus-operator created service/prometheus-operator created serviceaccount/prometheus-operator created   Run the following command to created the rest of the resources:\n$ kubectl create -f manifests/ The output will look similar to the following:\nalertmanager.monitoring.coreos.com/main created secret/alertmanager-main created service/alertmanager-main created serviceaccount/alertmanager-main created servicemonitor.monitoring.coreos.com/alertmanager created secret/grafana-datasources created configmap/grafana-dashboard-apiserver created configmap/grafana-dashboard-cluster-total created configmap/grafana-dashboard-controller-manager created configmap/grafana-dashboard-k8s-resources-cluster created configmap/grafana-dashboard-k8s-resources-namespace created configmap/grafana-dashboard-k8s-resources-node created configmap/grafana-dashboard-k8s-resources-pod created configmap/grafana-dashboard-k8s-resources-workload created configmap/grafana-dashboard-k8s-resources-workloads-namespace created configmap/grafana-dashboard-kubelet created configmap/grafana-dashboard-namespace-by-pod created configmap/grafana-dashboard-namespace-by-workload created configmap/grafana-dashboard-node-cluster-rsrc-use created configmap/grafana-dashboard-node-rsrc-use created configmap/grafana-dashboard-nodes created configmap/grafana-dashboard-persistentvolumesusage created configmap/grafana-dashboard-pod-total created configmap/grafana-dashboard-prometheus-remote-write created configmap/grafana-dashboard-prometheus created configmap/grafana-dashboard-proxy created configmap/grafana-dashboard-scheduler created configmap/grafana-dashboard-statefulset created configmap/grafana-dashboard-workload-total created configmap/grafana-dashboards created deployment.apps/grafana created service/grafana created serviceaccount/grafana created servicemonitor.monitoring.coreos.com/grafana created clusterrole.rbac.authorization.k8s.io/kube-state-metrics created clusterrolebinding.rbac.authorization.k8s.io/kube-state-metrics created deployment.apps/kube-state-metrics created service/kube-state-metrics created serviceaccount/kube-state-metrics created servicemonitor.monitoring.coreos.com/kube-state-metrics created clusterrole.rbac.authorization.k8s.io/node-exporter created clusterrolebinding.rbac.authorization.k8s.io/node-exporter created daemonset.apps/node-exporter created service/node-exporter created serviceaccount/node-exporter created servicemonitor.monitoring.coreos.com/node-exporter created apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created clusterrole.rbac.authorization.k8s.io/prometheus-adapter created clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created clusterrolebinding.rbac.authorization.k8s.io/prometheus-adapter created clusterrolebinding.rbac.authorization.k8s.io/resource-metrics:system:auth-delegator created clusterrole.rbac.authorization.k8s.io/resource-metrics-server-resources created configmap/adapter-config created deployment.apps/prometheus-adapter created rolebinding.rbac.authorization.k8s.io/resource-metrics-auth-reader created service/prometheus-adapter created serviceaccount/prometheus-adapter created servicemonitor.monitoring.coreos.com/prometheus-adapter created clusterrole.rbac.authorization.k8s.io/prometheus-k8s created clusterrolebinding.rbac.authorization.k8s.io/prometheus-k8s created servicemonitor.monitoring.coreos.com/prometheus-operator created prometheus.monitoring.coreos.com/k8s created rolebinding.rbac.authorization.k8s.io/prometheus-k8s-config created rolebinding.rbac.authorization.k8s.io/prometheus-k8s created rolebinding.rbac.authorization.k8s.io/prometheus-k8s created rolebinding.rbac.authorization.k8s.io/prometheus-k8s created role.rbac.authorization.k8s.io/prometheus-k8s-config created role.rbac.authorization.k8s.io/prometheus-k8s created role.rbac.authorization.k8s.io/prometheus-k8s created role.rbac.authorization.k8s.io/prometheus-k8s created prometheusrule.monitoring.coreos.com/prometheus-k8s-rules created service/prometheus-k8s created serviceaccount/prometheus-k8s created servicemonitor.monitoring.coreos.com/prometheus created servicemonitor.monitoring.coreos.com/kube-apiserver created servicemonitor.monitoring.coreos.com/coredns created servicemonitor.monitoring.coreos.com/kube-controller-manager created servicemonitor.monitoring.coreos.com/kube-scheduler created servicemonitor.monitoring.coreos.com/kubelet created   Kube-Prometheus requires all nodes to be labelled with kubernetes.io/os=linux. To check if your nodes are labelled, run the following:\n$ kubectl get nodes --show-labels If the nodes are labelled the output will look similar to the following:\nNAME STATUS ROLES AGE VERSION LABELS worker-node1 Ready \u0026lt;none\u0026gt; 42d v1.18.4 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=worker-node1,kubernetes.io/os=linux worker-node2 Ready \u0026lt;none\u0026gt; 42d v1.18.4 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=worker-node2,kubernetes.io/os=linux master-node Ready master 42d v1.18.4 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=masternode,kubernetes.io/os=linux,node-role.kubernetes.io/master= If the nodes are not labelled, run the following command:\n$ kubectl label nodes --all kubernetes.io/os=linux   Provide external access for Grafana, Prometheus, and Alertmanager, by running the following commands:\n$ kubectl patch svc grafana -n monitoring --type=json -p \u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/type\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;NodePort\u0026#34; },{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/ports/0/nodePort\u0026#34;, \u0026#34;value\u0026#34;: 32100 }]\u0026#39; $ kubectl patch svc prometheus-k8s -n monitoring --type=json -p \u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/type\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;NodePort\u0026#34; },{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/ports/0/nodePort\u0026#34;, \u0026#34;value\u0026#34;: 32101 }]\u0026#39; $ kubectl patch svc alertmanager-main -n monitoring --type=json -p \u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/type\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;NodePort\u0026#34; },{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/ports/0/nodePort\u0026#34;, \u0026#34;value\u0026#34;: 32102 }]\u0026#39; Note: This assigns port 32100 to Grafana, 32101 to Prometheus, and 32102 to Alertmanager.\nThe output will look similar to the following:\nservice/grafana patched service/prometheus-k8s patched service/alertmanager-main patched   Verify that the Prometheus, Grafana, and Alertmanager pods are running in the monitoring namespace and the respective services have the exports configured correctly:\n$ kubectl get pods,services -o wide -n monitoring The output should look similar to the following:\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/alertmanager-main-0 2/2 Running 0 97s 10.244.2.52 worker-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/alertmanager-main-1 2/2 Running 0 97s 10.244.1.61 worker-node1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/alertmanager-main-2 2/2 Running 0 97s 10.244.2.53 worker-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/grafana-86445dccbb-dln2l 1/1 Running 0 96s 10.244.2.55 worker-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/kube-state-metrics-5b67d79459-k7xrb 3/3 Running 0 96s 10.244.1.63 worker-node1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/node-exporter-dhp4k 2/2 Running 0 96s 10.250.111.111 worker-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/node-exporter-jknkv 2/2 Running 0 96s 10.196.4.112 masternode \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/node-exporter-vpn9l 2/2 Running 0 96s 10.250.111.112 worker-node1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/prometheus-adapter-66b855f564-snkjb 1/1 Running 0 96s 10.244.2.56 worker-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/prometheus-k8s-0 3/3 Running 0 96s 10.244.2.54 worker-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/prometheus-k8s-1 3/3 Running 0 96s 10.244.1.62 worker-node1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/prometheus-operator-78fcb48ccf-gcgc5 2/2 Running 0 107s 10.244.1.60 worker-node1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service/alertmanager-main NodePort 10.107.184.118 \u0026lt;none\u0026gt; 9093:32102/TCP 98s alertmanager=main,app=alertmanager service/alertmanager-operated ClusterIP None \u0026lt;none\u0026gt; 9093/TCP,9094/TCP,9094/UDP 97s app=alertmanager service/grafana NodePort 10.96.249.254 \u0026lt;none\u0026gt; 3000:32100/TCP 97s app=grafana service/kube-state-metrics ClusterIP None \u0026lt;none\u0026gt; 8443/TCP,9443/TCP 97s app.kubernetes.io/name=kube-state-metrics service/node-exporter ClusterIP None \u0026lt;none\u0026gt; 9100/TCP 97s app.kubernetes.io/name=node-exporter service/prometheus-adapter ClusterIP 10.100.222.239 \u0026lt;none\u0026gt; 443/TCP 97s name=prometheus-adapter service/prometheus-k8s NodePort 10.106.163.78 \u0026lt;none\u0026gt; 9090:32101/TCP 96s app=prometheus,prometheus=k8s service/prometheus-operated ClusterIP None \u0026lt;none\u0026gt; 9090/TCP 96s app=prometheus service/prometheus-operator ClusterIP None \u0026lt;none\u0026gt; 8443/TCP 108s app.kubernetes.io/component=contr oller,app.kubernetes.io/name=prometheus-operator   Deploy WebLogic Monitoring Exporter   Download WebLogic Monitoring Exporter:\n$ mkdir -p \u0026lt;work_directory\u0026gt;/wls_exporter $ cd \u0026lt;work_directory\u0026gt;/wls_exporter $ wget https://github.com/oracle/weblogic-monitoring-exporter/releases/download/\u0026lt;version\u0026gt;/wls-exporter.war $ wget https://github.com/oracle/weblogic-monitoring-exporter/releases/download/\u0026lt;version\u0026gt;/get\u0026lt;version\u0026gt;.sh For example:\n$ mkdir -p /scratch/OIGDockerK8S/wls_exporter $ cd /scratch/OIGDockerK8S/wls_exporter $ wget https://github.com/oracle/weblogic-monitoring-exporter/releases/download/v1.2.0/wls-exporter.war $ wget https://github.com/oracle/weblogic-monitoring-exporter/releases/download/v1.2.0/get1.2.0.sh   Create a configuration file config-admin.yaml in the \u0026lt;work_directory\u0026gt;/wls_exporter directory that contains the following. Modify the restPort to match the server port for the OIG Administration Server:\nmetricsNameSnakeCase: true restPort: 7001 queries: - key: name keyName: location prefix: wls_server_ applicationRuntimes: key: name keyName: app componentRuntimes: prefix: wls_webapp_config_ type: WebAppComponentRuntime key: name values: [deploymentState, contextRoot, sourceInfo, openSessionsHighCount, openSessionsCurrentCount, sessionsOpenedTotalCount, sessionCookieMaxAgeSecs, sessionInvalidationIntervalSecs, sessionTimeoutSecs, singleThreadedServletPoolSize, sessionIDLength, servletReloadCheckSecs, jSPPageCheckSecs] servlets: prefix: wls_servlet_ key: servletName - JVMRuntime: prefix: wls_jvm_ key: name - executeQueueRuntimes: prefix: wls_socketmuxer_ key: name values: [pendingRequestCurrentCount] - workManagerRuntimes: prefix: wls_workmanager_ key: name values: [stuckThreadCount, pendingRequests, completedRequests] - threadPoolRuntime: prefix: wls_threadpool_ key: name values: [executeThreadTotalCount, queueLength, stuckThreadCount, hoggingThreadCount] - JMSRuntime: key: name keyName: jmsruntime prefix: wls_jmsruntime_ JMSServers: prefix: wls_jms_ key: name keyName: jmsserver destinations: prefix: wls_jms_dest_ key: name keyName: destination - persistentStoreRuntimes: prefix: wls_persistentstore_ key: name - JDBCServiceRuntime: JDBCDataSourceRuntimeMBeans: prefix: wls_datasource_ key: name - JTARuntime: prefix: wls_jta_ key: name   Create a configuration file config-oimserver.yaml in the \u0026lt;work_directory\u0026gt;/wls_exporter directory that contains the following. Modify the restPort to match the server port for the OIG Managed Servers:\nmetricsNameSnakeCase: true restPort: 14000 queries: - key: name keyName: location prefix: wls_server_ applicationRuntimes: key: name keyName: app componentRuntimes: prefix: wls_webapp_config_ type: WebAppComponentRuntime key: name values: [deploymentState, contextRoot, sourceInfo, openSessionsHighCount, openSessionsCurrentCount, sessionsOpenedTotalCount, sessionCookieMaxAgeSecs, sessionInvalidationIntervalSecs, sessionTimeoutSecs, singleThreadedServletPoolSize, sessionIDLength, servletReloadCheckSecs, jSPPageCheckSecs] servlets: prefix: wls_servlet_ key: servletName - JVMRuntime: prefix: wls_jvm_ key: name - executeQueueRuntimes: prefix: wls_socketmuxer_ key: name values: [pendingRequestCurrentCount] - workManagerRuntimes: prefix: wls_workmanager_ key: name values: [stuckThreadCount, pendingRequests, completedRequests] - threadPoolRuntime: prefix: wls_threadpool_ key: name values: [executeThreadTotalCount, queueLength, stuckThreadCount, hoggingThreadCount] - JMSRuntime: key: name keyName: jmsruntime prefix: wls_jmsruntime_ JMSServers: prefix: wls_jms_ key: name keyName: jmsserver destinations: prefix: wls_jms_dest_ key: name keyName: destination - persistentStoreRuntimes: prefix: wls_persistentstore_ key: name - JDBCServiceRuntime: JDBCDataSourceRuntimeMBeans: prefix: wls_datasource_ key: name - JTARuntime: prefix: wls_jta_ key: name   Create a configuration file config-soaserver.yaml in the \u0026lt;work_directory\u0026gt;/wls_exporter directory that contains the following. Modify the restPort to match the server port for the SOA Managed Servers:\nmetricsNameSnakeCase: true restPort: 8001 queries: - key: name keyName: location prefix: wls_server_ applicationRuntimes: key: name keyName: app componentRuntimes: prefix: wls_webapp_config_ type: WebAppComponentRuntime key: name values: [deploymentState, contextRoot, sourceInfo, openSessionsHighCount, openSessionsCurrentCount, sessionsOpenedTotalCount, sessionCookieMaxAgeSecs, sessionInvalidationIntervalSecs, sessionTimeoutSecs, singleThreadedServletPoolSize, sessionIDLength, servletReloadCheckSecs, jSPPageCheckSecs] servlets: prefix: wls_servlet_ key: servletName - JVMRuntime: prefix: wls_jvm_ key: name - executeQueueRuntimes: prefix: wls_socketmuxer_ key: name values: [pendingRequestCurrentCount] - workManagerRuntimes: prefix: wls_workmanager_ key: name values: [stuckThreadCount, pendingRequests, completedRequests] - threadPoolRuntime: prefix: wls_threadpool_ key: name values: [executeThreadTotalCount, queueLength, stuckThreadCount, hoggingThreadCount] - JMSRuntime: key: name keyName: jmsruntime prefix: wls_jmsruntime_ JMSServers: prefix: wls_jms_ key: name keyName: jmsserver destinations: prefix: wls_jms_dest_ key: name keyName: destination - persistentStoreRuntimes: prefix: wls_persistentstore_ key: name - JDBCServiceRuntime: JDBCDataSourceRuntimeMBeans: prefix: wls_datasource_ key: name - JTARuntime: prefix: wls_jta_ key: name   Generate the deployment package for the OIG Administration Server:\n$ chmod 777 get\u0026lt;version\u0026gt;.sh $ ./get\u0026lt;version\u0026gt; config-admin.yaml For example:\n$ chmod 777 get1.2.0.sh $ ./get1.2.0.sh config-admin.yaml The output will look similar to the following:\n% Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 629 100 629 0 0 1241 0 --:--:-- --:--:-- --:--:-- 1240 100 2033k 100 2033k 0 0 1219k 0 0:00:01 0:00:01 --:--:-- 2882k created /tmp/ci-lKm0dOnLwU /tmp/ci-lKm0dOnLwU /scratch/OIGDockerK8S/wls_exporter in temp dir adding: config.yml (deflated 65%) /scratch/OIGDockerK8S/wls_exporter This will generate a wls-exporter.war file in the same directory that contains a config.yml that corresponds to config-admin.yaml. Rename the file as follows:\nmv wls-exporter.war wls-exporter-admin.war   Generate the deployment package for the OIG Managed Server and Policy Manager Server, for example:\n$ ./get1.2.0.sh config-oimserver.yaml $ mv wls-exporter.war wls-exporter-oimserver.war $ ./get1.2.0.sh config-soaserver.yaml $ mv wls-exporter.war wls-exporter-soaserver.war   Copy the war files to the persistent volume directory:\ncp wls-exporter*.war \u0026lt;work_directory\u0026gt;/\u0026lt;persistent_volume\u0026gt;/ For example:\n$ cp wls-exporter*.war /scratch/OIGDockerK8S/oimclusterdomainpv/   Deploy the wls-exporter war files in OIG WebLogic server   Login to the Oracle Enterprise Manager Console using the URL https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/em.\n  Navigate to WebLogic Domain \u0026gt; Deployments. Click on the padlock in the upper right hand corner and select Lock and Edit.\n  From the \u0026lsquo;Deployment\u0026rsquo; drop down menu select Deploy.\n  In the Select Archive screen, under Archive or exploded directory is on the server where Enterprise Manager is running, click Browse. Navigate to the /u01/oracle/user_projects/domains directory and select wls-exporter-admin.war. Click OK and then Next.\n  In Select Target check AdminServer and click Next.\n  In Application Attributes set the following and click Next:\n Application Name: wls-exporter-admin Context Root: wls-exporter Distribution: Install and start application (servicing all requests)    In Deployment Settings click Deploy.\n  Once you see the message Deployment Succeeded, click Close.\n  Click on the padlock in the upper right hand corner and select Activate Changes.\n  Repeat the above steps to deploy wls-exporter-oimserver.war with the following caveats:\n In Select Target choose oim_cluster In Application Attributes set Application Name: wls-exporter-oimserver, Context Root: wls-exporter In Distribution select Install and start application (servicing all requests)    Repeat the above steps to deploy wls-exporter-soaserver.war with the following caveats:\n In Select Target choose soa_cluster In Application Attributes set Application Name: wls-exporter-soaserver, Context Root: wls-exporter In Distribution select Install and start application (servicing all requests)    Check the wls-exporter is accessible using the URL: https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/wls-exporter.\nYou should see a page saying This is the WebLogic Monitoring Exporter.\n  Prometheus Operator Configuration Prometheus has to be configured to collect the metrics from the weblogic-monitor-exporter. The Prometheus operator identifies the targets using service discovery. To get the weblogic-monitor-exporter end point discovered as a target, you will need to create a service monitor to point to the service as follows:\n  Create a wls-exporter-service-monitor.yaml in the \u0026lt;work_directory\u0026gt;/wls_exporter directory with the following contents:\napiVersion: v1 kind: Secret metadata: name: basic-auth namespace: monitoring data: password: V2VsY29tZTE= ## \u0026lt;password\u0026gt; base64 user: d2VibG9naWM= ## weblogic base64 type: Opaque --- apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: wls-exporter-oimcluster namespace: monitoring labels: k8s-app: wls-exporter spec: namespaceSelector: matchNames: - oimcluster selector: matchLabels: weblogic.domainName: oimcluster endpoints: - basicAuth: password: name: basic-auth key: password username: name: basic-auth key: user port: default relabelings: - action: labelmap regex: __meta_kubernetes_service_label_(.+) interval: 10s honorLabels: true path: /wls-exporter/metrics Note: In the above example, change the password value to the base64 encoded version of your weblogic password. To find the base64 value run the following:\n$ echo -n \u0026#34;\u0026lt;password\u0026gt;\u0026#34; | base64 If using a different namespace from oimcluster or a different domain_UID from oimcluster, then change accordingly.\n  Add Rolebinding for the WebLogic OIG domain namespace:\n$ cd \u0026lt;work_directory\u0026gt;/kube-prometheus/manifests Edit the prometheus-roleBindingSpecificNamespaces.yaml file and add the following to the file for your OIG domain namespace, for example oimcluster:\n- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: prometheus-k8s namespace: oimcluster roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: prometheus-k8s subjects: - kind: ServiceAccount name: prometheus-k8s namespace: monitoring For example the file should now read:\napiVersion: rbac.authorization.k8s.io/v1 items: - apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: prometheus-k8s namespace: oimcluster roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: prometheus-k8s subjects: - kind: ServiceAccount name: prometheus-k8s namespace: monitoring - apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: prometheus-k8s namespace: default ....   Add the Role for WebLogic OIG domain namespace. Edit the prometheus-roleSpecificNamespaces.yaml and change the namespace to your OIG domain namespace, for example oimcluster:\n- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: prometheus-k8s namespace: oimcluster rules: - apiGroups: - \u0026quot;\u0026quot; resources: - services - endpoints - pods verbs: - get - list - watch ....\t  Apply the yaml files as follows:\n$ kubectl apply -f prometheus-roleBindingSpecificNamespaces.yaml $ kubectl apply -f prometheus-roleSpecificNamespaces.yaml The output should look similar to the following:\nkubectl apply -f prometheus-roleBindingSpecificNamespaces.yaml rolebinding.rbac.authorization.k8s.io/prometheus-k8s created Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply rolebinding.rbac.authorization.k8s.io/prometheus-k8s configured Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply rolebinding.rbac.authorization.k8s.io/prometheus-k8s configured Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply rolebinding.rbac.authorization.k8s.io/prometheus-k8s configured $ kubectl apply -f prometheus-roleSpecificNamespaces.yaml role.rbac.authorization.k8s.io/prometheus-k8s created Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply role.rbac.authorization.k8s.io/prometheus-k8s configured Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply role.rbac.authorization.k8s.io/prometheus-k8s configured Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply role.rbac.authorization.k8s.io/prometheus-k8s configured   Deploy the ServiceMonitor   Run the following command to create the ServiceMonitor:\n$ cd \u0026lt;work_directory\u0026gt;/wls_exporter $ kubectl create -f wls-exporter-service-monitor.yaml The output will look similar to the following:\nservicemonitor.monitoring.coreos.com/wls-exporter-oim-cluster created   Prometheus Service Discovery After ServiceMonitor is deployed, the wls-exporter should be discovered by Prometheus and be able to scrape metrics.\n  Access the following URL to view Prometheus service discovery: http://${MASTERNODE-HOSTNAME}:32101/service-discovery\n  Click on monitoring/wls-exporter-oimcluster/0  and then show more. Verify all the targets are mentioned.\n  Grafana Dashboard   Access the Grafana dashboard with the following URL: http://${MASTERNODE-HOSTNAME}:32100 and login with admin/admin. Change your password when prompted.\n  Import the Grafana dashboard by navigating on the left hand menu to Create \u0026gt; Import. Copy the content from \u0026lt;work_directory\u0026gt;/fmw-kubernetes/OracleIdentityGovernance/kubernetes/3.0.1/grafana/weblogic_dashboard.json and paste. Then click Load and Import.\n  "
},
{
	"uri": "/fmw-kubernetes/20.4.1/oam/validate-domain-urls/",
	"title": "Validate Domain URLs",
	"tags": [],
	"description": "Sample for validating domain urls.",
	"content": "In this section you validate the OAM domain URLs are accessible via the NGINX or Voyager ingress.\nMake sure you know the master hostname and ingress port for NGINX or Voyager before proceeding.\nValidate the OAM domain urls via the Ingress Launch a browser and access the following URL\u0026rsquo;s. Login with the weblogic username and password (weblogic/\u0026lt;password\u0026gt;).\n   Console or Page URL     WebLogic Administration Console https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/console   Oracle Enterprise Manager Console https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/em   Oracle Access Management Console https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/oamconsole   Oracle Access Management Console https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/access   Logout URL https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/oam/server/logout    Note: WebLogic Administration Console and Oracle Enterprise Manager Console should only be used to monitor the servers in the OAM domain. To control the Administration Server and OAM Managed Servers (start/stop) you must use Kubernetes. See Domain Life Cycle  for more information.\nThe browser will give certificate errors if you used a self signed certifcate and have not imported it into the browsers Certificate Authority store. If this occurs you can proceed with the connection and ignore the errors.\n"
},
{
	"uri": "/fmw-kubernetes/20.4.1/oig/validate-domain-urls/",
	"title": "Validate Domain URLs",
	"tags": [],
	"description": "Sample for validating domain urls.",
	"content": "In this section you validate the OIG domain URLs that are accessible via the NGINX or Voyager ingress.\nMake sure you know the master hostname and port before proceeding.\nValidate the OIG domain urls via the Ingress Launch a browser and access the following URL\u0026rsquo;s. Use http or https depending on whether you configured your ingress for non-ssl or ssl.\nLogin to the WebLogic Administration Console and Oracle Enterprise Manager Console with the WebLogic username and password (weblogic/\u0026lt;password\u0026gt;).\nLogin to Oracle Identity Governance with the xelsysadm username and password (xelsysadm/\u0026lt;password\u0026gt;).\n   Console or Page URL     WebLogic Administration Console https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/console   Oracle Enterprise Manager Console https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/em   Oracle Identity System Administration https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/sysadmin   Oracle Identity Self Service https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/identity    Note: WebLogic Administration Console and Oracle Enterprise Manager Console should only be used to monitor the servers in the OIG domain. To control the Administration Server and OIG Managed Servers (start/stop) you must use Kubernetes. See Domain Life Cycle  for more information.\nThe browser will give certificate errors if you used a self signed certifcate and have not imported it into the browsers Certificate Authority store. If this occurs you can proceed with the connection and ignore the errors.\n"
},
{
	"uri": "/fmw-kubernetes/20.4.1/oud/patch-and-upgrade/",
	"title": "Upgrade",
	"tags": [],
	"description": "This document provides steps to upgrade a Kubernetes Cluster.",
	"content": "Upgrade the underlying Kubernetes cluster to a new release.\n a) Upgrade a Kubernetes cluster  Instructions on how to upgrade a Kubernetes cluster.\n "
},
{
	"uri": "/fmw-kubernetes/20.4.1/oudsm/patch-and-upgrade/",
	"title": "Upgrade",
	"tags": [],
	"description": "This document provides steps to upgrade a Kubernetes Cluster.",
	"content": "Upgrade the underlying Kubernetes cluster to a new release.\n b) Upgrade a Kubernetes cluster  Instructions on how to upgrade a Kubernetes cluster.\n "
},
{
	"uri": "/fmw-kubernetes/20.4.1/oig/manage-oig-domains/delete-domain-home/",
	"title": "Delete the OIG domain home",
	"tags": [],
	"description": "Learn about the steps to cleanup the OIG domain home.",
	"content": "Sometimes in production, but most likely in testing environments, you might want to remove the domain home that is generated using the create-domain.sh script.\n  Run the following command to delete the jobs, domain, and configmaps:\n$ kubectl delete jobs \u0026lt;domain_job\u0026gt; -n \u0026lt;domain_namespace\u0026gt; $ kubectl delete domain \u0026lt;domain_uid\u0026gt; -n \u0026lt;domain_namespace\u0026gt; $ kubectl delete configmaps \u0026lt;domain_job\u0026gt;-cm -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl delete jobs oimcluster-create-fmw-infra-sample-domain-job -n oimcluster $ kubectl delete domain oimcluster -n oimcluster $ kubectl delete configmaps oimcluster-create-fmw-infra-sample-domain-job-cm -n oimcluster   Drop the RCU schemas as follows:\n$ kubectl exec -it helper -n \u0026lt;domain_namespace\u0026gt; -- /bin/bash [oracle@helper ~]$ [oracle@helper ~]$ export CONNECTION_STRING=\u0026lt;db_host.domain\u0026gt;:\u0026lt;db_port\u0026gt;/\u0026lt;service_name\u0026gt; [oracle@helper ~]$ export RCUPREFIX=\u0026lt;rcu_schema_prefix\u0026gt; /u01/oracle/oracle_common/bin/rcu -silent -dropRepository -databaseType ORACLE -connectString $CONNECTION_STRING \\ -dbUser sys -dbRole sysdba -selectDependentsForComponents true -schemaPrefix $RCUPREFIX \\ -component MDS -component IAU -component IAU_APPEND -component IAU_VIEWER -component OPSS \\ -component WLS -component STB -component OIM -component SOAINFRA -component UCSUMS -f \u0026lt; /tmp/pwd.txt For example:\n$ kubectl exec -it helper -n oimcluster -- /bin/bash [oracle@helper ~]$ export CONNECTION_STRING=mydatabasehost.example.com:1521/orcl.example.com [oracle@helper ~]$ export RCUPREFIX=OIGK8S /u01/oracle/oracle_common/bin/rcu -silent -dropRepository -databaseType ORACLE -connectString $CONNECTION_STRING \\ -dbUser sys -dbRole sysdba -selectDependentsForComponents true -schemaPrefix $RCUPREFIX \\ -component MDS -component IAU -component IAU_APPEND -component IAU_VIEWER -component OPSS \\ -component WLS -component STB -component OIM -component SOAINFRA -component UCSUMS -f \u0026lt; /tmp/pwd.txt   Delete the Persistent Volume and Persistent Volume Claim:\n$ kubectl delete pv \u0026lt;pv-name\u0026gt; -n \u0026lt;domain_namespace\u0026gt; $ kubectl delete pvc \u0026lt;pvc-name\u0026gt; -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl delete pv oimcluster-oim-pv -n oimcluster $ kubectl delete pvc oimcluster-oim-pvc -n oimcluster   Delete the contents of the persistent volume, for example:\n$ rm -rf /\u0026lt;work directory\u0026gt;/oimclusterdomainpv/* For example:\n$ rm -rf /scratch/OIGDockerK8S/oimclusterdomainpv/*   Delete the Oracle WebLogic Kubernetes Operator, by running the following command:\n$ helm delete weblogic-kubernetes-operator -n operator   To delete NGINX:\n$ helm delete oimcluster-nginx -n oimcluster $ helm delete nginx-ingress -n nginx $ kubectl delete namespace nginx or if using SSL:\n$ helm delete oimcluster-nginx -n oimcluster $ helm delete nginx-ingress -n nginxssl $ kubectl delete namespace nginxssl   To delete Voyager:\n$ helm delete oimcluster-voyager -n oimcluster $ helm delete voyager-ingress -n voyager $ kubectl delete namespace voyager or if using SSL:\n$ helm delete oimcluster-voyager -n oimcluster $ helm delete voyager-ingress -n voyagerssl $ kubectl delete namespace voyagerssl   "
},
{
	"uri": "/fmw-kubernetes/20.4.1/oam/post-install-config/",
	"title": "Post Install Configuration",
	"tags": [],
	"description": "Post install configuration.",
	"content": "Follow these mandatory post install configuration steps.\n WebLogic Server Tuning Modify oamconfig.properties  WebLogic Server Tuning For production environments, the following WebLogic Server tuning parameters must be set:\nAdd Minimum Thread constraint to worker manager \u0026ldquo;OAPOverRestWM\u0026rdquo;  Login to the WebLogic Server Console at https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/console. Click Lock \u0026amp; Edit. In Domain Structure, click Deployments. On the Deployments page click Next until you see oam_server. Expand oam_server by clicking on the + icon, then click /iam/access/binding. Click the Configuration tab, followed by the Workload tab. Click wm/OAPOverRestWM Under Application Scoped Work Managed Components, click New. In Create a New Work Manager Component, select Minumum Threads Constraint and click Next. In Minimum Threads Constraint Properties enter the Count as 400 and click Finish. In the Save Deployment Plan change the Path to the value /u01/oracle/user_projects/domains/accessinfra/Plan.xml, where accessinfra is your domain_UID. Click OK and then Activate Changes.  Remove Max Thread Constraint and Capacity Constraint  Repeat steps 1-7 above. Under Application Scoped Work Managed Components select the check box for Capacity and MaxThreadsCount. Click Delete. In the Delete Work Manage Components screen, click OK to delete. Click on Release Configuration and then Log Out.  oamDS DataSource Tuning  Login to the WebLogic Server Console at https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/console. Click Lock \u0026amp; Edit. In Domain Structure, Expand Data Sources and click Data Sources. Click on oamDS. In Settings for oamDS, select the Configuration tab, and then the Connection Pool tab. Change Initial Capacity, Maximum Capacity, and Minimum Capacity to 800 and click Save. Click Activate Changes.  Modify oamconfig.properties   Navigate to the following directory and change permissions for the oamconfig_modify.sh:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-access-domain/domain-home-on-pv/common $ chmod 777 oamconfig_modify.sh For example:\n$ cd /scratch/OAMDockerK8S/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-access-domain/domain-home-on-pv/common $ chmod 777 oamconfig_modify.sh   Edit the oamconfig.properties and change the OAM_NAMESPACE, INGRESS, INGRESS_NAME, and LBR_HOST to match the values for your OAM Kubernetes environment. For example:\n#Below are only the sample values, please modify them as per your setup # The name space where OAM servers are created OAM_NAMESPACE='accessns' # Define the INGRESS CONTROLLER used. typical values are voyager/nginx INGRESS=\u0026quot;nginx\u0026quot; # Define the INGRESS CONTROLLER name used during installation. INGRESS_NAME=\u0026quot;nginx-ingress\u0026quot; # FQDN of the LBR Host i.e the host from where you access oam console LBR_HOST=\u0026quot;masternode.example.com\u0026quot;   Run the oamconfig_modify.sh script as follows:\n$ ./oamconfig_modify.sh \u0026lt;OAM_ADMIN_USER\u0026gt;:\u0026lt;OAM_ADMIN_PASSWORD\u0026gt; where:\nOAM_ADMIN_USER is the OAM administrator username\nOAM_ADMIN_PASSWORD is the OAM administrator password\nFor example:\n$ ./oamconfig_modify.sh weblogic:\u0026lt;password\u0026gt; Note: Make sure port 30540 is free before running the command.\nThe output will look similar to the following:\nLBR_PROTOCOL: https domainUID: accessinfra OAM_SERVER: accessinfra-oam-server OAM_NAMESPACE: accessns INGRESS: nginx INGRESS_NAME: nginx-ingress ING_TYPE : NodePort LBR_HOST: masternode.example.com LBR_PORT: 32190 Started Executing Command % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 762k 0 762k 0 0 3276k 0 --:--:-- --:--:-- --:--:-- 3273k new_cluster_id: abcd-accessinfra oamoap-service NodePort 10.96.63.13 \u0026lt;none\u0026gt; 5575:30540/TCP 5h9m HTTP/1.1 100 Continue HTTP/1.1 201 Created Date: Thu, 15 Oct 2020 11:22:46 GMT Content-Type: text/plain Content-Length: 76 Connection: keep-alive X-ORACLE-DMS-ECID: 9aadbcc3-e0a5-46d7-882a-484b17587cf2-00005839 X-ORACLE-DMS-RID: 0 Set-Cookie: JSESSIONID=XmIr_3-T4iesEkMJxp5NCqZxjr5M-0icByML5hkwMQn9-KCg_zno!-992913931; path=/; HttpOnly Set-Cookie: _WL_AUTHCOOKIE_JSESSIONID=t2cmxfVqXI.sZLm.8tPo; path=/; secure; HttpOnly Strict-Transport-Security: max-age=15724800; includeSubDomains https://masternode.example.com:32190/iam/admin/config/api/v1/config?path=%2F /home/rest/output/oamconfig_modify.xml executed successfully --------------------------------------------------------------------------- Initializing WebLogic Scripting Tool (WLST) ... Welcome to WebLogic Server Administration Scripting Shell Type help() for help on available commands Connecting to t3://accessinfra-adminserver:7001 with userid weblogic ... Successfully connected to Admin Server \u0026quot;AdminServer\u0026quot; that belongs to domain \u0026quot;accessinfra\u0026quot;. Warning: An insecure protocol was used to connect to the server. To ensure on-the-wire security, the SSL port or Admin port should be used instead. Location changed to domainRuntime tree. This is a read-only tree with DomainMBean as the root MBean. For more help, use help('domainRuntime') Exiting WebLogic Scripting Tool. Please wait for some time for the server to restart pod \u0026quot;accessinfra-oam-server1\u0026quot; deleted pod \u0026quot;accessinfra-oam-server2\u0026quot; deleted The script will delete the accessinfra-oam-server1 and accessinfra-oam-server2 pods and then create new ones. Check the pods are running again by issuing the following command:\n$ kubectl get pods -n accessns The output will look similar to the following:\nNAME READY STATUS RESTARTS AGE pod/accessinfra-adminserver 1/1 Running 0 1h17m pod/accessinfra-create-oam-infra-domain-job-vj69h 0/1 Completed 0 1h42m pod/accessinfra-oam-policy-mgr1 1/1 Running 0 1h9m pod/accessinfra-oam-server1 0/1 Running 0 31s pod/accessinfra-oam-server2 0/1 Running 0 31s The accessinfra-oam-server1 and accessinfra-oam-server2 are started, but currently have a READY status of 0/1. This means oam_server1 and oam_server2 are not currently running but are in the process of starting. The servers will take several minutes to start so keep executing the command until READY shows 1/1:\nNAME READY STATUS RESTARTS AGE pod/accessinfra-adminserver 1/1 Running 0 1h23m pod/accessinfra-create-oam-infra-domain-job-vj69h 0/1 Completed 0 1h48m pod/accessinfra-oam-policy-mgr1 1/1 Running 0 1h15m pod/accessinfra-oam-server1 1/1 Running 0 6m pod/accessinfra-oam-server2 1/1 Running 0 6m   "
},
{
	"uri": "/fmw-kubernetes/20.4.1/oig/post-install-config/",
	"title": "Post Install Configuration",
	"tags": [],
	"description": "Post install configuration.",
	"content": "Follow these post install configuration steps.\n a. Set OIMfrontendURL  Set the OIMfrontendURL in Oracle Enterprise Manager.\n b. Install and Configure Connectors  Install and Configure Connectors.\n "
},
{
	"uri": "/fmw-kubernetes/20.4.1/oud/security-hardening/",
	"title": "Security Hardening",
	"tags": [],
	"description": "Review resources for the Docker and Kubernetes cluster hardening.",
	"content": "Securing a Kubernetes cluster involves hardening on multiple fronts - securing the API servers, etcd, nodes, container images, container run-time, and the cluster network. Apply principles of defense in depth, principle of least privilege, and minimize the attack surface. Use security tools such as Kube-Bench to verify the cluster’s security posture. Since Kubernetes is evolving rapidly, refer to Kubernetes Security Overview for the latest information on securing a Kubernetes cluster. Also ensure the deployed Docker containers follow the Docker Security guidance.\nThis section provides references on how to securely configure Docker and Kubernetes.\nReferences   Docker hardening:\n https://docs.docker.com/engine/security/security/ https://blog.aquasec.com/docker-security-best-practices    Kubernetes hardening:\n https://kubernetes.io/docs/concepts/security/overview/ https://kubernetes.io/docs/concepts/security/pod-security-standards/ https://blogs.oracle.com/developers/5-best-practices-for-kubernetes-security    "
},
{
	"uri": "/fmw-kubernetes/20.4.1/oudsm/security-hardening/",
	"title": "Security Hardening",
	"tags": [],
	"description": "Review resources for the Docker and Kubernetes cluster hardening.",
	"content": "Securing a Kubernetes cluster involves hardening on multiple fronts - securing the API servers, etcd, nodes, container images, container run-time, and the cluster network. Apply principles of defense in depth, principle of least privilege, and minimize the attack surface. Use security tools such as Kube-Bench to verify the cluster’s security posture. Since Kubernetes is evolving rapidly, refer to Kubernetes Security Overview for the latest information on securing a Kubernetes cluster. Also ensure the deployed Docker containers follow the Docker Security guidance.\nThis section provides references on how to securely configure Docker and Kubernetes.\nReferences   Docker hardening:\n https://docs.docker.com/engine/security/security/ https://blog.aquasec.com/docker-security-best-practices    Kubernetes hardening:\n https://kubernetes.io/docs/concepts/security/overview/ https://kubernetes.io/docs/concepts/security/pod-security-standards/ https://blogs.oracle.com/developers/5-best-practices-for-kubernetes-security    "
},
{
	"uri": "/fmw-kubernetes/20.4.1/oig/configure-design-console/",
	"title": "Configure Design Console",
	"tags": [],
	"description": "Configure Design Console.",
	"content": "Configure an Ingress to allow Design Console to connect to your Kubernetes cluster.\n a. Using Design Console with NGINX(non-SSL)  Configure Design Console with NGINX(non-SSL).\n a. Using Design Console with NGINX(SSL)  Configure Design Console with NGINX(SSL).\n a. Using Design Console with Voyager(non-SSL)  Configure Design Console with Voyager(non-SSL).\n a. Using Design Console with Voyager(SSL)  Configure Design Console with Voyager(SSL).\n "
},
{
	"uri": "/fmw-kubernetes/20.4.1/oig/manage-oig-domains/",
	"title": "Manage OIG Domains",
	"tags": [],
	"description": "This document provides steps to manage the OIG domain.",
	"content": "Important considerations for Oracle Identity Governance domains in Kubernetes.\n Domain Life Cycle  Learn about the domain life cyle of an OIG domain.\n WLST Administration Operations  Describes the steps for WLST administration using helper pod running in the same Kubernetes Cluster as OIG Domain.\n Runnning OIG Utilities  Describes the steps for running OIG utilities in Kubernetes.\n Logging and Visualization  Describes the steps for logging and visualization with Elasticsearch and Kibana.\n Monitoring an OIG domain  Describes the steps for Monitoring the OIG domain and Publising the logs to Elasticsearch.\n Delete the OIG domain home  Learn about the steps to cleanup the OIG domain home.\n "
},
{
	"uri": "/fmw-kubernetes/20.4.1/oam/validate-sso-using-webgate/",
	"title": "Validate a Basic SSO Flow using WebGate Registration ",
	"tags": [],
	"description": "Sample for validating a basic SSO flow using WebGate registration.",
	"content": "In this section you validate single-sign on works to the OAM Kubernetes cluster via Oracle WebGate. The instructions below assume you have a running Oracle HTTP Server (for example ohs_k8s) and Oracle WebGate installed on an independent server. The instructions also assume basic knowledge of how to register a WebGate agent.\nNote: At present Oracle HTTP Server and Oracle WebGate are not supported on a Kubernetes cluster.\nUpdate the OAM Hostname and Port for the Loadbalancer If using an NGINX or Voyager ingress with no load balancer, change {LOADBALANCER-HOSTNAME}:${LOADBALANCER-PORT} to {MASTERNODE-HOSTNAME}:${MASTERNODE-PORT} when referenced below.\n  Launch a browser and access the OAM console (https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-PORT}/oamconsole). Login with the weblogic username and password (weblogic/\u0026lt;password\u0026gt;)\n  Navigate to Configuration → Settings ( View ) → Access Manager.\n  Under Load Balancing modify the OAM Server Host and OAM Server Port, to point to the Loadbalancer HTTP endpoint (e.g loadbalancer.example.com and \u0026lt;port\u0026gt; respectively). In the OAM Server Protocol drop down list select https.\n  Under WebGate Traffic Load Balancer modify the OAM Server Host and OAM Server Port, to point to the Loadbalancer HTTP endpoint (e.g loadbalancer.example.com and \u0026lt;port\u0026gt; repectively). In the OAM Server Protocol drop down list select https.\n  Click Apply.\n  Register a WebGate Agent In all the examples below, change the directory path as appropriate for your installation.\n  Run the following command on the server with Oracle HTTP Server and WebGate installed:\n$ cd /scratch/export/home/oracle/product/middleware/webgate/ohs/tools/deployWebGate $ ./deployWebGateInstance.sh -w /scratch/export/home/oracle/admin/domains/oam_domain/config/fmwconfig/components/OHS/ohs_k8s -oh /scratch/export/home/oracle/product/middleware -ws ohs The output will look similar to the following:\nCopying files from WebGate Oracle Home to WebGate Instancedir   Run the following command to update the OHS configuration files appropriately:\n$ export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/scratch/export/home/oracle/product/middleware/lib $ cd /scratch/export/home/oracle/product/middleware/webgate/ohs/tools/setup/InstallTools/ $ ./EditHttpConf -w /scratch/export/home/oracle/admin/domains/oam_domain/config/fmwconfig/components/OHS/ohs_k8s -oh /scratch/export/home/oracle/product/middleware The output will look similar to the following:\nThe web server configuration file was successfully updated /scratch/export/home/oracle/admin/domains/oam_domain/config/fmwconfig/components/OHS/ohs_k8s/httpd.conf has been backed up as /scratch/export/home/oracle/admin/domains/oam_domain/config/fmwconfig/components/OHS/ohs_k8s/httpd.conf.ORIG   Launch a browser, and access the OAM console. Navigate to Application Security → Quick Start Wizards → SSO Agent Registration. Register the agent in the usual way, download the configuration zip file and copy to the OHS WebGate server, for example: /scratch/export/home/oracle/admin/domains/oam_domain/config/fmwconfig/components/OHS/ohs_k8/webgate/config. Extract the zip file.\n  Copy the Certificate Authority (CA) certificate (cacert.pem) for the load balancer/ingress certificate to the same directory e.g: /scratch/export/home/oracle/admin/domains/oam_domain/config/fmwconfig/components/OHS/ohs_k8/webgate/config.\nIf you used a self signed certificate for the ingress, instead copy the self signed certificate (e.g: /scratch/ssl/tls.crt) to the above directory. Rename the certificate to cacert.pem.\n  Restart Oracle HTTP Server.\n  Access the configured OHS e.g http://ohs.example.com:7778, and check you are redirected to the SSO login page. Login and make sure you are redirected successfully to the home page.\n  Changing WebGate agent to use OAP Note: This section should only be followed if you need to change the OAM/WebGate Agent communication from HTTPS to OAP.\nTo change the WebGate agent to use OAP:\n  In the OAM Console click Application Security and then Agents.\n  Search for the agent you want modify and select it.\n  In the User Defined Parameters change:\na) OAMServerCommunicationMode from HTTPS to OAP. For example OAMServerCommunicationMode=OAP\nb) OAMRestEndPointHostName=\u0026lt;hostname\u0026gt; to the {$MASTERNODE-HOSTNAME}. For example OAMRestEndPointHostName=masternode.example.com\n  In the Server Lists section click Add to a add new server with the following values:\n Access Server: oam_server Host Name: \u0026lt;{$MASTERNODE-HOSTNAME}\u0026gt; Host Port: \u0026lt;oamoap-service NodePort\u0026gt;  Note: To find the value for Host Port run the following:\n$ kubectl describe svc oamoap-service -n accessns The output will look similar to the following:\nName: oamoap-service Namespace: accessns Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Selector: weblogic.clusterName=oam_cluster Type: NodePort IP: 10.96.63.13 Port: \u0026lt;unset\u0026gt; 5575/TCP TargetPort: 5575/TCP NodePort: \u0026lt;unset\u0026gt; 30540/TCP Endpoints: 10.244.0.30:5575,10.244.0.31:5575 Session Affinity: None External Traffic Policy: Cluster Events: \u0026lt;none\u0026gt;   Delete all servers in Server Lists except for the one just created, and click Apply.\n  Click Download to download the webgate zip file. Copy the zip file to the desired WebGate.\n  "
},
{
	"uri": "/fmw-kubernetes/20.4.1/oud/troubleshooting/",
	"title": "Troubleshooting",
	"tags": [],
	"description": "How to Troubleshoot issues.",
	"content": " Check the Status of a Namespace View POD Logs View Pod Description  Check the Status of a Namespace To check the status of objects in a namespace use the following command:\n$ kubectl --namespace \u0026lt;namespace\u0026gt; get nodes,pod,service,secret,pv,pvc,ingress -o wide Output will be similar to the following:\n$ kubectl --namespace myhelmns get nodes,pod,service,secret,pv,pvc,ingress -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME node/10.89.73.203 Ready \u0026lt;none\u0026gt; 75d v1.18.4 10.89.73.203 \u0026lt;none\u0026gt; Oracle Linux Server 7.5 4.1.12-124.35.2.el7uek.x86_64 docker://19.3.11 node/10.89.73.204 Ready \u0026lt;none\u0026gt; 75d v1.18.4 10.89.73.204 \u0026lt;none\u0026gt; Oracle Linux Server 7.5 4.1.12-124.38.1.el7uek.x86_64 docker://19.3.11 node/10.89.73.42 Ready master 76d v1.18.4 10.89.73.42 \u0026lt;none\u0026gt; Oracle Linux Server 7.5 4.1.12-124.35.2.el7uek.x86_64 docker://19.3.11 NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/my-oud-ds-rs-0 1/1 Running 0 83m 10.244.1.90 10.89.73.203 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/my-oud-ds-rs-1 1/1 Running 0 83m 10.244.1.91 10.89.73.203 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/my-oud-ds-rs-2 1/1 Running 0 83m 10.244.1.89 10.89.73.203 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service/my-oud-ds-rs-0 ClusterIP 10.100.226.50 \u0026lt;none\u0026gt; 1444/TCP,1888/TCP,1898/TCP 83m app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=my-oud-ds-rs-0 service/my-oud-ds-rs-1 ClusterIP 10.96.231.214 \u0026lt;none\u0026gt; 1444/TCP,1888/TCP,1898/TCP 83m app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=my-oud-ds-rs-1 service/my-oud-ds-rs-2 ClusterIP 10.99.254.14 \u0026lt;none\u0026gt; 1444/TCP,1888/TCP,1898/TCP 83m app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=my-oud-ds-rs-2 service/my-oud-ds-rs-http-0 ClusterIP 10.109.186.111 \u0026lt;none\u0026gt; 1080/TCP,1081/TCP 83m app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=my-oud-ds-rs-0 service/my-oud-ds-rs-http-1 ClusterIP 10.101.227.72 \u0026lt;none\u0026gt; 1080/TCP,1081/TCP 83m app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=my-oud-ds-rs-1 service/my-oud-ds-rs-http-2 ClusterIP 10.103.18.99 \u0026lt;none\u0026gt; 1080/TCP,1081/TCP 83m app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=my-oud-ds-rs-2 service/my-oud-ds-rs-lbr-admin ClusterIP 10.105.211.54 \u0026lt;none\u0026gt; 1888/TCP,1444/TCP 83m app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs service/my-oud-ds-rs-lbr-http ClusterIP 10.99.23.245 \u0026lt;none\u0026gt; 1080/TCP,1081/TCP 83m app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs service/my-oud-ds-rs-lbr-ldap ClusterIP 10.103.171.90 \u0026lt;none\u0026gt; 1389/TCP,1636/TCP 83m app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs service/my-oud-ds-rs-ldap-0 ClusterIP 10.107.250.130 \u0026lt;none\u0026gt; 1389/TCP,1636/TCP 83m app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=my-oud-ds-rs-0 service/my-oud-ds-rs-ldap-1 ClusterIP 10.100.73.198 \u0026lt;none\u0026gt; 1389/TCP,1636/TCP 83m app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=my-oud-ds-rs-1 service/my-oud-ds-rs-ldap-2 ClusterIP 10.98.176.118 \u0026lt;none\u0026gt; 1389/TCP,1636/TCP 83m app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=my-oud-ds-rs-2 NAME TYPE DATA AGE secret/default-token-kfmhq kubernetes.io/service-account-token 3 84m secret/my-oud-ds-rs-creds opaque 8 83m secret/my-oud-ds-rs-tls-cert kubernetes.io/tls 2 83m secret/my-oud-ds-rs-token-c4tg4 kubernetes.io/service-account-token 3 83m secret/sh.helm.release.v1.my-oud-ds-rs.v1 helm.sh/release.v1 1 83m NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE VOLUMEMODE persistentvolume/my-oud-ds-rs-espv1 20Gi RWX Retain Available elk 83m Filesystem persistentvolume/my-oud-ds-rs-pv 30Gi RWX Retain Bound myhelmns/my-oud-ds-rs-pvc manual 83m Filesystem persistentvolume/oimcluster-oim-pv 10Gi RWX Retain Bound oimcluster/oimcluster-oim-pvc oimcluster-oim-storage-class 63d Filesystem NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE VOLUMEMODE persistentvolumeclaim/my-oud-ds-rs-pvc Bound my-oud-ds-rs-pv 30Gi RWX manual 83m Filesystem NAME CLASS HOSTS ADDRESS PORTS AGE ingress.extensions/my-oud-ds-rs-admin-ingress-nginx \u0026lt;none\u0026gt; my-oud-ds-rs-admin-0,my-oud-ds-rs-admin-1,my-oud-ds-rs-admin-2 + 2 more... 80, 443 83m ingress.extensions/my-oud-ds-rs-http-ingress-nginx \u0026lt;none\u0026gt; my-oud-ds-rs-http-0,my-oud-ds-rs-http-1,my-oud-ds-rs-http-2 + 3 more... 80, 443 83m Include/exclude elements (nodes,pod,service,secret,pv,pvc,ingress) as required.\nView POD Logs To view logs for a POD use the following command:\n$ kubectl logs \u0026lt;pod\u0026gt; -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl logs my-oud-ds-rs-0 -n myhelmns Output will depend on the application running in the POD.\nView Pod Description Details about a POD can be viewed using the kubectl describe command:\n$ kubectl describe pod \u0026lt;pod\u0026gt; -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl describe pod my-oud-ds-rs-0 -n myhelmns Name: my-oud-ds-rs-0 Namespace: myhelmns Priority: 0 Node: 10.89.73.203/10.89.73.203 Start Time: Wed, 07 Oct 2020 07:30:27 -0700 Labels: app.kubernetes.io/instance=my-oud-ds-rs app.kubernetes.io/managed-by=Helm app.kubernetes.io/name=oud-ds-rs app.kubernetes.io/version=12.2.1.4.0 helm.sh/chart=oud-ds-rs-0.1 oud/instance=my-oud-ds-rs-0 Annotations: meta.helm.sh/release-name: my-oud-ds-rs meta.helm.sh/release-namespace: myhelmns Status: Running IP: 10.244.1.90 IPs: IP: 10.244.1.90 Containers: oud-ds-rs: Container ID: docker://e3b79a283f56870e6d702cf8c2cc7aafa09a242f7a2cd543d8014a24aa219903 Image: oracle/oud:12.2.1.4.0 Image ID: docker://sha256:8a937042bef357fdeb09ce20d34332b14d1f1afe3ccb9f9b297f6940fdf32a76 Ports: 1444/TCP, 1888/TCP, 1389/TCP, 1636/TCP, 1080/TCP, 1081/TCP, 1898/TCP Host Ports: 0/TCP, 0/TCP, 0/TCP, 0/TCP, 0/TCP, 0/TCP, 0/TCP State: Running Started: Wed, 07 Oct 2020 07:30:28 -0700 Ready: True Restart Count: 0 Liveness: tcp-socket :ldap delay=900s timeout=15s period=30s #success=1 #failure=1 Readiness: exec [/u01/oracle/container-scripts/checkOUDInstance.sh] delay=180s timeout=30s period=60s #success=1 #failure=10 Environment: instanceType: Directory sleepBeforeConfig: 3 OUD_INSTANCE_NAME: my-oud-ds-rs-0 hostname: my-oud-ds-rs-0 baseDN: dc=example,dc=com rootUserDN: \u0026lt;set to the key 'rootUserDN' in secret 'my-oud-ds-rs-creds'\u0026gt; Optional: false rootUserPassword: \u0026lt;set to the key 'rootUserPassword' in secret 'my-oud-ds-rs-creds'\u0026gt; Optional: false adminConnectorPort: 1444 httpAdminConnectorPort: 1888 ldapPort: 1389 ldapsPort: 1636 httpPort: 1080 httpsPort: 1081 replicationPort: 1898 sampleData: 10 Mounts: /u01/oracle/user_projects from my-oud-ds-rs-pv (rw) /var/run/secrets/kubernetes.io/serviceaccount from my-oud-ds-rs-token-c4tg4 (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: my-oud-ds-rs-pv: Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace) ClaimName: my-oud-ds-rs-pvc ReadOnly: false my-oud-ds-rs-token-c4tg4: Type: Secret (a volume populated by a Secret) SecretName: my-oud-ds-rs-token-c4tg4 Optional: false QoS Class: BestEffort Node-Selectors: \u0026lt;none\u0026gt; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300s Events: \u0026lt;none\u0026gt; "
},
{
	"uri": "/fmw-kubernetes/20.4.1/oudsm/troubleshooting/",
	"title": "Troubleshooting",
	"tags": [],
	"description": "How to Troubleshoot issues.",
	"content": " Check the Status of a Namespace View POD Logs View Pod Description  Check the Status of a Namespace To check the status of objects in a namespace use the following command:\n$ kubectl --namespace \u0026lt;namespace\u0026gt; get nodes,pod,service,secret,pv,pvc,ingress -o wide Output will be similar to the following:\n$ kubectl --namespace myhelmns get nodes,pod,service,secret,pv,pvc,ingress -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME node/10.89.73.203 Ready \u0026lt;none\u0026gt; 75d v1.18.4 10.89.73.203 \u0026lt;none\u0026gt; Oracle Linux Server 7.5 4.1.12-124.35.2.el7uek.x86_64 docker://19.3.11 node/10.89.73.204 Ready \u0026lt;none\u0026gt; 75d v1.18.4 10.89.73.204 \u0026lt;none\u0026gt; Oracle Linux Server 7.5 4.1.12-124.38.1.el7uek.x86_64 docker://19.3.11 node/10.89.73.42 Ready master 76d v1.18.4 10.89.73.42 \u0026lt;none\u0026gt; Oracle Linux Server 7.5 4.1.12-124.35.2.el7uek.x86_64 docker://19.3.11 NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/my-oud-ds-rs-0 1/1 Running 0 83m 10.244.1.90 10.89.73.203 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/my-oud-ds-rs-1 1/1 Running 0 83m 10.244.1.91 10.89.73.203 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/my-oud-ds-rs-2 1/1 Running 0 83m 10.244.1.89 10.89.73.203 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service/my-oud-ds-rs-0 ClusterIP 10.100.226.50 \u0026lt;none\u0026gt; 1444/TCP,1888/TCP,1898/TCP 83m app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=my-oud-ds-rs-0 service/my-oud-ds-rs-1 ClusterIP 10.96.231.214 \u0026lt;none\u0026gt; 1444/TCP,1888/TCP,1898/TCP 83m app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=my-oud-ds-rs-1 service/my-oud-ds-rs-2 ClusterIP 10.99.254.14 \u0026lt;none\u0026gt; 1444/TCP,1888/TCP,1898/TCP 83m app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=my-oud-ds-rs-2 service/my-oud-ds-rs-http-0 ClusterIP 10.109.186.111 \u0026lt;none\u0026gt; 1080/TCP,1081/TCP 83m app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=my-oud-ds-rs-0 service/my-oud-ds-rs-http-1 ClusterIP 10.101.227.72 \u0026lt;none\u0026gt; 1080/TCP,1081/TCP 83m app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=my-oud-ds-rs-1 service/my-oud-ds-rs-http-2 ClusterIP 10.103.18.99 \u0026lt;none\u0026gt; 1080/TCP,1081/TCP 83m app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=my-oud-ds-rs-2 service/my-oud-ds-rs-lbr-admin ClusterIP 10.105.211.54 \u0026lt;none\u0026gt; 1888/TCP,1444/TCP 83m app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs service/my-oud-ds-rs-lbr-http ClusterIP 10.99.23.245 \u0026lt;none\u0026gt; 1080/TCP,1081/TCP 83m app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs service/my-oud-ds-rs-lbr-ldap ClusterIP 10.103.171.90 \u0026lt;none\u0026gt; 1389/TCP,1636/TCP 83m app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs service/my-oud-ds-rs-ldap-0 ClusterIP 10.107.250.130 \u0026lt;none\u0026gt; 1389/TCP,1636/TCP 83m app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=my-oud-ds-rs-0 service/my-oud-ds-rs-ldap-1 ClusterIP 10.100.73.198 \u0026lt;none\u0026gt; 1389/TCP,1636/TCP 83m app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=my-oud-ds-rs-1 service/my-oud-ds-rs-ldap-2 ClusterIP 10.98.176.118 \u0026lt;none\u0026gt; 1389/TCP,1636/TCP 83m app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=my-oud-ds-rs-2 NAME TYPE DATA AGE secret/default-token-kfmhq kubernetes.io/service-account-token 3 84m secret/my-oud-ds-rs-creds opaque 8 83m secret/my-oud-ds-rs-tls-cert kubernetes.io/tls 2 83m secret/my-oud-ds-rs-token-c4tg4 kubernetes.io/service-account-token 3 83m secret/sh.helm.release.v1.my-oud-ds-rs.v1 helm.sh/release.v1 1 83m NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE VOLUMEMODE persistentvolume/my-oud-ds-rs-espv1 20Gi RWX Retain Available elk 83m Filesystem persistentvolume/my-oud-ds-rs-pv 30Gi RWX Retain Bound myhelmns/my-oud-ds-rs-pvc manual 83m Filesystem persistentvolume/oimcluster-oim-pv 10Gi RWX Retain Bound oimcluster/oimcluster-oim-pvc oimcluster-oim-storage-class 63d Filesystem NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE VOLUMEMODE persistentvolumeclaim/my-oud-ds-rs-pvc Bound my-oud-ds-rs-pv 30Gi RWX manual 83m Filesystem NAME CLASS HOSTS ADDRESS PORTS AGE ingress.extensions/my-oud-ds-rs-admin-ingress-nginx \u0026lt;none\u0026gt; my-oud-ds-rs-admin-0,my-oud-ds-rs-admin-1,my-oud-ds-rs-admin-2 + 2 more... 80, 443 83m ingress.extensions/my-oud-ds-rs-http-ingress-nginx \u0026lt;none\u0026gt; my-oud-ds-rs-http-0,my-oud-ds-rs-http-1,my-oud-ds-rs-http-2 + 3 more... 80, 443 83m Include/exclude elements (nodes,pod,service,secret,pv,pvc,ingress) as required.\nView POD Logs To view logs for a POD use the following command:\n$ kubectl logs \u0026lt;pod\u0026gt; -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl logs my-oudsm -n myhelmns Output will depend on the application running in the POD.\nView Pod Description Details about a POD can be viewed using the kubectl describe command:\n$ kubectl describe pod \u0026lt;pod\u0026gt; -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl describe pod my-oud-ds-rs-0 -n myhelmns Name: my-oud-ds-rs-0 Namespace: myhelmns Priority: 0 Node: 10.89.73.203/10.89.73.203 Start Time: Wed, 07 Oct 2020 07:30:27 -0700 Labels: app.kubernetes.io/instance=my-oud-ds-rs app.kubernetes.io/managed-by=Helm app.kubernetes.io/name=oud-ds-rs app.kubernetes.io/version=12.2.1.4.0 helm.sh/chart=oud-ds-rs-0.1 oud/instance=my-oud-ds-rs-0 Annotations: meta.helm.sh/release-name: my-oud-ds-rs meta.helm.sh/release-namespace: myhelmns Status: Running IP: 10.244.1.90 IPs: IP: 10.244.1.90 Containers: oud-ds-rs: Container ID: docker://e3b79a283f56870e6d702cf8c2cc7aafa09a242f7a2cd543d8014a24aa219903 Image: oracle/oud:12.2.1.4.0 Image ID: docker://sha256:8a937042bef357fdeb09ce20d34332b14d1f1afe3ccb9f9b297f6940fdf32a76 Ports: 1444/TCP, 1888/TCP, 1389/TCP, 1636/TCP, 1080/TCP, 1081/TCP, 1898/TCP Host Ports: 0/TCP, 0/TCP, 0/TCP, 0/TCP, 0/TCP, 0/TCP, 0/TCP State: Running Started: Wed, 07 Oct 2020 07:30:28 -0700 Ready: True Restart Count: 0 Liveness: tcp-socket :ldap delay=900s timeout=15s period=30s #success=1 #failure=1 Readiness: exec [/u01/oracle/container-scripts/checkOUDInstance.sh] delay=180s timeout=30s period=60s #success=1 #failure=10 Environment: instanceType: Directory sleepBeforeConfig: 3 OUD_INSTANCE_NAME: my-oud-ds-rs-0 hostname: my-oud-ds-rs-0 baseDN: dc=example,dc=com rootUserDN: \u0026lt;set to the key 'rootUserDN' in secret 'my-oud-ds-rs-creds'\u0026gt; Optional: false rootUserPassword: \u0026lt;set to the key 'rootUserPassword' in secret 'my-oud-ds-rs-creds'\u0026gt; Optional: false adminConnectorPort: 1444 httpAdminConnectorPort: 1888 ldapPort: 1389 ldapsPort: 1636 httpPort: 1080 httpsPort: 1081 replicationPort: 1898 sampleData: 10 Mounts: /u01/oracle/user_projects from my-oud-ds-rs-pv (rw) /var/run/secrets/kubernetes.io/serviceaccount from my-oud-ds-rs-token-c4tg4 (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: my-oud-ds-rs-pv: Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace) ClaimName: my-oud-ds-rs-pvc ReadOnly: false my-oud-ds-rs-token-c4tg4: Type: Secret (a volume populated by a Secret) SecretName: my-oud-ds-rs-token-c4tg4 Optional: false QoS Class: BestEffort Node-Selectors: \u0026lt;none\u0026gt; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300s Events: \u0026lt;none\u0026gt; "
},
{
	"uri": "/fmw-kubernetes/20.4.1/oam/manage-oam-domains/",
	"title": "Manage OAM Domains",
	"tags": [],
	"description": "This document provides steps to manage the OAM domain.",
	"content": "Important considerations for Oracle Access Management domains in Kubernetes.\n Domain Life Cycle  Learn about the domain life cyle of an OAM domain.\n WLST Administration Operations  Describes the steps for WLST administration using helper pod running in the same Kubernetes Cluster as OAM Domain.\n Logging and Visualization  Describes the steps for logging and visualization with Elasticsearch and Kibana.\n Monitoring an OAM domain  Describes the steps for Monitoring the OAM domain.\n Delete the OAM domain home  Learn about the steps to cleanup the OAM domain home.\n "
},
{
	"uri": "/fmw-kubernetes/20.4.1/oig/patch-and-upgrade/",
	"title": "Patch and Upgrade",
	"tags": [],
	"description": "This document provides steps to patch or upgrade an OIG image, Oracle WebLogic Kubernetes Operator or Kubernetes Cluster.",
	"content": "Patch an existing Oracle OIG image, upgrade the Oracle WebLogic Kubernetes Operator release, or upgrade the underlying Kubernetes cluster to a new release.\n a. Patch an image  Instructions on how to update your OIG Kubernetes cluster with a new OIG docker image.\n b. Upgrade an operator release  Instructions on how to update the Oracle WebLogic Kubernetes Operator version.\n c. Upgrade a Kubernetes cluster  Instructions on how to upgrade a Kubernetes cluster.\n "
},
{
	"uri": "/fmw-kubernetes/20.4.1/oam/patch-and-upgrade/",
	"title": "Patch and Upgrade",
	"tags": [],
	"description": "This document provides steps to patch or upgrade an OAM image, Oracle WebLogic Server Kubernetes Operator or Kubernetes Cluster.",
	"content": "Patch an existing OAM image, upgrade the Oracle WebLogic Server Kubernetes Operator release, or upgrade the underlying Kubernetes cluster to a new release.\n a. Patch an image  Instructions on how to update your OAM Kubernetes cluster with a new OAM Docker image.\n b. Upgrade an operator release  Instructions on how to update the Oracle WebLogic Server Kubernetes Operator version.\n c. Upgrade a Kubernetes cluster  Instructions on how to upgrade a Kubernetes cluster.\n "
},
{
	"uri": "/fmw-kubernetes/20.4.1/oig/troubleshooting/",
	"title": "Troubleshooting",
	"tags": [],
	"description": "Sample for creating an OIG domain home on an existing PV or PVC, and the domain resource YAML file for deploying the generated OIG domain.",
	"content": "Domain creation failure If the OIG domain creation fails when running create-domain.sh, run the following to diagnose the issue:\n  Run the following command to diagnose the create domain job:\n$ kubectl logs \u0026lt;job_name\u0026gt; -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl logs oimcluster-create-fmw-infra-sample-domain-job-9wqzb -n oimcluster Also run:\n$ kubectl describe pod \u0026lt;job_domain\u0026gt; -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl describe pod oimcluster-create-fmw-infra-sample-domain-job-9wqzb -n oimcluster Using the output you should be able to diagnose the problem and resolve the issue.\nClean down the failed domain creation by following steps 1-4 in Delete the OIG domain home. Then recreate the PC and PVC then execute the OIG domain creation steps again.\n  If any of the above commands return the following error:\nFailed to start container \u0026#34;create-fmw-infra-sample-domain-job\u0026#34;: Error response from daemon: error while creating mount source path \u0026#39;/scratch/OIGDockerK8S/oimclusterdomainpv \u0026#39;: mkdir /scratch/OIGDockerK8S/oimclusterdomainpv : permission denied then there is a permissions error on the directory for the PV and PVC and the following should be checked:\na) The directory has 777 permissions: chmod -R 777 \u0026lt;work directory\u0026gt;/oimclusterdomainpv.\nb) If it does have the permissions, check if an oracle user exists and the uid and gid equal 1000, for example:\n$ uid=1000(oracle) gid=1000(spg) groups=1000(spg),59968(oinstall),8500(dba),100(users),1007(cgbudba) Create the oracle user if it doesn\u0026rsquo;t exist and set the uid and gid to 1000.\nc) Edit the \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-oim-domain-pv-pvc/create-pv-pvc-inputs.yaml and add a slash to the end of the directory for the weblogicDomainStoragePath parameter:\nweblogicDomainStoragePath: /scratch/OIGDockerK8S/oimclusterdomainpv/ Clean down the failed domain creation by following steps 1-4 in Delete the OIG domain home. Then recreate the PC and PVC and then execute the OIG domain creation steps again.\n  "
},
{
	"uri": "/fmw-kubernetes/20.4.1/oam/security-hardening/",
	"title": "Security Hardening",
	"tags": [],
	"description": "Review resources for the Docker and Kubernetes cluster hardening.",
	"content": "Securing a Kubernetes cluster involves hardening on multiple fronts - securing the API servers, etcd, nodes, container images, container run-time, and the cluster network. Apply principles of defense in depth, principle of least privilege, and minimize the attack surface. Use security tools such as Kube-Bench to verify the cluster’s security posture. Since Kubernetes is evolving rapidly, refer to Kubernetes Security Overview for the latest information on securing a Kubernetes cluster. Also ensure the deployed Docker containers follow the Docker Security guidance.\nThis section provides references on how to securely configure Docker and Kubernetes.\nReferences   Docker hardening:\n https://docs.docker.com/engine/security/security/ https://blog.aquasec.com/docker-security-best-practices    Kubernetes hardening:\n https://kubernetes.io/docs/concepts/security/overview/ https://kubernetes.io/docs/concepts/security/pod-security-standards/ https://blogs.oracle.com/developers/5-best-practices-for-kubernetes-security    "
},
{
	"uri": "/fmw-kubernetes/20.4.1/oig/security-hardening/",
	"title": "Security Hardening",
	"tags": [],
	"description": "Review resources for the Docker and Kubernetes cluster hardening.",
	"content": "Securing a Kubernetes cluster involves hardening on multiple fronts - securing the API servers, etcd, nodes, container images, container run-time, and the cluster network. Apply principles of defense in depth, principle of least privilege, and minimize the attack surface. Use security tools such as Kube-Bench to verify the cluster’s security posture. Since Kubernetes is evolving rapidly, refer to Kubernetes Security Overview for the latest information on securing a Kubernetes cluster. Also ensure the deployed Docker containers follow the Docker Security guidance.\nThis section provides references on how to securely configure Docker and Kubernetes.\nReferences   Docker hardening:\n https://docs.docker.com/engine/security/security/ https://blog.aquasec.com/docker-security-best-practices    Kubernetes hardening:\n https://kubernetes.io/docs/concepts/security/overview/ https://kubernetes.io/docs/concepts/security/pod-security-standards/ https://blogs.oracle.com/developers/5-best-practices-for-kubernetes-security    "
},
{
	"uri": "/fmw-kubernetes/20.4.1/oam/troubleshooting/",
	"title": "Troubleshooting",
	"tags": [],
	"description": "How to Troubleshoot domain creation failure.",
	"content": "Domain creation failure If the OAM domain creation fails when running create-domain.sh, run the following to diagnose the issue:\n  Run the following command to diagnose the create domain job:\n$ kubectl logs \u0026lt;domain_job\u0026gt; -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl logs accessinfra-create-fmw-infra-sample-domain-job-c6vfb -n accessns Also run:\n$ kubectl describe pod \u0026lt;domain_job\u0026gt; -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl describe pod accessinfra-create-fmw-infra-sample-domain-job-c6vfb -n accessns Using the output you should be able to diagnose the problem and resolve the issue.\nClean down the failed domain creation by following steps 1-4 in Delete the OAM domain home. Then recreate the PV and PVC then execute the OAM domain creation steps again.\n  If any of the above commands return the following error:\nFailed to start container \u0026#34;create-fmw-infra-sample-domain-job\u0026#34;: Error response from daemon: error while creating mount source path \u0026#39;/scratch/OAMDockerK8S/accessdomainpv \u0026#39;: mkdir /scratch/OAMDockerK8S/accessdomainpv : permission denied then there is a permissions error on the directory for the PV and PVC and the following should be checked:\na) The directory has 777 permissions: chmod -R 777 \u0026lt;work directory\u0026gt;/accessdomainpv.\nb) If it does have the permissions, check if an oracle user exists and the uid and gid equal 1000.\nCreate the oracle user if it doesn\u0026rsquo;t exist and set the uid and gid to 1000.\nc) Edit the \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-access-domain-pv-pvc/create-pv-pvc-inputs.yaml and add a slash to the end of the directory for the weblogicDomainStoragePath parameter:\nweblogicDomainStoragePath: /scratch/OAMDockerK8S/accessdomainpv/ Clean down the failed domain creation by following steps 1-4 in Delete the OAM domain home. Then recreate the PV and PVC and then execute the OAM domain creation steps again.\n  "
},
{
	"uri": "/fmw-kubernetes/20.4.1/oud/create-oud-instances-helm/oud-ds-rs/",
	"title": "Helm Chart: oud-ds-rs: For deployment of replicated Oracle Unified Directory (DS+RS) instances",
	"tags": [],
	"description": "This document provides details of the oud-ds-rs Helm chart.",
	"content": " Introduction Deploy oud-ds-rs Helm Chart Verify the Replication Ingress Controller Setup  Ingress with NGINX Ingress with Voyager   Access to Interfaces through Ingress Configuration Parameters  Introduction This Helm chart provides for the deployment of replicated Oracle Unified Directory (DS+RS) instances on Kubernetes.\nThis chart can be used to deploy an Oracle Unified Directory instance as a base, with configured sample entries, and multiple replicated Oracle Unified Directory instances/pods/services based on the specified replicaCount.\nBased on the configuration, this chart deploys the following objects in the specified namespace of a Kubernetes cluster.\n Service Account Secret Persistent Volume and Persistent Volume Claim Pod(s)/Container(s) for Oracle Unified Directory Instances Services for interfaces exposed through Oracle Unified Directory Instances Ingress configuration  Create Kubernetes Namespace Create a Kubernetes namespace to provide a scope for other objects such as pods and services that you create in the environment. To create your namespace issue the following command:\n$ kubectl create ns myhelmns namespace/myhelmns created Deploy oud-ds-rs Helm Chart Create or Deploy a group of replicated Oracle Unified Directory instances along with Kubernetes objects in a specified namespace using the oud-ds-rs Helm Chart.\nThe deployment can be initiated by running the following Helm command with reference to the oud-ds-rs Helm Chart, along with configuration parameters according to your environment. Before deploying the Helm chart, the namespace should be created. Objects to be created by the Helm chart will be created inside the specified namespace.\n$ helm install --namespace \u0026lt;namespace\u0026gt; \\ \u0026lt;Configuration Parameters\u0026gt; \\ \u0026lt;deployment/release name\u0026gt; \\ \u0026lt;Helm Chart Path/Name\u0026gt; Configuration Parameters (override values in chart) can be passed on with --set arguments on the command line and/or with -f / --values arguments when referring to files.\nNote: Example files in the sections below provide values which allow the user to override the default values provided by the Helm chart.\nExamples Example where configuration parameters are passed with --set argument: $ helm install --namespace myhelmns \\ --set oudConfig.rootUserPassword=Oracle123,persistence.filesystem.hostPath.path=/scratch/shared/oud_user_projects \\ my-oud-ds-rs oud-ds-rs  For more details about the helm command and parameters, please execute helm --help and helm install --help. In this example, it is assumed that the command is executed from the directory containing the \u0026lsquo;oud-ds-rs\u0026rsquo; helm chart directory (OracleUnifiedDirectory/kubernetes/helm/).  Example where configuration parameters are passed with --values argument: $ helm install --namespace myhelmns \\ --values oud-ds-rs-values-override.yaml \\ my-oud-ds-rs oud-ds-rs  For more details about the helm command and parameters, please execute helm --help and helm install --help. In this example, it is assumed that the command is executed from the directory containing the \u0026lsquo;oud-ds-rs\u0026rsquo; helm chart directory (OracleUnifiedDirectory/kubernetes/helm/). The --values argument passes a file path/name which overrides values in the chart.  oud-ds-rs-values-override.yaml\noudConfig: rootUserPassword: Oracle123 persistence: type: filesystem filesystem: hostPath: path: /scratch/shared/oud_user_projects Example to scale-up through Helm Chart based deployment: In this example, we are setting replicaCount value to 3. If initially, the replicaCount value was 2, we will observe a new Oracle Unified Directory pod with assosiated services brought up by Kubernetes. So overall, 4 pods will be running now.\nWe have two ways to achieve our goal:\n$ helm upgrade --namespace myhelmns \\ --set replicaCount=3 \\ my-oud-ds-rs oud-ds-rs OR\n$ helm upgrade --namespace myhelmns \\ --values oud-ds-rs-values-override.yaml \\ my-oud-ds-rs oud-ds-rs oud-ds-rs-values-override.yaml\nreplicaCount: 3  For more details about the helm command and parameters, please execute helm --help and helm install --help. In this example, it is assumed that the command is executed from the directory containing the \u0026lsquo;oud-ds-rs\u0026rsquo; helm chart directory (OracleUnifiedDirectory/kubernetes/helm/).  Example to apply new Oracle Unified Directory patch through Helm Chart based deployment: In this example, we will apply PSU2020July-20200730 patch on earlier running Oracle Unified Directory version. If we describe pod we will observe that the container is up with new version.\nWe have two ways to achieve our goal:\n$ helm upgrade --namespace myhelmns \\ --set image.repository=oracle/oud,image.tag=12.2.1.4.0-PSU2020July-20200730 \\ my-oud-ds-rs oud-ds-rs OR\n$ helm upgrade --namespace myhelmns \\ --values oud-ds-rs-values-override.yaml \\ my-oud-ds-rs oud-ds-rs  For more details about the helm command and parameters, please execute helm --help and helm install --help. In this example, it is assumed that the command is executed from the directory containing the \u0026lsquo;oud-ds-rs\u0026rsquo; helm chart directory (OracleUnifiedDirectory/kubernetes/helm/).  oud-ds-rs-values-override.yaml\nimage: repository: oracle/oud tag: 12.2.1.4.0-PSU2020July-20200730 Example for using NFS as PV Storage: $ helm install --namespace myhelmns \\ --values oud-ds-rs-values-override-nfs.yaml \\ my-oud-ds-rs oud-ds-rs  For more details about the helm command and parameters, please execute helm --help and helm install --help. In this example, it is assumed that the command is executed from the directory containing the \u0026lsquo;oud-ds-rs\u0026rsquo; helm chart directory (OracleUnifiedDirectory/kubernetes/helm/). The --values argument passes a file path/name which overrides values in the chart.  oud-ds-rs-values-override-nfs.yaml\noudConfig: rootUserPassword: Oracle123 persistence: type: networkstorage networkstorage: nfs: path: /scratch/shared/oud_user_projects server: \u0026lt;NFS IP address \u0026gt; Example for using PV type of your choice: $ helm install --namespace myhelmns \\ --values oud-ds-rs-values-override-pv-custom.yaml \\ my-oud-ds-rs oud-ds-rs  For more details about the helm command and parameters, please execute helm --help and helm install --help. In this example, it is assumed that the command is executed from the directory containing the \u0026lsquo;oud-ds-rs\u0026rsquo; helm chart directory (OracleUnifiedDirectory/kubernetes/helm/). The --values argument passes a file path/name which overrides values in the chart.  oud-ds-rs-values-override-pv-custom.yaml\noudConfig: rootUserPassword: Oracle123 persistence: type: custom custom: nfs: # Path of NFS Share location path: /scratch/shared/oud_user_projects # IP of NFS Server server: \u0026lt;NFS IP address \u0026gt;  Under custom:, the configuration of your choice can be specified. This configuration will be used \u0026lsquo;as-is\u0026rsquo; for the PersistentVolume object.  Check Deployment Output for the helm install/upgrade command Ouput similar to the following is observed following successful execution of helm install/upgrade command.\nNAME: my-oud-ds-rs LAST DEPLOYED: Tue Mar 31 01:40:05 2020 NAMESPACE: myhelmns STATUS: deployed REVISION: 1 TEST SUITE: None  Check for the status of objects created through oud-ds-rs helm chart Command:\n$ kubectl --namespace myhelmns get nodes,pod,service,secret,pv,pvc,ingress -o wide Output is similar to the following:\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/my-oud-ds-rs-0 1/1 Running 0 8m44s 10.244.0.195 \u0026lt;Worker Node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/my-oud-ds-rs-1 1/1 Running 0 8m44s 10.244.0.194 \u0026lt;Worker Node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/my-oud-ds-rs-2 0/1 Running 0 8m44s 10.244.0.193 \u0026lt;Worker Node\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service/my-oud-ds-rs-0 ClusterIP 10.99.232.83 \u0026lt;none\u0026gt; 1444/TCP,1888/TCP,1898/TCP 8m44s kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=my-oud-ds-rs-0 service/my-oud-ds-rs-1 ClusterIP 10.100.186.42 \u0026lt;none\u0026gt; 1444/TCP,1888/TCP,1898/TCP 8m45s app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=my-oud-ds-rs-1 service/my-oud-ds-rs-2 ClusterIP 10.104.55.53 \u0026lt;none\u0026gt; 1444/TCP,1888/TCP,1898/TCP 8m45s app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=my-oud-ds-rs-2 service/my-oud-ds-rs-http-0 ClusterIP 10.102.116.145 \u0026lt;none\u0026gt; 1080/TCP,1081/TCP 8m45s app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=my-oud-ds-rs-0 service/my-oud-ds-rs-http-1 ClusterIP 10.111.103.84 \u0026lt;none\u0026gt; 1080/TCP,1081/TCP 8m44s app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=my-oud-ds-rs-1 service/my-oud-ds-rs-http-2 ClusterIP 10.105.53.24 \u0026lt;none\u0026gt; 1080/TCP,1081/TCP 8m45s app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=my-oud-ds-rs-2 service/my-oud-ds-rs-lbr-admin ClusterIP 10.98.39.206 \u0026lt;none\u0026gt; 1888/TCP,1444/TCP 8m45s app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs service/my-oud-ds-rs-lbr-http ClusterIP 10.110.77.132 \u0026lt;none\u0026gt; 1080/TCP,1081/TCP 8m45s app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs service/my-oud-ds-rs-lbr-ldap ClusterIP 10.111.55.122 \u0026lt;none\u0026gt; 1389/TCP,1636/TCP 8m45s app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs service/my-oud-ds-rs-ldap-0 ClusterIP 10.108.155.81 \u0026lt;none\u0026gt; 1389/TCP,1636/TCP 8m44s app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=my-oud-ds-rs-0 service/my-oud-ds-rs-ldap-1 ClusterIP 10.104.88.44 \u0026lt;none\u0026gt; 1389/TCP,1636/TCP 8m45s app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=my-oud-ds-rs-1 service/my-oud-ds-rs-ldap-2 ClusterIP 10.105.253.120 \u0026lt;none\u0026gt; 1389/TCP,1636/TCP 8m45s app.kubernetes.io/instance=my-oud-ds-rs,app.kubernetes.io/name=oud-ds-rs,oud/instance=my-oud-ds-rs-2 NAME TYPE DATA AGE secret/default-token-tbjr5 kubernetes.io/service-account-token 3 25d secret/my-oud-ds-rs-creds opaque 8 8m48s secret/my-oud-ds-rs-token-cct26 kubernetes.io/service-account-token 3 8m50s secret/sh.helm.release.v1.my-oud-ds-rs.v1 helm.sh/release.v1 1 8m51s NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE persistentvolume/my-oud-ds-rs-pv 20Gi RWX Retain Bound myhelmns/my-oud-ds-rs-pvc manual 8m47s NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistentvolumeclaim/my-oud-ds-rs-pvc Bound my-oud-ds-rs-pv 20Gi RWX manual 8m48s NAME HOSTS ADDRESS PORTS AGE ingress.extensions/my-oud-ds-rs-admin-ingress-nginx my-oud-ds-rs-admin-0,my-oud-ds-rs-admin-1,my-oud-ds-rs-admin-2 + 2 more... 10.229.141.78 80 8m45s ingress.extensions/my-oud-ds-rs-http-ingress-nginx my-oud-ds-rs-http-0,my-oud-ds-rs-http-1,my-oud-ds-rs-http-2 + 3 more... 10.229.141.78 80 8m45s Kubernetes Objects Kubernetes objects created by the Helm chart are detailed in the table below:\n   Type Name Example Name Purpose     Service Account \u0026lt;deployment/release name\u0026gt; my-oud-ds-rs Kubernetes Service Account for the Helm Chart deployment   Secret \u0026lt;deployment/release name\u0026gt;-creds my-oud-ds-rs-creds Secret object for Oracle Unified Directory related critical values like passwords   Persistent Volume \u0026lt;deployment/release name\u0026gt;-pv my-oud-ds-rs-pv Persistent Volume for user_projects mount.   Persistent Volume Claim \u0026lt;deployment/release name\u0026gt;-pvc my-oud-ds-rs-pvc Persistent Volume Claim for user_projects mount.   Persistent Volume \u0026lt;deployment/release name\u0026gt;-pv-config my-oud-ds-rs-pv-config Persistent Volume for mounting volume in containers for configuration files like ldif, schema, jks, java.security, etc.   Persistent Volume Claim \u0026lt;deployment/release name\u0026gt;-pvc-config my-oud-ds-rs-pvc-config Persistent Volume Claim for mounting volume in containers for configuration files like ldif, schema, jks, java.security, etc.   Pod \u0026lt;deployment/release name\u0026gt;-0 my-oud-ds-rs-0 Pod/Container for base Oracle Unified Directory Instance which would be populated first with base configuration (like number of sample entries)   Pod \u0026lt;deployment/release name\u0026gt;-N my-oud-ds-rs-1, my-oud-ds-rs-2, \u0026hellip; Pod(s)/Container(s) for Oracle Unified Directory Instances - each would have replication enabled against base Oracle Unified Directory instance \u0026lt;deployment/release name\u0026gt;-0   Service \u0026lt;deployment/release name\u0026gt;-0 my-oud-ds-rs-0 Service for LDAPS Admin, REST Admin and Replication interfaces from base Oracle Unified Directory instance \u0026lt;deployment/release name\u0026gt;-0   Service \u0026lt;deployment/release name\u0026gt;-http-0 my-oud-ds-rs-http-0 Service for HTTP and HTTPS interfaces from base Oracle Unified Directory instance \u0026lt;deployment/release name\u0026gt;-0   Service \u0026lt;deployment/release name\u0026gt;-ldap-0 my-oud-ds-rs-ldap-0 Service for LDAP and LDAPS interfaces from base Oracle Unified Directory instance \u0026lt;deployment/release name\u0026gt;-0   Service \u0026lt;deployment/release name\u0026gt;-N my-oud-ds-rs-1, my-oud-ds-rs-2, \u0026hellip; Service(s) for LDAPS Admin, REST Admin and Replication interfaces from base Oracle Unified Directory instance \u0026lt;deployment/release name\u0026gt;-N   Service \u0026lt;deployment/release name\u0026gt;-http-N my-oud-ds-rs-http-1, my-oud-ds-rs-http-2, \u0026hellip; Service(s) for HTTP and HTTPS interfaces from base Oracle Unified Directory instance \u0026lt;deployment/release name\u0026gt;-N   Service \u0026lt;deployment/release name\u0026gt;-ldap-N my-oud-ds-rs-ldap-1, my-oud-ds-rs-ldap-2, \u0026hellip; Service(s) for LDAP and LDAPS interfaces from base Oracle Unified Directory instance \u0026lt;deployment/release name\u0026gt;-N   Service \u0026lt;deployment/release name\u0026gt;-lbr-admin my-oud-ds-rs-lbr-admin Service for LDAPS Admin, REST Admin and Replication interfaces from all Oracle Unified Directory instances   Service \u0026lt;deployment/release name\u0026gt;-lbr-http my-oud-ds-rs-lbr-http Service for HTTP and HTTPS interfaces from all Oracle Unified Directory instances   Service \u0026lt;deployment/release name\u0026gt;-lbr-ldap my-oud-ds-rs-lbr-ldap Service for LDAP and LDAPS interfaces from all Oracle Unified Directory instances   Ingress \u0026lt;deployment/release name\u0026gt;-admin-ingress-nginx my-oud-ds-rs-admin-ingress-nginx Ingress Rules for HTTP Admin interfaces.   Ingress \u0026lt;deployment/release name\u0026gt;-http-ingress-nginx my-oud-ds-rs-http-ingress-nginx Ingress Rules for HTTP (Data/REST) interfaces.     In the table above the \u0026lsquo;Example Name\u0026rsquo; for each Object is based on the value \u0026lsquo;my-oud-ds-rs\u0026rsquo; as deployment/release name for the Helm chart installation.  Verify the Replication Once all the PODs created are visible as READY (i.e. 1/1), you can verify your replication across multiple Oracle Unified Directory instances.\nTo verify the replication group, connect to the container and issue an Oracle Unified Directory Administration command to show details. You can get the name of the container by issuing the following:\n$ kubectl get pods -n \u0026lt;namespace\u0026gt; -o jsonpath='{.items[*].spec.containers[*].name}' For example:\n$ kubectl get pods -n myhelmns -o jsonpath='{.items[*].spec.containers[*].name}' oud-ds-rs With the container name you can then connect to the container:\n$ kubectl --namespace \u0026lt;namespace\u0026gt; exec -it -c \u0026lt;containername\u0026gt; \u0026lt;podname\u0026gt; /bin/bash For example:\n$ kubectl --namespace myhelmns exec -it -c oud-ds-rs my-oud-ds-rs-0 /bin/bash From the prompt, use the dsreplication command to check the status of your replication group:\n$ cd /u01/oracle/user_projects/my-oud-ds-rs-0/OUD/bin $ ./dsreplication status --trustAll \\ --hostname my-oud-ds-rs-0 --port 1444 --adminUID admin \\ --dataToDisplay compat-view --dataToDisplay rs-connections Output will be similar to the following (enter credentials where prompted):\n\u0026gt;\u0026gt;\u0026gt;\u0026gt; Specify Oracle Unified Directory LDAP connection parameters Password for user 'admin': Establishing connections and reading configuration ..... Done. dc=example,dc=com - Replication Enabled ======================================= Server : Entries : M.C. [1] : A.O.M.C. [2] : Port [3] : Encryption [4] : Trust [5] : U.C. [6] : Status [7] : ChangeLog [8] : Group ID [9] : Connected To [10] ---------------------:---------:----------:--------------:----------:----------------:-----------:----------:------------:---------------:--------------:------------------------------- my-oud-ds-rs-0:1444 : 1 : 0 : 0 : 1898 : Disabled : Trusted : -- : Normal : Enabled : 1 : my-oud-ds-rs-0:1898 : : : : : : : : : : : (GID=1) my-oud-ds-rs-1:1444 : 1 : 0 : 0 : 1898 : Disabled : Trusted : -- : Normal : Enabled : 1 : my-oud-ds-rs-1:1898 : : : : : : : : : : : (GID=1) my-oud-ds-rs-2:1444 : 1 : 0 : 0 : 1898 : Disabled : Trusted : -- : Normal : Enabled : 1 : my-oud-ds-rs-2:1898 : : : : : : : : : : : (GID=1) Replication Server [11] : RS #1 : RS #2 : RS #3 -------------------------------:-------:-------:------ my-oud-ds-rs-0:1898 : -- : Yes : Yes (#1) : : : my-oud-ds-rs-1:1898 : Yes : -- : Yes (#2) : : : my-oud-ds-rs-2:1898 : Yes : Yes : -- (#3) : : : [1] The number of changes that are still missing on this element (and that have been applied to at least one other server). [2] Age of oldest missing change: the age (in seconds) of the oldest change that has not yet arrived on this element. [3] The replication port used to communicate between the servers whose contents are being replicated. [4] Whether the replication communication initiated by this element is encrypted or not. [5] Whether the directory server is trusted or not. Updates coming from an untrusted server are discarded and not propagated. [6] The number of untrusted changes. These are changes generated on this server while it is untrusted. Those changes are not propagated to the rest of the topology but are effective on the untrusted server. [7] The status of the replication on this element. [8] Whether the external change log is enabled for the base DN on this server or not. [9] The ID of the replication group to which the server belongs. [10] The replication server this server is connected to with its group ID between brackets. [11] This table represents the connections between the replication servers. The headers of the columns use a number as identifier for each replication server. See the values of the first column to identify the corresponding replication server for each number. The dsreplication status command can be additionally invoked using the following syntax:\n$ kubectl --namespace \u0026lt;namespace\u0026gt; exec -it -c \u0026lt;containername\u0026gt; \u0026lt;podname\u0026gt; -- \\ /u01/oracle/user_projects/\u0026lt;OUD Instance/Pod Name\u0026gt;/OUD/bin/dsreplication status \\ --trustAll --hostname \u0026lt;OUD Instance/Pod Name\u0026gt; --port 1444 --adminUID admin \\ --dataToDisplay compat-view --dataToDisplay rs-connections For example:\n$ kubectl --namespace myhelmns exec -it -c oud-ds-rs my-oud-ds-rs-0 -- \\ /u01/oracle/user_projects/my-oud-ds-rs-0/OUD/bin/dsreplication status \\ --trustAll --hostname my-oud-ds-rs-0 --port 1444 --adminUID admin \\ --dataToDisplay compat-view --dataToDisplay rs-connections Ingress Controller Setup There are two types of Ingress controllers supported by this Helm chart. In the sub-sections below, configuration steps for each Controller are described.\nBy default Ingress configuration only supports HTTP and HTTPS Ports/Communication. To allow LDAP and LDAPS communication over TCP, configuration is required at Ingress Controller/Implementation level.\nIngress with NGINX Nginx-ingress controller implementation can be deployed/installed in a Kubernetes environment.\nCreate a Kubernetes Namespace Create a Kubernetes namespace to provide a scope for NGINX objects such as pods and services that you create in the environment. To create your namespace issue the following command:\n$ kubectl create ns mynginx namespace/mynginx created Add Repo reference to Helm for retrieving/installing Chart for nginx-ingress implementation. $ helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx Confirm the charts available by issuing the following command:\n$ helm search repo | grep nginx ingress-nginx/ingress-nginx 3.4.1 0.40.2 Ingress controller for Kubernetes using NGINX a... stable/ingress-nginx 3.4.1 0.40.2 Ingress controller for Kubernetes using NGINX a... Command helm install to install nginx-ingress related objects like pod, service, deployment, etc. To install and configure NGINX Ingress issue the following command:\n$ helm install --namespace mynginx \\ --values nginx-ingress-values-override.yaml \\ lbr-nginx ingress-nginx/ingress-nginx Where:\n lbr-nginx is your deployment name ingress-nginx/ingress-nginx is the chart reference  Output will be similar to the following:\n$ helm install --namespace mynginx --values samples/nginx-ingress-values-override.yaml lbr-nginx ingress-nginx/ingress-nginx NAME: lbr-nginx LAST DEPLOYED: Wed Oct 7 08:07:29 2020 NAMESPACE: mynginx STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: The ingress-nginx controller has been installed. It may take a few minutes for the LoadBalancer IP to be available. You can watch the status by running 'kubectl --namespace mynginx get services -o wide -w lbr-nginx-ingress-nginx-controller' An example Ingress that makes use of the controller: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx name: example namespace: foo spec: rules: - host: www.example.com http: paths: - backend: serviceName: exampleService servicePort: 80 path: / # This section is only required if TLS is to be enabled for the Ingress tls: - hosts: - www.example.com secretName: example-tls If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided: apiVersion: v1 kind: Secret metadata: name: example-tls namespace: foo data: tls.crt: \u0026lt;base64 encoded cert\u0026gt; tls.key: \u0026lt;base64 encoded key\u0026gt; type: kubernetes.io/tls  For more details about the helm command and parameters, please execute helm --help and helm install --help. The --values argument passes a file path/name which overrides values in the chart.  nginx-ingress-values-override.yaml\n# Configuration for additional TCP ports to be exposed through Ingress # Format for each port would be like: # \u0026lt;PortNumber\u0026gt;: \u0026lt;Namespace\u0026gt;/\u0026lt;Service\u0026gt; tcp: # Map 1389 TCP port to LBR LDAP service to get requests handled through any available POD/Endpoint serving LDAP Port 1389: myhelmns/my-oud-ds-rs-lbr-ldap:ldap # Map 1636 TCP port to LBR LDAP service to get requests handled through any available POD/Endpoint serving LDAPS Port 1636: myhelmns/my-oud-ds-rs-lbr-ldap:ldaps controller: admissionWebhooks: enabled: false extraArgs: # The secret referred to by this flag contains the default certificate to be used when accessing the catch-all server. # If this flag is not provided NGINX will use a self-signed certificate. # If the TLS Secret is in different namespace, name can be mentioned as \u0026lt;namespace\u0026gt;/\u0026lt;tlsSecretName\u0026gt; default-ssl-certificate: myhelmns/my-oud-ds-rs-tls-cert service: # controller service external IP addresses # externalIPs: # - \u0026lt; External IP Address \u0026gt; # To configure Ingress Controller Service as LoadBalancer type of Service # Based on the Kubernetes configuration, External LoadBalancer would be linked to the Ingress Controller Service type: LoadBalancer # Configuration for NodePort to be used for Ports exposed through Ingress # If NodePorts are not defied/configured, Node Port would be assigend automatically by Kubernetes # These NodePorts are helpful while accessing services directly through Ingress and without having External Load Balancer. nodePorts: # For HTTP Interface exposed through LoadBalancer/Ingress http: 30080 # For HTTPS Interface exposed through LoadBalancer/Ingress https: 30443 tcp: # For LDAP Interface 1389: 31389 # For LDAPS Interface 1636: 31636  The configuration above assumes that you have oud-ds-rs installed with value my-oud-ds-rs as a deployment/release name. Based on the deployment/release name in your environment, TCP port mapping may be required to be changed/updated.  Optional: Command helm upgrade to update nginx-ingress related objects like pod, service, deployment, etc. If required, an nginx-ingress deployment can be updated/upgraded with following command. In this example, nginx-ingress configuration is updated with an additional TCP port and Node Port for accessing the LDAP/LDAPS port of a specific POD.\n$ helm upgrade --namespace mynginx \\ --values nginx-ingress-values-override.yaml \\ lbr-nginx ingress-nginx/nginx-ingress  For more details about the helm command and parameters, please execute helm --help and helm install --help. The --values argument passes a file path/name which overrides values in the chart.  nginx-ingress-values-override.yaml\n# Configuration for additional TCP ports to be exposed through Ingress # Format for each port would be like: # \u0026lt;PortNumber\u0026gt;: \u0026lt;Namespace\u0026gt;/\u0026lt;Service\u0026gt; tcp: # Map 1389 TCP port to LBR LDAP service to get requests handled through any available POD/Endpoint serving LDAP Port 1389: myhelmns/my-oud-ds-rs-lbr-ldap:ldap # Map 1636 TCP port to LBR LDAP service to get requests handled through any available POD/Endpoint serving LDAPS Port 1636: myhelmns/my-oud-ds-rs-lbr-ldap:ldaps # Map specific ports for LDAP and LDAPS communication from individual Services/Pods # To redirect requests on 3890 port to myhelmns/my-oud-ds-rs-ldap-0:ldap 3890: myhelmns/my-oud-ds-rs-ldap-0:ldap # To redirect requests on 6360 port to myhelmns/my-oud-ds-rs-ldaps-0:ldap 6360: myhelmns/my-oud-ds-rs-ldap-0:ldaps # To redirect requests on 3891 port to myhelmns/my-oud-ds-rs-ldap-1:ldap 3891: myhelmns/my-oud-ds-rs-ldap-1:ldap # To redirect requests on 6361 port to myhelmns/my-oud-ds-rs-ldaps-1:ldap 6361: myhelmns/my-oud-ds-rs-ldap-1:ldaps # To redirect requests on 3892 port to myhelmns/my-oud-ds-rs-ldap-2:ldap 3892: myhelmns/my-oud-ds-rs-ldap-2:ldap # To redirect requests on 6362 port to myhelmns/my-oud-ds-rs-ldaps-2:ldap 6362: myhelmns/my-oud-ds-rs-ldap-2:ldaps # Map 1444 TCP port to LBR Admin service to get requests handled through any available POD/Endpoint serving Admin LDAPS Port 1444: myhelmns/my-oud-ds-rs-lbr-admin:adminldaps # To redirect requests on 4440 port to myhelmns/my-oud-ds-rs-0:adminldaps 4440: myhelmns/my-oud-ds-rs-0:adminldaps # To redirect requests on 4441 port to myhelmns/my-oud-ds-rs-1:adminldaps 4441: myhelmns/my-oud-ds-rs-1:adminldaps # To redirect requests on 4442 port to myhelmns/my-oud-ds-rs-2:adminldaps 4442: myhelmns/my-oud-ds-rs-2:adminldaps controller: admissionWebhooks: enabled: false extraArgs: # The secret referred to by this flag contains the default certificate to be used when accessing the catch-all server. # If this flag is not provided NGINX will use a self-signed certificate. # If the TLS Secret is in different namespace, name can be mentioned as \u0026lt;namespace\u0026gt;/\u0026lt;tlsSecretName\u0026gt; default-ssl-certificate: myhelmns/my-oud-ds-rs-tls-cert service: # controller service external IP addresses # externalIPs: # - \u0026lt; External IP Address \u0026gt; # To configure Ingress Controller Service as LoadBalancer type of Service # Based on the Kubernetes configuration, External LoadBalancer would be linked to the Ingress Controller Service type: LoadBalancer # Configuration for NodePort to be used for Ports exposed through Ingress # If NodePorts are not defied/configured, Node Port would be assigend automatically by Kubernetes # These NodePorts are helpful while accessing services directly through Ingress and without having External Load Balancer. nodePorts: # For HTTP Interface exposed through LoadBalancer/Ingress http: 30080 # For HTTPS Interface exposed through LoadBalancer/Ingress https: 30443 tcp: # For LDAP Interface referring to LBR LDAP services serving LDAP port 1389: 31389 # For LDAPS Interface referring to LBR LDAP services serving LDAPS port 1636: 31636 # For LDAP Interface from specific service oud-ds-rs-ldap-0 3890: 30890 # For LDAPS Interface from specific service oud-ds-rs-ldap-0 6360: 30360 # For LDAP Interface from specific service oud-ds-rs-ldap-1 3891: 30891 # For LDAPS Interface from specific service oud-ds-rs-ldap-1 6361: 30361 # For LDAP Interface from specific service oud-ds-rs-ldap-2 3892: 30892 # For LDAPS Interface from specific service oud-ds-rs-ldap-2 6362: 30362 # For LDAPS Interface referring to LBR Admin services serving adminldaps port 1444: 31444 # For Admin LDAPS Interface from specific service oud-ds-rs-0 4440: 30440 # For Admin LDAPS Interface from specific service oud-ds-rs-1 4441: 30441 # For Admin LDAPS Interface from specific service oud-ds-rs-2 4442: 30442  The configuration above assumes that you have oud-ds-rs installed with value my-oud-ds-rs as a deployment/release name. Based on the deployment/release name in your environment, TCP port mapping may be required to be changed/updated.  Ingress with Voyager Voyager ingress implementation can be deployed/installed in a Kubernetes environment.\nAdd Repo reference to helm for retriving/installing Chart for Voyager implementation. $ helm repo add appscode https://charts.appscode.com/stable Command helm install to install voyager related objects like pod, service, deployment, etc. $ helm install --namespace mynginx \\ --set cloudProvider=baremetal \\ voyager-operator appscode/voyager  For more details about the helm command and parameters, please execute helm --help and helm install --help.  Access to Interfaces through Ingress Using the Helm chart, Ingress objects are also created according to configuration. The following table details the rules configured in Ingress object(s) for access to Oracle Unified Directory Interfaces through Ingress.\n   Port NodePort Host Example Hostname Path Backend Service:Port Example Service Name:Port     http/https 30080/30443 \u0026lt;deployment/release name\u0026gt;-admin-0 my-oud-ds-rs-admin-0 * \u0026lt;deployment/release name\u0026gt;-0:adminhttps my-oud-ds-rs-0:adminhttps   http/https 30080/30443 \u0026lt;deployment/release name\u0026gt;-admin-N my-oud-ds-rs-admin-N * \u0026lt;deployment/release name\u0026gt;-N:adminhttps my-oud-ds-rs-1:adminhttps   http/https 30080/30443 \u0026lt;deployment/release name\u0026gt;-admin my-oud-ds-rs-admin * \u0026lt;deployment/release name\u0026gt;-lbr-admin:adminhttps my-oud-ds-rs-lbr-admin:adminhttps   http/https 30080/30443 * * /rest/v1/admin \u0026lt;deployment/release name\u0026gt;-lbr-admin:adminhttps my-oud-ds-rs-lbr-admin:adminhttps   http/https 30080/30443 \u0026lt;deployment/release name\u0026gt;-http-0 my-oud-ds-rs-http-0 * \u0026lt;deployment/release name\u0026gt;-http-0:http my-oud-ds-rs-http-0:http   http/https 30080/30443 \u0026lt;deployment/release name\u0026gt;-http-N my-oud-ds-rs-http-N * \u0026lt;deployment/release name\u0026gt;-http-N:http my-oud-ds-rs-http-N:http   http/https 30080/30443 \u0026lt;deployment/release name\u0026gt;-http my-oud-ds-rs-http * \u0026lt;deployment/release name\u0026gt;-lbr-http:http my-oud-ds-rs-lbr-http:http   http/https 30080/30443 * * /rest/v1/directory \u0026lt;deployment/release name\u0026gt;-lbr-http:http my-oud-ds-rs-lbr-http:http   http/https 30080/30443 * * /iam/directory \u0026lt;deployment/release name\u0026gt;-lbr-http:http my-oud-ds-rs-lbr-http:http     In the table above, example values are based on the value \u0026lsquo;my-oud-ds-rs\u0026rsquo; as the deployment/release name for Helm chart installation.The NodePorts mentioned in the table are according to Ingress configuration described in previous section.When External LoadBalancer is not available/configured, Interfaces can be accessed through NodePort on a Kubernetes Node.\n For LDAP/LDAPS access (based on the updated/upgraded configuration mentioned in previous section)\n   Port NodePort Backend Service:Port Example Service Name:Port     1389 31389 \u0026lt;deployment/release name\u0026gt;-lbr-ldap:ldap my-oud-ds-rs-lbr-ldap:ldap   1636 31636 \u0026lt;deployment/release name\u0026gt;-lbr-ldap:ldap my-oud-ds-rs-lbr-ldap:ldaps   1444 31444 \u0026lt;deployment/release name\u0026gt;-lbr-admin:adminldaps my-oud-ds-rs-lbr-admin:adminldaps   3890 30890 \u0026lt;deployment/release name\u0026gt;-ldap-0:ldap my-oud-ds-rs-ldap-0:ldap   6360 30360 \u0026lt;deployment/release name\u0026gt;-ldap-0:ldaps my-oud-ds-rs-ldap-0:ldaps   3891 30891 \u0026lt;deployment/release name\u0026gt;-ldap-1:ldap my-oud-ds-rs-ldap-1:ldap   6361 30361 \u0026lt;deployment/release name\u0026gt;-ldap-1:ldaps my-oud-ds-rs-ldap-1:ldaps   3892 30892 \u0026lt;deployment/release name\u0026gt;-ldap-2:ldap my-oud-ds-rs-ldap-2:ldap   6362 30362 \u0026lt;deployment/release name\u0026gt;-ldap-2:ldaps my-oud-ds-rs-ldap-2:ldaps   4440 30440 \u0026lt;deployment/release name\u0026gt;-0:adminldaps my-oud-ds-rs-ldap-0:adminldaps   4441 30441 \u0026lt;deployment/release name\u0026gt;-1:adminldaps my-oud-ds-rs-ldap-1:adminldaps   4442 30442 \u0026lt;deployment/release name\u0026gt;-2:adminldaps my-oud-ds-rs-ldap-2:adminldaps     In the table above, example values are based on value \u0026lsquo;my-oud-ds-rs\u0026rsquo; as the deployment/release name for helm chart installation. The NodePorts mentioned in the table are according to Ingress configuration described in previous section. When external LoadBalancer is not available/configured, Interfaces can be accessed through NodePort on a Kubernetes Node.  Changes in /etc/hosts to validate hostname based Ingress rules If it is not possible to have a LoadBalancer configuration updated to have host names added for Oracle Unified Directory Interfaces then the following entries can be added in /etc/hosts files on host from where Oracle Unified Directory interfaces will be accessed.\n\u0026lt;IP Address of External LBR or Kubernetes Node\u0026gt;\tmy-oud-ds-rs-http my-oud-ds-rs-http-0 my-oud-ds-rs-http-1 my-oud-ds-rs-http-2 my-oud-ds-rs-http-N \u0026lt;IP Address of External LBR or Kubernetes Node\u0026gt;\tmy-oud-ds-rs-admin my-oud-ds-rs-admin-0 my-oud-ds-rs-admin-1 my-oud-ds-rs-admin-2 my-oud-ds-rs-admin-N  In the table above, host names are based on the value \u0026lsquo;my-oud-ds-rs\u0026rsquo; as the deployment/release name for Helm chart installation. When External LoadBalancer is not available/configured, Interfaces can be accessed through NodePort on Kubernetes Node.  Validate access HTTPS/REST API against External LBR Host:Port Note: For commands mentioned in this section you need to have an external IP assigned at Ingress level.\na) Command to invoke Data REST API:\n$curl --noproxy \u0026quot;*\u0026quot; --location \\ --request GET 'https://\u0026lt;External LBR Host\u0026gt;/rest/v1/directory/uid=user.1,ou=People,dc=example,dc=com?scope=sub\u0026amp;attributes=*' \\ --header 'Authorization: Basic \u0026lt;Base64 of userDN:userPassword\u0026gt;' | json_pp  | json_pp is used to format output in readable json format on the client side. It can be ignored if you do not have the json_pp library. Base64 of userDN:userPassword can be generated using echo -n \u0026quot;userDN:userPassword\u0026quot; | base64  Output:\n{ \u0026#34;msgType\u0026#34; : \u0026#34;urn:ietf:params:rest:schemas:oracle:oud:1.0:SearchResponse\u0026#34;, \u0026#34;totalResults\u0026#34; : 1, \u0026#34;searchResultEntries\u0026#34; : [ { \u0026#34;dn\u0026#34; : \u0026#34;uid=user.1,ou=People,dc=example,dc=com\u0026#34;, \u0026#34;attributes\u0026#34; : { \u0026#34;st\u0026#34; : \u0026#34;OH\u0026#34;, \u0026#34;employeeNumber\u0026#34; : \u0026#34;1\u0026#34;, \u0026#34;postalCode\u0026#34; : \u0026#34;93694\u0026#34;, \u0026#34;description\u0026#34; : \u0026#34;This is the description for Aaren Atp.\u0026#34;, \u0026#34;telephoneNumber\u0026#34; : \u0026#34;+1 390 103 6917\u0026#34;, \u0026#34;homePhone\u0026#34; : \u0026#34;+1 280 375 4325\u0026#34;, \u0026#34;initials\u0026#34; : \u0026#34;ALA\u0026#34;, \u0026#34;objectClass\u0026#34; : [ \u0026#34;top\u0026#34;, \u0026#34;inetorgperson\u0026#34;, \u0026#34;organizationalperson\u0026#34;, \u0026#34;person\u0026#34; ], \u0026#34;uid\u0026#34; : \u0026#34;user.1\u0026#34;, \u0026#34;sn\u0026#34; : \u0026#34;Atp\u0026#34;, \u0026#34;street\u0026#34; : \u0026#34;70110 Fourth Street\u0026#34;, \u0026#34;mobile\u0026#34; : \u0026#34;+1 680 734 6300\u0026#34;, \u0026#34;givenName\u0026#34; : \u0026#34;Aaren\u0026#34;, \u0026#34;mail\u0026#34; : \u0026#34;user.1@maildomain.net\u0026#34;, \u0026#34;l\u0026#34; : \u0026#34;New Haven\u0026#34;, \u0026#34;postalAddress\u0026#34; : \u0026#34;Aaren Atp$70110 Fourth Street$New Haven, OH 93694\u0026#34;, \u0026#34;pager\u0026#34; : \u0026#34;+1 850 883 8888\u0026#34;, \u0026#34;cn\u0026#34; : \u0026#34;Aaren Atp\u0026#34; } } ] } b) Command to invoke Data REST API against specific Oracle Unified Directory Interface:\n$ curl --noproxy \u0026quot;*\u0026quot; --location \\ --request GET 'https://my-oud-ds-rs-http-0/rest/v1/directory/uid=user.1,ou=People,dc=example,dc=com?scope=sub\u0026amp;attributes=*' \\ --header 'Authorization: Basic \u0026lt;Base64 of userDN:userPassword\u0026gt;' | json_pp  | json_pp is used to format output in readable json format on the client side. It can be ignored if you do not have the json_pp library. Base64 of userDN:userPassword can be generated using echo -n \u0026quot;userDN:userPassword\u0026quot; | base64. For this example, it is assumed that the value \u0026lsquo;my-oud-ds-rs\u0026rsquo; is used as the deployment/release name for helm chart installation. It is assumed that \u0026lsquo;my-oud-ds-rs-http-0\u0026rsquo; points to an External LoadBalancer  HTTPS/REST API against Kubernetes NodePort for Ingress Controller Service a) Command to invoke Data SCIM API:\n$ curl --noproxy \u0026quot;*\u0026quot; --location \\ --request GET 'https://\u0026lt;Kubernetes Node\u0026gt;:30443/iam/directory/oud/scim/v1/Users' \\ --header 'Authorization: Basic \u0026lt;Base64 of userDN:userPassword\u0026gt;' | json_pp  | json_pp is used to format output in readable json format on the client side. It can be ignored if you do not have the json_pp library. Base64 of userDN:userPassword can be generated using echo -n \u0026quot;userDN:userPassword\u0026quot; | base64.  Output:\n{ \u0026#34;Resources\u0026#34; : [ { \u0026#34;id\u0026#34; : \u0026#34;ad55a34a-763f-358f-93f9-da86f9ecd9e4\u0026#34;, \u0026#34;userName\u0026#34; : [ { \u0026#34;value\u0026#34; : \u0026#34;user.0\u0026#34; } ], \u0026#34;schemas\u0026#34; : [ \u0026#34;urn:ietf:params:scim:schemas:core:2.0:User\u0026#34;, \u0026#34;urn:ietf:params:scim:schemas:extension:oracle:2.0:OUD:User\u0026#34;, \u0026#34;urn:ietf:params:scim:schemas:extension:enterprise:2.0:User\u0026#34; ], \u0026#34;meta\u0026#34; : { \u0026#34;location\u0026#34; : \u0026#34;http://idm-oke-lbr/iam/directory/oud/scim/v1/Users/ad55a34a-763f-358f-93f9-da86f9ecd9e4\u0026#34;, \u0026#34;resourceType\u0026#34; : \u0026#34;User\u0026#34; }, \u0026#34;addresses\u0026#34; : [ { \u0026#34;postalCode\u0026#34; : \u0026#34;50369\u0026#34;, \u0026#34;formatted\u0026#34; : \u0026#34;Aaccf Amar$01251 Chestnut Street$Panama City, DE 50369\u0026#34;, \u0026#34;streetAddress\u0026#34; : \u0026#34;01251 Chestnut Street\u0026#34;, \u0026#34;locality\u0026#34; : \u0026#34;Panama City\u0026#34;, \u0026#34;region\u0026#34; : \u0026#34;DE\u0026#34; } ], \u0026#34;urn:ietf:params:scim:schemas:extension:oracle:2.0:OUD:User\u0026#34; : { \u0026#34;description\u0026#34; : [ { \u0026#34;value\u0026#34; : \u0026#34;This is the description for Aaccf Amar.\u0026#34; } ], \u0026#34;mobile\u0026#34; : [ { \u0026#34;value\u0026#34; : \u0026#34;+1 010 154 3228\u0026#34; } ], \u0026#34;pager\u0026#34; : [ { \u0026#34;value\u0026#34; : \u0026#34;+1 779 041 6341\u0026#34; } ], \u0026#34;objectClass\u0026#34; : [ { \u0026#34;value\u0026#34; : \u0026#34;top\u0026#34; }, { \u0026#34;value\u0026#34; : \u0026#34;organizationalperson\u0026#34; }, { \u0026#34;value\u0026#34; : \u0026#34;person\u0026#34; }, { \u0026#34;value\u0026#34; : \u0026#34;inetorgperson\u0026#34; } ], \u0026#34;initials\u0026#34; : [ { \u0026#34;value\u0026#34; : \u0026#34;ASA\u0026#34; } ], \u0026#34;homePhone\u0026#34; : [ { \u0026#34;value\u0026#34; : \u0026#34;+1 225 216 5900\u0026#34; } ] }, \u0026#34;name\u0026#34; : [ { \u0026#34;givenName\u0026#34; : \u0026#34;Aaccf\u0026#34;, \u0026#34;familyName\u0026#34; : \u0026#34;Amar\u0026#34;, \u0026#34;formatted\u0026#34; : \u0026#34;Aaccf Amar\u0026#34; } ], \u0026#34;emails\u0026#34; : [ { \u0026#34;value\u0026#34; : \u0026#34;user.0@maildomain.net\u0026#34; } ], \u0026#34;phoneNumbers\u0026#34; : [ { \u0026#34;value\u0026#34; : \u0026#34;+1 685 622 6202\u0026#34; } ], \u0026#34;urn:ietf:params:scim:schemas:extension:enterprise:2.0:User\u0026#34; : { \u0026#34;employeeNumber\u0026#34; : [ { \u0026#34;value\u0026#34; : \u0026#34;0\u0026#34; } ] } } , . . . } b) Command to invoke Data SCIM API against specific Oracle Unified Directory Interface:\n$ curl --noproxy \u0026quot;*\u0026quot; --location \\ --request GET 'https://my-oud-ds-rs-http-0:30443/iam/directory/oud/scim/v1/Users' \\ --header 'Authorization: Basic \u0026lt;Base64 of userDN:userPassword\u0026gt;' | json_pp  | json_pp is used to format output in readable json format on the client side. It can be ignored if you do not have the json_pp library. Base64 of userDN:userPassword can be generated using echo -n \u0026quot;userDN:userPassword\u0026quot; | base64. For this example, it is assumed that the value \u0026lsquo;my-oud-ds-rs\u0026rsquo; is used as the deployment/release name for helm chart installation. It is assumed that \u0026lsquo;my-oud-ds-rs-http-0\u0026rsquo; points to an External LoadBalancer  HTTPS/REST Admin API a) Command to invoke Admin REST API against External LBR:\n$ curl --noproxy \u0026quot;*\u0026quot; --insecure --location \\ --request GET 'https://\u0026lt;External LBR Host\u0026gt;/rest/v1/admin/?scope=base\u0026amp;attributes=vendorName\u0026amp;attributes=vendorVersion\u0026amp;attributes=ds-private-naming-contexts\u0026amp;attributes=subschemaSubentry' \\ --header 'Content-Type: application/json' \\ --header 'Authorization: Basic \u0026lt;Base64 of userDN:userPassword\u0026gt;' | json_pp  | json_pp is used to format output in readable json format on the client side. It can be ignored if you do not have the json_pp library. Base64 of userDN:userPassword can be generated using echo -n \u0026quot;userDN:userPassword\u0026quot; | base64.  Output:\n{ \u0026#34;totalResults\u0026#34; : 1, \u0026#34;searchResultEntries\u0026#34; : [ { \u0026#34;dn\u0026#34; : \u0026#34;\u0026#34;, \u0026#34;attributes\u0026#34; : { \u0026#34;vendorVersion\u0026#34; : \u0026#34;Oracle Unified Directory 12.2.1.4.0\u0026#34;, \u0026#34;ds-private-naming-contexts\u0026#34; : [ \u0026#34;cn=admin data\u0026#34;, \u0026#34;cn=ads-truststore\u0026#34;, \u0026#34;cn=backups\u0026#34;, \u0026#34;cn=config\u0026#34;, \u0026#34;cn=monitor\u0026#34;, \u0026#34;cn=schema\u0026#34;, \u0026#34;cn=tasks\u0026#34;, \u0026#34;cn=virtual acis\u0026#34;, \u0026#34;dc=replicationchanges\u0026#34; ], \u0026#34;subschemaSubentry\u0026#34; : \u0026#34;cn=schema\u0026#34;, \u0026#34;vendorName\u0026#34; : \u0026#34;Oracle Corporation\u0026#34; } } ], \u0026#34;msgType\u0026#34; : \u0026#34;urn:ietf:params:rest:schemas:oracle:oud:1.0:SearchResponse\u0026#34; } b) Command to invoke Admin REST API against specific Oracle Unified Directory Admin Interface:\n$ curl --noproxy \u0026quot;*\u0026quot; --insecure --location \\ --request GET 'https://my-oud-ds-rs-admin-0/rest/v1/admin/?scope=base\u0026amp;attributes=vendorName\u0026amp;attributes=vendorVersion\u0026amp;attributes=ds-private-naming-contexts\u0026amp;attributes=subschemaSubentry' \\ --header 'Content-Type: application/json' \\ --header 'Authorization: Basic \u0026lt;Base64 of userDN:userPassword\u0026gt;' | json_pp  | json_pp is used to format output in readable json format on the client side. It can be ignored if you do not have the json_pp library. Base64 of userDN:userPassword can be generated using echo -n \u0026quot;userDN:userPassword\u0026quot; | base64.  c) Command to invoke Admin REST API against Kubernetes NodePort for Ingress Controller Service\n$ curl --noproxy \u0026quot;*\u0026quot; --insecure --location \\ --request GET 'https://my-oud-ds-rs-admin-0:30443/rest/v1/admin/?scope=base\u0026amp;attributes=vendorName\u0026amp;attributes=vendorVersion\u0026amp;attributes=ds-private-naming-contexts\u0026amp;attributes=subschemaSubentry' \\ --header 'Content-Type: application/json' \\ --header 'Authorization: Basic \u0026lt;Base64 of userDN:userPassword\u0026gt;' | json_pp  | json_pp is used to format output in readable json format on the client side. It can be ignored if you do not have the json_pp library. Base64 of userDN:userPassword can be generated using echo -n \u0026quot;userDN:userPassword\u0026quot; | base64.  LDAP against External Load Balancer a) Command to perform ldapsearch against External LBR and LDAP port\n$ \u0026lt;OUD Home\u0026gt;/bin/ldapsearch --hostname \u0026lt;External LBR\u0026gt; --port 1389 \\ -D \u0026quot;\u0026lt;Root User DN\u0026gt;\u0026quot; -w \u0026lt;Password for Root User DN\u0026gt; \\ -b \u0026quot;\u0026quot; -s base \u0026quot;(objectClass=*)\u0026quot; \u0026quot;*\u0026quot; Output:\ndn: objectClass: top objectClass: ds-root-dse lastChangeNumber: 0 firstChangeNumber: 0 changelog: cn=changelog entryDN: pwdPolicySubentry: cn=Default Password Policy,cn=Password Policies,cn=config subschemaSubentry: cn=schema supportedAuthPasswordSchemes: SHA256 supportedAuthPasswordSchemes: SHA1 supportedAuthPasswordSchemes: SHA384 supportedAuthPasswordSchemes: SHA512 supportedAuthPasswordSchemes: MD5 numSubordinates: 1 supportedFeatures: 1.3.6.1.1.14 supportedFeatures: 1.3.6.1.4.1.4203.1.5.1 supportedFeatures: 1.3.6.1.4.1.4203.1.5.2 supportedFeatures: 1.3.6.1.4.1.4203.1.5.3 lastExternalChangelogCookie: vendorName: Oracle Corporation vendorVersion: Oracle Unified Directory 12.2.1.4.0 componentVersion: 4 releaseVersion: 1 platformVersion: 0 supportedLDAPVersion: 2 supportedLDAPVersion: 3 supportedControl: 1.2.826.0.1.3344810.2.3 supportedControl: 1.2.840.113556.1.4.1413 supportedControl: 1.2.840.113556.1.4.319 supportedControl: 1.2.840.113556.1.4.473 supportedControl: 1.2.840.113556.1.4.805 supportedControl: 1.3.6.1.1.12 supportedControl: 1.3.6.1.1.13.1 supportedControl: 1.3.6.1.1.13.2 supportedControl: 1.3.6.1.4.1.26027.1.5.2 supportedControl: 1.3.6.1.4.1.26027.1.5.4 supportedControl: 1.3.6.1.4.1.26027.1.5.5 supportedControl: 1.3.6.1.4.1.26027.1.5.6 supportedControl: 1.3.6.1.4.1.26027.2.3.1 supportedControl: 1.3.6.1.4.1.26027.2.3.2 supportedControl: 1.3.6.1.4.1.26027.2.3.4 supportedControl: 1.3.6.1.4.1.42.2.27.8.5.1 supportedControl: 1.3.6.1.4.1.42.2.27.9.5.2 supportedControl: 1.3.6.1.4.1.42.2.27.9.5.8 supportedControl: 1.3.6.1.4.1.4203.1.10.1 supportedControl: 1.3.6.1.4.1.4203.1.10.2 supportedControl: 2.16.840.1.113730.3.4.12 supportedControl: 2.16.840.1.113730.3.4.16 supportedControl: 2.16.840.1.113730.3.4.17 supportedControl: 2.16.840.1.113730.3.4.18 supportedControl: 2.16.840.1.113730.3.4.19 supportedControl: 2.16.840.1.113730.3.4.2 supportedControl: 2.16.840.1.113730.3.4.3 supportedControl: 2.16.840.1.113730.3.4.4 supportedControl: 2.16.840.1.113730.3.4.5 supportedControl: 2.16.840.1.113730.3.4.9 supportedControl: 2.16.840.1.113894.1.8.21 supportedControl: 2.16.840.1.113894.1.8.31 supportedControl: 2.16.840.1.113894.1.8.36 maintenanceVersion: 2 supportedSASLMechanisms: PLAIN supportedSASLMechanisms: EXTERNAL supportedSASLMechanisms: CRAM-MD5 supportedSASLMechanisms: DIGEST-MD5 majorVersion: 12 orclGUID: D41D8CD98F003204A9800998ECF8427E entryUUID: d41d8cd9-8f00-3204-a980-0998ecf8427e ds-private-naming-contexts: cn=schema hasSubordinates: true nsUniqueId: d41d8cd9-8f003204-a9800998-ecf8427e structuralObjectClass: ds-root-dse supportedExtension: 1.3.6.1.4.1.4203.1.11.1 supportedExtension: 1.3.6.1.4.1.4203.1.11.3 supportedExtension: 1.3.6.1.1.8 supportedExtension: 1.3.6.1.4.1.26027.1.6.3 supportedExtension: 1.3.6.1.4.1.26027.1.6.2 supportedExtension: 1.3.6.1.4.1.26027.1.6.1 supportedExtension: 1.3.6.1.4.1.1466.20037 namingContexts: cn=changelog namingContexts: dc=example,dc=com b) Command to perform ldapsearch against External LBR and LDAP port for specific Oracle Unified Directory Interface\n$ \u0026lt;OUD Home\u0026gt;/bin/ldapsearch --hostname \u0026lt;External LBR\u0026gt; --port 3890 \\ -D \u0026quot;\u0026lt;Root User DN\u0026gt;\u0026quot; -w \u0026lt;Password for Root User DN\u0026gt; \\ -b \u0026quot;\u0026quot; -s base \u0026quot;(objectClass=*)\u0026quot; \u0026quot;*\u0026quot; LDAPS against Kubernetes NodePort for Ingress Controller Service a) Command to perform ldapsearch against External LBR and LDAP port\n$ \u0026lt;OUD Home\u0026gt;/bin/ldapsearch --hostname \u0026lt;Kubernetes Node\u0026gt; --port 31636 \\ --useSSL --trustAll \\ -D \u0026quot;\u0026lt;Root User DN\u0026gt;\u0026quot; -w \u0026lt;Password for Root User DN\u0026gt; \\ -b \u0026quot;\u0026quot; -s base \u0026quot;(objectClass=*)\u0026quot; \u0026quot;*\u0026quot; Output:\ndn: objectClass: top objectClass: ds-root-dse lastChangeNumber: 0 firstChangeNumber: 0 changelog: cn=changelog entryDN: pwdPolicySubentry: cn=Default Password Policy,cn=Password Policies,cn=config subschemaSubentry: cn=schema supportedAuthPasswordSchemes: SHA256 supportedAuthPasswordSchemes: SHA1 supportedAuthPasswordSchemes: SHA384 supportedAuthPasswordSchemes: SHA512 supportedAuthPasswordSchemes: MD5 numSubordinates: 1 supportedFeatures: 1.3.6.1.1.14 supportedFeatures: 1.3.6.1.4.1.4203.1.5.1 supportedFeatures: 1.3.6.1.4.1.4203.1.5.2 supportedFeatures: 1.3.6.1.4.1.4203.1.5.3 lastExternalChangelogCookie: vendorName: Oracle Corporation vendorVersion: Oracle Unified Directory 12.2.1.4.0 componentVersion: 4 releaseVersion: 1 platformVersion: 0 supportedLDAPVersion: 2 supportedLDAPVersion: 3 supportedControl: 1.2.826.0.1.3344810.2.3 supportedControl: 1.2.840.113556.1.4.1413 supportedControl: 1.2.840.113556.1.4.319 supportedControl: 1.2.840.113556.1.4.473 supportedControl: 1.2.840.113556.1.4.805 supportedControl: 1.3.6.1.1.12 supportedControl: 1.3.6.1.1.13.1 supportedControl: 1.3.6.1.1.13.2 supportedControl: 1.3.6.1.4.1.26027.1.5.2 supportedControl: 1.3.6.1.4.1.26027.1.5.4 supportedControl: 1.3.6.1.4.1.26027.1.5.5 supportedControl: 1.3.6.1.4.1.26027.1.5.6 supportedControl: 1.3.6.1.4.1.26027.2.3.1 supportedControl: 1.3.6.1.4.1.26027.2.3.2 supportedControl: 1.3.6.1.4.1.26027.2.3.4 supportedControl: 1.3.6.1.4.1.42.2.27.8.5.1 supportedControl: 1.3.6.1.4.1.42.2.27.9.5.2 supportedControl: 1.3.6.1.4.1.42.2.27.9.5.8 supportedControl: 1.3.6.1.4.1.4203.1.10.1 supportedControl: 1.3.6.1.4.1.4203.1.10.2 supportedControl: 2.16.840.1.113730.3.4.12 supportedControl: 2.16.840.1.113730.3.4.16 supportedControl: 2.16.840.1.113730.3.4.17 supportedControl: 2.16.840.1.113730.3.4.18 supportedControl: 2.16.840.1.113730.3.4.19 supportedControl: 2.16.840.1.113730.3.4.2 supportedControl: 2.16.840.1.113730.3.4.3 supportedControl: 2.16.840.1.113730.3.4.4 supportedControl: 2.16.840.1.113730.3.4.5 supportedControl: 2.16.840.1.113730.3.4.9 supportedControl: 2.16.840.1.113894.1.8.21 supportedControl: 2.16.840.1.113894.1.8.31 supportedControl: 2.16.840.1.113894.1.8.36 maintenanceVersion: 2 supportedSASLMechanisms: PLAIN supportedSASLMechanisms: EXTERNAL supportedSASLMechanisms: CRAM-MD5 supportedSASLMechanisms: DIGEST-MD5 majorVersion: 12 orclGUID: D41D8CD98F003204A9800998ECF8427E entryUUID: d41d8cd9-8f00-3204-a980-0998ecf8427e ds-private-naming-contexts: cn=schema hasSubordinates: true nsUniqueId: d41d8cd9-8f003204-a9800998-ecf8427e structuralObjectClass: ds-root-dse supportedExtension: 1.3.6.1.4.1.4203.1.11.1 supportedExtension: 1.3.6.1.4.1.4203.1.11.3 supportedExtension: 1.3.6.1.1.8 supportedExtension: 1.3.6.1.4.1.26027.1.6.3 supportedExtension: 1.3.6.1.4.1.26027.1.6.2 supportedExtension: 1.3.6.1.4.1.26027.1.6.1 supportedExtension: 1.3.6.1.4.1.1466.20037 namingContexts: cn=changelog namingContexts: dc=example,dc=com b) Command to perform ldapsearch against External LBR and LDAP port for specific Oracle Unified Directory Interface\n$ \u0026lt;OUD Home\u0026gt;/bin/ldapsearch --hostname \u0026lt;Kubernetes Node\u0026gt; --port 30360 \\ --useSSL --trustAll \\ -D \u0026quot;\u0026lt;Root User DN\u0026gt;\u0026quot; -w \u0026lt;Password for Root User DN\u0026gt; \\ -b \u0026quot;\u0026quot; -s base \u0026quot;(objectClass=*)\u0026quot; \u0026quot;*\u0026quot; Configuration Parameters The following table lists the configurable parameters of the oud-ds-rs chart and its default values.\n   Parameter Description Default Value     replicaCount Number of DS+RS instances/pods/services to be created with replication enabled against a base Oracle Unified Directory instance/pod. 3   restartPolicyName restartPolicy to be configured for each POD containing Oracle Unified Directory instance OnFailure   image.repository Oracle Unified Directory Image Registry/Repository and name. Based on this, image parameter would be configured for Oracle Unified Directory pods/containers oracle/oud   image.tag Oracle Unified Directory Image Tag. Based on this, image parameter would be configured for Oracle Unified Directory pods/containers 12.2.1.4.0   image.pullPolicy policy to pull the image IfnotPresent   imagePullSecrets.name name of Secret resource containing private registry credentials regcred   nameOverride override the fullname with this name    fullnameOverride Overrides the fullname with the provided string    serviceAccount.create Specifies whether a service account should be created true   serviceAccount.name If not set and create is true, a name is generated using the fullname template oud-ds-rs-\u0026lt; fullname \u0026gt;-token-\u0026lt; randomalphanum \u0026gt;   podSecurityContext Security context policies to add to the controller pod    securityContext Security context policies to add by default    service.type type of controller service to create ClusterIP   nodeSelector node labels for pod assignment    tolerations node taints to tolerate    affinity node/pod affinities    ingress.enabled  true   ingress.type Supported value: either nginx or voyager nginx   ingress.nginx.http.host Hostname to be used with Ingress Rules. If not set, hostname would be configured according to fullname. Hosts would be configured as \u0026lt; fullname \u0026gt;-http.\u0026lt; domain \u0026gt;, \u0026lt; fullname \u0026gt;-http-0.\u0026lt; domain \u0026gt;, \u0026lt; fullname \u0026gt;-http-1.\u0026lt; domain \u0026gt;, etc.    ingress.nginx.http.domain Domain name to be used with Ingress Rules. In ingress rules, hosts would be configured as \u0026lt; host \u0026gt;.\u0026lt; domain \u0026gt;, \u0026lt; host \u0026gt;-0.\u0026lt; domain \u0026gt;, \u0026lt; host \u0026gt;-1.\u0026lt; domain \u0026gt;, etc.    ingress.nginx.http.backendPort  http   ingress.nginx.http.nginxAnnotations  { kubernetes.io/ingress.class: \u0026ldquo;nginx\u0026quot;}   ingress.nginx.admin.host Hostname to be used with Ingress Rules. If not set, hostname would be configured according to fullname. Hosts would be configured as \u0026lt; fullname \u0026gt;-admin.\u0026lt; domain \u0026gt;, \u0026lt; fullname \u0026gt;-admin-0.\u0026lt; domain \u0026gt;, \u0026lt; fullname \u0026gt;-admin-1.\u0026lt; domain \u0026gt;, etc.    ingress.nginx.admin.domain Domain name to be used with Ingress Rules. In ingress rules, hosts would be configured as \u0026lt; host \u0026gt;.\u0026lt; domain \u0026gt;, \u0026lt; host \u0026gt;-0.\u0026lt; domain \u0026gt;, \u0026lt; host \u0026gt;-1.\u0026lt; domain \u0026gt;, etc.    ingress.nginx.admin.nginxAnnotations  { kubernetes.io/ingress.class: \u0026ldquo;nginx\u0026rdquo; nginx.ingress.kubernetes.io/backend-protocol: \u0026ldquo;https\u0026quot;}   ingress.voyagerAnnotations  { kubernetes.io/ingress.class: \u0026ldquo;voyager\u0026rdquo; ingress.appscode.com/type: \u0026ldquo;NodePort\u0026rdquo; }   ingress.voyagerNodePortHttp NodePort value for HTTP Port exposed through Voyager LoadBalancer Service 30080   ingress.voyagerNodePortHttps NodePort value for HTTPS Port exposed through Voyager LoadBalancer Service 30443   ingress.voyagerHttpPort Port value for HTTP Port exposed through Voyager LoadBalancer Service 80   ingress.voyagerHttpsPort Port value for HTTPS Port exposed through Voyager LoadBalancer Service 443   ingress.ingress.tlsSecret Secret name to use an already created TLS Secret. If such secret is not provided, one would be created with name \u0026lt; fullname \u0026gt;-tls-cert. If the TLS Secret is in different namespace, name can be mentioned as \u0026lt; namespace \u0026gt;/\u0026lt; tlsSecretName \u0026gt;    ingress.certCN Subject\u0026rsquo;s common name (cn) for SelfSigned Cert. \u0026lt; fullname \u0026gt;   ingress.certValidityDays Validity of Self-Signed Cert in days 365   secret.enabled If enabled it will use the secret created with base64 encoding. if value is false, secret would not be used and input values (through \u0026ndash;set, \u0026ndash;values, etc.) would be used while creation of pods. true   secret.name secret name to use an already created Secret oud-ds-rs-\u0026lt; fullname \u0026gt;-creds   secret.type Specifies the type of the secret Opaque   persistence.enabled If enabled, it will use the persistent volume. if value is false, PV and PVC would not be used and pods would be using the default emptyDir mount volume. true   persistence.pvname pvname to use an already created Persistent Volume , If blank will use the default name oud-ds-rs-\u0026lt; fullname \u0026gt;-pv   persistence.pvcname pvcname to use an already created Persistent Volume Claim , If blank will use default name oud-ds-rs-\u0026lt; fullname \u0026gt;-pvc   persistence.type supported values: either filesystem or networkstorage or custom filesystem   persistence.filesystem.hostPath.path The path location mentioned should be created and accessible from the local host provided with necessary privileges for the user. /scratch/shared/oud_user_projects   persistence.networkstorage.nfs.path Path of NFS Share location /scratch/shared/oud_user_projects   persistence.networkstorage.nfs.server IP or hostname of NFS Server 0.0.0.0   persistence.custom.* Based on values/data, YAML content would be included in PersistenceVolume Object    persistence.accessMode Specifies the access mode of the location provided ReadWriteMany   persistence.size Specifies the size of the storage 10Gi   persistence.storageClass Specifies the storageclass of the persistence volume. empty   persistence.annotations specifies any annotations that will be used { }   configVolume.enabled If enabled, it will use the persistent volume. If value is false, PV and PVC would not be used and pods would be using the default emptyDir mount volume. true   configVolume.mountPath If enabled, it will use the persistent volume. If value is false, PV and PVC would not be used and there would not be any mount point available for config false   configVolume.pvname pvname to use an already created Persistent Volume , If blank will use the default name oud-ds-rs-\u0026lt; fullname \u0026gt;-pv-config   configVolume.pvcname pvcname to use an already created Persistent Volume Claim , If blank will use default name oud-ds-rs-\u0026lt; fullname \u0026gt;-pvc-config   configVolume.type supported values: either filesystem or networkstorage or custom filesystem   configVolume.filesystem.hostPath.path The path location mentioned should be created and accessible from the local host provided with necessary privileges for the user. /scratch/shared/oud_user_projects   configVolume.networkstorage.nfs.path Path of NFS Share location /scratch/shared/oud_config   configVolume.networkstorage.nfs.server IP or hostname of NFS Server 0.0.0.0   configVolume.custom.* Based on values/data, YAML content would be included in PersistenceVolume Object    configVolume.accessMode Specifies the access mode of the location provided ReadWriteMany   configVolume.size Specifies the size of the storage 10Gi   configVolume.storageClass Specifies the storageclass of the persistence volume. empty   configVolume.annotations specifies any annotations that will be used { }   oudPorts.adminldaps Port on which Oracle Unified Directory Instance in the container should listen for Administration Communication over LDAPS Protocol 1444   oudPorts.adminhttps Port on which Oracle Unified Directory Instance in the container should listen for Administration Communication over HTTPS Protocol. 1888   oudPorts.ldap Port on which Oracle Unified Directory Instance in the container should listen for LDAP Communication. 1389   oudPorts.ldaps Port on which Oracle Unified Directory Instance in the container should listen for LDAPS Communication. 1636   oudPorts.http Port on which Oracle Unified Directory Instance in the container should listen for HTTP Communication. 1080   oudPorts.https Port on which Oracle Unified Directory Instance in the container should listen for HTTPS Communication. 1081   oudPorts.replication Port value to be used while setting up replication server. 1898   oudConfig.baseDN BaseDN for Oracle Unified Directory Instances dc=example,dc=com   oudConfig.rootUserDN Root User DN for Oracle Unified Directory Instances cn=Directory Manager   oudConfig.rootUserPassword Password for Root User DN RandomAlphanum   oudConfig.sampleData To specify that the database should be populated with the specified number of sample entries. 0   oudConfig.sleepBeforeConfig Based on the value for this parameter, initialization/configuration of each Oracle Unified Directory replica would be delayed. 120   oudConfig.adminUID AdminUID to be configured with each replicated Oracle Unified Directory instance admin   oudConfig.adminPassword Password for AdminUID. If the value is not passed, value of rootUserPassword would be used as password for AdminUID. rootUserPassword   baseOUD.envVarsConfigMap Reference to ConfigMap which can contain additional environment variables to be passed on to POD for Base Oracle Unified Directory Instance. Following are the environment variables which would not be honored from the ConfigMap. instanceType, sleepBeforeConfig, OUD_INSTANCE_NAME, hostname, baseDN, rootUserDN, rootUserPassword, adminConnectorPort, httpAdminConnectorPort, ldapPort, ldapsPort, httpPort, httpsPort, replicationPort, sampleData. -   baseOUD.envVars Environment variables in Yaml Map format. This is helpful when its requried to pass environment variables through \u0026ndash;values file. List of env variables which would not be honored from envVars map is same as list of env var names mentioned for envVarsConfigMap. -   replOUD.envVarsConfigMap Reference to ConfigMap which can contain additional environment variables to be passed on to PODs for Replicated Oracle Unified Directory Instances. Following are the environment variables which would not be honored from the ConfigMap. instanceType, sleepBeforeConfig, OUD_INSTANCE_NAME, hostname, baseDN, rootUserDN, rootUserPassword, adminConnectorPort, httpAdminConnectorPort, ldapPort, ldapsPort, httpPort, httpsPort, replicationPort, sampleData, sourceHost, sourceServerPorts, sourceAdminConnectorPort, sourceReplicationPort, dsreplication_1, dsreplication_2, dsreplication_3, dsreplication_4, post_dsreplication_dsconfig_1, post_dsreplication_dsconfig_2 -   replOUD.envVars Environment variables in Yaml Map format. This is helpful when its required to pass environment variables through \u0026ndash;values file. List of env variables which would not be honored from envVars map is same as list of env var names mentioned for envVarsConfigMap. -   replOUD.groupId Group ID to be used/configured with each Oracle Unified Directory instance in replicated topology. 1   elk.elasticsearch.enabled If enabled it will create the elastic search statefulset deployment false   elk.elasticsearch.image.repository Elastic Search Image name/Registry/Repository . Based on this elastic search instances will be created docker.elastic.co/elasticsearch/elasticsearch   elk.elasticsearch.image.tag Elastic Search Image tag .Based on this, image parameter would be configured for Elastic Search pods/instances 6.4.3   elk.elasticsearch.image.pullPolicy policy to pull the image IfnotPresent   elk.elasticsearch.esreplicas Number of Elastic search Instances will be created 3   elk.elasticsearch.minimumMasterNodes The value for discovery.zen.minimum_master_nodes. Should be set to (esreplicas / 2) + 1. 2   elk.elasticsearch.esJAVAOpts Java options for Elasticsearch. This is where you should configure the jvm heap size -Xms512m -Xmx512m   elk.elasticsearch.sysctlVmMaxMapCount Sets the sysctl vm.max_map_count needed for Elasticsearch 262144   elk.elasticsearch.resources.requests.cpu cpu resources requested for the elastic search 100m   elk.elasticsearch.resources.limits.cpu total cpu limits that are configures for the elastic search 1000m   elk.elasticsearch.esService.type Type of Service to be created for elastic search ClusterIP   elk.elasticsearch.esService.lbrtype Type of load balancer Service to be created for elastic search ClusterIP   elk.kibana.enabled If enabled it will create a kibana deployment false   elk.kibana.image.repository Kibana Image Registry/Repository and name. Based on this Kibana instance will be created docker.elastic.co/kibana/kibana   elk.kibana.image.tag Kibana Image tag. Based on this, Image parameter would be configured. 6.4.3   elk.kibana.image.pullPolicy policy to pull the image IfnotPresent   elk.kibana.kibanaReplicas Number of Kibana instances will be created 1   elk.kibana.service.tye Type of service to be created NodePort   elk.kibana.service.targetPort Port on which the kibana will be accessed 5601   elk.kibana.service.nodePort nodePort is the port on which kibana service will be accessed from outside 31119   elk.logstash.enabled If enabled it will create a logstash deployment false   elk.logstash.image.repository logstash Image Registry/Repository and name. Based on this logstash instance will be created logstash   elk.logstash.image.tag logstash Image tag. Based on this, Image parameter would be configured. 6.6.0   elk.logstash.image.pullPolicy policy to pull the image IfnotPresent   elk.logstash.containerPort Port on which the logstash container will be running 5044   elk.logstash.service.tye Type of service to be created NodePort   elk.logstash.service.targetPort Port on which the logstash will be accessed 9600   elk.logstash.service.nodePort nodePort is the port on which logstash service will be accessed from outside 32222   elk.logstash.logstashConfigMap Provide the configmap name which is already created with the logstash conf. if empty default logstash configmap will be created and used    elk.elkPorts.rest Port for REST 9200   elk.elkPorts.internode port used for communication between the nodes 9300   elk.busybox.image busy box image name. Used for initcontianers busybox   elk.elkVolume.enabled If enabled, it will use the persistent volume. if value is false, PV and pods would be using the default emptyDir mount volume. true   elk.elkVolume.pvname pvname to use an already created Persistent Volume , If blank will use the default name oud-ds-rs-\u0026lt; fullname \u0026gt;-espv   elk.elkVolume.type supported values: either filesystem or networkstorage or custom filesystem   elk.elkVolume.filesystem.hostPath.path The path location mentioned should be created and accessible from the local host provided with necessary privileges for the user. /scratch/shared/oud_elk/data   elk.elkVolume.networkstorage.nfs.path Path of NFS Share location /scratch/shared/oud_elk/data   elk.elkVolume.networkstorage.nfs.server IP or hostname of NFS Server 0.0.0.0   elk.elkVolume.custom.* Based on values/data, YAML content would be included in PersistenceVolume Object    elk.elkVolume.accessMode Specifies the access mode of the location provided ReadWriteMany   elk.elkVolume.size Specifies the size of the storage 20Gi   elk.elkVolume.storageClass Specifies the storageclass of the persistence volume. elk   elk.elkVolume.annotations specifies any annotations that will be used { }    "
},
{
	"uri": "/fmw-kubernetes/20.4.1/oudsm/create-oudsm-instances-helm/oudsm/",
	"title": "Helm Chart: oudsm: For deployment of Oracle Unified Directory Services Manager instances on Kubernetes",
	"tags": [],
	"description": "This document provides details of the oudsm Helm chart.",
	"content": " Introduction Create Kubernetes Namespace Deploy oudsm Helm Chart Verify the Installation Ingress Controller Setup  Ingress with NGINX Ingress with Voyager   Access to Interfaces through Ingress Configuration Parameters  Introduction This Helm chart provides for the deployment of replicated Oracle Unified Directory Services Manager instances on Kubernetes.\nBased on the configuration, this chart deploys the following objects in the specified namespace of a Kubernetes cluster.\n Service Account Secret Persistent Volume and Persistent Volume Claim Pod(s)/Container(s) for Oracle Unified Directory Services Manager Instances Services for interfaces exposed through Oracle Unified Directory Services Manager Instances Ingress configuration  Create Kubernetes Namespace Create a Kubernetes namespace to provide a scope for other objects such as pods and services that you create in the environment. To create your namespace issue the following command:\n$ kubectl create ns myhelmns namespace/myhelmns created Deploy oudsm Helm Chart Create/Deploy Oracle Unified Directory Services Manager instances along with Kubernetes objects in a specified namespace using the oudsm Helm Chart.\nThe deployment can be initiated by running the following Helm command with reference to the oudsm Helm Chart, along with configuration parameters according to your environment. Before deploying the Helm chart, the namespace should be created. Objects to be created by the Helm chart will be created inside the specified namespace.\n$ helm install --namespace \u0026lt;namespace\u0026gt; \\ \u0026lt;Configuration Parameters\u0026gt; \\ \u0026lt;deployment/release name\u0026gt; \\ \u0026lt;Helm Chart Path/Name\u0026gt; Configuration Parameters (override values in chart) can be passed on with --set arguments on the command line and/or with -f / --values arguments when referring to files.\nExamples Example where configuration parameters are passed with --set argument: $ helm install --namespace myhelmns \\ --set oudsm.adminUser=weblogic,oudsm.adminPass=Oracle123,persistence.filesystem.hostPath.path=/scratch/shared/oudsm_user_projects \\ my-oudsm oudsm  For more details about the helm command and parameters, please execute helm --help and helm install --help. In this example, it is assumed that the command is executed from the directory containing the \u0026lsquo;oudsm\u0026rsquo; helm chart directory (OracleUnifiedDirectorySM/kubernetes/helm/).  Example where configuration parameters are passed with --values argument: $ helm install --namespace myhelmns \\ --values oudsm-values-override.yaml \\ my-oudsm oudsm  For more details about the helm command and parameters, please execute helm --help and helm install --help. In this example, it is assumed that the command is executed from the directory containing the \u0026lsquo;oudsm\u0026rsquo; helm chart directory (OracleUnifiedDirectorySM/kubernetes/helm/). The --values argument passes a file path/name which overrides default values in the chart.  oudsm-values-override.yaml\noudsm: adminUser: weblogic adminPass: Oracle123 persistence: type: filesystem filesystem: hostPath: path: /scratch/shared/oudsm_user_projects Example to update/upgrade Helm Chart based deployment: $ helm upgrade --namespace myhelmns \\ --set oudsm.adminUser=weblogic,oudsm.adminPass=Oracle123,persistence.filesystem.hostPath.path=/scratch/shared/oudsm_user_projects,replicaCount=2 \\ my-oudsm oudsm  For more details about the helm command and parameters, please execute helm --help and helm install --help. In this example, it is assumed that the command is executed from the directory containing the \u0026lsquo;oudsm\u0026rsquo; helm chart directory (OracleUnifiedDirectorySM/kubernetes/helm/).  Example to apply new Oracle Unified Directory Services Manager patch through Helm Chart based deployment: In this example, we will apply PSU2020July-20200730 patch on earlier running Oracle Unified Directory Services Manager version. If we describe pod we will observe that the container is up with new version.\nWe have two ways to achieve our goal:\n$ helm upgrade --namespace myhelmns \\ --set image.repository=oracle/oudsm,image.tag=12.2.1.4.0-PSU2020July-20200730 \\ my-oudsm oudsm OR\n$ helm upgrade --namespace myhelmns \\ --values oudsm-values-override.yaml \\ my-oudsm oudsm  For more details about the helm command and parameters, please execute helm --help and helm install --help. In this example, it is assumed that the command is executed from the directory containing the \u0026lsquo;oudsm\u0026rsquo; helm chart directory (OracleUnifiedDirectorySM/kubernetes/helm/).  oudsm-values-override.yaml\nimage: repository: oracle/oudsm tag: 12.2.1.4.0-PSU2020July-20200730 Example for using NFS as PV Storage: $ helm install --namespace myhelmns \\ --values oudsm-values-override-nfs.yaml \\ my-oudsm oudsm  For more details about the helm command and parameters, please execute helm --help and helm install --help. In this example, it is assumed that the command is executed from the directory containing the \u0026lsquo;oudsm\u0026rsquo; helm chart directory (OracleUnifiedDirectorySM/kubernetes/helm/). The --values argument passes a file path/name which overrides values in the chart.  oudsm-values-override-nfs.yaml\noudsm: adminUser: weblogic adminPass: Oracle123 persistence: type: networkstorage networkstorage: nfs: path: /scratch/shared/oud_user_projects server: \u0026lt;NFS IP address\u0026gt; Example for using PV type of your choice: $ helm install --namespace myhelmns \\ --values oudsm-values-override-pv-custom.yaml \\ my-oudsm oudsm  For more details about the helm command and parameters, please execute helm --help and helm install --help. In this example, it is assumed that the command is executed from the directory containing the \u0026lsquo;oudsm\u0026rsquo; helm chart directory (OracleUnifiedDirectorySM/kubernetes/helm/). The --values argument passes a file path/name which overrides values in the chart.  oudsm-values-override-pv-custom.yaml\noudsm: adminUser: weblogic adminPass: Oracle123 persistence: type: custom custom: nfs: # Path of NFS Share location path: /scratch/shared/oudsm_user_projects # IP of NFS Server server: \u0026lt;NFS IP address\u0026gt;  Under custom:, the configuration of your choice can be specified. This configuration will be used \u0026lsquo;as-is\u0026rsquo; for the PersistentVolume object.\n Check Deployment Output for the helm install/upgrade command Ouput similar to the following is observed following successful execution of helm install/upgrade command.\nNAME: my-oudsm LAST DEPLOYED: Wed Oct 14 06:22:10 2020 NAMESPACE: myhelmns STATUS: deployed REVISION: 1 TEST SUITE: None Check for the status of objects created through oudsm helm chart Command:\n$ kubectl --namespace myhelmns get nodes,pod,service,secret,pv,pvc,ingress -o wide Output is similar to the following:\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/oudsm-1 1/1 Running 0 22h 10.244.0.19 100.102.51.238 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/oudsm-2 1/1 Running 0 22h 10.244.0.20 100.102.51.238 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service/oudsm-1 ClusterIP 10.96.108.200 \u0026lt;none\u0026gt; 7001/TCP,7002/TCP 22h app.kubernetes.io/instance=oudsm,app.kubernetes.io/name=oudsm,oudsm/instance=oudsm-1 service/oudsm-2 ClusterIP 10.96.96.12 \u0026lt;none\u0026gt; 7001/TCP,7002/TCP 22h app.kubernetes.io/instance=oudsm,app.kubernetes.io/name=oudsm,oudsm/instance=oudsm-2 service/oudsm-lbr ClusterIP 10.96.41.201 \u0026lt;none\u0026gt; 7001/TCP,7002/TCP 22h app.kubernetes.io/instance=oudsm,app.kubernetes.io/name=oudsm NAME TYPE DATA AGE secret/default-token-w4jft kubernetes.io/service-account-token 3 32d secret/oudsm-creds opaque 2 22h secret/oudsm-token-ksr4g kubernetes.io/service-account-token 3 22h secret/sh.helm.release.v1.oudsm.v1 helm.sh/release.v1 1 22h secret/sh.helm.release.v1.oudsm.v2 helm.sh/release.v1 1 21h secret/sh.helm.release.v1.oudsm.v3 helm.sh/release.v1 1 19h NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE VOLUMEMODE persistentvolume/oudsm-pv 30Gi RWX Retain Bound myoudsmns/oudsm-pvc manual 22h Filesystem NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE VOLUMEMODE persistentvolumeclaim/oudsm-pvc Bound oudsm-pv 30Gi RWX manual 22h Filesystem NAME HOSTS ADDRESS PORTS AGE ingress.extensions/oudsm-ingress-nginx oudsm-1,oudsm-2,oudsm + 1 more... 100.102.51.230 80 19h Kubernetes Objects Kubernetes objects created by the Helm chart are detailed in the table below:\n   Type Name Example Name Purpose     Service Account \u0026lt;deployment/release name\u0026gt; my-oudsm Kubernetes Service Account for the Helm Chart deployment   Secret \u0026lt;deployment/release name\u0026gt;-creds my-oudsm-creds Secret object for Oracle Unified Directory Services Manager related critical values like passwords   Persistent Volume \u0026lt;deployment/release name\u0026gt;-pv my-oudsm-pv Persistent Volume for user_projects mount.   Persistent Volume Claim \u0026lt;deployment/release name\u0026gt;-pvc my-oudsm-pvc Persistent Volume Claim for user_projects mount.   Pod \u0026lt;deployment/release name\u0026gt;-N my-oudsm-1, my-oudsm-2, \u0026hellip; Pod(s)/Container(s) for Oracle Unified Directory Services Manager Instances   Service \u0026lt;deployment/release name\u0026gt;-N my-oudsm-1, my-oudsm-2, \u0026hellip; Service(s) for HTTP and HTTPS interfaces from Oracle Unified Directory Services Manager instance \u0026lt;deployment/release name\u0026gt;-N   Ingress \u0026lt;deployment/release name\u0026gt;-ingress-nginx my-oudsm-ingress-nginx Ingress Rules for HTTP and HTTPS interfaces.     In the table above, the Example Name for each Object is based on the value \u0026lsquo;my-oudsm\u0026rsquo; as the deployment/release name for the Helm chart installation.  Verify the Installation Ingress Controller Setup There are two types of Ingress controllers supported by this Helm chart. In the sub-sections below, configuration steps for each Controller are described.\nBy default Ingress configuration only supports HTTP and HTTPS Ports/Communication. To allow LDAP and LDAPS communication over TCP, configuration is required at Ingress Controller/Implementation level.\nIngress with NGINX Nginx-ingress controller implementation can be deployed/installed in Kubernetes environment.\nAdd Repo reference to helm for retriving/installing Chart for nginx-ingress implementation. $ helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx Command helm install to install nginx-ingress related objects like pod, service, deployment, etc. $ helm install --namespace default \\ --values nginx-ingress-values-override.yaml \\ lbr-nginx ingress-nginx/ingress-nginx  For more details about the helm command and parameters, please execute helm --help and helm install --help. The --values argument passes a file path/name which overrides values in the chart.  nginx-ingress-values-override.yaml\ncontroller: admissionWebhooks: enabled: false extraArgs: # The secret referred to by this flag contains the default certificate to be used when accessing the catch-all server. # If this flag is not provided NGINX will use a self-signed certificate. # If the TLS Secret is in different namespace, name can be mentioned as \u0026lt;namespace\u0026gt;/\u0026lt;tlsSecretName\u0026gt; default-ssl-certificate=myhelmns/my-oudsm-tls-cert service: # controller service external IP addresses externalIPs: - \u0026lt; External IP Address \u0026gt; # To configure Ingress Controller Service as LoadBalancer type of Service # Based on the Kubernetes configuration, External LoadBalancer would be linked to the Ingress Controller Service type: LoadBalancer # Configuration for NodePort to be used for Ports exposed through Ingress # If NodePorts are not defined/configured, Node Port would be assigned automatically by Kubernetes # These NodePorts are helpful while accessing services directly through Ingress and without having External Load Balancer. nodePorts: # For HTTP Interface exposed through LoadBalancer/Ingress http: 30080 # For HTTPS Interface exposed through LoadBalancer/Ingress https: 30443 Ingress with Voyager Voyager ingress implementation can be deployed/installed in Kubernetes environment.\nAdd Repo reference to helm for retrieving/installing Chart for voyager implementation. $ helm repo add appscode https://charts.appscode.com/stable Command helm install to install Voyager related objects like pod, service, deployment, etc. $ helm install --namespace default \\ --set cloudProvider=baremetal \\ voyager-operator appscode/voyager  For more details about the helm command and parameters, please execute helm --help and helm install --help.  Access to Interfaces through Ingress With the helm chart, Ingress objects are also created according to configuration. Following are the rules configured in Ingress object(s) for access to Oracle Unified Directory Services Manager Interfaces through Ingress.\n   Port NodePort Host Example Hostname Path Backend Service:Port Example Service Name:Port     http/https 30080/30443 \u0026lt;deployment/release name\u0026gt;-N my-oudsm-N * \u0026lt;deployment/release name\u0026gt;-N:http my-oudsm-1:http   http/https 30080/30443 * * /oudsm/console \u0026lt;deployment/release name\u0026gt;-lbr:http my-oudsm-lbr:http     In the table above, the Example Name for each Object is based on the value \u0026lsquo;my-oudsm\u0026rsquo; as the deployment/release name for the Helm chart installation. NodePort mentioned in the table are according to Ingress configuration described in previous section. When an External LoadBalancer is not available/configured, Interfaces can be accessed through NodePort on the Kubernetes Node.  Changes in /etc/hosts to validate hostname based Ingress rules In case, its not possible for you to have LoadBalancer configuration updated to have host names added for Oracle Unified Directory Services Manager Interfaces, following kind of entries can be added in /etc/hosts files on host from where Oracle Unified Directory Services Manager interfaces would be accessed.\n\u0026lt;IP Address of External LBR or Kubernetes Node\u0026gt;\tmy-oudsm my-oudsm-1 my-oudsm-2 my-oudsm-N  In the table above, the Example Name for each Object is based on the value \u0026lsquo;my-oudsm\u0026rsquo; as the deployment/release name for the Helm chart installation. When an External LoadBalancer is not available/configured, Interfaces can be accessed through NodePort on the Kubernetes Node.  Configuration Parameters The following table lists the configurable parameters of the Oracle Unified Directory Services Manager chart and their default values.\n   Parameter Description Default Value     replicaCount Number of Oracle Unified Directory Services Manager instances/pods/services to be created 1   restartPolicyName restartPolicy to be configured for each POD containing Oracle Unified Directory Services Manager instance OnFailure   image.repository Oracle Unified Directory Services Manager Image Registry/Repository and name. Based on this, image parameter would be configured for Oracle Unified Directory Services Manager pods/containers oracle/oudsm   image.tag Oracle Unified Directory Services Manager Image Tag. Based on this, image parameter would be configured for Oracle Unified Directory Services Manager pods/containers 12.2.1.4.0   image.pullPolicy policy to pull the image IfnotPresent   imagePullSecrets.name name of Secret resource containing private registry credentials regcred   nameOverride override the fullname with this name    fullnameOverride Overrides the fullname with the provided string    serviceAccount.create Specifies whether a service account should be created true   serviceAccount.name If not set and create is true, a name is generated using the fullname template oudsm-\u0026lt; fullname \u0026gt;-token-\u0026lt; randomalphanum \u0026gt;   podSecurityContext Security context policies to add to the controller pod    securityContext Security context policies to add by default    service.type type of controller service to create ClusterIP   nodeSelector node labels for pod assignment    tolerations node taints to tolerate    affinity node/pod affinities    ingress.enabled  true   ingress.type Supported value: either nginx or voyager nginx   ingress.host Hostname to be used with Ingress Rules. If not set, hostname would be configured according to fullname. Hosts would be configured as \u0026lt; fullname \u0026gt;-http.\u0026lt; domain \u0026gt;, \u0026lt; fullname \u0026gt;-http-0.\u0026lt; domain \u0026gt;, \u0026lt; fullname \u0026gt;-http-1.\u0026lt; domain \u0026gt;, etc.    ingress.domain Domain name to be used with Ingress Rules. In ingress rules, hosts would be configured as \u0026lt; host \u0026gt;.\u0026lt; domain \u0026gt;, \u0026lt; host \u0026gt;-0.\u0026lt; domain \u0026gt;, \u0026lt; host \u0026gt;-1.\u0026lt; domain \u0026gt;, etc.    ingress.backendPort  http   ingress.nginxAnnotations  { kubernetes.io/ingress.class: \u0026ldquo;nginx\u0026quot;nginx.ingress.kubernetes.io/affinity-mode: \u0026ldquo;persistent\u0026rdquo; nginx.ingress.kubernetes.io/affinity: \u0026ldquo;cookie\u0026rdquo; }   ingress.voyagerAnnotations  { kubernetes.io/ingress.class: \u0026ldquo;voyager\u0026rdquo; ingress.appscode.com/affinity: \u0026ldquo;cookie\u0026rdquo; ingress.appscode.com/type: \u0026ldquo;NodePort\u0026rdquo; }   ingress.voyagerNodePortHttp NodePort value for HTTP Port exposed through Voyager LoadBalancer Service 30080   ingress.voyagerNodePortHttps NodePort value for HTTPS Port exposed through Voyager LoadBalancer Service 30443   ingress.voyagerHttpPort Port value for HTTP Port exposed through Voyager LoadBalancer Service 80   ingress.voyagerHttpsPort Port value for HTTPS Port exposed through Voyager LoadBalancer Service 443   ingress.ingress.tlsSecret Secret name to use an already created TLS Secret. If such secret is not provided, one would be created with name \u0026lt; fullname \u0026gt;-tls-cert. If the TLS Secret is in different namespace, name can be mentioned as \u0026lt; namespace \u0026gt;/\u0026lt; tlsSecretName \u0026gt;    ingress.certCN Subject\u0026rsquo;s common name (cn) for SelfSigned Cert. \u0026lt; fullname \u0026gt;   ingress.certValidityDays Validity of Self-Signed Cert in days 365   secret.enabled If enabled it will use the secret created with base64 encoding. if value is false, secret would not be used and input values (through \u0026ndash;set, \u0026ndash;values, etc.) would be used while creation of pods. true   secret.name secret name to use an already created Secret oudsm-\u0026lt; fullname \u0026gt;-creds   secret.type Specifies the type of the secret Opaque   persistence.enabled If enabled, it will use the persistent volume. if value is false, PV and PVC would not be used and pods would be using the default emptyDir mount volume. true   persistence.pvname pvname to use an already created Persistent Volume , If blank will use the default name oudsm-\u0026lt; fullname \u0026gt;-pv   persistence.pvcname pvcname to use an already created Persistent Volume Claim , If blank will use default name oudsm-\u0026lt; fullname \u0026gt;-pvc   persistence.type supported values: either filesystem or networkstorage or custom filesystem   persistence.filesystem.hostPath.path The path location mentioned should be created and accessible from the local host provided with necessary privileges for the user. /scratch/shared/oudsm_user_projects   persistence.networkstorage.nfs.path Path of NFS Share location /scratch/shared/oudsm_user_projects   persistence.networkstorage.nfs.server IP or hostname of NFS Server 0.0.0.0   persistence.custom.* Based on values/data, YAML content would be included in PersistenceVolume Object    persistence.accessMode Specifies the access mode of the location provided ReadWriteMany   persistence.size Specifies the size of the storage 10Gi   persistence.storageClass Specifies the storageclass of the persistence volume. empty   persistence.annotations specifies any annotations that will be used { }   ingress.voyagerNodePortHttp  31080   ingress.voyagerNodePortHttps  31443   ingress.voyagerHttpPort  80   ingress.voyagerHttpsPort  443   oudsm.adminUser Weblogic Administration User weblogic   oudsm.adminPass Password for Weblogic Administration User    oudsm.startupTime Expected startup time. After specified seconds readinessProbe would start 900   oudsm.livenessProbeInitialDelay Paramter to decide livenessProbe initialDelaySeconds 1200   elk.elasticsearch.enabled If enabled it will create the elastic search statefulset deployment false   elk.elasticsearch.image.repository Elastic Search Image name/Registry/Repository . Based on this elastic search instances will be created docker.elastic.co/elasticsearch/elasticsearch   elk.elasticsearch.image.tag Elastic Search Image tag .Based on this, image parameter would be configured for Elastic Search pods/instances 6.4.3   elk.elasticsearch.image.pullPolicy policy to pull the image IfnotPresent   elk.elasticsearch.esreplicas Number of Elastic search Instances will be created 3   elk.elasticsearch.minimumMasterNodes The value for discovery.zen.minimum_master_nodes. Should be set to (esreplicas / 2) + 1. 2   elk.elasticsearch.esJAVAOpts Java options for Elasticsearch. This is where you should configure the jvm heap size -Xms512m -Xmx512m   elk.elasticsearch.sysctlVmMaxMapCount Sets the sysctl vm.max_map_count needed for Elasticsearch 262144   elk.elasticsearch.resources.requests.cpu cpu resources requested for the elastic search 100m   elk.elasticsearch.resources.limits.cpu total cpu limits that are configures for the elastic search 1000m   elk.elasticsearch.esService.type Type of Service to be created for elastic search ClusterIP   elk.elasticsearch.esService.lbrtype Type of load balancer Service to be created for elastic search ClusterIP   elk.kibana.enabled If enabled it will create a kibana deployment false   elk.kibana.image.repository Kibana Image Registry/Repository and name. Based on this Kibana instance will be created docker.elastic.co/kibana/kibana   elk.kibana.image.tag Kibana Image tag. Based on this, Image parameter would be configured. 6.4.3   elk.kibana.image.pullPolicy policy to pull the image IfnotPresent   elk.kibana.kibanaReplicas Number of Kibana instances will be created 1   elk.kibana.service.tye Type of service to be created NodePort   elk.kibana.service.targetPort Port on which the kibana will be accessed 5601   elk.kibana.service.nodePort nodePort is the port on which kibana service will be accessed from outside 31119   elk.logstash.enabled If enabled it will create a logstash deployment false   elk.logstash.image.repository logstash Image Registry/Repository and name. Based on this logstash instance will be created logstash   elk.logstash.image.tag logstash Image tag. Based on this, Image parameter would be configured. 6.6.0   elk.logstash.image.pullPolicy policy to pull the image IfnotPresent   elk.logstash.containerPort Port on which the logstash container will be running 5044   elk.logstash.service.tye Type of service to be created NodePort   elk.logstash.service.targetPort Port on which the logstash will be accessed 9600   elk.logstash.service.nodePort nodePort is the port on which logstash service will be accessed from outside 32222   elk.logstash.logstashConfigMap Provide the configmap name which is already created with the logstash conf. if empty default logstash configmap will be created and used    elk.elkPorts.rest Port for REST 9200   elk.elkPorts.internode port used for communication between the nodes 9300   elk.busybox.image busy box image name. Used for initcontianers busybox   elk.elkVolume.enabled If enabled, it will use the persistent volume. if value is false, PV and pods would be using the default emptyDir mount volume. true   elk.elkVolume.pvname pvname to use an already created Persistent Volume , If blank will use the default name oudsm-\u0026lt; fullname \u0026gt;-espv   elk.elkVolume.type supported values: either filesystem or networkstorage or custom filesystem   elk.elkVolume.filesystem.hostPath.path The path location mentioned should be created and accessible from the local host provided with necessary privileges for the user. /scratch/shared/oud_elk/data   elk.elkVolume.networkstorage.nfs.path Path of NFS Share location /scratch/shared/oud_elk/data   elk.elkVolume.networkstorage.nfs.server IP or hostname of NFS Server 0.0.0.0   elk.elkVolume.custom.* Based on values/data, YAML content would be included in PersistenceVolume Object    elk.elkVolume.accessMode Specifies the access mode of the location provided ReadWriteMany   elk.elkVolume.size Specifies the size of the storage 20Gi   elk.elkVolume.storageClass Specifies the storageclass of the persistence volume. elk   elk.elkVolume.annotations specifies any annotations that will be used { }    "
},
{
	"uri": "/fmw-kubernetes/20.4.1/oud/manage-oud-containers/logging-and-visualization/",
	"title": "a) Logging and Visualization for Helm Chart oud-ds-rs Deployment",
	"tags": [],
	"description": "Describes the steps for logging and visualization with Elasticsearch and Kibana.",
	"content": " Introduction Installation  Enable Elasticsearch, Logstash, and Kibana Create Data Mount Points Configure Logstash Install or Upgrade Oracle Unified Directory Container with ELK Configuration Configure ElasticSearch   Verify Using the Kibana Application  Introduction This section describes how to install and configure logging and visualization for the oud-ds-rs Helm Chart deployment.\nThe ELK stack consists of Elasticsearch, Logstash, and Kibana. Using ELK we can gain insights in real-time from the log data from your applications.\n Elasticsearch is a distributed, RESTful search and analytics engine capable of solving a growing number of use cases. As the heart of the Elastic Stack, it centrally stores your data so you can discover the expected and uncover the unexpected. Logstash is an open source, server-side data processing pipeline that ingests data from a multitude of sources simultaneously, transforms it, and then sends it to your favorite “stash.” Kibana lets you visualize your Elasticsearch data and navigate the Elastic Stack. It gives you the freedom to select the way you give shape to your data. And you don’t always have to know what you\u0026rsquo;re looking for.  Installation ELK can be enabled for environments created using the Helm charts provided with this project. The example below will demonstrate installation and configuration of ELK for the oud-ds-rs chart.\nEnable Elasticsearch, Logstash, and Kibana Edit logging-override-values.yaml and set the enabled flag for each component to \u0026lsquo;true\u0026rsquo;.\nelk: elasticsearch: enabled: true ... kibana: enabled: true ... logstash: enabled: true ... elkVolume: # If enabled, it will use the persistent volume. # if value is false, PV and PVC would not be used and there would not be any mount point available for config enabled: true type: networkstorage networkstorage: nfs: server: myserver path: /scratch/oud_elk_data Note: if elkVolume.enabled is set to \u0026lsquo;true\u0026rsquo; you should supply a directory for the ELK log files. The userid for the directory can be anything but it must have uid:guid as 1000:1000, which is the same as the ‘oracle’ user running in the container. This ensures the ‘oracle’ user has access to the shared volume/directory.\nInstall or Upgrade Oracle Unified Directory Container with ELK Configuration If you have not installed the oud-ds-rs chart then you should install with the following command, picking up the ELK configuration from the previous steps:\n$ helm install --namespace \u0026lt;namespace\u0026gt; --values \u0026lt;valuesfile.yaml\u0026gt; \u0026lt;releasename\u0026gt; oud-ds-rs For example:\n$ helm install --namespace myhelmns --values logging-override-values.yaml my-oud-ds-rs oud-ds-rs If the oud-ds-rs chart is already installed then update the configuration with the ELK configuration from the previous steps:\n$ helm upgrade --namespace \u0026lt;namespace\u0026gt; --values \u0026lt;valuesfile.yaml\u0026gt; \u0026lt;releasename\u0026gt; oud-ds-rs For example:\n$ helm upgrade --namespace myhelmns --values logging-override-values.yaml my-oud-ds-rs oud-ds-rs Configure ElasticSearch List the PODs in your namespace:\n$ kubectl get pods -o wide -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl get pods -o wide -n myhelmns Output will be similar to the following:\n$ kubectl get pods -o wide -n myhelmns NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES my-oud-ds-rs-0 1/1 Running 0 39m 10.244.1.107 10.89.73.203 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; my-oud-ds-rs-1 1/1 Running 0 39m 10.244.1.108 10.89.73.203 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; my-oud-ds-rs-2 1/1 Running 0 39m 10.244.1.106 10.89.73.203 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; my-oud-ds-rs-es-cluster-0 1/1 Running 0 39m 10.244.1.109 10.89.73.203 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; my-oud-ds-rs-kibana-665f9d5fb-pmz4v 1/1 Running 0 39m 10.244.1.110 10.89.73.203 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; my-oud-ds-rs-logstash-756fd7c5f5-kvwrw 1/1 Running 0 39m 10.244.2.103 10.89.73.204 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; From this, identify the ElastiSearch POD, my-oud-ds-rs-es-cluster-0.\nRun the port-forward command to allow ElasticSearch to be listening on port 9200:\n$ kubectl port-forward oud-ds-rs-es-cluster-0 9200:9200 --namespace=\u0026lt;namespace\u0026gt; \u0026amp; For example:\n$ kubectl port-forward my-oud-ds-rs-es-cluster-0 9200:9200 --namespace=myhelmns \u0026amp; [1] 98458 bash-4.2$ Forwarding from 127.0.0.1:9200 -\u0026gt; 9200 Forwarding from [::1]:9200 -\u0026gt; 9200 Verify that ElasticSearch is running by interrogating port 9200:\n$ curl http://localhost:9200 Handling connection for 9200 { \u0026quot;name\u0026quot; : \u0026quot;mike-oud-ds-rs-es-cluster-0\u0026quot;, \u0026quot;cluster_name\u0026quot; : \u0026quot;OUD-elk\u0026quot;, \u0026quot;cluster_uuid\u0026quot; : \u0026quot;H2EBtAlJQUGpV6IkS46Yzw\u0026quot;, \u0026quot;version\u0026quot; : { \u0026quot;number\u0026quot; : \u0026quot;6.4.3\u0026quot;, \u0026quot;build_flavor\u0026quot; : \u0026quot;default\u0026quot;, \u0026quot;build_type\u0026quot; : \u0026quot;tar\u0026quot;, \u0026quot;build_hash\u0026quot; : \u0026quot;fe40335\u0026quot;, \u0026quot;build_date\u0026quot; : \u0026quot;2018-10-30T23:17:19.084789Z\u0026quot;, \u0026quot;build_snapshot\u0026quot; : false, \u0026quot;lucene_version\u0026quot; : \u0026quot;7.4.0\u0026quot;, \u0026quot;minimum_wire_compatibility_version\u0026quot; : \u0026quot;5.6.0\u0026quot;, \u0026quot;minimum_index_compatibility_version\u0026quot; : \u0026quot;5.0.0\u0026quot; }, \u0026quot;tagline\u0026quot; : \u0026quot;You Know, for Search\u0026quot; } Verify Using the Kibana Application List the Kibana application service using the following command:\n$ kubectl get svc -o wide -n \u0026lt;namespace\u0026gt; | grep kibana For example:\n$ kubectl get svc -o wide -n myhelmns | grep kibana Output will be similar to the following:\nmy-oud-ds-rs-kibana NodePort 10.103.169.218 \u0026lt;none\u0026gt; 5601:31199/TCP 67m app=kibana In this example, the port to access Kibana application via a Web browser will be 31199.\nEnter the following URL in a browser to access the Kibana application:\nhttp://\u0026lt;hostname\u0026gt;:\u0026lt;NodePort\u0026gt;/app/kibana\nFor example:\nhttp://myserver:31199/app/kibana\nFrom the Kibana Portal navigate to:\nManagement -\u0026gt; Index Patterns\nCreate an Index Pattern using the pattern \u0026lsquo;*\u0026rsquo;\nNavigate to Discover : from here you should be able to see logs from the Oracle Unified Directory environment.\n"
},
{
	"uri": "/fmw-kubernetes/20.4.1/oudsm/manage-oudsm-containers/logging-and-visualization/",
	"title": "a) Logging and Visualization for Helm Chart oudsm Deployment",
	"tags": [],
	"description": "Describes the steps for logging and visualization with Elasticsearch and Kibana.",
	"content": " Introduction Installation  Enable Elasticsearch, Logstash, and Kibana Create Data Mount Points Configure Logstash Install or Upgrade Oracle Unified Directory Services Manager Container with ELK Configuration Configure ElasticSearch   Verify Using the Kibana Application  Introduction This section describes how to install and configure logging and visualization for the oudsm Helm Chart deployment.\nThe ELK stack consists of Elasticsearch, Logstash, and Kibana. Using ELK we can gain insights in real-time from the log data from your applications.\n Elasticsearch is a distributed, RESTful search and analytics engine capable of solving a growing number of use cases. As the heart of the Elastic Stack, it centrally stores your data so you can discover the expected and uncover the unexpected. Logstash is an open source, server-side data processing pipeline that ingests data from a multitude of sources simultaneously, transforms it, and then sends it to your favorite “stash.” Kibana lets you visualize your Elasticsearch data and navigate the Elastic Stack. It gives you the freedom to select the way you give shape to your data. And you don’t always have to know what you\u0026rsquo;re looking for.  Installation ELK can be enabled for environments created using the Helm charts provided with this project. The example below will demonstrate installation and configuration of ELK for the oudsm chart.\nEdit logging-override-values.yaml and set the enabled flag for each component to \u0026lsquo;true\u0026rsquo;.\nelk: elasticsearch: enabled: true ... kibana: enabled: true ... logstash: enabled: true ... elkVolume: # If enabled, it will use the persistent volume. # if value is false, PV and PVC would not be used and there would not be any mount point available for config enabled: true type: networkstorage networkstorage: nfs: server: myserver path: /scratch/oud_elk_data Note: If elkVolume.enabled is set to \u0026lsquo;true\u0026rsquo; you should supply a directory for the ELK log files. The userid for the directory can be anything but it must have uid:guid as 1000:1000, which is the same as the ‘oracle’ user running in the container. This ensures the ‘oracle’ user has access to the shared volume/directory.\nInstall or Upgrade Oracle Unified Directory Services Manager Container with ELK Configuration If you have not installed the oudsm chart then you should install with the following command, picking up the ELK configuration from the previous steps:\n$ helm install --namespace \u0026lt;namespace\u0026gt; --values \u0026lt;valuesfile.yaml\u0026gt; \u0026lt;releasename\u0026gt; oudsm For example:\n$ helm install --namespace myhelmns --values logging-override-values.yaml my-oud-ds-rs oudsm If the oudsm chart is already installed then update the configuration with the ELK configuration from the previous steps:\n$ helm upgrade --namespace \u0026lt;namespace\u0026gt; --values \u0026lt;valuesfile.yaml\u0026gt; \u0026lt;releasename\u0026gt; oudsm For example:\n$ helm upgrade --namespace myhelmns --values logging-override-values.yaml my-oud-ds-rs oudsm Configure ElasticSearch List the PODs in your namespace:\n$ kubectl get pods -o wide -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl get pods -o wide -n myhelmns Output will be similar to the following:\n$ kubectl get pods -o wide -n myhelmns NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES my-oudsm-1 1/1 Running 0 19m 10.244.1.66 10.89.73.203 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; my-oudsm-es-cluster-0 1/1 Running 0 19m 10.244.1.69 10.89.73.203 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; my-oudsm-es-cluster-1 1/1 Running 0 18m 10.244.2.125 10.89.73.204 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; my-oudsm-es-cluster-2 1/1 Running 0 17m 10.244.1.70 10.89.73.203 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; my-oudsm-kibana-6bbd487d66-dr662 1/1 Running 0 19m 10.244.1.68 10.89.73.203 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; my-oudsm-logstash-56f4665997-vbx4q 1/1 Running 0 19m 10.244.1.67 10.89.73.203 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; From this, identify the ElastiSearch POD, my-oudsm-es-cluster-0.\nRun the port-forward command to allow ElasticSearch to be listening on port 9200:\n$ kubectl port-forward my-oudsm-es-cluster-0 9200:9200 --namespace=\u0026lt;namespace\u0026gt; \u0026amp; For example:\n$ kubectl port-forward my-oudsm-es-cluster-0 9200:9200 --namespace=myhelmns \u0026amp; [1] 98458 bash-4.2$ Forwarding from 127.0.0.1:9200 -\u0026gt; 9200 Forwarding from [::1]:9200 -\u0026gt; 9200 Verify that ElasticSearch is running by interrogating port 9200:\n$ curl http://localhost:9200 Handling connection for 9200 { \u0026quot;name\u0026quot; : \u0026quot;my-oudsm-es-cluster-0\u0026quot;, \u0026quot;cluster_name\u0026quot; : \u0026quot;OUD-elk\u0026quot;, \u0026quot;cluster_uuid\u0026quot; : \u0026quot;w5LKK98RRp-LMoCGA2AnsA\u0026quot;, \u0026quot;version\u0026quot; : { \u0026quot;number\u0026quot; : \u0026quot;6.4.3\u0026quot;, \u0026quot;build_flavor\u0026quot; : \u0026quot;default\u0026quot;, \u0026quot;build_type\u0026quot; : \u0026quot;tar\u0026quot;, \u0026quot;build_hash\u0026quot; : \u0026quot;fe40335\u0026quot;, \u0026quot;build_date\u0026quot; : \u0026quot;2018-10-30T23:17:19.084789Z\u0026quot;, \u0026quot;build_snapshot\u0026quot; : false, \u0026quot;lucene_version\u0026quot; : \u0026quot;7.4.0\u0026quot;, \u0026quot;minimum_wire_compatibility_version\u0026quot; : \u0026quot;5.6.0\u0026quot;, \u0026quot;minimum_index_compatibility_version\u0026quot; : \u0026quot;5.0.0\u0026quot; }, \u0026quot;tagline\u0026quot; : \u0026quot;You Know, for Search\u0026quot; } Verify Using the Kibana Application List the Kibana application service using the following command:\n$ kubectl get svc -o wide -n \u0026lt;namespace\u0026gt; | grep kibana For example:\n$ kubectl get svc -o wide -n myhelmns | grep kibana Output will be similar to the following:\nmy-oudsm-kibana NodePort 10.103.92.84 \u0026lt;none\u0026gt; 5601:31199/TCP 21m app=kibana In this example, the port to access Kibana application via a Web browser will be 31199.\nEnter the following URL in a browser to access the Kibana application:\nhttp://\u0026lt;hostname\u0026gt;:\u0026lt;NodePort\u0026gt;/app/kibana\nFor example:\nhttp://myserver:31199/app/kibana\nFrom the Kibana Portal navigate to:\nManagement -\u0026gt; Index Patterns\nCreate an Index Pattern using the pattern \u0026lsquo;*\u0026rsquo;\nNavigate to Discover : from here you should be able to see logs from the Oracle Unified Directory Services Manager environment.\n"
},
{
	"uri": "/fmw-kubernetes/20.4.1/oud/manage-oud-containers/monitoring-oud-instance/",
	"title": "b) Monitoring an Oracle Unified Directory Instance",
	"tags": [],
	"description": "Describes the steps for Monitoring the Oracle Unified Directory environment.",
	"content": " Introduction Install Prometheus and Grafana  Create a Kubernetes Namespace Add Prometheus and Grafana Helm Repositories Install the Prometheus Operator View Prometheus and Grafana Objects Created Add the NodePort   Verify Using Grafana GUI  Introduction After the Oracle Unified Directory instance is set up you can monitor it using Prometheus and Grafana.\nInstall Prometheus and Grafana Create a Kubernetes Namespace Create a Kubernetes namespace to provide a scope for Prometheus and Grafana objects such as pods and services that you create in the environment. To create your namespace issue the following command:\n$ kubectl create ns mypgns namespace/mypgns created Add Prometheus and Grafana Helm Repositories Add the Prometheus and Grafana Helm repositories by issuing the following commands:\n$ helm repo add prometheus https://prometheus-community.github.io/helm-charts \u0026quot;prometheus\u0026quot; has been added to your repositories $ helm repo update Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \u0026quot;prometheus\u0026quot; chart repository Update Complete. Happy Helming! $ Install the Prometheus Operator Install the Prometheus Operator using the helm command:\n$ helm install \u0026lt;release_name\u0026gt; prometheus/kube-prometheus-stack grafana.adminPassword=\u0026lt;password\u0026gt; -n \u0026lt;namespace\u0026gt; For example:\n$ helm install mypg prometheus/kube-prometheus-stack grafana.adminPassword=\u0026lt;password\u0026gt; -n mypgns Output should be similar to the following:\nNAME: mypg LAST DEPLOYED: Mon Oct 12 02:05:41 2020 NAMESPACE: mypgns STATUS: deployed REVISION: 1 NOTES: kube-prometheus-stack has been installed. Check its status by running: kubectl --namespace mypgns get pods -l \u0026quot;release=mypg\u0026quot; Visit https://github.com/prometheus-operator/kube-prometheus for instructions on how to create \u0026amp; configure Alertmanager and Prometheus instances using the Operator. View Prometheus and Grafana Objects Created View the objects created for Prometheus and Grafana by issuing the following command:\n$ kubectl get all,service,pod -o wide -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl get all,service,pod -o wide -n mypgns NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/alertmanager-mypg-kube-prometheus-stack-alertmanager-0 2/2 Running 0 25m 10.244.1.25 10.89.73.203 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/mypg-grafana-b7d4fbfb-jzccm 2/2 Running 0 25m 10.244.2.140 10.89.73.204 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/mypg-kube-prometheus-stack-operator-7fb485bbcd-lbh9d 2/2 Running 0 25m 10.244.2.139 10.89.73.204 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/mypg-kube-state-metrics-86dfdf9c75-nvbss 1/1 Running 0 25m 10.244.1.146 10.89.73.203 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/mypg-prometheus-node-exporter-29dzd 1/1 Running 0 25m 10.244.2.141 10.89.73.204 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/prometheus-mypg-kube-prometheus-stack-prometheus-0 3/3 Running 0 25m 10.244.2.140 10.89.73.203 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service/alertmanager-operated ClusterIP None \u0026lt;none\u0026gt; 9093/TCP,9094/TCP,9094/UDP 25m app=alertmanager service/mypg-grafana ClusterIP 10.111.28.76 \u0026lt;none\u0026gt; 80/TCP 25m app.kubernetes.io/instance=mypg,app.kubernetes.io/name=grafana service/mypg-kube-prometheus-stack-alertmanager ClusterIP 10.103.83.97 \u0026lt;none\u0026gt; 9093/TCP 25m alertmanager=mypg-kube-prometheus-stack-alertmanager,app=alertmanager service/mypg-kube-prometheus-stack-operator ClusterIP 10.110.216.204 \u0026lt;none\u0026gt; 8080/TCP,443/TCP 25m app=kube-prometheus-stack-operator,release=mypg service/mypg-kube-prometheus-stack-prometheus ClusterIP 10.104.11.9 \u0026lt;none\u0026gt; 9090/TCP 25m app=prometheus,prometheus=mypg-kube-prometheus-stack-prometheus service/mypg-kube-state-metrics ClusterIP 10.109.172.125 \u0026lt;none\u0026gt; 8080/TCP 25m app.kubernetes.io/instance=mypg,app.kubernetes.io/name=kube-state-metrics service/mypg-prometheus-node-exporter ClusterIP 10.110.249.92 \u0026lt;none\u0026gt; 9100/TCP 25m app=prometheus-node-exporter,release=mypg service/prometheus-operated ClusterIP None \u0026lt;none\u0026gt; 9090/TCP 25m app=prometheus NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE CONTAINERS IMAGES SELECTOR daemonset.apps/mypg-prometheus-node-exporter 3 3 0 3 0 \u0026lt;none\u0026gt; 25m node-exporter quay.io/prometheus/node-exporter:v1.0.1 app=prometheus-node-exporter,release=mypg NAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTOR deployment.apps/mypg-grafana 1/1 1 1 25m grafana-sc-dashboard,grafana kiwigrid/k8s-sidecar:0.1.209,grafana/grafana:7.2.0 app.kubernetes.io/instance=mypg,app.kubernetes.io/name=grafana deployment.apps/mypg-kube-prometheus-stack-operator 1/1 1 1 25m kube-prometheus-stack,tls-proxy quay.io/prometheus-operator/prometheus-operator:v0.42.1,squareup/ghostunnel:v1.5.2 app=kube-prometheus-stack-operator,release=mypg deployment.apps/mypg-kube-state-metrics 1/1 1 1 25m kube-state-metrics quay.io/coreos/kube-state-metrics:v1.9.7 app.kubernetes.io/name=kube-state-metrics NAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTOR replicaset.apps/mypg-grafana-b7d4fbfb 1 1 1 25m grafana-sc-dashboard,grafana kiwigrid/k8s-sidecar:0.1.209,grafana/grafana:7.2.0 app.kubernetes.io/instance=mypg,app.kubernetes.io/name=grafana,pod-template-hash=b7d4fbfb replicaset.apps/mypg-kube-prometheus-stack-operator-7fb485bbcd 1 1 1 25m kube-prometheus-stack,tls-proxy quay.io/prometheus-operator/prometheus-operator:v0.42.1,squareup/ghostunnel:v1.5.2 app=kube-prometheus-stack-operator,pod-template-hash=7fb485bbcd,release=mypg replicaset.apps/mypg-kube-state-metrics-86dfdf9c75 1 1 1 25m kube-state-metrics quay.io/coreos/kube-state-metrics:v1.9.7 app.kubernetes.io/name=kube-state-metrics,pod-template-hash=86dfdf9c75 NAME READY AGE CONTAINERS IMAGES statefulset.apps/alertmanager-mypg-kube-prometheus-stack-alertmanager 1/1 25m alertmanager,config-reloader quay.io/prometheus/alertmanager:v0.21.0,jimmidyson/configmap-reload:v0.4.0 statefulset.apps/prometheus-mypg-kube-prometheus-stack-prometheus 0/1 25m prometheus,prometheus-config-reloader,rules-configmap-reloader quay.io/prometheus/prometheus:v2.21.0,quay.io/prometheus-operator/prometheus-config-reloader:v0.42.1,docker.io/jimmidyson/configmap-reload:v0.4.0 Add the NodePort Edit the grafana service to add the NodePort in the service.nodeport=\u0026lt;nodeport\u0026gt; and type=NodePort and save:\n$ kubectl edit service/prometheus-grafana -n \u0026lt;namespace\u0026gt; ports: - name: service nodePort: 30091 port: 80 protocol: TCP targetPort: 3000 selector: app.kubernetes.io/instance: prometheus-operator app.kubernetes.io/name: grafana sessionAffinity: None type: NodePort Verify Using Grafana GUI Access the Grafana GUI using http://\u0026lt;HostIP\u0026gt;:\u0026lt;nodeport\u0026gt; with the default username=admin and password=grafana.adminPassword:\nCheck the Prometheus datasource from the DataSource pane:\nAdd the customized k8cluster view dashboard json to view the cluster monitoring dashboard, by importing the following json file.\nDownload the JSON file from monitoring a Kubernetes cluster using Prometheus from https://grafana.com/grafana/dashboards/10856. Import the downloaded json using the import option.\nVerify your installation by viewing some of the customized dashboard views.\n"
},
{
	"uri": "/fmw-kubernetes/20.4.1/oudsm/manage-oudsm-containers/monitoring-oudsm-instance/",
	"title": "b) Monitoring an Oracle Unified Directory Services Manager Instance",
	"tags": [],
	"description": "Describes the steps for Monitoring the Oracle Unified Directory Services Manager environment.",
	"content": " Introduction Install Prometheus and Grafana  Create a Kubernetes Namespace Add Prometheus and Grafana Helm Repositories Install the Prometheus Operator View Prometheus and Grafana Objects Created Add the NodePort   Verify Using Grafana GUI  Introduction After the Oracle Unified Directory Services Manager instance is set up you can monitor it using Prometheus and Grafana.\nInstall Prometheus and Grafana Create a Kubernetes Namespace Create a Kubernetes namespace to provide a scope for Prometheus and Grafana objects such as pods and services that you create in the environment. To create your namespace issue the following command:\n$ kubectl create ns mypgns namespace/mypgns created Add Prometheus and Grafana Helm Repositories Add the Prometheus and Grafana Helm repositories by issuing the following commands:\n$ helm repo add prometheus https://prometheus-community.github.io/helm-charts \u0026quot;prometheus\u0026quot; has been added to your repositories $ helm repo update Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \u0026quot;prometheus\u0026quot; chart repository Update Complete. Happy Helming! $ Install the Prometheus Operator Install the Prometheus Operator using the helm command:\n$ helm install \u0026lt;release_name\u0026gt; prometheus/kube-prometheus-stack grafana.adminPassword=\u0026lt;password\u0026gt; -n \u0026lt;namespace\u0026gt; For example:\n$ helm install mypg prometheus/kube-prometheus-stack grafana.adminPassword=\u0026lt;password\u0026gt; -n mypgns Output should be similar to the following:\nNAME: mypg LAST DEPLOYED: Mon Oct 12 02:05:41 2020 NAMESPACE: mypgns STATUS: deployed REVISION: 1 NOTES: kube-prometheus-stack has been installed. Check its status by running: kubectl --namespace mypgns get pods -l \u0026quot;release=mypg\u0026quot; Visit https://github.com/prometheus-operator/kube-prometheus for instructions on how to create \u0026amp; configure Alertmanager and Prometheus instances using the Operator. View Prometheus and Grafana Objects Created View the objects created for Prometheus and Grafana by issuing the following command:\n$ kubectl get all,service,pod -o wide -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl get all,service,pod -o wide -n mypgns NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/alertmanager-mypg-kube-prometheus-stack-alertmanager-0 2/2 Running 0 25m 10.244.1.25 10.89.73.203 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/mypg-grafana-b7d4fbfb-jzccm 2/2 Running 0 25m 10.244.2.140 10.89.73.204 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/mypg-kube-prometheus-stack-operator-7fb485bbcd-lbh9d 2/2 Running 0 25m 10.244.2.139 10.89.73.204 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/mypg-kube-state-metrics-86dfdf9c75-nvbss 1/1 Running 0 25m 10.244.1.146 10.89.73.203 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/mypg-prometheus-node-exporter-29dzd 1/1 Running 0 25m 10.244.2.141 10.89.73.204 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/prometheus-mypg-kube-prometheus-stack-prometheus-0 3/3 Running 0 25m 10.244.2.142 10.89.73.203 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service/alertmanager-operated ClusterIP None \u0026lt;none\u0026gt; 9093/TCP,9094/TCP,9094/UDP 25m app=alertmanager service/mypg-grafana ClusterIP 10.111.28.76 \u0026lt;none\u0026gt; 80/TCP 25m app.kubernetes.io/instance=mypg,app.kubernetes.io/name=grafana service/mypg-kube-prometheus-stack-alertmanager ClusterIP 10.103.83.97 \u0026lt;none\u0026gt; 9093/TCP 25m alertmanager=mypg-kube-prometheus-stack-alertmanager,app=alertmanager service/mypg-kube-prometheus-stack-operator ClusterIP 10.110.216.204 \u0026lt;none\u0026gt; 8080/TCP,443/TCP 25m app=kube-prometheus-stack-operator,release=mypg service/mypg-kube-prometheus-stack-prometheus ClusterIP 10.104.11.9 \u0026lt;none\u0026gt; 9090/TCP 25m app=prometheus,prometheus=mypg-kube-prometheus-stack-prometheus service/mypg-kube-state-metrics ClusterIP 10.109.172.125 \u0026lt;none\u0026gt; 8080/TCP 25m app.kubernetes.io/instance=mypg,app.kubernetes.io/name=kube-state-metrics service/mypg-prometheus-node-exporter ClusterIP 10.110.249.92 \u0026lt;none\u0026gt; 9100/TCP 25m app=prometheus-node-exporter,release=mypg service/prometheus-operated ClusterIP None \u0026lt;none\u0026gt; 9090/TCP 25m app=prometheus NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE CONTAINERS IMAGES SELECTOR daemonset.apps/mypg-prometheus-node-exporter 3 3 0 3 0 \u0026lt;none\u0026gt; 25m node-exporter quay.io/prometheus/node-exporter:v1.0.1 app=prometheus-node-exporter,release=mypg NAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTOR deployment.apps/mypg-grafana 1/1 1 1 25m grafana-sc-dashboard,grafana kiwigrid/k8s-sidecar:0.1.209,grafana/grafana:7.2.0 app.kubernetes.io/instance=mypg,app.kubernetes.io/name=grafana deployment.apps/mypg-kube-prometheus-stack-operator 1/1 1 1 25m kube-prometheus-stack,tls-proxy quay.io/prometheus-operator/prometheus-operator:v0.42.1,squareup/ghostunnel:v1.5.2 app=kube-prometheus-stack-operator,release=mypg deployment.apps/mypg-kube-state-metrics 1/1 1 1 25m kube-state-metrics quay.io/coreos/kube-state-metrics:v1.9.7 app.kubernetes.io/name=kube-state-metrics NAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTOR replicaset.apps/mypg-grafana-b7d4fbfb 1 1 1 25m grafana-sc-dashboard,grafana kiwigrid/k8s-sidecar:0.1.209,grafana/grafana:7.2.0 app.kubernetes.io/instance=mypg,app.kubernetes.io/name=grafana,pod-template-hash=b7d4fbfb replicaset.apps/mypg-kube-prometheus-stack-operator-7fb485bbcd 1 1 1 25m kube-prometheus-stack,tls-proxy quay.io/prometheus-operator/prometheus-operator:v0.42.1,squareup/ghostunnel:v1.5.2 app=kube-prometheus-stack-operator,pod-template-hash=7fb485bbcd,release=mypg replicaset.apps/mypg-kube-state-metrics-86dfdf9c75 1 1 1 25m kube-state-metrics quay.io/coreos/kube-state-metrics:v1.9.7 app.kubernetes.io/name=kube-state-metrics,pod-template-hash=86dfdf9c75 NAME READY AGE CONTAINERS IMAGES statefulset.apps/alertmanager-mypg-kube-prometheus-stack-alertmanager 1/1 25m alertmanager,config-reloader quay.io/prometheus/alertmanager:v0.21.0,jimmidyson/configmap-reload:v0.4.0 statefulset.apps/prometheus-mypg-kube-prometheus-stack-prometheus 0/1 25m prometheus,prometheus-config-reloader,rules-configmap-reloader quay.io/prometheus/prometheus:v2.21.0,quay.io/prometheus-operator/prometheus-config-reloader:v0.42.1,docker.io/jimmidyson/configmap-reload:v0.4.0 Add the NodePort Edit the grafana service to add the NodePort in the service.nodeport=\u0026lt;nodeport\u0026gt; and type=NodePort and save:\n$ kubectl edit service/prometheus-grafana -n \u0026lt;namespace\u0026gt; ports: - name: service nodePort: 30091 port: 80 protocol: TCP targetPort: 3000 selector: app.kubernetes.io/instance: prometheus-operator app.kubernetes.io/name: grafana sessionAffinity: None type: NodePort Verify Using Grafana GUI Access the Grafana GUI using http://\u0026lt;HostIP\u0026gt;:\u0026lt;nodeport\u0026gt; with the default username=admin and password=grafana.adminPassword:\nCheck the Prometheus datasource from the DataSource pane:\nAdd the customized k8cluster view dashboard json to view the cluster monitoring dashboard, by importing the following json file.\nDownload the JSON file from monitoring a Kubernetes cluster using Prometheus from https://grafana.com/grafana/dashboards/10856. Import the downloaded json using the import option.\nVerify your installation by viewing some of the customized dashboard views.\n"
},
{
	"uri": "/fmw-kubernetes/20.4.1/oud/patch-and-upgrade/upgrade_a_kubernetes_cluster/",
	"title": "a) Upgrade a Kubernetes cluster",
	"tags": [],
	"description": "Instructions on how to upgrade a Kubernetes cluster.",
	"content": "These instructions describe how to upgrade a Kubernetes cluster created using kubeadm on which an Oracle Unified Directory instance is deployed. A rolling upgrade approach is used to upgrade nodes (master and worker) of the Kubernetes cluster.\nIt is expected that there will be a down time during the upgrade of the Kubernetes cluster as the nodes need to be drained as part of the upgrade process.\n Prerequisites  Review Prerequisites and ensure that your Kubernetes cluster is ready for upgrade. Make sure your environment meets all prerequisites.  Upgrade the Kubernetes version An upgrade of Kubernetes is supported from one MINOR version to the next MINOR version, or between PATCH versions of the same MINOR. For example, you can upgrade from 1.x to 1.x+1, but not from 1.x to 1.x+2. To upgrade a Kubernetes version, first all the master nodes of the Kubernetes cluster must be upgraded sequentially, followed by the sequential upgrade of each worker node.\n See here for Kubernetes official documentation to upgrade from v1.16.x to v1.17.x. See here for Kubernetes official documentation to upgrade from v1.17.x to v1.18.x.  "
},
{
	"uri": "/fmw-kubernetes/20.4.1/oam/patch-and-upgrade/patch_an_image/",
	"title": "a. Patch an image",
	"tags": [],
	"description": "Instructions on how to update your OAM Kubernetes cluster with a new OAM Docker image.",
	"content": "To update your OAM Kubernetes cluster with a new OAM Docker image, first install the new Docker image on all nodes in your Kubernetes cluster.\nOnce the new image is installed, choose one of the following options to update your OAM kubernetes cluster to use the new image:\n Run the kubectl edit domain command Run the kubectl patch domain command  In all of the above cases, the Oracle WebLogic Server Kubernetes Operator will restart the Administration Server pod first and then perform a rolling restart on the OAM Managed Servers.\nRun the kubectl edit domain command   To update the domain with the kubectl edit domain command, run the following:\n$ kubectl edit domain \u0026lt;domainname\u0026gt; -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl edit domain accessinfra -n accessns   Update the image tag to point at the new image, for example:\ndomainHomeInImage: false image: oracle/oam:12.2.1.4.0-new imagePullPolicy: IfNotPresent   Save the file and exit (:wq!)\n  Run the kubectl patch command   To update the domain with the kubectl patch domain command, run the following:\n$ kubectl patch domain \u0026lt;domain\u0026gt; -n \u0026lt;namespace\u0026gt; --type merge -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;image\u0026#34;:\u0026#34;newimage:tag\u0026#34;}}\u0026#39; For example:\n$ kubectl patch domain accessinfra -n accessns --type merge -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;image\u0026#34;:\u0026#34;oracle/oam:12.2.1.4-new\u0026#34;}}\u0026#39; The output will look similar to the following:\ndomain.weblogic.oracle/oimcluster patched   "
},
{
	"uri": "/fmw-kubernetes/20.4.1/oig/patch-and-upgrade/patch_an_image/",
	"title": "a. Patch an image",
	"tags": [],
	"description": "Instructions on how to update your OIG Kubernetes cluster with a new OIG docker image.",
	"content": "To update your OIG Kubernetes cluster with a new OIG Docker image, first install the new Docker image on all nodes in your Kubernetes cluster.\nOnce the new image is installed choose one of the following options to update your OIG Kubernetes cluster to use the new image:\n Run the kubectl edit domain command Run the kubectl patch domain command  In all of the above cases, the Oracle WebLogic Kubernetes Operator will restart the Administration Server pod first and then perform a rolling restart on the OIG Managed Servers.\nRun the kubectl edit domain command   To update the domain with the kubectl edit domain command, run the following:\n$ kubectl edit domain \u0026lt;domainname\u0026gt; -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl edit domain oimcluster -n oimcluster   Update the image tag to point at the new image, for example:\ndomainHomeInImage: false image: oracle/oig:12.2.1.4.0-new imagePullPolicy: IfNotPresent   Save the file and exit (:wq!)\n  Run the kubectl patch command   To update the domain with the kubectl patch domain command, run the following:\n$ kubectl patch domain \u0026lt;domain\u0026gt; -n \u0026lt;namespace\u0026gt; --type merge -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;image\u0026#34;:\u0026#34;newimage:tag\u0026#34;}}\u0026#39; For example:\n$ kubectl patch domain oimcluster -n oimcluster --type merge -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;image\u0026#34;:\u0026#34;oracle/oig:12.2.1.4-new\u0026#34;}}\u0026#39; The output will look similar to the following:\ndomain.weblogic.oracle/oimcluster patched   "
},
{
	"uri": "/fmw-kubernetes/20.4.1/oig/post-install-config/set_oimfronendurl_using_mbeans/",
	"title": "a. Set OIMfrontendURL",
	"tags": [],
	"description": "Set the OIMfrontendURL in Oracle Enterprise Manager.",
	"content": "Set OIMFrontendURL using MBeans   Login to Oracle Enterprise Manager using the following URL:\nhttps://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/em\n  Click the Target Navigation icon in the top left of the screen and navigate to the following:\n Expand Identity and Access \u0026gt; Access \u0026gt; OIM \u0026gt; oim Right click the instance oim and select System MBean Browser Under Application Defined MBeans, navigate to oracle.iam, Server:oim_server1, Application:oim \u0026gt; XMLConfig \u0026gt; Config \u0026gt; XMLConfig.DiscoveryConfig \u0026gt; Discovery.    Enter a new value for the OimFrontEndURL attribute, in the format:\nhttp://\u0026lt;OIM-Cluster-Service-Name\u0026gt;:\u0026lt;Cluster-Service-Port\u0026gt;\nFor example:\nhttp://oimcluster-cluster-oim-cluster:14000\nThen click Apply.\nNote: To find the \u0026lt;OIM-Cluster-Service-Name\u0026gt; run the following command:\n$ kubectl -n oimcluster get svc Your output will look similar to this:\n$ kubectl -n oimcluster get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE oimcluster-adminserver ClusterIP None \u0026lt;none\u0026gt; 7001/TCP 6d23h oimcluster-cluster-oim-cluster ClusterIP 10.107.191.53 \u0026lt;none\u0026gt; 14000/TCP 6d23h oimcluster-cluster-soa-cluster ClusterIP 10.97.108.226 \u0026lt;none\u0026gt; 8001/TCP 6d23h oimcluster-oim-server1 ClusterIP None \u0026lt;none\u0026gt; 14000/TCP 6d23h oimcluster-oim-server2 ClusterIP 10.96.147.43 \u0026lt;none\u0026gt; 14000/TCP 6d23h oimcluster-oim-server3 ClusterIP 10.103.65.77 \u0026lt;none\u0026gt; 14000/TCP 6d23h oimcluster-oim-server4 ClusterIP 10.98.157.253 \u0026lt;none\u0026gt; 14000/TCP 6d23h oimcluster-oim-server5 ClusterIP 10.102.19.32 \u0026lt;none\u0026gt; 14000/TCP 6d23h oimcluster-soa-server1 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP 6d23h oimcluster-soa-server2 ClusterIP 10.96.73.62 \u0026lt;none\u0026gt; 8001/TCP 6d23h oimcluster-soa-server3 ClusterIP 10.105.198.83 \u0026lt;none\u0026gt; 8001/TCP 6d23h oimcluster-soa-server4 ClusterIP 10.98.171.18 \u0026lt;none\u0026gt; 8001/TCP 6d23h oimcluster-soa-server5 ClusterIP 10.105.196.107 \u0026lt;none\u0026gt; 8001/TCP 6d23h   "
},
{
	"uri": "/fmw-kubernetes/20.4.1/oam/configure-ingress/ingress-nginx-setup-for-oam-domain-setup-on-k8s/",
	"title": "a. Using an Ingress with NGINX",
	"tags": [],
	"description": "Steps to set up an Ingress for NGINX to direct traffic to the OAM domain.",
	"content": "Setting up an ingress for NGINX for the OAM Domain The instructions below explain how to set up NGINX as an ingress for the OAM domain with SSL termination.\nNote: All the steps below should be performed on the master node.\n Generate a SSL Certificate Install NGINX Create an Ingress for the Domain Verify that You can Access the Domain URL  Generate a SSL Certificate   Generate a private key and certificate signing request (CSR) using a tool of your choice. Send the CSR to your certificate authority (CA) to generate the certificate.\nIf you want to use a certificate for testing purposes you can generate a self signed certificate using openssl:\n$ mkdir \u0026lt;work directory\u0026gt;/ssl $ cd \u0026lt;work directory\u0026gt;/ssl $ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \u0026#34;/CN=\u0026lt;nginx-hostname\u0026gt;\u0026#34; For example:\n$ mkdir /scratch/OAMDockerK8S/ssl $ cd /scratch/OAMDockerK8S/ssl $ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \u0026#34;/CN=masternode.example.com\u0026#34; Note: The CN should match the host.domain of the master node in order to prevent hostname problems during certificate verification.\nThe output will look similar to the following:\nGenerating a 2048 bit RSA private key ..........................................+++ .......................................................................................................+++ writing new private key to \u0026#39;tls.key\u0026#39; -----   Create a secret for SSL by running the following command:\n$ kubectl -n accessns create secret tls \u0026lt;domain_uid\u0026gt;-tls-cert --key \u0026lt;work directory\u0026gt;/tls.key --cert \u0026lt;work directory\u0026gt;/tls.crt For example:\n$ kubectl -n accessns create secret tls accessinfra-tls-cert --key /scratch/OAMDockerK8S/ssl/tls.key --cert /scratch/OAMDockerK8S/ssl/tls.crt The output will look similar to the following:\nsecret/accessinfra-tls-cert created   Install NGINX Use helm to install NGINX.\n  Add the helm chart repository for NGINX using the following command:\n$ helm repo add stable https://kubernetes.github.io/ingress-nginx The output will look similar to the following:\n\u0026#34;stable\u0026#34; has been added to your repositories   Update the repository using the following command:\n$ helm repo update The output will look similar to the following:\nHang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \u0026#34;appscode\u0026#34; chart repository ...Successfully got an update from the \u0026#34;stable\u0026#34; chart repository Update Complete. ⎈ Happy Helming!⎈   Install NGINX using the following helm command:\n$ helm install nginx-ingress -n \u0026lt;domain_namespace\u0026gt; --set controller.extraArgs.default-ssl-certificate=\u0026lt;domain_namespace\u0026gt;/\u0026lt;ssl_secret\u0026gt; --set controller.service.type=NodePort --set controller.admissionWebhooks.enabled=false stable/ingress-nginx For example:\n$ helm install nginx-ingress -n accessns --set controller.extraArgs.default-ssl-certificate=accessns/accessinfra-tls-cert --set controller.service.type=NodePort --set controller.admissionWebhooks.enabled=false stable/ingress-nginx The output will look similar to the following:\nNAME: nginx-ingress LAST DEPLOYED: Thu Sep 24 07:31:51 2020 NAMESPACE: accessns STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: The nginx-ingress controller has been installed. Get the application URL by running these commands: export HTTP_NODE_PORT=$(kubectl --namespace accessns get services -o jsonpath=\u0026#34;{.spec.ports[0].nodePort}\u0026#34; nginx-ingress-controller) export HTTPS_NODE_PORT=$(kubectl --namespace accessns get services -o jsonpath=\u0026#34;{.spec.ports[1].nodePort}\u0026#34; nginx-ingress-controller) export NODE_IP=$(kubectl --namespace accessns get nodes -o jsonpath=\u0026#34;{.items[0].status.addresses[1].address}\u0026#34;) echo \u0026#34;Visit http://$NODE_IP:$HTTP_NODE_PORTto access your application via HTTP.\u0026#34; echo \u0026#34;Visit https://$NODE_IP:$HTTPS_NODE_PORTto access your application via HTTPS.\u0026#34; An example Ingress that makes use of the controller: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx name: example namespace: foo spec: rules: - host: www.example.com http: paths: - backend: serviceName: exampleService servicePort: 80 path: / # This section is only required if TLS is to be enabled for the Ingress tls: - hosts: - www.example.com secretName: example-tls If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided: apiVersion: v1 kind: Secret metadata: name: example-tls namespace: foo data: tls.crt: \u0026lt;base64 encoded cert\u0026gt; tls.key: \u0026lt;base64 encoded key\u0026gt; type: kubernetes.io/tls   Create an Ingress for the Domain   Create an Ingress for the domain (access-ingress), in the domain namespace by using the sample Helm chart.\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain $ kubectl create -f ssl-nginx-ingress.yaml Note: The ssl-nginx-ingress.yaml has nginx.ingress.kubernetes.io/enable-access-log set to false. If you want to enable access logs then set this value to true before executing the command. Enabling access-logs can cause issues with disk space if not regularly maintained.\nFor example:\n$ cd /scratch/OAMDockerK8S/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain $ kubectl create -f ssl-nginx-ingress.yaml The output will look similar to the following:\ningress.extensions/access-ingress created   Run the following command to show the ingress is created successfully:\n$ kubectl get ing -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get ing -n accessns The output will look similar to the following:\nNAME CLASS HOSTS ADDRESS PORTS AGE access-ingress \u0026lt;none\u0026gt; * 80 2m53s   Find the node port of NGINX using the following command:\n$ kubectl --namespace accessns get services -o jsonpath=\u0026#34;{.spec.ports[1].nodePort}\u0026#34; nginx-ingress-ingress-nginx-controller The output will look similar to the following:\n30099   Run the following command to check the ingress:\n$ kubectl describe ing access-ingress -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl describe ing access-ingress -n accessns The output will look similar to the following:\nName: access-ingress Namespace: accessns Address: 10.107.181.157 Default backend: default-http-backend:80 (\u0026lt;error: endpoints \u0026#34;default-http-backend\u0026#34; not found\u0026gt;) Rules: Host Path Backends ---- ---- -------- * /console accessinfra-adminserver:7001 (10.244.1.7:7001) /rreg/rreg accessinfra-adminserver:7001 (10.244.1.7:7001) /em accessinfra-adminserver:7001 (10.244.1.7:7001) /oamconsole accessinfra-adminserver:7001 (10.244.1.7:7001) /dms accessinfra-adminserver:7001 (10.244.1.7:7001) /oam/services/rest accessinfra-adminserver:7001 (10.244.1.7:7001) /iam/admin/config accessinfra-adminserver:7001 (10.244.1.7:7001) /oam/admin/api accessinfra-adminserver:7001 (10.244.1.7:7001) /iam/admin/diag accessinfra-adminserver:7001 (10.244.1.7:7001) /iam/access accessinfra-cluster-oam-cluster:14100 (10.244.1.8:14100,10.244.2.3:14100) /oam/services/rest/access/api accessinfra-cluster-oam-cluster:14100 (10.244.1.8:14100,10.244.2.3:14100) /access accessinfra-cluster-policy-cluster:15100 (10.244.1.9:15100) / accessinfra-cluster-oam-cluster:14100 (10.244.1.8:14100,10.244.2.3:14100) Annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/configuration-snippet: more_set_input_headers \u0026#34;X-Forwarded-Proto: https\u0026#34;; more_set_input_headers \u0026#34;WL-Proxy-SSL: true\u0026#34;; nginx.ingress.kubernetes.io/ingress.allow-http: false nginx.ingress.kubernetes.io/proxy-buffer-size: 2000k Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal CREATE 85s nginx-ingress-controller Ingress accessns/access-ingress Normal UPDATE 40s nginx-ingress-controller Ingress accessns/access-ingress   To confirm that the new Ingress is successfully routing to the domain\u0026rsquo;s server pods, run the following command to send a request to the URL for the \u0026ldquo;WebLogic ReadyApp framework\u0026rdquo;:\n$ curl -v -k https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/weblogic/ready For example:\n$ curl -v -k https://masternode.example.com:30099/weblogic/ready The output will look similar to the following:\n* Trying 12.345.67.89... * Connected to 12.345.67.89 (12.345.67.89) port 30099 (#0) * Initializing NSS with certpath: sql:/etc/pki/nssdb * skipping SSL peer certificate verification * SSL connection using TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 * Server certificate: * subject: CN=masternode.example.com * start date: Sep 24 14:30:46 2020 GMT * expire date: Sep 24 14:30:46 2021 GMT * common name: masternode.example.com * issuer: CN=masternode.example.com \u0026gt; GET /weblogic/ready HTTP/1.1 \u0026gt; User-Agent: curl/7.29.0 \u0026gt; Host: masternode.example.com:30099 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; Server: nginx/1.19.2 \u0026lt; Date: Thu, 24 Sep 2020 14:51:06 GMT \u0026lt; Content-Length: 0 \u0026lt; Connection: keep-alive \u0026lt; Strict-Transport-Security: max-age=15724800; includeSubDomains \u0026lt; * Connection #0 to host 10.247.94.49 left intact   Verify that You can Access the Domain URL After setting up the NGINX ingress, verify that the domain applications are accessible through the NGINX ingress port (for example 30099) as per Validate Domain URLs \n"
},
{
	"uri": "/fmw-kubernetes/20.4.1/oig/configure-ingress/ingress-nginx-setup-for-oig-domain-setup-on-k8s/",
	"title": "a. Using an Ingress with NGINX (non-SSL)",
	"tags": [],
	"description": "Steps to set up an Ingress for NGINX to direct traffic to the OIG domain (non-SSL).",
	"content": "Setting Up an ingress for NGINX for the OIG Domain on Kubernetes (non-SSL) The instructions below explain how to set up NGINX as an ingress for the OIG domain with non-SSL termination.\nNote: All the steps below should be performed on the master node.\n Install NGINX  Configure the repository Create a Namespace Install NGINX using helm Setup Routing Rules for the Domain   Create an Ingress for the Domain Verify that You can Access the Domain URL Cleanup  Install NGINX Use helm to install NGINX.\nConfigure the repository   Add the Helm chart repository for NGINX using the following command:\n$ helm repo add stable https://kubernetes.github.io/ingress-nginx The output will look similar to the following:\n$ helm repo add stable https://kubernetes.github.io/ingress-nginx \u0026quot;stable\u0026quot; has been added to your repositories   Update the repository using the following command:\n$ helm repo update The output will look similar to the following:\nHang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \u0026#34;stable\u0026#34; chart repository Update Complete. Happy Helming!   Create a Namespace   Create a Kubernetes namespace for NGINX by running the following command:\n$ kubectl create namespace nginx The output will look similar to the following:\nnamespace/nginx created   Install NGINX using helm If you can connect directly to the master node IP address from a browser, then install NGINX with the --set controller.service.type=NodePort parameter.\nIf you are using a Managed Service for your Kubernetes cluster,for example Oracle Kubernetes Engine (OKE) on Oracle Cloud Infrastructure (OCI), and connect from a browser to the Load Balancer IP address, then use the --set controller.service.type=LoadBalancer parameter. This instructs the Managed Service to setup a Load Balancer to direct traffic to the NGINX ingress.\n  To install NGINX use the following helm command depending on if you are using NodePort or LoadBalancer:\na) Using NodePort\n$ helm install nginx-ingress -n nginx --set controller.service.type=NodePort --set controller.admissionWebhooks.enabled=false stable/ingress-nginx The output will look similar to the following:\nNAME: nginx-ingress LAST DEPLOYED: Tue Sep 29 08:07:03 2020 NAMESPACE: nginx STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: The nginx-ingress controller has been installed. Get the application URL by running these commands: export HTTP_NODE_PORT=$(kubectl --namespace nginx get services -o jsonpath=\u0026quot;{.spec.ports[0].nodePort}\u0026quot; nginx-ingress-controller) export HTTPS_NODE_PORT=$(kubectl --namespace nginx get services -o jsonpath=\u0026quot;{.spec.ports[1].nodePort}\u0026quot; nginx-ingress-controller) export NODE_IP=$(kubectl --namespace nginx get nodes -o jsonpath=\u0026quot;{.items[0].status.addresses[1].address}\u0026quot;) echo \u0026quot;Visit http://$NODE_IP:$HTTP_NODE_PORT to access your application via HTTP.\u0026quot; echo \u0026quot;Visit https://$NODE_IP:$HTTPS_NODE_PORT to access your application via HTTPS.\u0026quot; An example Ingress that makes use of the controller: apiVersion: extensions/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx name: example namespace: foo spec: rules: - host: www.example.com http: paths: - backend: serviceName: exampleService servicePort: 80 path: / # This section is only required if TLS is to be enabled for the Ingress tls: - hosts: - www.example.com secretName: example-tls If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided: apiVersion: v1 kind: Secret metadata: name: example-tls namespace: foo data: tls.crt: \u0026lt;base64 encoded cert\u0026gt; tls.key: \u0026lt;base64 encoded key\u0026gt; type: kubernetes.io/tls b) Using LoadBalancer\n$ helm install nginx-ingress -n nginx --set controller.service.type=LoadBalancer --set controller.admissionWebhooks.enabled=false stable/ingress-nginx The output will look similar to the following:\nNAME: nginx-ingress LAST DEPLOYED: Tue Sep 29 08:07:03 2020 NAMESPACE: nginx STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: The nginx-ingress controller has been installed. It may take a few minutes for the LoadBalancer IP to be available. You can watch the status by running 'kubectl --namespace nginx get services -o wide -w nginx-ingress-controller' An example Ingress that makes use of the controller: apiVersion: extensions/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx name: example namespace: foo spec: rules: - host: www.example.com http: paths: - backend: serviceName: exampleService servicePort: 80 path: / # This section is only required if TLS is to be enabled for the Ingress tls: - hosts: - www.example.com secretName: example-tls If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided: apiVersion: v1 kind: Secret metadata: name: example-tls namespace: foo data: tls.crt: \u0026lt;base64 encoded cert\u0026gt; tls.key: \u0026lt;base64 encoded key\u0026gt; type: kubernetes.io/tls   Setup Routing Rules for the Domain   Setup routing rules by running the following commands:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain $ cp values.yaml values.yaml.orig $ vi values.yaml Edit values.yaml and ensure that type=NGINX and tls=NONSSL are set, for example:\n$ cat values.yaml # Copyright 2020 Oracle Corporation and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # Default values for ingress-per-domain. # This is a YAML-formatted file. # Declare variables to be passed into your templates. # Load balancer type. Supported values are: VOYAGER, NGINX type: NGINX # Type of Configuration Supported Values are : NONSSL,SSL # tls: NONSSL tls: NONSSL # TLS secret name if the mode is SSL secretName: domain1-tls-cert # WLS domain as backend to the load balancer wlsDomain: domainUID: oimcluster oimClusterName: oim_cluster soaClusterName: soa_cluster soaManagedServerPort: 8001 oimManagedServerPort: 14000 adminServerName: adminserver adminServerPort: 7001 # Traefik specific values # traefik: # hostname used by host-routing # hostname: idmdemo.m8y.xyz # Voyager specific values voyager: # web port webPort: 30305 # stats port statsPort: 30315   Create an Ingress for the Domain   Create an Ingress for the domain (oimcluster-nginx), in the domain namespace by using the sample Helm chart:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator $ helm install oimcluster-nginx kubernetes/samples/charts/ingress-per-domain --namespace \u0026lt;namespace\u0026gt; --values kubernetes/samples/charts/ingress-per-domain/values.yaml Note: The \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain/templates/nginx-ingress.yaml has nginx.ingress.kubernetes.io/enable-access-log set to false. If you want to enable access logs then set this value to true before executing the command. Enabling access-logs can cause issues with disk space if not regularly maintained.\nFor example:\n$ cd /scratch/OIGDockerK8S/weblogic-kubernetes-operator $ helm install oimcluster-nginx kubernetes/samples/charts/ingress-per-domain --namespace oimcluster --values kubernetes/samples/charts/ingress-per-domain/values.yaml The output will look similar to the following:\n$ helm install oimcluster-nginx kubernetes/samples/charts/ingress-per-domain --namespace oimcluster --values kubernetes/samples/charts/ingress-per-domain/values.yaml NAME: oimcluster-nginx LAST DEPLOYED: Tue Sep 29 08:10:06 2020 NAMESPACE: oimcluster STATUS: deployed REVISION: 1 TEST SUITE: None   Run the following command to show the ingress is created successfully:\n$ kubectl get ing -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get ing -n oimcluster The output will look similar to the following:\nNAME CLASS HOSTS ADDRESS PORTS AGE oimcluster-nginx \u0026lt;none\u0026gt; * 80 47s   Find the NodePort of NGINX using the following command (only if you installed NGINX using NodePort):\n$ kubectl get services -n nginx -o jsonpath=”{.spec.ports[0].nodePort}” nginx-ingress-ingress-nginx-controller The output will look similar to the following:\n31578$   Run the following command to check the ingress:\n$ kubectl describe ing access-ingress -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl describe ing oimcluster-nginx -n oimcluster The output will look similar to the following:\nName: oimcluster-nginx Namespace: oimcluster Address: 10.97.68.171 Default backend: default-http-backend:80 (\u0026lt;error: endpoints \u0026quot;default-http-backend\u0026quot; not found\u0026gt;) Rules: Host Path Backends ---- ---- -------- * /console oimcluster-adminserver:7001 (10.244.1.42:7001) /em oimcluster-adminserver:7001 (10.244.1.42:7001) /soa oimcluster-cluster-soa-cluster:8001 (10.244.1.43:8001) /integration oimcluster-cluster-soa-cluster:8001 (10.244.1.43:8001) /soa-infra oimcluster-cluster-soa-cluster:8001 (10.244.1.43:8001) oimcluster-cluster-oim-cluster:14000 (10.244.1.44:14000) Annotations: meta.helm.sh/release-name: oimcluster-nginx meta.helm.sh/release-namespace: oimcluster Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal CREATE 53s nginx-ingress-controller Ingress oimcluster/oimcluster-nginx Normal UPDATE 42s nginx-ingress-controller Ingress oimcluster/oimcluster-nginx   To confirm that the new Ingress is successfully routing to the domain\u0026rsquo;s server pods, run the following command to send a request to the URL for the \u0026ldquo;WebLogic ReadyApp framework\u0026rdquo;:\n$ curl -v http://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/weblogic/ready For example:\na) For NodePort\n$ curl -v http://masternode.example.com:31578/weblogic/ready b) For LoadBalancer\n$ curl -v http://masternode.example.com:80/weblogic/ready The output will look similar to the following:\n$ curl -v -k http://masternode.example.com:31578/weblogic/ready * About to connect() to masternode.example.com port 31578 (#0) * Trying 12.345.67.890... * Connected to masternode.example.com (12.345.67.890) port 31578 (#0) \u0026gt; GET /weblogic/ready HTTP/1.1 \u0026gt; User-Agent: curl/7.29.0 \u0026gt; Host: masternode.example.com:31578 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; Server: nginx/1.19.2 \u0026lt; Date: Tue, 29 Sep 2020 15:16:20 GMT \u0026lt; Content-Length: 0 \u0026lt; Connection: keep-alive \u0026lt; * Connection #0 to host masternode.example.com left intact   Verify that You can Access the Domain URL After setting up the NGINX ingress, verify that the domain applications are accessible through the NGINX ingress port (for example 31578) as per Validate Domain URLs \nCleanup If you need to remove the NGINX Ingress (for example to setup NGINX with SSL) then remove the ingress with the following commands:\n$ helm delete oimcluster-nginx -n oimcluster $ helm delete nginx-ingress -n nginx $ kubectl delete namespace nginx The output will look similar to the following:\n$ helm delete oimcluster-nginx -n oimcluster release \u0026quot;oimcluster-nginx\u0026quot; uninstalled $ helm delete nginx-ingress -n nginx release \u0026quot;nginx-ingress\u0026quot; uninstalled $ kubectl delete namespace nginx namespace \u0026quot;nginx\u0026quot; deleted $ "
},
{
	"uri": "/fmw-kubernetes/20.4.1/oig/configure-design-console/using-the-design-console-with-nginx-non-ssl/",
	"title": "a. Using Design Console with NGINX(non-SSL)",
	"tags": [],
	"description": "Configure Design Console with NGINX(non-SSL).",
	"content": "Configure an NGINX ingress (non-SSL) to allow Design Console to connect to your Kubernetes cluster.\nDesign Console is not installed as part of the OAM Kubernetes cluster so must be installed on a seperate client before following the steps below.\n Add the NGINX ingress using helm Note: If already using NGINX with non-SSL for OIG you can skip this section:\n  Add the Helm chart repository for NGINX using the following command:\n$ helm repo add stable https://kubernetes.github.io/ingress-nginx The output will look similar to the following:\n\u0026#34;stable\u0026#34; has been added to your repositories   Update the repository using the following command:\n$ helm repo update The output will look similar to the following:\nHang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \u0026#34;stable\u0026#34; chart repository Update Complete. Happy Helming!   Create a Kubernetes namespace for NGINX by running the following command:\n$ kubectl create namespace nginx The output will look similar to the following:\nnamespace/nginx created   Install NGINX ingress using helm Install a NGINX ingress for the Design Console:\nIf you can connect directly to the master node IP address from a browser, then install NGINX with the --set controller.service.type=NodePort parameter.\nIf you are using a Managed Service for your Kubernetes cluster,for example Oracle Kubernetes Engine (OKE) on Oracle Cloud Infrastructure (OCI), and connect from a browser to the Load Balancer IP address, then use the --set controller.service.type=LoadBalancer parameter. This instructs the Managed Service to setup a Load Balancer to direct traffic to the NGINX ingress.\n  To install NGINX use the following helm command depending on if you are using NodePort or LoadBalancer:\na) Using NodePort\n$ helm install nginx-dc-operator stable/ingress-nginx -n nginx --set controller.service.type=NodePort --set controller.admissionWebhooks.enabled=false --set controller.service.nodePorts.http=30315 --set controller.ingressClass=nginx-designconsole The output will look similar to the following:\nNAME: nginx-dc-operator LAST DEPLOYED: Tue Oct 20 07:31:08 2020 NAMESPACE: nginx STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: The ingress-nginx controller has been installed. Get the application URL by running these commands: export HTTP_NODE_PORT=30315 export HTTPS_NODE_PORT=$(kubectl --namespace nginx get services -o jsonpath=\u0026quot;{.spec.ports[1].nodePort}\u0026quot; nginx-dc-operator-ingress-nginx-controller) export NODE_IP=$(kubectl --namespace nginx get nodes -o jsonpath=\u0026quot;{.items[0].status.addresses[1].address}\u0026quot;) echo \u0026quot;Visit http://$NODE_IP:$HTTP_NODE_PORT to access your application via HTTP.\u0026quot; echo \u0026quot;Visit https://$NODE_IP:$HTTPS_NODE_PORT to access your application via HTTPS.\u0026quot; An example Ingress that makes use of the controller: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx-designconsole name: example namespace: foo spec: rules: - host: www.example.com http: paths: - backend: serviceName: exampleService servicePort: 80 path: / # This section is only required if TLS is to be enabled for the Ingress tls: - hosts: - www.example.com secretName: example-tls If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided: apiVersion: v1 kind: Secret metadata: name: example-tls namespace: foo data: tls.crt: \u0026lt;base64 encoded cert\u0026gt; tls.key: \u0026lt;base64 encoded key\u0026gt; type: kubernetes.io/tls b) Using LoadBalancer\n$ helm install nginx-dc-operator stable/ingress-nginx -n nginx --set controller.service.type=LoadBalancer --set controller.admissionWebhooks.enabled=false The output will look similar to the following:\nLAST DEPLOYED: Tue Oct 20 07:39:27 2020 NAMESPACE: nginx STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: The ingress-nginx controller has been installed. It may take a few minutes for the LoadBalancer IP to be available. You can watch the status by running 'kubectl --namespace nginx get services -o wide -w nginx-dc-operator-ingress-nginx-controller' An example Ingress that makes use of the controller: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx name: example namespace: foo spec: rules: - host: www.example.com http: paths: - backend: serviceName: exampleService servicePort: 80 path: / # This section is only required if TLS is to be enabled for the Ingress tls: - hosts: - www.example.com secretName: example-tls If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided: apiVersion: v1 kind: Secret metadata: name: example-tls namespace: foo data: tls.crt: \u0026lt;base64 encoded cert\u0026gt; tls.key: \u0026lt;base64 encoded key\u0026gt; type: kubernetes.io/tls   Setup Routing Rules for the Design Console ingress   Setup routing rules by running the following commands:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/charts/design-console-ingress $ cp values.yaml values.yaml.orig $ vi values.yaml Edit values.yaml and ensure that type=NGINX and tls=NONSSL are set, for example:\n$ cat values.yaml # Copyright 2020 Oracle Corporation and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # Default values for design-console-ingress. # This is a YAML-formatted file. # Declare variables to be passed into your templates. # Load balancer type. Supported values are: VOYAGER, NGINX type: NGINX # Type of Configuration Supported Values are : NONSSL,SSL # tls: NONSSL tls: NONSSL # TLS secret name if the mode is SSL secretName: dc-tls-cert # WLS domain as backend to the load balancer wlsDomain: domainUID: oimcluster oimClusterName: oim_cluster oimServerT3Port: 14001 # Voyager specific values voyager: # web port webPort: 30320 # stats port statsPort: 30321   Create the ingress   Run the following command to create the ingress:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator $ helm install oimcluster-nginx-designconsole kubernetes/samples/charts/design-console-ingress --namespace oimcluster --values kubernetes/samples/charts/design-console-ingress/values.yaml For example:\n$ cd /scratch/OIGDockerK8S/weblogic-kubernetes-operator $ helm install oimcluster-nginx-designconsole kubernetes/samples/charts/design-console-ingress --namespace oimcluster --values kubernetes/samples/charts/design-console-ingress/values.yaml The output will look similar to the following:\nNAME: oimcluster-nginx-designconsole LAST DEPLOYED: Tue Oct 20 08:01:47 2020 NAMESPACE: oimcluster STATUS: deployed REVISION: 1 TEST SUITE: None   Run the following command to show the ingress is created successfully:\n$ kubectl describe ing oimcluster-nginx-designconsole -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl describe ing oimcluster-nginx-designconsole -n oimcluster The output will look similar to the following:\nName: oimcluster-nginx-designconsole Namespace: oimcluster Address: 10.99.240.21 Default backend: default-http-backend:80 (\u0026lt;error: endpoints \u0026quot;default-http-backend\u0026quot; not found\u0026gt;) Rules: Host Path Backends ---- ---- -------- * oimcluster-cluster-oim-cluster:14001 () Annotations: kubernetes.io/ingress.class: nginx-designconsole meta.helm.sh/release-name: oimcluster-nginx-designconsole meta.helm.sh/release-namespace: oimcluster nginx.ingress.kubernetes.io/affinity: cookie nginx.ingress.kubernetes.io/enable-access-log: false Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal CREATE 117s nginx-ingress-controller Ingress oimcluster/oimcluster-nginx-designconsole Normal UPDATE 64s nginx-ingress-controller Ingress oimcluster/oimcluster-nginx-designconsole   Login to the Design Console   Launch the Design Console and in the Oracle Identity Manager Design Console login page enter the following details:\nEnter the following details and click Login:\n Server URL: \u0026lt;url\u0026gt; User ID: xelsysadm Password: \u0026lt;password\u0026gt;.  where \u0026lt;url\u0026gt; is as per the following:\na) For NodePort: http://\u0026lt;masternode.example.com\u0026gt;:\u0026lt;NodePort\u0026gt;\nwhere \u0026lt;NodePort\u0026gt; is the value passed in the command earlier, for example: --set controller.service.nodePorts.http=30315\nb) For LoadBalancer: http://\u0026lt;loadbalancer.example.com\u0026gt;:\u0026lt;LBRPort\u0026gt;\n  If successful the Design Console will be displayed. If the VNC session disappears then the connection failed so double check the connection details and try again.\n  "
},
{
	"uri": "/fmw-kubernetes/20.4.1/oig/configure-design-console/using-the-design-console-with-nginx-ssl/",
	"title": "a. Using Design Console with NGINX(SSL)",
	"tags": [],
	"description": "Configure Design Console with NGINX(SSL).",
	"content": "Configure an NGINX ingress (SSL) to allow Design Console to connect to your Kubernetes cluster.\nDesign Console is not installed as part of the OAM Kubernetes cluster so must be installed on a seperate client before following the steps below.\n Generate SSL Certificate Note: If already using NGINX with SSL for OIG you can skip this section:\n  Generate a private key and certificate signing request (CSR) using a tool of your choice. Send the CSR to your certificate authority (CA) to generate the certificate.\nIf you want to use a certificate for testing purposes you can generate a self signed certificate using openssl:\n$ mkdir \u0026lt;work directory\u0026gt;/ssl $ cd \u0026lt;work directory\u0026gt;/ssl $ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \u0026quot;/CN=\u0026lt;nginx-hostname\u0026gt;\u0026quot; For example:\n$ mkdir /scratch/OIGDockerK8S/ssl $ cd /scratch/OIGDockerK8S/ssl $ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \u0026quot;/CN=masternode.example.com\u0026quot; Note: The CN should match the host.domain of the master node in order to prevent hostname problems during certificate verification.\nThe output will look similar to the following:\n$ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \u0026quot;/CN=masternode.example.com\u0026quot; Generating a 2048 bit RSA private key ..........................................+++ .......................................................................................................+++ writing new private key to 'tls.key' -----   Create a secret for SSL containing the SSL certificate by running the following command:\n$ kubectl -n oimcluster create secret tls \u0026lt;domain_id\u0026gt;-tls-cert --key \u0026lt;work directory\u0026gt;/tls.key --cert \u0026lt;work directory\u0026gt;/tls.crt For example:\n$ kubectl -n oimcluster create secret tls oimcluster-tls-cert --key /scratch/OIGDockerK8S/ssl/tls.key --cert /scratch/OIGDockerK8S/ssl/tls.crt The output will look similar to the following:\n$ kubectl -n oimcluster create secret tls oimcluster-tls-cert --key /scratch/OIGDockerK8S/ssl/tls.key --cert /scratch/OIGDockerK8S/ssl/tls.crt secret/oimcluster-tls-cert created $   Confirm that the secret is created by running the following command:\n$ kubectl get secret oimcluster-tls-cert -o yaml -n oimcluster The output will look similar to the following:\napiVersion: v1 data: tls.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURGVENDQWYyZ0F3SUJBZ0lKQUl3ZjVRMWVxZnljTUEwR0NTcUdTSWIzRFFFQkN3VUFNQ0V4SHpBZEJnTlYKQkFNTUZtUmxiakF4WlhadkxuVnpMbTl5WVdOc1pTNWpiMjB3SGhjTk1qQXdPREV3TVRReE9UUXpXaGNOTWpFdwpPREV3TVRReE9UUXpXakFoTVI4d0hRWURWUVFEREJaa1pXNHdNV1YyYnk1MWN5NXZjbUZqYkdVdVkyOXRNSUlCCklqQU5CZ2txaGtpRzl3MEJBUUVGQUFPQ0FROEFNSUlCQ2dLQ0FRRUEyY0lpVUhwcTRVZzBhaGR6aXkycHY2cHQKSVIza2s5REd2eVRNY0syaWZQQ2dtUU5CdHV6VXNFN0l4c294eldITmU5RFpXRXJTSjVON3FYYm1lTzJkMVd2NQp1aFhzbkFTbnkwY1NLUE9xVDNQSlpDVk1MK0llZVFKdnhaVjZaWWU4V2FFL1NQSGJzczRjYy9wcG1mc3pxCnErUi83cXEyMm9ueHNHaE9vQ1h1TlQvMFF2WXVzMnNucGtueWRKRHUxelhGbDREYkFIZGMvamNVK0NPWWROeS8KT3Iza2JIV0FaTkR4OWxaZUREOTRmNXZLcUF2V0FkSVJZa2UrSmpNTHg0VHo2ZlM0VXoxbzdBSTVuSApPQ1ZMblV5U0JkaGVuWTNGNEdFU0wwbnorVlhFWjRWVjRucWNjRmo5cnJ0Q29pT1BBNlgvNGdxMEZJbi9Qd0lECkFRQUJvMUF3VGpBZEJnTlZIUTRFRmdRVWw1VnVpVDBDT0xGTzcxMFBlcHRxSC9DRWZyY3dId1lEVlIwakJCZ3cKRm9BVWw1VnVpVDBDT0xGTzcxMFBlcHRxSC9DRWZyY3dEQVlEVlIwVEJBVXdBd0VCL3pBTkJna3Foa2lHOXcwQgpBUXNGQUFPQ0FRRUFXdEN4b2ZmNGgrWXZEcVVpTFFtUnpqQkVBMHJCOUMwL1FWOG9JQzJ3d1hzYi9KaVNuMHdOCjNMdHppejc0aStEbk1yQytoNFQ3enRaSkc3NVluSGRKcmxQajgzVWdDLzhYTlFCSUNDbTFUa3RlVU1jWG0reG4KTEZEMHpReFhpVzV0N1FHcWtvK2FjeTlhUnUvN3JRMXlNSE9HdVVkTTZETzErNXF4cTdFNXFMamhyNEdKejV5OAoraW8zK25UcUVKMHFQOVRocG96RXhBMW80OEY0ZHJybWdqd3ROUldEQVpBYmYyV1JNMXFKWXhxTTJqdU1FQWNsCnFMek1TdEZUQ2o1UGFTQ0NUV1VEK3ZlSWtsRWRpaFdpRm02dzk3Y1diZ0lGMlhlNGk4L2szMmF1N2xUTDEvd28KU3Q2dHpsa20yV25uUFlVMzBnRURnVTQ4OU02Z1dybklpZz09Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K tls.key: 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 kind: Secret metadata: creationTimestamp: \u0026quot;2020-09-29T15:51:22Z\u0026quot; managedFields: - apiVersion: v1 fieldsType: FieldsV1 fieldsV1: f:data: .: {} f:tls.crt: {} f:tls.key: {} f:type: {} manager: kubectl operation: Update time: \u0026quot;2020-09-29T15:51:22Z\u0026quot; name: oimcluster-tls-cert namespace: oimcluster resourceVersion: \u0026quot;1291036\u0026quot; selfLink: /api/v1/namespaces/oimcluster/secrets/oimcluster-tls-cert uid: a127e5fd-705b-43e1-ab56-590834efda5e type: kubernetes.io/tls   Add the NGINX ingress using helm Note: If already using NGINX with SSL for OIG you can skip this section:\n  Add the Helm chart repository for NGINX using the following command:\n$ helm repo add stable https://kubernetes.github.io/ingress-nginx The output will look similar to the following:\n\u0026#34;stable\u0026#34; has been added to your repositories   Update the repository using the following command:\n$ helm repo update The output will look similar to the following:\nHang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \u0026#34;stable\u0026#34; chart repository Update Complete. Happy Helming!   Create a Kubernetes namespace for NGINX:\n$ kubectl create namespace nginxssl The output will look similar to the following:\nnamespace/nginxssl created   Install NGINX ingress using helm Install a NGINX ingress for the Design Console:\nIf you can connect directly to the master node IP address from a browser, then install NGINX with the --set controller.service.type=NodePort parameter.\nIf you are using a Managed Service for your Kubernetes cluster,for example Oracle Kubernetes Engine (OKE) on Oracle Cloud Infrastructure (OCI), and connect from a browser to the Load Balancer IP address, then use the --set controller.service.type=LoadBalancer parameter. This instructs the Managed Service to setup a Load Balancer to direct traffic to the NGINX ingress.\n  To install NGINX use the following helm command depending on if you are using NodePort or LoadBalancer:\na) Using NodePort\n$ helm install nginx-dc-operator-ssl -n nginxssl --set controller.extraArgs.default-ssl-certificate=oimcluster/oimcluster-tls-cert --set controller.service.type=NodePort --set controller.admissionWebhooks.enabled=false --set controller.service.nodePorts.https=30321 --set controller.ingressClass=nginx-designconsole stable/ingress-nginx The output will look similar to the following:\nLAST DEPLOYED: Wed Oct 21 03:52:25 2020 NAMESPACE: nginxssl STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: The ingress-nginx controller has been installed. Get the application URL by running these commands: export HTTP_NODE_PORT=$(kubectl --namespace nginxssl get services -o jsonpath=\u0026quot;{.spec.ports[0].nodePort}\u0026quot; nginx-dc-operator-ssl-ingress-nginx-controller) export HTTPS_NODE_PORT=30321 export NODE_IP=$(kubectl --namespace nginxssl get nodes -o jsonpath=\u0026quot;{.items[0].status.addresses[1].address}\u0026quot;) echo \u0026quot;Visit http://$NODE_IP:$HTTP_NODE_PORT to access your application via HTTP.\u0026quot; echo \u0026quot;Visit https://$NODE_IP:$HTTPS_NODE_PORT to access your application via HTTPS.\u0026quot; An example Ingress that makes use of the controller: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx-designconsole name: example namespace: foo spec: rules: - host: www.example.com http: paths: - backend: serviceName: exampleService servicePort: 80 path: / # This section is only required if TLS is to be enabled for the Ingress tls: - hosts: - www.example.com secretName: example-tls If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided: apiVersion: v1 kind: Secret metadata: name: example-tls namespace: foo data: tls.crt: \u0026lt;base64 encoded cert\u0026gt; tls.key: \u0026lt;base64 encoded key\u0026gt; type: kubernetes.io/tls b) Using LoadBalancer\n$ helm install nginx-dc-operator-ssl -n nginxssl --set controller.extraArgs.default-ssl-certificate=oimcluster/oimcluster-tls-cert --set controller.service.type=LoadBalancer --set controller.admissionWebhooks.enabled=false stable/ingress-nginx The output will look similar to the following:\nNAME: nginx-dc-operator-ssl-lbr LAST DEPLOYED: Wed Oct 21 04:02:35 2020 NAMESPACE: nginxssl STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: The ingress-nginx controller has been installed. It may take a few minutes for the LoadBalancer IP to be available. You can watch the status by running 'kubectl --namespace nginxssl get services -o wide -w nginx-dc-operator-ssl-lbr-ingress-nginx-controller' An example Ingress that makes use of the controller: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx name: example namespace: foo spec: rules: - host: www.example.com http: paths: - backend: serviceName: exampleService servicePort: 80 path: / # This section is only required if TLS is to be enabled for the Ingress tls: - hosts: - www.example.com secretName: example-tls If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided: apiVersion: v1 kind: Secret metadata: name: example-tls namespace: foo data: tls.crt: \u0026lt;base64 encoded cert\u0026gt; tls.key: \u0026lt;base64 encoded key\u0026gt; type: kubernetes.io/tls   Setup Routing Rules for the Design Console ingress   Setup routing rules by running the following commands:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/charts/design-console-ingress $ cp values.yaml values.yaml.orig $ vi values.yaml Edit values.yaml and ensure that type=NGINX, tls=SSL and secretName:oimcluster-tls-cert are set, for example:\n$ cat values.yaml # Copyright 2020 Oracle Corporation and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # Default values for design-console-ingress. # This is a YAML-formatted file. # Declare variables to be passed into your templates. # Load balancer type. Supported values are: VOYAGER, NGINX type: NGINX # Type of Configuration Supported Values are : NONSSL,SSL # tls: NONSSL tls: SSL # TLS secret name if the mode is SSL secretName: oimcluster-tls-cert # WLS domain as backend to the load balancer wlsDomain: domainUID: oimcluster oimClusterName: oim_cluster oimServerT3Port: 14001 # Voyager specific values voyager: # web port webPort: 30320 # stats port statsPort: 30321   Create the ingress   Run the following command to create the ingress:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator $ helm install oimcluster-nginx-designconsole kubernetes/samples/charts/design-console-ingress --namespace oimcluster --values kubernetes/samples/charts/design-console-ingress/values.yaml For example:\n$ cd /scratch/OIGDockerK8S/weblogic-kubernetes-operator $ helm install oimcluster-nginx-designconsole kubernetes/samples/charts/design-console-ingress --namespace oimcluster --values kubernetes/samples/charts/design-console-ingress/values.yaml The output will look similar to the following:\nNAME: oimcluster-nginx-designconsole LAST DEPLOYED: Wed Oct 21 04:12:00 2020 NAMESPACE: oimcluster STATUS: deployed REVISION: 1 TEST SUITE: None   Run the following command to show the ingress is created successfully:\n$ kubectl describe ing oimcluster-nginx-designconsole -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl describe ing oimcluster-nginx-designconsole -n oimcluster The output will look similar to the following:\nName: oimcluster-nginx-designconsole Namespace: oimcluster Address: 10.106.181.99 Default backend: default-http-backend:80 (\u0026lt;error: endpoints \u0026quot;default-http-backend\u0026quot; not found\u0026gt;) Rules: Host Path Backends ---- ---- -------- * oimcluster-cluster-oim-cluster:14001 () Annotations: kubernetes.io/ingress.class: nginx-designconsole meta.helm.sh/release-name: oimcluster-nginx-designconsole meta.helm.sh/release-namespace: oimcluster nginx.ingress.kubernetes.io/affinity: cookie nginx.ingress.kubernetes.io/configuration-snippet: more_set_input_headers \u0026quot;X-Forwarded-Proto: https\u0026quot;; more_set_input_headers \u0026quot;WL-Proxy-SSL: true\u0026quot;; nginx.ingress.kubernetes.io/enable-access-log: false nginx.ingress.kubernetes.io/ingress.allow-http: false nginx.ingress.kubernetes.io/proxy-buffer-size: 2000k Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal CREATE 38s nginx-ingress-controller Ingress oimcluster/oimcluster-nginx-designconsole Normal UPDATE 10s nginx-ingress-controller Ingress oimcluster/oimcluster-nginx-designconsole   Design Console Client The instructions below should be performed on the client where Design Console is installed.\nImport the CA certificate into the java keystore If in Generate a SSL Certificate you requested a certificate from a Certificate Authority (CA), then you must import the CA certificate (e.g cacert.crt) that signed your certificate, into the java truststore used by Design Console.\nIf in Generate a SSL Certificate you generated a self-signed certicate (e.g tls.crt), you must import the self-signed certificate into the java truststore used by Design Console.\nImport the certificate using the following command:\n$ keytool -import -trustcacerts -alias dc -file \u0026lt;certificate\u0026gt; -keystore $JAVA_HOME/jre/lib/security/cacerts where \u0026lt;certificate\u0026gt; is the CA certificate, or self-signed certicate.\nLogin to the Design Console   Launch the Design Console and in the Oracle Identity Manager Design Console login page enter the following details:\nEnter the following details and click Login:\n Server URL: \u0026lt;url\u0026gt; User ID: xelsysadm Password: \u0026lt;password\u0026gt;.  where \u0026lt;url\u0026gt; is as per the following:\na) For NodePort: https://\u0026lt;masternode.example.com\u0026gt;:\u0026lt;NodePort\u0026gt;\nwhere \u0026lt;NodePort\u0026gt; is the value passed in the command earlier, for example: --set controller.service.nodePorts.http=30321\nb) For LoadBalancer: https://\u0026lt;loadbalancer.example.com\u0026gt;:\u0026lt;LBRPort\u0026gt;\n  If successful the Design Console will be displayed. If the VNC session disappears then the connection failed so double check the connection details and try again.\n  "
},
{
	"uri": "/fmw-kubernetes/20.4.1/oig/configure-design-console/using-the-design-console-with-voyager-non-ssl/",
	"title": "a. Using Design Console with Voyager(non-SSL)",
	"tags": [],
	"description": "Configure Design Console with Voyager(non-SSL).",
	"content": "Configure a Voyager ingress (non-SSL) to allow Design Console to connect to your Kubernetes cluster.\nDesign Console is not installed as part of the OAM Kubernetes cluster so must be installed on a seperate client before following the steps below.\n Add the Voyager ingress using helm Note: If already using Voyager with non-SSL for OIG you can skip this section:\n  Add the Helm chart repository for Voyager using the following command:\n$ helm repo add appscode https://charts.appscode.com/stable The output will look similar to the following:\n\u0026#34;appscode\u0026#34; has been added to your repositories   Update the repository using the following command:\n$ helm repo update The output will look similar to the following:\nHang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \u0026#34;appscode\u0026#34; chart repository Update Complete. Happy Helming!   Create a namespace for Voyager:\n$ kubectl create namespace voyager The output will look similar to the following:\nnamespace/voyager created   Install Voyager ingress using helm  $ helm install voyager-designconsole-operator appscode/voyager --version v12.0.0-rc.1 --namespace voyager --set cloudProvider=baremetal --set apiserver.enableValidatingWebhook=false Note: For bare metal Kubernetes use --set cloudProvider=baremetal. If using a managed Kubernetes service then the value should be set for your specific service as per the Voyager install guide.\nThe output will look similar to the following:\nNAME: voyager-designconsole-operator LAST DEPLOYED: Wed Oct 21 08:31:32 2020 NAMESPACE: voyager STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: Set cloudProvider for installing Voyager To verify that Voyager has started, run: kubectl --namespace=voyager get deployments -l \u0026quot;release=voyager-designconsole-operator, app=voyager\u0026quot; Setup Routing Rules for the Design Console ingress   Setup routing rules by running the following commands:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/charts/design-console-ingress $ cp values.yaml values.yaml.orig $ vi values.yaml Edit values.yaml and ensure that type=VOYAGER and tls=NONSSL are set, and that webPort and statsPort are set to free ports, for example:\n$ cat values.yaml # Copyright 2020 Oracle Corporation and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # Default values for design-console-ingress. # This is a YAML-formatted file. # Declare variables to be passed into your templates. # Load balancer type. Supported values are: VOYAGER, NGINX type: VOYAGER # Type of Configuration Supported Values are : NONSSL,SSL # tls: NONSSL tls: NONSSL # TLS secret name if the mode is SSL secretName: dc-tls-cert # WLS domain as backend to the load balancer wlsDomain: domainUID: oimcluster oimClusterName: oim_cluster oimServerT3Port: 14001 # Voyager specific values voyager: # web port webPort: 30325 # stats port statsPort: 30326   Create the ingress   Run the following command to create the ingress:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator $ helm install oimcluster-voyager-designconsole kubernetes/samples/charts/design-console-ingress --namespace oimcluster --values kubernetes/samples/charts/design-console-ingress/values.yaml For example:\n$ cd /scratch/OIGDockerK8S/weblogic-kubernetes-operator $ helm install oimcluster-voyager-designconsole kubernetes/samples/charts/design-console-ingress --namespace oimcluster --values kubernetes/samples/charts/design-console-ingress/values.yaml The output will look similar to the following:\nNAME: oimcluster-voyager-designconsole LAST DEPLOYED: Wed Oct 21 08:36:03 2020 NAMESPACE: oimcluster STATUS: deployed REVISION: 1 TEST SUITE: None   Run the following command to show the ingress is created successfully:\n$ kubectl get ingress.voyager.appscode.com -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get ingress.voyager.appscode.com -n oimcluster The output will look similar to the following:\nNAME HOSTS LOAD_BALANCER_IP AGE oimcluster-voyager-designconsole * 10s   Return details of the ingress using the following command:\n$ kubectl describe ingress.voyager.appscode.com oimcluster-voyager-designconsole -n oimcluster The output will look similar to the following:\nName: oimcluster-voyager-designconsole Namespace: oimcluster Labels: app.kubernetes.io/managed-by=Helm weblogic.resourceVersion=domain-v2 Annotations: ingress.appscode.com/affinity: cookie ingress.appscode.com/stats: true ingress.appscode.com/type: NodePort meta.helm.sh/release-name: oimcluster-voyager-designconsole meta.helm.sh/release-namespace: oimcluster API Version: voyager.appscode.com/v1beta1 Kind: Ingress Metadata: Creation Timestamp: 2020-10-21T15:46:29Z Generation: 1 Managed Fields: API Version: voyager.appscode.com/v1beta1 Fields Type: FieldsV1 fieldsV1: f:metadata: f:annotations: .: f:ingress.appscode.com/affinity: f:ingress.appscode.com/stats: f:ingress.appscode.com/type: f:meta.helm.sh/release-name: f:meta.helm.sh/release-namespace: f:labels: .: f:app.kubernetes.io/managed-by: f:weblogic.resourceVersion: f:spec: .: f:frontendRules: f:rules: f:tls: Manager: Go-http-client Operation: Update Time: 2020-10-21T15:46:29Z Resource Version: 6082128 Self Link: /apis/voyager.appscode.com/v1beta1/namespaces/oimcluster/ingresses/oimcluster-voyager-designconsole UID: a4968c01-28eb-4e4a-ac31-d60cfcd8705f Spec: Frontend Rules: Port: 443 Rules: http-request set-header WL-Proxy-SSL true Rules: Host: * Http: Node Port: 30325 Paths: Backend: Service Name: oimcluster-cluster-oim-cluster Service Port: 14001 Path: / Tls: Hosts: * Secret Name: oimcluster-tls-cert Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal DeploymentReconcileSuccessful 55m voyager-operator Successfully patched HAProxy Deployment voyager-oimcluster-voyager-designconsole Normal DeploymentReconcileSuccessful 45m voyager-operator Successfully patched HAProxy Deployment voyager-oimcluster-voyager-designconsole   Login to the Design Console   Launch the Design Console and in the Oracle Identity Manager Design Console login page enter the following details:\nEnter the following details and click Login:\n Server URL: \u0026lt;url\u0026gt; User ID: xelsysadm Password: \u0026lt;password\u0026gt;.  where \u0026lt;url\u0026gt; is http://\u0026lt;masternode.example.com\u0026gt;:\u0026lt;NodePort\u0026gt;\n\u0026lt;NodePort\u0026gt; is the value passed for webPort in the `values.yaml earlier, for example: 30325\n  If successful the Design Console will be displayed. If the VNC session disappears then the connection failed so double check the connection details and try again.\n  "
},
{
	"uri": "/fmw-kubernetes/20.4.1/oig/configure-design-console/using-the-design-console-with-voyager-ssl/",
	"title": "a. Using Design Console with Voyager(SSL)",
	"tags": [],
	"description": "Configure Design Console with Voyager(SSL).",
	"content": "Configure a Voyager ingress (SSL) to allow Design Console to connect to your Kubernetes cluster.\nDesign Console is not installed as part of the OAM Kubernetes cluster so must be installed on a seperate client before following the steps below.\n Generate SSL Certificate Note: If already using Voyager with SSL for OIG you can skip this section:\n  Generate a private key and certificate signing request (CSR) using a tool of your choice. Send the CSR to your certificate authority (CA) to generate the certificate.\nIf you want to use a certificate for testing purposes you can generate a self signed certificate using openssl:\n$ mkdir \u0026lt;work directory\u0026gt;/ssl $ cd \u0026lt;work directory\u0026gt;/ssl $ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \u0026quot;/CN=\u0026lt;nginx-hostname\u0026gt;\u0026quot; For example:\n$ mkdir /scratch/OIGDockerK8S/ssl $ cd /scratch/OIGDockerK8S/ssl $ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \u0026quot;/CN=masternode.example.com\u0026quot; Note: The CN should match the host.domain of the master node in order to prevent hostname problems during certificate verification.\nThe output will look similar to the following:\n$ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \u0026quot;/CN=masternode.example.com\u0026quot; Generating a 2048 bit RSA private key ..........................................+++ .......................................................................................................+++ writing new private key to 'tls.key' ----- $   Create a Kubernetes Secret for SSL Note: If already using Voyager with SSL for OIG you can skip this section:\n  Create a secret for SSL containing the SSL certificate by running the following command:\n$ kubectl -n oimcluster create secret tls \u0026lt;domain_id\u0026gt;-tls-cert --key \u0026lt;work directory\u0026gt;/tls.key --cert \u0026lt;work directory\u0026gt;/tls.crt For example:\n$ kubectl -n oimcluster create secret tls oimcluster-tls-cert --key /scratch/OIGDockerK8S/ssl/tls.key --cert /scratch/OIGDockerK8S/ssl/tls.crt The output will look similar to the following:\nsecret/oimcluster-tls-cert created Confirm that the secret is created by running the following command:\n$ kubectl get secret oimcluster-tls-cert -o yaml -n oimcluster The output will look similar to the following:\napiVersion: v1 data: tls.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURGVENDQWYyZ0F3SUJBZ0lKQUl3ZjVRMWVxZnljTUEwR0NTcUdTSWIzRFFFQkN3VUFNQ0V4SHpBZEJnTlYKQkFNTUZtUmxiakF4WlhadkxuVnpMbTl5WVdOc1pTNWpiMjB3SGhjTk1qQXdPREV3TVRReE9UUXpXaGNOTWpFdwpPREV3TVRReE9UUXpXakFoTVI4d0hRWURWUVFEREJaa1pXNHdNV1YyYnk1MWN5NXZjbUZqYkdVdVkyOXRNSUlCCklqQU5CZ2txaGtpRzl3MEJBUUVGQUFPQ0FROEFNSUlCQ2dLQ0FRRUEyY0lpVUhwcTRVZzBhaGR6aXkycHY2cHQKSVIza2s5REd2eVRNY0syaWZQQ2dtUU5CdHV6VXNFN0l4c294eldITmU5RFpXRXJTSjVON3FYYm1lTzJkMVd2NQp1aFhzbkFTbnkwY1NLUE9xOUNZVDNQSlpDVk1MK0llZVFKdnhaVjZaWWU4V2FFL1NQSGJzczRjYy9wcG1mc3pxCnErUi83cXEyMm9ueHNHaE9vQ1h1TlQvMFF2WXVzMnNucGtueWRKRHUxelhGbDREYkFIZGMvamNVK0NPWWROeS8KT3Iza2JIV0FaTkR4OWxaZUREOTRmNXZLbGJwMy9rcUF2V0FkSVJZa2UrSmpNTHg0VHo2ZlM0VXoxbzdBSTVuSApPQ1ZMblV5U0JkaGVuWTNGNEdFU0wwbnorVlhFWjRWVjRucWNjRmo5cnJ0Q29pT1BBNlgvNGdxMEZJbi9Qd0lECkFRQUJvMUF3VGpBZEJnTlZIUTRFRmdRVWw1VnVpVDBDT0xGTzcxMFBlcHRxSC9DRWZyY3dId1lEVlIwakJCZ3cKRm9BVWw1VnVpVDBDT0xGTzcxMFBlcHRxSC9DRWZyY3dEQVlEVlIwVEJBVXdBd0VCL3pBTkJna3Foa2lHOXcwQgpBUXNGQUFPQ0FRRUFXdEN4b2ZmNGgrWXZEcVVpTFFtUnpqQkVBMHJCOUMwL1FWOG9JQzJ3d1hzYi9KaVNuMHdOCjNMdHppejc0aStEbk1yQytoNFQ3enRaSkc3NVluSGRKcmxQajgzVWdDLzhYTlFCSUNDbTFUa3RlVU1jWG0reG4KTEZEMHpReFhpVzV0N1FHcWtvK2FjeN3JRMXlNSE9HdVVkTTZETzErNXF4cTdFNXFMamhyNEdKejV5OAoraW8zK25UcUVKMHFQOVRocG96RXhBMW80OEY0ZHJybWdqd3ROUldEQVpBYmYyV1JNMXFKWXhxTTJqdU1FQWNsCnFMek1TdEZUQ2o1UGFTQ0NUV1VEK3ZlSWtsRWRpaFdpRm02dzk3Y1diZ0lGMlhlNGk4L2szMmF1N2xUTDEvd28KU3Q2dHpsa20yV25uUFlVMzBnRURnVTQ4OU02Z1dybklpZz09Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K tls.key: LS0tLS1CRUdJQVRFIEtFWS0tLS0tCk1JSUV1d0lCQURBTkJna3Foa2lHOXcwQkFRRUZBQVNDQktVd2dnU2hBZ0VBQW9JQkFRRFp3aUpRZW1yaFNEUnEKRjNPTExhbS9xbTBoSGVTVDBNYS9KTXh3cmFKODhLQ1pBMEcyN05Td1Rzakd5akhOWWMxNzBObFlTdEluazN1cApkdVo0N1ozVmEvbTZGZXljQktmTFJ4SW84NnIwSmhQYzhsa0pVd3Y0aDU1QW0vRmxYcGxoN3hab1Q5SThkdXl6Cmh4eittbVorek9xcjVIL3VxcmJhaWZHd2FFNmdKZTQxUC9SQzlpNnpheWVtU2ZKMGtPN1hOY1dYZ05zQWQxeisKTnhUNEk1aDAzTDg2dmVSc2RZQmswUEgyVmw0TVAzaC9tOHFWdW5mK1NvQzlZQjBoRmlSNzRtTXd2SGhQUHA5TApoVFBXanNBam1jYzRKVXVkVEpJRjJGNmRqY1hnWVJJdlNmUDVWY1JuaFZYaWVweHdXUDJ1dTBLaUk0OERwZi9pCkNyUVVpZjgvQWdNQkFBRUNnZjl6cnE2TUVueTFNYWFtdGM2c0laWU1QSDI5R2lSVVlwVXk5bG1sZ3BqUHh3V0sKUkRDay9Td0FmZG9yd1Q2ejNVRk1oYWJ4UU01a04vVjZFYkJlamQxTGhCRW15bjdvWTVEQWJRRTR3RG9SZWlrVApONndWU0FrVC92Z1RXc1RqRlY1bXFKMCt6U2ppOWtySkZQNVNRN1F2cUswQ3BHRlNhVjY2dW8ycktiNmJWSkJYCkxPZmZPMytlS0tVazBaTnE1Q1NVQk9mbnFoNVFJSGdpaDNiMTRlNjBDdGhYcEh6bndrNWhaMHBHZE9BQm9aTkoKZ21lanUyTEdzVWxXTjBLOVdsUy9lcUllQzVzQm9jaWlocmxMVUpGWnpPRUV6LzErT2cyemhmT29yTE9rMTIrTgpjQnV0cTJWQ2I4ZFJDaFg1ZzJ0WnBrdzgzcXN5RSt3M09zYlQxa0VDZ1lFQTdxUnRLWGFONUx1SENvWlM1VWhNCm9Hak1WcnYxTEg0eGNhaDJITmZnMksrMHJqQkJONGpkZkFDMmF3R3ZzU1EyR0lYRzVGYmYyK0pwL1kxbktKOEgKZU80MzNLWVgwTDE4NlNNLzFVay9HSEdTek1CWS9KdGR6WkRrbTA4UnBwaTl4bExTeDBWUWtFNVJVcnJJcTRJVwplZzBOM2RVTHZhTVl1UTBrR2dncUFETUNnWUVBNlpqWCtjU2VMZ1BVajJENWRpUGJ1TmVFd2RMeFNPZDFZMUFjCkUzQ01YTWozK2JxQ3BGUVIrTldYWWVuVmM1QiszajlSdHVnQ0YyTkNSdVdkZWowalBpL243UExIRHdCZVY0bVIKM3VQVHJmamRJbFovSFgzQ2NjVE94TmlaajU4VitFdkRHNHNHOGxtRTRieStYRExIYTJyMWxmUk9sUVRMSyswVgpyTU93eU1VQ2dZRUF1dm14WGM4NWxZRW9hU0tkU0cvQk9kMWlYSUtmc2VDZHRNT2M1elJ0UXRsSDQwS0RscE54CmxYcXBjbVc3MWpyYzk1RzVKNmE1ZG5xTE9OSFZoWW8wUEpmSXhPU052RXI2MTE5NjRBMm5sZXRHYlk0M0twUkEKaHBPRHlmdkZoSllmK29kaUJpZFUyL3ZBMCtUczNSUHJzRzBSOUVDOEZqVDNaZVhaNTF1R0xPa0NnWUFpTmU0NwplQjRxWXdrNFRsMTZmZG5xQWpaQkpLR05xY2c1V1R3alpMSkp6R3owdCtuMkl4SFd2WUZFSjdqSkNmcHFsaDlqCmlDcjJQZVV3K09QTlNUTG1JcUgydzc5L1pQQnNKWXVsZHZ4RFdGVWFlRXg1aHpkNDdmZlNRRjZNK0NHQmthYnIKVzdzU3R5V000ZFdITHpDaGZMS20yWGJBd0VqNUQrbkN1WTRrZVFLQmdFSkRHb0puM1NCRXcra2xXTE85N09aOApnc3lYQm9mUW1lRktIS2NHNzFZUFhJbTRlV1kyUi9KOCt5anc5b1FJQ3o5NlRidkdSZEN5QlJhbWhoTmFGUzVyCk9MZUc0ejVENE4zdThUc0dNem9QcU13KzBGSXJiQ3FzTnpGWTg3ekZweEdVaXZvRWZLNE82YkdERTZjNHFqNGEKNmlmK0RSRSt1TWRMWTQyYTA3ekoKLS0tLS1FTkQgUFJJVkFURSBLRVktLS0tLQo= kind: Secret metadata: creationTimestamp: \u0026quot;2020-08-10T14:22:52Z\u0026quot; managedFields: - apiVersion: v1 fieldsType: FieldsV1 fieldsV1: f:data: .: {} f:tls.crt: {} f:tls.key: {} f:type: {} manager: kubectl operation: Update time: \u0026quot;2020-08-10T14:22:52Z\u0026quot; name: oimcluster-tls-cert namespace: oimcluster resourceVersion: \u0026quot;3722477\u0026quot; selfLink: /api/v1/namespaces/oimcluster/secrets/oimcluster-tls-cert uid: 596fe0fe-effd-4eb9-974d-691da3a3b15a type: kubernetes.io/tls   Add the Voyager ingress using helm Note: If already using Voyager with SSL for OIG you can skip this section:\n  Add the Helm chart repository for Voyager using the following command:\n$ helm repo add appscode https://charts.appscode.com/stable The output will look similar to the following:\n\u0026#34;appscode\u0026#34; has been added to your repositories   Update the repository using the following command:\n$ helm repo update The output will look similar to the following:\nHang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \u0026#34;appscode\u0026#34; chart repository Update Complete. Happy Helming!   Create a namespace for Voyager:\n$ kubectl create namespace voyagerssl The output will look similar to the following:\nnamespace/voyagerssl created   Install Voyager ingress using helm   Run the following command to install the ingress:\n$ helm install voyager-designconsole-operator appscode/voyager --version v12.0.0-rc.1 --namespace voyagerssl --set cloudProvider=baremetal --set apiserver.enableValidatingWebhook=false Note: For bare metal Kubernetes use --set cloudProvider=baremetal. If using a managed Kubernetes service then the value should be set for your specific service as per the Voyager install guide.\nThe output will look similar to the following:\nNAME: voyager-designconsole-operator LAST DEPLOYED: Wed Oct 21 09:24:55 2020 NAMESPACE: voyagerssl STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: Set cloudProvider for installing Voyager To verify that Voyager has started, run: kubectl --namespace=voyagerssl get deployments -l \u0026quot;release=voyager-designconsole-operator, app=voyager\u0026quot;   Setup Routing Rules for the Design Console ingress   Setup routing rules by running the following commands:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/charts/design-console-ingress $ cp values.yaml values.yaml.orig $ vi values.yaml Edit values.yaml and ensure that type=VOYAGER, tls=SSL and secretName:oimcluster-tls-cert are set, and that webPort and statsPort are set to free ports, for example:\n$ cat values.yaml # Copyright 2020 Oracle Corporation and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # Default values for design-console-ingress. # This is a YAML-formatted file. # Declare variables to be passed into your templates. # Load balancer type. Supported values are: VOYAGER, NGINX type: VOYAGER # Type of Configuration Supported Values are : NONSSL,SSL # tls: NONSSL tls: SSL # TLS secret name if the mode is SSL secretName: oimcluster-tls-cert # WLS domain as backend to the load balancer wlsDomain: domainUID: oimcluster oimClusterName: oim_cluster oimServerT3Port: 14001 # Voyager specific values voyager: # web port webPort: 30330 # stats port statsPort: 30331   Create the ingress   Run the following command to create the ingress:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator $ helm install oimcluster-voyager-designconsole kubernetes/samples/charts/design-console-ingress --namespace oimcluster --values kubernetes/samples/charts/design-console-ingress/values.yaml For example:\n$ cd /scratch/OIGDockerK8S/weblogic-kubernetes-operator $ helm install oimcluster-voyager-designconsole kubernetes/samples/charts/design-console-ingress --namespace oimcluster --values kubernetes/samples/charts/design-console-ingress/values.yaml The output will look similar to the following:\nNAME: oimcluster-voyager-designconsole LAST DEPLOYED: Wed Oct 21 09:59:43 2020 NAMESPACE: oimcluster STATUS: deployed REVISION: 1 TEST SUITE: None   Run the following command to show the ingress is created successfully:\n$ kubectl get ingress.voyager.appscode.com -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl get ingress.voyager.appscode.com -n oimcluster The output will look similar to the following:\nNAME HOSTS LOAD_BALANCER_IP AGE oimcluster-voyager-designconsole * 10s   Return details of the ingress using the following command:\n$ kubectl describe ingress.voyager.appscode.com oimcluster-voyager-designconsole -n oimcluster The output will look similar to the following:\nName: oimcluster-voyager-designconsole Namespace: oimcluster Labels: app.kubernetes.io/managed-by=Helm weblogic.resourceVersion=domain-v2 Annotations: ingress.appscode.com/affinity: cookie ingress.appscode.com/stats: true ingress.appscode.com/type: NodePort meta.helm.sh/release-name: oimcluster-voyager-designconsole meta.helm.sh/release-namespace: oimcluster API Version: voyager.appscode.com/v1beta1 Kind: Ingress Metadata: Creation Timestamp: 2020-10-21T09:26:48Z Generation: 1 Resource Version: 15430914 Self Link: /apis/voyager.appscode.com/v1beta1/namespaces/oimcluster/ingresses/oimcluster-voyager-designconsole UID: 89f42060-c8e6-470f-b661-14b9969fe1aa Spec: Frontend Rules: Port: 443 Rules: http-request set-header WL-Proxy-SSL true Rules: Host: * Http: Node Port: 30330 Paths: Backend: Service Name: oimcluster-cluster-oim-cluster Service Port: 14001 Path: / Tls: Hosts: * Secret Name: dc-tls-cert Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ServiceReconcileSuccessful 54m voyager-operator Successfully patched NodePort Service voyager-oimcluster-voyager-designconsole Normal DeploymentReconcileSuccessful 54m voyager-operator Successfully patched HAProxy Deployment voyager-oimcluster-voyager-designconsole Normal DeploymentReconcileSuccessful 44m voyager-operator Successfully patched HAProxy Deployment voyager-oimcluster-voyager-designconsole   Design Console Client The instructions below should be performed on the client where Design Console is installed.\nImport the CA certificate into the java keystore If in Generate a SSL Certificate you requested a certificate from a Certificate Authority (CA), then you must import the CA certificate (e.g cacert.crt) that signed your certificate, into the java truststore used by Design Console.\nIf in Generate a SSL Certificate you generated a self-signed certicate (e.g tls.crt), you must import the self-signed certificate into the java truststore used by Design Console.\nImport the certificate using the following command:\n$ keytool -import -trustcacerts -alias dc -file \u0026lt;certificate\u0026gt; -keystore $JAVA_HOME/jre/lib/security/cacerts where \u0026lt;certificate\u0026gt; is the CA certificate, or self-signed certicate.\nLogin to the Design Console   Launch the Design Console and in the Oracle Identity Manager Design Console login page enter the following details:\nEnter the following details and click Login:\n Server URL: \u0026lt;url\u0026gt; User ID: xelsysadm Password: \u0026lt;password\u0026gt;.  where \u0026lt;url\u0026gt; is http://\u0026lt;masternode.example.com\u0026gt;:\u0026lt;NodePort\u0026gt;\n\u0026lt;NodePort\u0026gt; is the value passed for webPort in the `values.yaml earlier, for example: 30330\n  If successful the Design Console will be displayed. If the VNC session disappears then the connection failed so double check the connection details and try again.\n  "
},
{
	"uri": "/fmw-kubernetes/20.4.1/oudsm/patch-and-upgrade/upgrade_a_kubernetes_cluster/",
	"title": "b) Upgrade a Kubernetes cluster",
	"tags": [],
	"description": "Instructions on how to upgrade a Kubernetes cluster.",
	"content": "These instructions describe how to upgrade a Kubernetes cluster created using kubeadm on which an Oracle Unified Directory Services Manager instance is deployed. A rolling upgrade approach is used to upgrade nodes (master and worker) of the Kubernetes cluster.\nIt is expected that there will be a down time during the upgrade of the Kubernetes cluster as the nodes need to be drained as part of the upgrade process.\n Prerequisites  Review Prerequisites and ensure that your Kubernetes cluster is ready for upgrade. Make sure your environment meets all prerequisites.  Upgrade the Kubernetes version An upgrade of Kubernetes is supported from one MINOR version to the next MINOR version, or between PATCH versions of the same MINOR. For example, you can upgrade from 1.x to 1.x+1, but not from 1.x to 1.x+2. To upgrade a Kubernetes version, first all the master nodes of the Kubernetes cluster must be upgraded sequentially, followed by the sequential upgrade of each worker node.\n See here for Kubernetes official documentation to upgrade from v1.16.x to v1.17.x. See here for Kubernetes official documentation to upgrade from v1.17.x to v1.18.x.  "
},
{
	"uri": "/fmw-kubernetes/20.4.1/oig/post-install-config/install_and_configure_connectors/",
	"title": "b. Install and Configure Connectors",
	"tags": [],
	"description": "Install and Configure Connectors.",
	"content": "Download the Connector   Download the Connector you are interested in from Oracle Identity Manager Connector Downloads.\n  Copy the connector zip file to a staging directory on the master node e.g. /scratch/OIGDocker/stage and unzip it:\n$ cp $HOME/Downloads/\u0026lt;connector\u0026gt;.zip \u0026lt;work directory\u0026gt;/\u0026lt;stage\u0026gt;/ $ cd \u0026lt;work directory\u0026gt;/\u0026lt;stage\u0026gt; $ unzip \u0026lt;connector\u0026gt;.zip For example:\n$ cp $HOME/Downloads/Exchange-12.2.1.3.0.zip /scratch/OIGDocker/stage/ $ cd /scratch/OIGDockerK8S/stage/ $ unzip Exchange-12.2.1.3.0.zip   Create a directory in the persistent volume   On the master node run the following command to create a ConnectorDefaultDirectory:\n$ kubectl exec -ti oimcluster-oim-server1 -n \u0026lt;domain_namespace\u0026gt; -- mkdir -p /u01/oracle/user_projects/domains/ConnectorDefaultDirectory For example:\n$ kubectl exec -ti oimcluster-oim-server1 -n oimcluster -- mkdir -p /u01/oracle/user_projects/domains/ConnectorDefaultDirectory Note: This will create a directory in the persistent volume e:g /scratch/OIGDockerK8S/oimclusterdomainpv/ConnectorDefaultDirectory,\n  Copy OIG Connectors There are two options to copy OIG Connectors to your Kubernetes cluster:\n a) Copy the connector directly to the Persistent Volume b) Use the kubectl cp command to copy the connector to the Persistent Volume  It is recommended to use option a), however there may be cases, for example when using a Managed Service such as Oracle Kubernetes Engine on Oracle Cloud Infrastructure, where it may not be feasible to directly mount the domain directory. In such cases option b) should be used.\na) Copy the connector directly to the Persistent Volume   Copy the connector zip file to the persistent volume. For example:\n$ cp -R \u0026lt;path_to\u0026gt;/\u0026lt;connector\u0026gt; \u0026lt;work directory\u0026gt;/oimclusterdomainpv/ConnectorDefaultDirectory/ For example:\n$ cp -R /scratch/OIGDockerK8S/stage/Exchange-12.2.1.3.0 /scratch/OIGDockerK8S/oimclusterdomainpv/ConnectorDefaultDirectory/   b) Use the kubectl cp command to copy the connector to the Persistent Volume   Run the following command to copy over the connector:\n$ kubectl -n \u0026lt;domain_namespace\u0026gt; cp \u0026lt;path_to\u0026gt;/\u0026lt;connector\u0026gt; \u0026lt;cluster_name\u0026gt;:/u01/oracle/idm/server/ConnectorDefaultDirectory/ For example:\n$ kubectl -n oimcluster cp /scratch/OIGDockerK8S/stage/Exchange-12.2.1.3.0 oimcluster-oim-server1:/u01/oracle/idm/server/ConnectorDefaultDirectory/   Install the Connector The connectors are installed as they are on a standard on-premises setup, via Application On Boarding or via Connector Installer.\nRefer to your Connector specific documentation for instructions.\n"
},
{
	"uri": "/fmw-kubernetes/20.4.1/oam/patch-and-upgrade/upgrade_an_operator_release/",
	"title": "b. Upgrade an operator release",
	"tags": [],
	"description": "Instructions on how to update the Oracle WebLogic Server Kubernetes Operator version.",
	"content": "These instructions apply to upgrading the operator within the 3.x release family as additional versions are released.\nThe new Oracle WebLogic Server Kubernetes Operator Docker image must be installed on the master node AND each of the worker nodes in your Kubernetes cluster. Alternatively you can place the image in a Docker registry that your cluster can access.\n   Pull the Oracle WebLogic Server Kubernetes Operator 3.X.X image by running the following command on the master node:\n$ docker pull ghcr.io/oracle/weblogic-kubernetes-operator:3.X.X where 3.X.X is the version of the operator you require.\n  Run the docker tag command as follows:\n$ docker tag ghcr.io/oracle/weblogic-kubernetes-operator:3.X.X weblogic-kubernetes-operator:3.X.X where 3.X.X is the version of the operator downloaded.\nAfter installing the new Oracle WebLogic Server Kubernetes Operator Docker image, repeat the above on the worker nodes.\n  On the master node, download the new Oracle WebLogic Server Kubernetes Operator source code from the operator github project:\n$ mkdir \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator-3.X.X $ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator-3.X.X $ git clone https://github.com/oracle/weblogic-kubernetes-operator.git --branch release/3.X.X For example:\n$ cd /scratch/OAMDockerK8S/weblogic-kubernetes-operator-3.X.X $ git clone https://github.com/oracle/weblogic-kubernetes-operator.git --branch release/3.X.X This will create the directory \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator-3.X.X/weblogic-kubernetes-operator\n  Run the following helm command to upgrade the operator:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator-3.X.X/weblogic-kubernetes-operator $ helm upgrade --reuse-values --set image=weblogic-kubernetes-operator:3.X.X --namespace \u0026lt;sample-kubernetes-operator-ns\u0026gt; --wait weblogic-kubernetes-operator kubernetes/charts/weblogic-operator For example:\n$ cd /scratch/OAMDockerK8S/weblogic-kubernetes-operator-3.X.X/weblogic-kubernetes-operator $ helm upgrade --reuse-values --set image=weblogic-kubernetes-operator:3.X.X --namespace opns --wait weblogic-kubernetes-operator kubernetes/charts/weblogic-operator The output will look similar to the following:\nRelease \u0026#34;weblogic-kubernetes-operator\u0026#34; has been upgraded. Happy Helming! NAME: weblogic-kubernetes-operator LAST DEPLOYED: Mon Sep 28 02:50:07 2020 NAMESPACE: opns STATUS: deployed REVISION: 3 TEST SUITE: None   "
},
{
	"uri": "/fmw-kubernetes/20.4.1/oig/patch-and-upgrade/upgrade_an_operator_release/",
	"title": "b. Upgrade an operator release",
	"tags": [],
	"description": "Instructions on how to update the Oracle WebLogic Kubernetes Operator version.",
	"content": "These instructions apply to upgrading operators within the 3.x release family as additional versions are released.\nThe new Oracle WebLogic Kubernetes Operator Docker image must be installed on the master node AND each of the worker nodes in your Kubernetes cluster. Alternatively you can place the image in a Docker registry that your cluster can access.\n   Pull the Oracle WebLogic Kubernetes Operator 3.X.X image by running the following command on the master node:\n$ docker pull oracle/weblogic-kubernetes-operator:3.X.X where 3.X.X is the version of the operator you require.\n  Run the docker tag command as follows:\n$ docker tag oracle/weblogic-kubernetes-operator:3.X.X weblogic-kubernetes-operator:3.X.X where 3.X.X is the version of the operator downloaded.\nAfter installing the new Oracle WebLogic Kubernetes Operator Docker image, repeat the above on the worker nodes.\n  On the master node, download the new Oracle WebLogic Kubernetes Operator source code from the operator github project:\n$ mkdir \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator-3.X.X $ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator-3.X.X $ git clone https://github.com/oracle/weblogic-kubernetes-operator.git --branch release/3.X.X For example:\n$ mkdir /scratch/OIGDockerK8S/weblogic-kubernetes-operator-3.X.X $ cd /scratch/OIGDockerK8S/weblogic-kubernetes-operator-3.X.X $ git clone https://github.com/oracle/weblogic-kubernetes-operator.git --branch release/3.X.X This will create the directory \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator-3.X.X/weblogic-kubernetes-operator\n  Run the following helm command to upgrade the operator:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator-3.X.X/weblogic-kubernetes-operator $ helm upgrade --reuse-values --set image=oracle/weblogic-kubernetes-operator:3.X.X --namespace \u0026lt;sample-kubernetes-operator-ns\u0026gt; --wait weblogic-kubernetes-operator kubernetes/charts/weblogic-operator For example:\n$ cd /scratch/OIGDockerK8S/weblogic-kubernetes-operator-3.X.X/weblogic-kubernetes-operator $ helm upgrade --reuse-values --set image=oracle/weblogic-kubernetes-operator:3.X.X --namespace operator --wait weblogic-kubernetes-operator kubernetes/charts/weblogic-operator The output will look similar to the following:\nRelease \u0026#34;weblogic-kubernetes-operator\u0026#34; has been upgraded. Happy Helming! NAME: weblogic-kubernetes-operator LAST DEPLOYED: Thu Oct 01 02:50:07 2020 NAMESPACE: operator STATUS: deployed REVISION: 3 TEST SUITE: None   "
},
{
	"uri": "/fmw-kubernetes/20.4.1/oig/configure-ingress/ingress-nginx-setup-for-oig-domain-setup-on-k8s-ssl/",
	"title": "b. Using an Ingress with NGINX (SSL)",
	"tags": [],
	"description": "Steps to set up an Ingress for NGINX to direct traffic to the OIG domain (SSL).",
	"content": "Setting Up an Ingress for NGINX for the OIG Domain on Kubernetes The instructions below explain how to set up NGINX as an ingress for the OIG domain with SSL termination.\nNote: All the steps below should be performed on the master node.\n Create a SSL Certificate  Generate SSL Certificate Create a Kubernetes Secret for SSL   Install NGINX  Configure the repository Create a Namespace Install NGINX using helm   Create an Ingress for the Domain Verify that You can Access the Domain URL Cleanup  Create a SSL Certificate Generate SSL Certificate   Generate a private key and certificate signing request (CSR) using a tool of your choice. Send the CSR to your certificate authority (CA) to generate the certificate.\nIf you want to use a certificate for testing purposes you can generate a self signed certificate using openssl:\n$ mkdir \u0026lt;work directory\u0026gt;/ssl $ cd \u0026lt;work directory\u0026gt;/ssl $ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \u0026quot;/CN=\u0026lt;nginx-hostname\u0026gt;\u0026quot; For example:\n$ mkdir /scratch/OIGDockerK8S/ssl $ cd /scratch/OIGDockerK8S/ssl $ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \u0026quot;/CN=masternode.example.com\u0026quot; Note: The CN should match the host.domain of the master node in order to prevent hostname problems during certificate verification.\nThe output will look similar to the following:\n$ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \u0026quot;/CN=masternode.example.com\u0026quot; Generating a 2048 bit RSA private key ..........................................+++ .......................................................................................................+++ writing new private key to 'tls.key' -----   Create a Kubernetes Secret for SSL   Create a secret for SSL containing the SSL certificate by running the following command:\n$ kubectl -n oimcluster create secret tls \u0026lt;domain_id\u0026gt;-tls-cert --key \u0026lt;work directory\u0026gt;/tls.key --cert \u0026lt;work directory\u0026gt;/tls.crt For example:\n$ kubectl -n oimcluster create secret tls oimcluster-tls-cert --key /scratch/OIGDockerK8S/ssl/tls.key --cert /scratch/OIGDockerK8S/ssl/tls.crt The output will look similar to the following:\n$ kubectl -n oimcluster create secret tls oimcluster-tls-cert --key /scratch/OIGDockerK8S/ssl/tls.key --cert /scratch/OIGDockerK8S/ssl/tls.crt secret/oimcluster-tls-cert created $   Confirm that the secret is created by running the following command:\n$ kubectl get secret oimcluster-tls-cert -o yaml -n oimcluster The output will look similar to the following:\napiVersion: v1 data: tls.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURGVENDQWYyZ0F3SUJBZ0lKQUl3ZjVRMWVxZnljTUEwR0NTcUdTSWIzRFFFQkN3VUFNQ0V4SHpBZEJnTlYKQkFNTUZtUmxiakF4WlhadkxuVnpMbTl5WVdOc1pTNWpiMjB3SGhjTk1qQXdPREV3TVRReE9UUXpXaGNOTWpFdwpPREV3TVRReE9UUXpXakFoTVI4d0hRWURWUVFEREJaa1pXNHdNV1YyYnk1MWN5NXZjbUZqYkdVdVkyOXRNSUlCCklqQU5CZ2txaGtpRzl3MEJBUUVGQUFPQ0FROEFNSUlCQ2dLQ0FRRUEyY0lpVUhwcTRVZzBhaGR6aXkycHY2cHQKSVIza2s5REd2eVRNY0syaWZQQ2dtUU5CdHV6VXNFN0l4c294eldITmU5RFpXRXJTSjVON3FYYm1lTzJkMVd2NQp1aFhzbkFTbnkwY1NLUE9xVDNQSlpDVk1MK0llZVFKdnhaVjZaWWU4V2FFL1NQSGJzczRjYy9wcG1mc3pxCnErUi83cXEyMm9ueHNHaE9vQ1h1TlQvMFF2WXVzMnNucGtueWRKRHUxelhGbDREYkFIZGMvamNVK0NPWWROeS8KT3Iza2JIV0FaTkR4OWxaZUREOTRmNXZLcUF2V0FkSVJZa2UrSmpNTHg0VHo2ZlM0VXoxbzdBSTVuSApPQ1ZMblV5U0JkaGVuWTNGNEdFU0wwbnorVlhFWjRWVjRucWNjRmo5cnJ0Q29pT1BBNlgvNGdxMEZJbi9Qd0lECkFRQUJvMUF3VGpBZEJnTlZIUTRFRmdRVWw1VnVpVDBDT0xGTzcxMFBlcHRxSC9DRWZyY3dId1lEVlIwakJCZ3cKRm9BVWw1VnVpVDBDT0xGTzcxMFBlcHRxSC9DRWZyY3dEQVlEVlIwVEJBVXdBd0VCL3pBTkJna3Foa2lHOXcwQgpBUXNGQUFPQ0FRRUFXdEN4b2ZmNGgrWXZEcVVpTFFtUnpqQkVBMHJCOUMwL1FWOG9JQzJ3d1hzYi9KaVNuMHdOCjNMdHppejc0aStEbk1yQytoNFQ3enRaSkc3NVluSGRKcmxQajgzVWdDLzhYTlFCSUNDbTFUa3RlVU1jWG0reG4KTEZEMHpReFhpVzV0N1FHcWtvK2FjeTlhUnUvN3JRMXlNSE9HdVVkTTZETzErNXF4cTdFNXFMamhyNEdKejV5OAoraW8zK25UcUVKMHFQOVRocG96RXhBMW80OEY0ZHJybWdqd3ROUldEQVpBYmYyV1JNMXFKWXhxTTJqdU1FQWNsCnFMek1TdEZUQ2o1UGFTQ0NUV1VEK3ZlSWtsRWRpaFdpRm02dzk3Y1diZ0lGMlhlNGk4L2szMmF1N2xUTDEvd28KU3Q2dHpsa20yV25uUFlVMzBnRURnVTQ4OU02Z1dybklpZz09Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K tls.key: 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 kind: Secret metadata: creationTimestamp: \u0026quot;2020-09-29T15:51:22Z\u0026quot; managedFields: - apiVersion: v1 fieldsType: FieldsV1 fieldsV1: f:data: .: {} f:tls.crt: {} f:tls.key: {} f:type: {} manager: kubectl operation: Update time: \u0026quot;2020-09-29T15:51:22Z\u0026quot; name: oimcluster-tls-cert namespace: oimcluster resourceVersion: \u0026quot;1291036\u0026quot; selfLink: /api/v1/namespaces/oimcluster/secrets/oimcluster-tls-cert uid: a127e5fd-705b-43e1-ab56-590834efda5e type: kubernetes.io/tls   Install NGINX Use helm to install NGINX.\nConfigure the repository   Add the Helm chart repository for installing NGINX using the following command:\n$ helm repo add stable https://kubernetes.github.io/ingress-nginx The output will look similar to the following:\n\u0026quot;stable\u0026quot; has been added to your repositories   Update the repository using the following command:\n$ helm repo update The output will look similar to the following:\nHang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \u0026quot;stable\u0026quot; chart repository Update Complete. Happy Helming!   Create a Namespace   Create a Kubernetes namespace for NGINX:\n$ kubectl create namespace nginxssl The output will look similar to the following:\nnamespace/nginxssl created   Install NGINX using helm If you can connect directly to the master node IP address from a browser, then install NGINX with the --set controller.service.type=NodePort parameter.\nIf you are using a Managed Service for your Kubernetes cluster, for example Oracle Kubernetes Engine (OKE) on Oracle Cloud Infrastructure (OCI), and connect from a browser to the Load Balancer IP address, then use the --set controller.service.type=LoadBalancer parameter. This instructs the Managed Service to setup a Load Balancer to direct traffic to the NGINX ingress.\n  To install NGINX use the following helm command depending on if you are using NodePort or LoadBalancer:\na) Using NodePort\n$ helm install nginx-ingress -n nginxssl --set controller.extraArgs.default-ssl-certificate=oimcluster/oimcluster-tls-cert --set controller.service.type=NodePort --set controller.admissionWebhooks.enabled=false stable/ingress-nginx The output will look similar to the following:\n$ helm install nginx-ingress -n nginxssl --set controller.extraArgs.default-ssl-certificate=oimcluster/oimcluster-tls-cert --set controller.service.type=NodePort --set controller.admissionWebhooks.enabled=false stable/ingress-nginx NAME: nginx-ingress LAST DEPLOYED: Tue Sep 29 08:53:30 2020 NAMESPACE: nginxssl STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: The nginx-ingress controller has been installed. Get the application URL by running these commands: export HTTP_NODE_PORT=$(kubectl --namespace nginxssl get services -o jsonpath=\u0026quot;{.spec.ports[0].nodePort}\u0026quot; nginx-ingress-controller) export HTTPS_NODE_PORT=$(kubectl --namespace nginxssl get services -o jsonpath=\u0026quot;{.spec.ports[1].nodePort}\u0026quot; nginx-ingress-controller) export NODE_IP=$(kubectl --namespace nginxssl get nodes -o jsonpath=\u0026quot;{.items[0].status.addresses[1].address}\u0026quot;) echo \u0026quot;Visit http://$NODE_IP:$HTTP_NODE_PORT to access your application via HTTP.\u0026quot; echo \u0026quot;Visit https://$NODE_IP:$HTTPS_NODE_PORT to access your application via HTTPS.\u0026quot; An example Ingress that makes use of the controller: apiVersion: extensions/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx name: example namespace: foo spec: rules: - host: www.example.com http: paths: - backend: serviceName: exampleService servicePort: 80 path: / # This section is only required if TLS is to be enabled for the Ingress tls: - hosts: - www.example.com secretName: example-tls If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided: apiVersion: v1 kind: Secret metadata: name: example-tls namespace: foo data: tls.crt: \u0026lt;base64 encoded cert\u0026gt; tls.key: \u0026lt;base64 encoded key\u0026gt; type: kubernetes.io/tls b) Using LoadBalancer\n$ helm install nginx-ingress -n nginxssl --set controller.extraArgs.default-ssl-certificate=oimcluster/oimcluster-tls-cert --set controller.service.type=LoadBalancer --set controller.admissionWebhooks.enabled=false stable/ingress-nginx The output will look similar to the following:\n$ helm install nginx-ingress -n nginxssl --set controller.extraArgs.default-ssl-certificate=oimcluster/oimcluster-tls-cert --set controller.service.type=LoadBalancer --set controller.admissionWebhooks.enabled=false stable/ingress-nginx NAME: nginx-ingress LAST DEPLOYED: Tue Sep 29 08:53:30 2020 NAMESPACE: nginxssl STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: The ingress-nginx controller has been installed. It may take a few minutes for the LoadBalancer IP to be available. You can watch the status by running 'kubectl --namespace nginxssl get services -o wide -w nginx-ingress-ingress-nginx-controller' An example Ingress that makes use of the controller: apiVersion: extensions/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx name: example namespace: foo spec: rules: - host: www.example.com http: paths: - backend: serviceName: exampleService servicePort: 80 path: / # This section is only required if TLS is to be enabled for the Ingress tls: - hosts: - www.example.com secretName: example-tls If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided: apiVersion: v1 kind: Secret metadata: name: example-tls namespace: foo data: tls.crt: \u0026lt;base64 encoded cert\u0026gt; tls.key: \u0026lt;base64 encoded key\u0026gt; type: kubernetes.io/tls   Setup Routing Rules for the Domain   Setup routing rules by running the following commands:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain $ cp values.yaml values.yaml.orig $ vi values.yaml   Edit values.yaml and ensure that the values type=NGINX, tls=SSL and secretName=oimcluster-tls-cert are set, for example:\n$ cat values.yaml # Copyright 2020 Oracle Corporation and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # Default values for ingress-per-domain. # This is a YAML-formatted file. # Declare variables to be passed into your templates. # Load balancer type. Supported values are: VOYAGER, NGINX type: NGINX # Type of Configuration Supported Values are : NONSSL,SSL # tls: NONSSL tls: SSL # TLS secret name if the mode is SSL secretName: oimcluster-tls-cert # WLS domain as backend to the load balancer wlsDomain: domainUID: oimcluster oimClusterName: oim_cluster soaClusterName: soa_cluster soaManagedServerPort: 8001 oimManagedServerPort: 14000 adminServerName: adminserver adminServerPort: 7001 # Traefik specific values # traefik: # hostname used by host-routing # hostname: idmdemo.m8y.xyz # Voyager specific values voyager: # web port webPort: 30305 # stats port statsPort: 30315   Create an Ingress for the Domain   Create an Ingress for the domain (oimcluster-nginx), in the domain namespace by using the sample Helm chart:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator $ helm install oimcluster-nginx kubernetes/samples/charts/ingress-per-domain --namespace oimcluster --values kubernetes/samples/charts/ingress-per-domain/values.yaml Note: The \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain/templates/nginx-ingress.yaml has nginx.ingress.kubernetes.io/enable-access-log set to false. If you want to enable access logs then set this value to true before executing the command. Enabling access-logs can cause issues with disk space if not regularly maintained.\nFor example:\n$ cd /scratch/OIGDockerK8S/weblogic-kubernetes-operator $ helm install oimcluster-nginx kubernetes/samples/charts/ingress-per-domain --namespace oimcluster --values kubernetes/samples/charts/ingress-per-domain/values.yaml The output will look similar to the following:\nNAME: oimcluster-nginx LAST DEPLOYED: Tue Sep 29 08:56:38 2020 NAMESPACE: oimcluster STATUS: deployed REVISION: 1 TEST SUITE: None   Run the following command to show the ingress is created successfully:\n$ kubectl get ing -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl get ing -n oimcluster The output will look similar to the following:\nNAME CLASS HOSTS ADDRESS PORTS AGE oimcluster-nginx \u0026lt;none\u0026gt; * 80 49s   Find the node port of NGINX using the following command:\n$ kubectl get services -n nginxssl -o jsonpath=\u0026quot;{.spec.ports[1].nodePort}\u0026quot; nginx-ingress-ingress-nginx-controller The output will look similar to the following:\n32033$   Run the following command to check the ingress:\n$ kubectl describe ing oimcluster-nginx -n \u0026lt;namespace\u0026gt; For example:\n$ kubectl describe ing oimcluster-nginx -n oimcluster The output will look similar to the following:\nName: oimcluster-nginx Namespace: oimcluster Address: 10.103.131.225 Default backend: default-http-backend:80 (\u0026lt;error: endpoints \u0026quot;default-http-backend\u0026quot; not found\u0026gt;) Rules: Host Path Backends ---- ---- -------- * /console oimcluster-adminserver:7001 (10.244.1.42:7001) /em oimcluster-adminserver:7001 (10.244.1.42:7001) /soa oimcluster-cluster-soa-cluster:8001 (10.244.1.43:8001) /integration oimcluster-cluster-soa-cluster:8001 (10.244.1.43:8001) /soa-infra oimcluster-cluster-soa-cluster:8001 (10.244.1.43:8001) oimcluster-cluster-oim-cluster:14000 (10.244.1.44:14000) Annotations: kubernetes.io/ingress.class: nginx meta.helm.sh/release-name: oimcluster-nginx meta.helm.sh/release-namespace: oimcluster nginx.ingress.kubernetes.io/configuration-snippet: more_set_input_headers \u0026quot;X-Forwarded-Proto: https\u0026quot;; more_set_input_headers \u0026quot;WL-Proxy-SSL: true\u0026quot;; nginx.ingress.kubernetes.io/ingress.allow-http: false nginx.ingress.kubernetes.io/proxy-buffer-size: 2000k Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal CREATE 5m4s nginx-ingress-controller Ingress oimcluster/oimcluster-nginx Normal UPDATE 4m9s nginx-ingress-controller Ingress oimcluster/oimcluster-nginx   To confirm that the new Ingress is successfully routing to the domain\u0026rsquo;s server pods, run the following command to send a request to the URL for the \u0026ldquo;WebLogic ReadyApp framework\u0026rdquo;:\n$ curl -v -k https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/weblogic/ready For example:\n$ curl -v -k https://masternode.example.com:32033/weblogic/ready The output will look similar to the following:\n$ curl -v https://masternode.example.com:32033/weblogic/ready * About to connect() to 12.345.678.9 port 32033 (#0) * Trying 12.345.678.9... * Connected to 12.345.678.9 (12.345.678.9) port 32033 (#0) * Initializing NSS with certpath: sql:/etc/pki/nssdb * skipping SSL peer certificate verification * SSL connection using TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 * Server certificate: * subject: CN=masternode.example.com * start date: Sep 29 14:52:35 2020 GMT * expire date: Sep 29 14:52:35 2021 GMT * common name: masternode.example.com * issuer: CN=masternode.example.com \u0026gt; GET /weblogic/ready HTTP/1.1 \u0026gt; User-Agent: curl/7.29.0 \u0026gt; Host: 12.345.678.9:32033 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; Server: nginx/1.19.1 \u0026lt; Date: Tue, 29 Sep 2020 16:10:10 GMT \u0026lt; Content-Length: 0 \u0026lt; Connection: keep-alive \u0026lt; Strict-Transport-Security: max-age=15724800; includeSubDomains \u0026lt; * Connection #0 to host 12.345.678.9 left intact   Verify that You can Access the Domain URL After setting up the NGINX ingress, verify that the domain applications are accessible through the NGINX ingress port (for example 32033) as per Validate Domain URLs \nCleanup If you need to remove the NGINX Ingress then remove the ingress with the following commands:\n$ helm delete oimcluster-nginx -n oimcluster $ helm delete nginx-ingress -n nginxssl $ kubectl delete namespace nginxssl The output will look similar to the following:\n$ helm delete oimcluster-nginx -n oimcluster release \u0026quot;oimcluster-nginx\u0026quot; uninstalled $ helm delete nginx-ingress -n nginxssl release \u0026quot;nginx-ingress\u0026quot; uninstalled $ kubectl delete namespace nginxssl namespace \u0026quot;nginxssl\u0026quot; deleted "
},
{
	"uri": "/fmw-kubernetes/20.4.1/oam/configure-ingress/ingress-voyager-setup-for-oam-domain-setup-on-k8s/",
	"title": "b. Using an Ingress with Voyager",
	"tags": [],
	"description": "Steps to set up an Ingress for Voyager to direct traffic to the OAM domain.",
	"content": "Setting Up an Ingress for Voyager for the OAM Domain on K8S The instructions below explain how to set up Voyager as an Ingress for the OAM domain with SSL termination.\nNote: All the steps below should be performed on the master node.\n Generate a SSL Certificate Install Voyager Create an Ingress for the Domain Verify that You can Access the Domain URL  Generate a SSL Certificate   Generate a private key and certificate signing request (CSR) using a tool of your choice. Send the CSR to your certificate authority (CA) to generate the certificate.\nIf you want to use a certificate for testing purposes you can generate a self signed certificate using openssl:\n$ mkdir \u0026lt;work directory\u0026gt;/ssl $ cd \u0026lt;work directory\u0026gt;/ssl $ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \u0026#34;/CN=\u0026lt;nginx-hostname\u0026gt;\u0026#34; For example:\n$ mkdir /scratch/OAMDockerK8S/ssl $ cd /scratch/OAMDockerK8S/ssl $ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \u0026#34;/CN=masternode.example.com\u0026#34; Note: The CN should match the host.domain of the master node in order to prevent hostname problems during certificate verification.\nThe output will look similar to the following:\nGenerating a 2048 bit RSA private key ..........................................+++ .......................................................................................................+++ writing new private key to \u0026#39;tls.key\u0026#39; -----   Create a secret for SSL by running the following command:\n$ kubectl -n accessns create secret tls \u0026lt;domain_uid\u0026gt;-tls-cert --key \u0026lt;work directory\u0026gt;/tls.key --cert \u0026lt;work directory\u0026gt;/tls.crt For example:\n$ kubectl -n accessns create secret tls accessinfra-tls-cert --key /scratch/OAMDockerK8S/ssl/tls.key --cert /scratch/OAMDockerK8S/ssl/tls.crt The output will look similar to the following:\nsecret/accessinfra-tls-cert created   Install Voyager Use helm to install Voyager.\n  Add the helm chart repository for Voyager using the following command:\nAdd the appscode chart repository using the following command:\n$ helm repo add appscode https://charts.appscode.com/stable/ The output will look similar to the following:\n\u0026#34;appscode\u0026#34; has been added to your repositories   Update the repository using the following command:\n$ helm repo update The output will look similar to the following:\nHang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \u0026#34;appscode\u0026#34; chart repository ...Successfully got an update from the \u0026#34;stable\u0026#34; chart repository Update Complete. ⎈ Happy Helming!⎈   Run the following command to show the voyager chart was added successfully.\n$ helm search repo appscode/voyager The output will look similar to the following:\nNAME CHART VERSION APP VERSION DESCRIPTION appscode/voyager v12.0.0 v12.0.0 Voyager by AppsCode - Secure HAProxy Ingress Co...   Create a namespace for the voyager:\n$ kubectl create namespace voyager The output will look similar to the following:\nnamespace/voyager created   Install Voyager using the following helm command:\n$ helm install voyager-operator appscode/voyager --version 12.0.0 --namespace voyager --set cloudProvider=baremetal --set apiserver.enableValidatingWebhook=false Note: For bare metal Kubernetes use --set cloudProvider=baremetal. If using a managed Kubernetes service then the value should be set for your specific service as per the Voyager install guide.\nThe output will look similar to the following:\nNAME: voyager-operator LAST DEPLOYED: Fri Sep 25 01:15:31 2020 NAMESPACE: voyager STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: Set cloudProvider for installing Voyager To verify that Voyager has started, run: kubectl get deployment --namespace voyager -l \u0026#34;app.kubernetes.io/name=voyager,app.kubernetes.io/instance=voyager-operator\u0026#34;   Create an Ingress for the Domain   Edit the values.yaml and change domainUID to the domainUID you created previously:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain For example:\n$ cd /scratch/OAMDockerK8S/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain Edit values.yaml and change to domainUID: \u0026lt;domain_UID\u0026gt;, for example domainUID: accessinfra.\n  Navigate to the following directory:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain/templates For example:\n$ cd /scratch/OAMDockerK8S/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain/templates Edit the voyager-ingress.yaml and change the secretName to the value created earlier, for example:\n# Copyright (c) 2020, Oracle Corporation and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. {{- if eq .Values.type \u0026#34;VOYAGER\u0026#34; }} --- apiVersion: voyager.appscode.com/v1beta1 kind: Ingress metadata: name: {{ .Values.wlsDomain.domainUID }}-voyager namespace: {{ .Release.Namespace }} annotations: ingress.appscode.com/type: \u0026#39;NodePort\u0026#39; kubernetes.io/ingress.class: \u0026#39;voyager\u0026#39; ingress.appscode.com/stats: \u0026#39;true\u0026#39; ingress.appscode.com/default-timeout: \u0026#39;{\u0026#34;connect\u0026#34;: \u0026#34;1800s\u0026#34;, \u0026#34;server\u0026#34;: \u0026#34;1800s\u0026#34;}\u0026#39; ingress.appscode.com/proxy-body-size: \u0026#34;2000000\u0026#34; labels: weblogic.resourceVersion: domain-v2 spec: {{- if eq .Values.tls \u0026#34;SSL\u0026#34; }} frontendRules: - port: 443 rules: - http-request set-header WL-Proxy-SSL true tls: - secretName: accessinfra-tls-cert hosts: - \u0026#39;*\u0026#39; {{- end }} ...   Create an Ingress for the domain (oam-voyager-ingress), in the domain namespace by using the sample Helm chart.\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator $ helm install oam-voyager-ingress kubernetes/samples/charts/ingress-per-domain --namespace \u0026lt;domain_namespace\u0026gt; --values kubernetes/samples/charts/ingress-per-domain/values.yaml For example:\n$ cd /scratch/OAMDockerK8S/weblogic-kubernetes-operator $ helm install oam-voyager-ingress kubernetes/samples/charts/ingress-per-domain --namespace accessns --values kubernetes/samples/charts/ingress-per-domain/values.yaml The output will look similar to the following:\nNAME: oam-voyager-ingress Fri Sep 25 01:18:01 2020 NAMESPACE: accessns STATUS: deployed REVISION: 1 TEST SUITE: None   Run the following command to show the ingress is created successfully:\n$ kubectl get ingress.voyager.appscode.com --all-namespaces The output will look similar to the following:\nNAMESPACE NAME HOSTS LOAD_BALANCER_IP AGE accessns accessinfra-voyager * 80s   Find the node port of the ingress using the following command:\n$ kubectl describe svc voyager-accessinfra-voyager -n \u0026lt;domain_namespace\u0026gt; For example:\n$ kubectl describe svc voyager-accessinfra-voyager -n accessns The output will look similar to the following:\nName: voyager-accessinfra-voyager Namespace: accessns Labels: app.kubernetes.io/managed-by=Helm origin=voyager origin-api-group=voyager.appscode.com origin-name=accessinfra-voyager weblogic.resourceVersion=domain-v2 Annotations: ingress.appscode.com/last-applied-annotation-keys: ingress.appscode.com/origin-api-schema: voyager.appscode.com/v1beta1 ingress.appscode.com/origin-name: accessinfra-voyager Selector: origin-api-group=voyager.appscode.com,origin-name=accessinfra-voyager,origin=voyager Type: NodePort IP: 10.105.242.191 Port: tcp-443 443/TCP TargetPort: 443/TCP NodePort: tcp-443 30305/TCP Endpoints: 10.244.2.4:443 Port: tcp-80 80/TCP TargetPort: 80/TCP NodePort: tcp-80 32064/TCP Endpoints: 10.244.2.4:80 Session Affinity: None External Traffic Policy: Cluster Events: \u0026lt;none\u0026gt; In the above example the NodePort for tcp-443 is 30305.\n  To confirm that the new Ingress is successfully routing to the domain\u0026rsquo;s server pods, run the following command to send a request to the URL for the \u0026ldquo;WebLogic ReadyApp framework\u0026rdquo;:\n$ curl -v https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/weblogic/ready For example:\n$ curl -v -k https://masternode.example.com:30305/weblogic/ready The output will look similar to the following:\n* Trying 12.345.67.89... * Connected to 12.345.67.89 (12.345.67.89) port 30305 (#0) * Initializing NSS with certpath: sql:/etc/pki/nssdb * skipping SSL peer certificate verification * SSL connection using TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 * Server certificate: * subject: CN=masternode.example.com * start date: Sep 24 14:30:46 2020 GMT * expire date: Sep 24 14:30:46 2021 GMT * common name: masternode.example.com * issuer: CN=masternode.example.com \u0026gt; GET /weblogic/ready HTTP/1.1 \u0026gt; User-Agent: curl/7.29.0 \u0026gt; Host: masternode.example.com:30305 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; Date: 25 Sep 2020 08:22:11 GMT \u0026lt; Content-Length: 0 \u0026lt; Strict-Transport-Security: max-age=15768000 \u0026lt; * Connection #0 to host 12.345.67.89 left intact   Verify that You can Access the Domain URL After setting up the Voyager ingress, verify that the domain applications are accessible through the Voyager ingress port (for example 30305) as per Validate Domain URLs \n"
},
{
	"uri": "/fmw-kubernetes/20.4.1/oam/patch-and-upgrade/upgrade_a_kubernetes_cluster/",
	"title": "c. Upgrade a Kubernetes cluster",
	"tags": [],
	"description": "Instructions on how to upgrade a Kubernetes cluster.",
	"content": "These instructions describe how to upgrade a Kubernetes cluster created using kubeadm on which an OAM domain is deployed. A rolling upgrade approach is used to upgrade nodes (master and worker) of the Kubernetes cluster.\nIt is expected that there will be a down time during the upgrade of the Kubernetes cluster as the nodes need to be drained as part of the upgrade process.\n Prerequisites  Review Prerequisites and ensure that your Kubernetes cluster is ready for upgrade. Make sure your environment meets all prerequisites. Make sure the database used for the OAM domain deployment is up and running during the upgrade process.  Upgrade the Kubernetes version An upgrade of Kubernetes is supported from one MINOR version to the next MINOR version, or between PATCH versions of the same MINOR. For example, you can upgrade from 1.x to 1.x+1, but not from 1.x to 1.x+2. To upgrade a Kubernetes version, first all the master nodes of the Kubernetes cluster must be upgraded sequentially, followed by the sequential upgrade of each worker node.\n See here for Kubernetes official documentation to upgrade from v1.16.x to v1.17.x. See here for Kubernetes official documentation to upgrade from v1.17.x to v1.18.x.  "
},
{
	"uri": "/fmw-kubernetes/20.4.1/oig/patch-and-upgrade/upgrade_a_kubernetes_cluster/",
	"title": "c. Upgrade a Kubernetes cluster",
	"tags": [],
	"description": "Instructions on how to upgrade a Kubernetes cluster.",
	"content": "These instructions describe how to upgrade a Kubernetes cluster created using kubeadm on which an OIG domain is deployed. A rolling upgrade approach is used to upgrade nodes (master and worker) of the Kubernetes cluster.\nIt is expected that there will be a down time during the upgrade of the Kubernetes cluster as the nodes need to be drained as part of the upgrade process.\n Prerequisites  Review Prerequisites and ensure that your Kubernetes cluster is ready for upgrade. Make sure your environment meets all prerequisites. Make sure the database used for the OIG domain deployment is up and running during the upgrade process.  Upgrade the Kubernetes version An upgrade of Kubernetes is supported from one MINOR version to the next MINOR version, or between PATCH versions of the same MINOR. For example, you can upgrade from 1.x to 1.x+1, but not from 1.x to 1.x+2. To upgrade a Kubernetes version, first all the master nodes of the Kubernetes cluster must be upgraded sequentially, followed by the sequential upgrade of each worker node.\n See here for Kubernetes official documentation to upgrade from v1.16.x to v1.17.x. See here for Kubernetes official documentation to upgrade from v1.17.x to v1.18.x.  "
},
{
	"uri": "/fmw-kubernetes/20.4.1/oig/configure-ingress/ingress-voyager-setup-for-oig-domain-setup-on-k8s/",
	"title": "c. Using an Ingress with Voyager (non-SSL)",
	"tags": [],
	"description": "Steps to set up an Ingress for Voyager to direct traffic to the OIG domain (non-SSL).",
	"content": "Setting Up an Ingress for Voyager for the OIG Domain on Kubernetes The instructions below explain how to set up Voyager as an Ingress for the OIG domain with non-SSL termination.\nNote: All the steps below should be performed on the master node.\n Install Voyager  Configure the repository Create Namespace and Install Voyager Setup Routing Rules for the Domain   Create an Ingress for the Domain Verify that You can Access the Domain URL Cleanup  Install Voyager Use Helm to install Voyager. For detailed information, see this document.\nConfigure the repository   Add the Helm chart repository for installing Voyager using the following command:\n$ helm repo add appscode https://charts.appscode.com/stable The output will look similar to the following:\n\u0026quot;appscode\u0026quot; has been added to your repositories   Update the repository using the following command:\n$ helm repo update The output will look similar to the following:\nHang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \u0026quot;appscode\u0026quot; chart repository Update Complete. Happy Helming!   Run the following command to show the Voyager chart was added successfully.\n$ helm search repo appscode/voyager The output will look similar to the following:\nNAME CHART VERSION APP VERSION DESCRIPTION appscode/voyager v12.0.0 v12.0.0 Voyager by AppsCode - Secure HAProxy Ingress Co...   Create Namespace and Install Voyager   Create a namespace for Voyager:\n$ kubectl create namespace voyager The output will look similar to the following:\nnamespace/voyager created   Install Voyager using the following Helm command:\n$ helm install voyager-ingress appscode/voyager --version 12.0.0 --namespace voyager --set cloudProvider=baremetal --set apiserver.enableValidatingWebhook=false Note: For bare metal Kubernetes use --set cloudProvider=baremetal. If using a managed Kubernetes service then the value should be set for your specific service as per the Voyager install guide.\nThe output will look similar to the following:\nNAME: voyager-ingress LAST DEPLOYED: Tue Sep 29 09:23:22 2020 NAMESPACE: voyager STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: Set cloudProvider for installing Voyager To verify that Voyager has started, run: $ kubectl get deployment --namespace voyager -l \u0026quot;app.kubernetes.io/name=voyager,app.kubernetes.io/instance=voyager-ingress\u0026quot;   Verify the ingress has started by running the following command:\n$ kubectl get deployment --namespace voyager -l \u0026quot;app.kubernetes.io/name=voyager,app.kubernetes.io/instance=voyager-ingress\u0026quot; The output will look similar to the following:\nNAME READY UP-TO-DATE AVAILABLE AGE voyager-ingress 1/1 1 1 89s   Setup Routing Rules for the Domain   Setup routing rules using the following commands:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain $ cp values.yaml values.yaml.orig $ vi values.yaml Edit values.yaml and ensure that the values type=VOYAGER and tls=NONSSL are set, for example:\n$ cat values.yaml # Copyright 2020 Oracle Corporation and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # Default values for ingress-per-domain. # This is a YAML-formatted file. # Declare variables to be passed into your templates. # Load balancer type. Supported values are: VOYAGER, NGINX type: VOYAGER # Type of Configuration Supported Values are : NONSSL,SSL # tls: NONSSL tls: NONSSL # TLS secret name if the mode is SSL secretName: domain1-tls-cert # WLS domain as backend to the load balancer wlsDomain: domainUID: oimcluster oimClusterName: oim_cluster soaClusterName: soa_cluster soaManagedServerPort: 8001 oimManagedServerPort: 14000 adminServerName: adminserver adminServerPort: 7001 # Traefik specific values # traefik: # hostname used by host-routing # hostname: idmdemo.m8y.xyz # Voyager specific values voyager: # web port webPort: 30305 # stats port statsPort: 30315   Create an Ingress for the Domain   Create an Ingress for the domain (oimcluster-voyager), in the domain namespace by using the sample Helm chart:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator $ helm install oimcluster-voyager kubernetes/samples/charts/ingress-per-domain --namespace \u0026lt;namespace\u0026gt; --values kubernetes/samples/charts/ingress-per-domain/values.yaml For example:\n$ cd /scratch/OIGDockerK8S/weblogic-kubernetes-operator $ helm install oimcluster-voyager kubernetes/samples/charts/ingress-per-domain --namespace oimcluster --values kubernetes/samples/charts/ingress-per-domain/values.yaml The output will look similar to the following:\nNAME: oimcluster-voyager LAST DEPLOYED: Tue Sep 29 09:28:12 2020 NAMESPACE: oimcluster STATUS: deployed REVISION: 1 TEST SUITE: None   Run the following command to show the ingress is created successfully:\n$ kubectl get ingress.voyager.appscode.com -n oimcluster The output will look similar to the following:\nNAME HOSTS LOAD_BALANCER_IP AGE oimcluster-voyager * 78s   Return details of the ingress using the following command:\n$ kubectl describe ingress.voyager.appscode.com oimcluster-voyager -n oimcluster The output will look similar to the following:\nName: oimcluster-voyager Namespace: oimcluster Labels: app.kubernetes.io/managed-by=Helm weblogic.resourceVersion=domain-v2 Annotations: ingress.appscode.com/affinity: cookie ingress.appscode.com/stats: true ingress.appscode.com/type: NodePort meta.helm.sh/release-name: oimcluster-voyager meta.helm.sh/release-namespace: oimcluster API Version: voyager.appscode.com/v1beta1 Kind: Ingress Metadata: Creation Timestamp: 2020-09-29T09:28:12Z Generation: 1 Managed Fields: API Version: voyager.appscode.com/v1beta1 Fields Type: FieldsV1 fieldsV1: f:metadata: f:annotations: .: f:ingress.appscode.com/affinity: f:ingress.appscode.com/stats: f:ingress.appscode.com/type: f:meta.helm.sh/release-name: f:meta.helm.sh/release-namespace: f:labels: .: f:app.kubernetes.io/managed-by: f:weblogic.resourceVersion: f:spec: .: f:rules: Manager: Go-http-client Operation: Update Time: 2020-09-29T09:28:12Z Resource Version: 4168835 Self Link: /apis/voyager.appscode.com/v1beta1/namespaces/oimcluster/ingresses/oimcluster-voyager UID: 2ea71f79-6836-4df2-8200-8418abf6ad9f Spec: Rules: Host: * Http: Node Port: 30305 Paths: Backend: Service Name: oimcluster-adminserver Service Port: 7001 Path: /console Backend: Service Name: oimcluster-adminserver Service Port: 7001 Path: /em Backend: Service Name: oimcluster-cluster-soa-cluster Service Port: 8001 Path: /soa-infra Backend: Service Name: oimcluster-cluster-oim-cluster Service Port: 14000 Path: / Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ServiceReconcileSuccessful 5m22s voyager-operator Successfully created NodePort Service voyager-oimcluster-voyager Normal ConfigMapReconcileSuccessful 5m22s voyager-operator Successfully created ConfigMap voyager-oimcluster-voyager Normal RBACSuccessful 5m22s voyager-operator Successfully created ServiceAccount voyager-oimcluster-voyager Normal RBACSuccessful 5m22s voyager-operator Successfully created Role voyager-oimcluster-voyager Normal RBACSuccessful 5m22s voyager-operator Successfully created RoleBinding voyager-oimcluster-voyager Normal DeploymentReconcileSuccessful 5m22s voyager-operator Successfully created HAProxy Deployment voyager-oimcluster-voyager Normal StatsServiceReconcileSuccessful 5m22s voyager-operator Successfully created stats Service voyager-oimcluster-voyager-stats   Find the NodePort of Voyager using the following command:\n$ kubectl get svc -n oimcluster The output will look similar to the following:\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE oimcluster-adminserver ClusterIP None \u0026lt;none\u0026gt; 7001/TCP 19h oimcluster-cluster-oim-cluster ClusterIP 10.97.121.159 \u0026lt;none\u0026gt; 14000/TCP 19h oimcluster-cluster-soa-cluster ClusterIP 10.111.231.242 \u0026lt;none\u0026gt; 8001/TCP 19h oimcluster-oim-server1 ClusterIP None \u0026lt;none\u0026gt; 14000/TCP 19h oimcluster-oim-server2 ClusterIP 10.108.139.30 \u0026lt;none\u0026gt; 14000/TCP 19h oimcluster-oim-server3 ClusterIP 10.97.170.104 \u0026lt;none\u0026gt; 14000/TCP 19h oimcluster-oim-server4 ClusterIP 10.99.82.214 \u0026lt;none\u0026gt; 14000/TCP 19h oimcluster-oim-server5 ClusterIP 10.98.75.228 \u0026lt;none\u0026gt; 14000/TCP 19h oimcluster-soa-server1 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP 19h oimcluster-soa-server2 ClusterIP 10.107.232.220 \u0026lt;none\u0026gt; 8001/TCP 19h oimcluster-soa-server3 ClusterIP 10.108.203.6 \u0026lt;none\u0026gt; 8001/TCP 19h oimcluster-soa-server4 ClusterIP 10.96.178.0 \u0026lt;none\u0026gt; 8001/TCP 19h oimcluster-soa-server5 ClusterIP 10.107.83.62 \u0026lt;none\u0026gt; 8001/TCP 19h oimcluster-voyager-stats NodePort 10.99.34.145 \u0026lt;none\u0026gt; 56789:30315/TCP 3m36s voyager-oimcluster-voyager NodePort 10.106.40.20 \u0026lt;none\u0026gt; 80:30305/TCP 3m36s voyager-oimcluster-voyager-stats ClusterIP 10.100.89.234 \u0026lt;none\u0026gt; 56789/TCP 3m30s Identify the service voyager-oimcluster-voyager in the above output and get the NodePort which corresponds to port 80. In this example it will be 30305.\n  To confirm that the new Ingress is successfully routing to the domain\u0026rsquo;s server pods, run the following command to send a request to the URL for the \u0026ldquo;WebLogic ReadyApp framework\u0026rdquo;:\n$ curl -v http://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/weblogic/ready For example:\n$ curl -v http://masternode.example.com:30305/weblogic/ready The output will look similar to the following:\n$ curl -v -k http://masternode.example.com:30305/weblogic/ready * About to connect() to masternode.example.com port 30305 (#0) * Trying 12.345.67.890... * Connected to masternode.example.com (12.345.67.890) port 30305 (#0) \u0026gt; GET /weblogic/ready HTTP/1.1 \u0026gt; User-Agent: curl/7.29.0 \u0026gt; Host: masternode.example.com:30305 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; Date: Wed, 29 Sep 2020 09:30:56 GMT \u0026lt; Content-Length: 0 \u0026lt; Set-Cookie: SERVERID=pod-oimcluster-oim-server1; path=/ \u0026lt; Cache-control: private \u0026lt; * Connection #0 to host masternode.example.com left intact   Verify that You can Access the Domain URL After setting up the Voyager ingress, verify that the domain applications are accessible through the Voyager ingress port (for example 30305) as per Validate Domain URLs \nCleanup If you need to remove the Voyager Ingress (for example to setup Voyager with SSL) then remove the ingress with the following commands:\n$ helm delete oimcluster-voyager -n oimcluster $ helm delete voyager-ingress -n voyager $ kubectl delete namespace voyager The output will look similar to the following:\n$ helm delete oimcluster-voyager -n oimcluster release \u0026quot;oimcluster-voyager\u0026quot; uninstalled $ helm delete voyager-ingress -n voyager release \u0026quot;voyager-ingress\u0026quot; uninstalled $ kubectl delete namespace voyager namespace \u0026quot;voyager\u0026quot; deleted "
},
{
	"uri": "/fmw-kubernetes/20.4.1/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/fmw-kubernetes/20.4.1/oig/configure-ingress/ingress-voyager-setup-for-oig-domain-setup-on-k8s-ssl/",
	"title": "d. Using an Ingress with Voyager (SSL)",
	"tags": [],
	"description": "Steps to set up an Ingress for Voyager to direct traffic to the OIG domain (SSL).",
	"content": "Setting Up an Ingress for Voyager for the OIG Domain on Kubernetes The instructions below explain how to set up Voyager as an Ingress for the OIG domain with SSL termination.\nNote: All the steps below should be performed on the master node.\n Create a SSL Certificate  Generate SSL Certificate Create a Kubernetes Secret for SSL   Install Voyager  Configure the repository Create Namespace and Install Voyager Setup Routing Rules for the Domain   Create an Ingress for the Domain Verify that You can Access the Domain URL Cleanup  Create a SSL Certificate Generate SSL Certificate   Generate a private key and certificate signing request (CSR) using a tool of your choice. Send the CSR to your certificate authority (CA) to generate the certificate.\nIf you want to use a certificate for testing purposes you can generate a self signed certificate using openssl:\n$ mkdir \u0026lt;work directory\u0026gt;/ssl $ cd \u0026lt;work directory\u0026gt;/ssl $ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \u0026quot;/CN=\u0026lt;nginx-hostname\u0026gt;\u0026quot; For example:\n$ mkdir /scratch/OIGDockerK8S/ssl $ cd /scratch/OIGDockerK8S/ssl $ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \u0026quot;/CN=masternode.example.com\u0026quot; Note: The CN should match the host.domain of the master node in order to prevent hostname problems during certificate verification.\nThe output will look similar to the following:\n$ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \u0026quot;/CN=masternode.example.com\u0026quot; Generating a 2048 bit RSA private key ..........................................+++ .......................................................................................................+++ writing new private key to 'tls.key' ----- $   Create a Kubernetes Secret for SSL   Create a secret for SSL containing the SSL certificate by running the following command:\n$ kubectl -n oimcluster create secret tls \u0026lt;domain_id\u0026gt;-tls-cert --key \u0026lt;work directory\u0026gt;/tls.key --cert \u0026lt;work directory\u0026gt;/tls.crt For example:\n$ kubectl -n oimcluster create secret tls oimcluster-tls-cert --key /scratch/OIGDockerK8S/ssl/tls.key --cert /scratch/OIGDockerK8S/ssl/tls.crt The output will look similar to the following:\nsecret/oimcluster-tls-cert created Confirm that the secret is created by running the following command:\n$ kubectl get secret oimcluster-tls-cert -o yaml -n oimcluster The output will look similar to the following:\napiVersion: v1 data: tls.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURGVENDQWYyZ0F3SUJBZ0lKQUl3ZjVRMWVxZnljTUEwR0NTcUdTSWIzRFFFQkN3VUFNQ0V4SHpBZEJnTlYKQkFNTUZtUmxiakF4WlhadkxuVnpMbTl5WVdOc1pTNWpiMjB3SGhjTk1qQXdPREV3TVRReE9UUXpXaGNOTWpFdwpPREV3TVRReE9UUXpXakFoTVI4d0hRWURWUVFEREJaa1pXNHdNV1YyYnk1MWN5NXZjbUZqYkdVdVkyOXRNSUlCCklqQU5CZ2txaGtpRzl3MEJBUUVGQUFPQ0FROEFNSUlCQ2dLQ0FRRUEyY0lpVUhwcTRVZzBhaGR6aXkycHY2cHQKSVIza2s5REd2eVRNY0syaWZQQ2dtUU5CdHV6VXNFN0l4c294eldITmU5RFpXRXJTSjVON3FYYm1lTzJkMVd2NQp1aFhzbkFTbnkwY1NLUE9xOUNZVDNQSlpDVk1MK0llZVFKdnhaVjZaWWU4V2FFL1NQSGJzczRjYy9wcG1mc3pxCnErUi83cXEyMm9ueHNHaE9vQ1h1TlQvMFF2WXVzMnNucGtueWRKRHUxelhGbDREYkFIZGMvamNVK0NPWWROeS8KT3Iza2JIV0FaTkR4OWxaZUREOTRmNXZLbGJwMy9rcUF2V0FkSVJZa2UrSmpNTHg0VHo2ZlM0VXoxbzdBSTVuSApPQ1ZMblV5U0JkaGVuWTNGNEdFU0wwbnorVlhFWjRWVjRucWNjRmo5cnJ0Q29pT1BBNlgvNGdxMEZJbi9Qd0lECkFRQUJvMUF3VGpBZEJnTlZIUTRFRmdRVWw1VnVpVDBDT0xGTzcxMFBlcHRxSC9DRWZyY3dId1lEVlIwakJCZ3cKRm9BVWw1VnVpVDBDT0xGTzcxMFBlcHRxSC9DRWZyY3dEQVlEVlIwVEJBVXdBd0VCL3pBTkJna3Foa2lHOXcwQgpBUXNGQUFPQ0FRRUFXdEN4b2ZmNGgrWXZEcVVpTFFtUnpqQkVBMHJCOUMwL1FWOG9JQzJ3d1hzYi9KaVNuMHdOCjNMdHppejc0aStEbk1yQytoNFQ3enRaSkc3NVluSGRKcmxQajgzVWdDLzhYTlFCSUNDbTFUa3RlVU1jWG0reG4KTEZEMHpReFhpVzV0N1FHcWtvK2FjeN3JRMXlNSE9HdVVkTTZETzErNXF4cTdFNXFMamhyNEdKejV5OAoraW8zK25UcUVKMHFQOVRocG96RXhBMW80OEY0ZHJybWdqd3ROUldEQVpBYmYyV1JNMXFKWXhxTTJqdU1FQWNsCnFMek1TdEZUQ2o1UGFTQ0NUV1VEK3ZlSWtsRWRpaFdpRm02dzk3Y1diZ0lGMlhlNGk4L2szMmF1N2xUTDEvd28KU3Q2dHpsa20yV25uUFlVMzBnRURnVTQ4OU02Z1dybklpZz09Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K tls.key: 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 kind: Secret metadata: creationTimestamp: \u0026quot;2020-08-10T14:22:52Z\u0026quot; managedFields: - apiVersion: v1 fieldsType: FieldsV1 fieldsV1: f:data: .: {} f:tls.crt: {} f:tls.key: {} f:type: {} manager: kubectl operation: Update time: \u0026quot;2020-08-10T14:22:52Z\u0026quot; name: oimcluster-tls-cert namespace: oimcluster resourceVersion: \u0026quot;3722477\u0026quot; selfLink: /api/v1/namespaces/oimcluster/secrets/oimcluster-tls-cert uid: 596fe0fe-effd-4eb9-974d-691da3a3b15a type: kubernetes.io/tls   Install Voyager Use Helm to install Voyager. For detailed information, see this document.\nConfigure the repository   Add the Helm chart repository for installing Voyager using the following command:\n$ helm repo add appscode https://charts.appscode.com/stable The output will look similar to the following:\n$ helm repo add appscode https://charts.appscode.com/stable \u0026quot;appscode\u0026quot; has been added to your repositories   Update the repository using the following command:\n$ helm repo update The output will look similar to the following:\nHang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \u0026quot;appscode\u0026quot; chart repository Update Complete. Happy Helming!   Run the following command to show the Voyager chart was added successfully.\n$ helm search repo appscode/voyager The output will look similar to the following:\nNAME CHART VERSION APP VERSION DESCRIPTION appscode/voyager v12.0.0 v12.0.0 Voyager by AppsCode - Secure HAProxy Ingress Co...   Create Namespace and Install Voyager   Create a namespace for Voyager:\n$ kubectl create namespace voyagerssl The output will look similar to the following:\nnamespace/voyagerssl created   Install Voyager using the following Helm command:\n$ helm install voyager-ingress appscode/voyager --version 12.0.0 --namespace voyagerssl --set cloudProvider=baremetal --set apiserver.enableValidatingWebhook=false Note: For bare metal Kubernetes use --set cloudProvider=baremetal. If using a managed Kubernetes service then the value should be set for your specific service as per the Voyager install guide.\nThe output will look similar to the following:\nNAME: voyager-ingress LAST DEPLOYED: Wed Aug 12 09:00:58 2020 NAMESPACE: voyagerssl STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: Set cloudProvider for installing Voyager To verify that Voyager has started, run: kubectl get deployment --namespace voyagerssl -l \u0026quot;app.kubernetes.io/name=voyager,app.kubernetes.io/instance=voyager-ingress\u0026quot;   Verify that the ingress has started by running the following command:\n$ kubectl get deployment --namespace voyagerssl -l \u0026quot;app.kubernetes.io/name=voyager,app.kubernetes.io/instance=voyager-ingress\u0026quot; The output will look similar to the following:\nNAME READY UP-TO-DATE AVAILABLE AGE voyager-ingress 1/1 1 1 89s   Setup Routing Rules for the Domain   Setup routing rules using the following commands:\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain $ cp values.yaml values.yaml.orig $ vi values.yaml   Edit values.yaml and ensure that type=VOYAGER,tls=SSL, and secretName: \u0026lt;SSL Secret\u0026gt; , for example:\n$ cat values.yaml # Copyright 2020 Oracle Corporation and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # Default values for ingress-per-domain. # This is a YAML-formatted file. # Declare variables to be passed into your templates. # Load balancer type. Supported values are: VOYAGER, NGINX type: VOYAGER # Type of Configuration Supported Values are : NONSSL,SSL # tls: NONSSL tls: SSL # TLS secret name if the mode is SSL secretName: oimcluster-tls-cert # WLS domain as backend to the load balancer wlsDomain: domainUID: oimcluster oimClusterName: oim_cluster soaClusterName: soa_cluster soaManagedServerPort: 8001 oimManagedServerPort: 14000 adminServerName: adminserver adminServerPort: 7001 # Traefik specific values # traefik: # hostname used by host-routing # hostname: idmdemo.m8y.xyz # Voyager specific values voyager: # web port webPort: 30305 # stats port statsPort: 30315 $   Create an Ingress for the Domain   Create an Ingress for the domain (oimcluster-voyager), in the domain namespace by using the sample Helm chart.\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator $ helm install oimcluster-voyager kubernetes/samples/charts/ingress-per-domain --namespace \u0026lt;namespace\u0026gt; --values kubernetes/samples/charts/ingress-per-domain/values.yaml For example:\n$ cd /scratch/OIGDockerK8S/weblogic-kubernetes-operator $ helm install oimcluster-voyager kubernetes/samples/charts/ingress-per-domain --namespace oimcluster --values kubernetes/samples/charts/ingress-per-domain/values.yaml The output will look similar to the following:\nNAME: oimcluster-voyager LAST DEPLOYED: Wed Sep 30 01:51:05 2020 NAMESPACE: oimcluster STATUS: deployed REVISION: 1 TEST SUITE: None   Run the following command to show the ingress is created successfully:\n$ kubectl get ingress.voyager.appscode.com -n oimcluster The output will look similar to the following:\nNAME HOSTS LOAD_BALANCER_IP AGE oimcluster-voyager * 3m44s   Return details of the ingress using the following command:\n$ kubectl describe ingress.voyager.appscode.com oimcluster-voyager -n oimcluster The output will look similar to the following:\nName: oimcluster-voyager Namespace: oimcluster Labels: app.kubernetes.io/managed-by=Helm weblogic.resourceVersion=domain-v2 Annotations: ingress.appscode.com/affinity: cookie ingress.appscode.com/stats: true ingress.appscode.com/type: NodePort meta.helm.sh/release-name: oimcluster-voyager meta.helm.sh/release-namespace: oimcluster API Version: voyager.appscode.com/v1beta1 Kind: Ingress Metadata: Creation Timestamp: 2020-09-30T08:51:05Z Generation: 1 Managed Fields: API Version: voyager.appscode.com/v1beta1 Fields Type: FieldsV1 fieldsV1: f:metadata: f:annotations: .: f:ingress.appscode.com/affinity: f:ingress.appscode.com/stats: f:ingress.appscode.com/type: f:meta.helm.sh/release-name: f:meta.helm.sh/release-namespace: f:labels: .: f:app.kubernetes.io/managed-by: f:weblogic.resourceVersion: f:spec: .: f:frontendRules: f:rules: f:tls: Manager: Go-http-client Operation: Update Time: 2020-09-30T08:51:05Z Resource Version: 1440614 Self Link: /apis/voyager.appscode.com/v1beta1/namespaces/oimcluster/ingresses/oimcluster-voyager UID: 875e7d90-b166-40ff-b792-c764d514c0c3 Spec: Frontend Rules: Port: 443 Rules: http-request set-header WL-Proxy-SSL true Rules: Host: * Http: Node Port: 30305 Paths: Backend: Service Name: oimcluster-adminserver Service Port: 7001 Path: /console Backend: Service Name: oimcluster-adminserver Service Port: 7001 Path: /em Backend: Service Name: oimcluster-cluster-soa-cluster Service Port: 8001 Path: /soa-infra Backend: Service Name: oimcluster-cluster-soa-cluster Service Port: 8001 Path: /soa Backend: Service Name: oimcluster-cluster-soa-cluster Service Port: 8001 Path: /integration Backend: Service Name: oimcluster-cluster-oim-cluster Service Port: 14000 Path: / Tls: Hosts: * Secret Name: oimcluster-tls-cert Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ServiceReconcileSuccessful 65s voyager-operator Successfully created NodePort Service voyager-oimcluster-voyager Normal ConfigMapReconcileSuccessful 64s voyager-operator Successfully created ConfigMap voyager-oimcluster-voyager Normal RBACSuccessful 64s voyager-operator Successfully created ServiceAccount voyager-oimcluster-voyager Normal RBACSuccessful 64s voyager-operator Successfully created Role voyager-oimcluster-voyager Normal RBACSuccessful 64s voyager-operator Successfully created RoleBinding voyager-oimcluster-voyager Normal DeploymentReconcileSuccessful 64s voyager-operator Successfully created HAProxy Deployment voyager-oimcluster-voyager Normal StatsServiceReconcileSuccessful 64s voyager-operator Successfully created stats Service voyager-oimcluster-voyager-stats Normal DeploymentReconcileSuccessful 64s voyager-operator Successfully patched HAProxy Deployment voyager-oimcluster-voyager   Find the NodePort of Voyager using the following command:\n$ kubectl get svc -n oimcluster The output will look similar to the following:\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE oimcluster-adminserver ClusterIP None \u0026lt;none\u0026gt; 7001/TCP 18h oimcluster-cluster-oim-cluster ClusterIP 10.97.121.159 \u0026lt;none\u0026gt; 14000/TCP 18h oimcluster-cluster-soa-cluster ClusterIP 10.111.231.242 \u0026lt;none\u0026gt; 8001/TCP 18h oimcluster-oim-server1 ClusterIP None \u0026lt;none\u0026gt; 14000/TCP 18h oimcluster-oim-server2 ClusterIP 10.108.139.30 \u0026lt;none\u0026gt; 14000/TCP 18h oimcluster-oim-server3 ClusterIP 10.97.170.104 \u0026lt;none\u0026gt; 14000/TCP 18h oimcluster-oim-server4 ClusterIP 10.99.82.214 \u0026lt;none\u0026gt; 14000/TCP 18h oimcluster-oim-server5 ClusterIP 10.98.75.228 \u0026lt;none\u0026gt; 14000/TCP 18h oimcluster-soa-server1 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP 18h oimcluster-soa-server2 ClusterIP 10.107.232.220 \u0026lt;none\u0026gt; 8001/TCP 18h oimcluster-soa-server3 ClusterIP 10.108.203.6 \u0026lt;none\u0026gt; 8001/TCP 18h oimcluster-soa-server4 ClusterIP 10.96.178.0 \u0026lt;none\u0026gt; 8001/TCP 18h oimcluster-soa-server5 ClusterIP 10.107.83.62 \u0026lt;none\u0026gt; 8001/TCP 18h oimcluster-voyager-stats NodePort 10.96.62.0 \u0026lt;none\u0026gt; 56789:30315/TCP 3m19s voyager-oimcluster-voyager NodePort 10.97.231.109 \u0026lt;none\u0026gt; 443:30305/TCP,80:30419/TCP 3m12s voyager-oimcluster-voyager-stats ClusterIP 10.99.185.46 \u0026lt;none\u0026gt; 56789/TCP 3m6s Identify the service voyager-oimcluster-voyager in the above output and get the NodePort which corresponds to port 443. In this example it will be 30305.\n  To confirm that the new Ingress is successfully routing to the domain\u0026rsquo;s server pods, run the following command to send a request to the URL for the \u0026ldquo;WebLogic ReadyApp framework\u0026rdquo;:\n$ curl -v -k https://${MASTERNODE-HOSTNAME}:${MASTERNODE-PORT}/weblogic/ready For example:\n$ curl -v -k https://masternode.example.com:30305/weblogic/ready The output will look similar to the following:\n* About to connect() to masternode.example.com port 30305 (#0) * Trying 12.345.678.9... * Connected to masternode.example.com (12.345.678.9) port 30305 (#0) * Initializing NSS with certpath: sql:/etc/pki/nssdb * skipping SSL peer certificate verification * SSL connection using TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 * Server certificate: * subject: CN=masternode.example.com * start date: Sep 29 14:52:35 2020 GMT * expire date: Sep 29 14:52:35 2021 GMT * common name: masternode.example.com * issuer: CN=masternode.example.com \u0026gt; GET /weblogic/ready HTTP/1.1 \u0026gt; User-Agent: curl/7.29.0 \u0026gt; Host: masternode.example.com:30305 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; Date: Wed, 30 Sep 2020 08:56:08 GMT \u0026lt; Content-Length: 0 \u0026lt; Strict-Transport-Security: max-age=15768000 \u0026lt; Set-Cookie: SERVERID=pod-oimcluster-oim-server1; path=/ \u0026lt; Cache-control: private \u0026lt; * Connection #0 to host masternode.example.com left intact   Verify that You can Access the Domain URL After setting up the Voyager ingress, verify that the domain applications are accessible through the Voyager ingress port (for example 30305) as per Validate Domain URLs \nCleanup If you need to remove the Voyager Ingress then remove the ingress with the following commands:\n$ helm delete oimcluster-voyager -n oimcluster $ helm delete voyager-ingress -n voyagerssl $ kubectl delete namespace voyagerssl The output will look similar to the following:\n$ helm delete oimcluster-voyager -n oimcluster release \u0026quot;oimcluster-voyager\u0026quot; uninstalled $ helm delete voyager-ingress -n voyagerssl release \u0026quot;voyager-ingress\u0026quot; uninstalled $ kubectl delete namespace voyagerssl namespace \u0026quot;voyagerssl\u0026quot; deleted "
},
{
	"uri": "/fmw-kubernetes/20.4.1/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]